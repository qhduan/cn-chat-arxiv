# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning.](http://arxiv.org/abs/2401.12216) | æœ¬æ–‡ç ”ç©¶äº†åœ¨æ¨¡å‹è§„èŒƒé”™è¯¯çš„æ¡ä»¶ä¸‹ï¼Œç”±äºåˆ†å¸ƒè½¬å˜å¯¼è‡´çš„é”™è¯¯æ”¾å¤§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ |
| [^2] | [The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization.](http://arxiv.org/abs/2401.12058) | æœ¬ç ”ç©¶æ¢è®¨äº†éšæœºå‡¸ä¼˜åŒ–ä¸­æ¢¯åº¦æ–¹æ³•çš„æ³›åŒ–æ€§èƒ½ä»¥åŠç»´åº¦ä¾èµ–æ€§ã€‚æˆ‘ä»¬å‘ç°æ ‡å‡†çš„å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨ç»´åº¦$d = O(n^2)$æ—¶éœ€è¦è‡³å°‘$\Omega(\sqrt{d})$ä¸ªè®­ç»ƒæ ·æœ¬æ‰èƒ½è¾¾åˆ°éå¹³å‡¡çš„æµ‹è¯•è¯¯å·®ã€‚è€Œå¯¹äºæ ‡å‡†çš„ä¸€ééšæœºæ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼ŒåŒæ ·çš„ç»´åº¦ä¸‹ç•Œä¹Ÿé€‚ç”¨ã€‚ |
| [^3] | [Integrating Statistical Significance and Discriminative Power in Pattern Discovery.](http://arxiv.org/abs/2401.12000) | æœ¬è®ºæ–‡å°†ç»Ÿè®¡æ˜¾è‘—æ€§å’Œåˆ¤åˆ«èƒ½åŠ›èå…¥æ¨¡å¼å‘ç°ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–¹æ³•æ¥åŒæ—¶æ»¡è¶³æ¨¡å¼è´¨é‡å’Œç»Ÿè®¡æ ‡å‡†ï¼Œåœ¨ä¸‰å…ƒèšç±»ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶æœ‰æ•ˆæ€§ã€‚ |
| [^4] | [Cross-Validation Conformal Risk Control.](http://arxiv.org/abs/2401.11974) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰éªŒè¯çš„æ–°å‹åˆè§„é£é™©æ§åˆ¶æ–¹æ³•(CV-CRC)ï¼Œå®ƒæ‰©å±•äº†ä¸€è‡´æ€§é¢„æµ‹çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæ§åˆ¶æ›´å¹¿æ³›çš„é£é™©å‡½æ•°ï¼Œå¹¶åœ¨é¢„æµ‹å™¨é›†åˆçš„å¹³å‡é£é™©ä¸Šæä¾›äº†ç†è®ºä¿è¯ã€‚ |
| [^5] | [RUMBoost: Gradient Boosted Random Utility Models.](http://arxiv.org/abs/2401.11954) | RUMBoostæ¨¡å‹å°†éšæœºæ•ˆç”¨æ¨¡å‹ï¼ˆRUMsï¼‰çš„è§£é‡Šæ€§å’Œè¡Œä¸ºé²æ£’æ€§ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ç›¸ç»“åˆï¼Œé€šè¿‡ä½¿ç”¨æ¢¯åº¦æå‡å›å½’æ ‘çš„é›†åˆæ¥è·å¾—éçº¿æ€§æ•ˆç”¨å‡½æ•°çš„å®Œæ•´å‡½æ•°å½¢å¼ï¼Œå®ç°äº†å¯¹è¾“å…¥å˜é‡çš„ä»»ä½•å¯èƒ½ç»„åˆè¿›è¡Œå¸¸æ•°æ’è¡¥ã€‚ |
| [^6] | [Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent.](http://arxiv.org/abs/2401.11940) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†è§£æ¢¯åº¦ä¸‹é™æ–¹æ³•è§£å†³ä½èƒçŠ¶ç§©å¼ é‡æ¢å¤é—®é¢˜çš„é«˜æ•ˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å¤§å¼ é‡åˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒå°çš„å› å­å¼ é‡ï¼Œåœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œå­˜å‚¨éœ€æ±‚çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ”¶æ•›æ€§ã€‚ |
| [^7] | [Subgroup analysis methods for time-to-event outcomes in heterogeneous randomized controlled trials.](http://arxiv.org/abs/2401.11842) | æœ¬è®ºæ–‡è¯„ä¼°äº†å¤šç§æ—¶é—´åˆ°äº‹ä»¶ç»“æœçš„äºšç»„åˆ†æç®—æ³•ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥æ¢ç´¢ä¸åŒçš„å¼‚è´¨æ€§æƒ…æ™¯ã€‚ |
| [^8] | [Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo.](http://arxiv.org/abs/2401.11665) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¬ é˜»å°¼ Langevin Monte Carlo åŠ é€Ÿçš„è¿‘ä¼¼ Thompson é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å®šåŠ¿å‡½æ•°çš„è®¾è®¡æ”¹å–„äº†é«˜ç»´é—®é¢˜ä¸­çš„æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶åœ¨é«˜ç»´èµŒåšæœºé—®é¢˜ä¸­è¿›è¡Œäº†éªŒè¯ã€‚ |
| [^9] | [Nonparametric Estimation via Variance-Reduced Sketching.](http://arxiv.org/abs/2401.11646) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVariance-Reduced Sketchingçš„æ¡†æ¶ï¼Œç”¨äºåœ¨é«˜ç»´åº¦ä¸­ä¼°è®¡å¯†åº¦å‡½æ•°å’Œéå‚æ•°å›å½’å‡½æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å‡½æ•°æ¦‚å¿µåŒ–ä¸ºçŸ©é˜µï¼Œå¹¶é‡‡ç”¨è‰å›¾æŠ€æœ¯æ¥é™ä½ç»´åº¦ç¾éš¾å¼•èµ·çš„æ–¹å·®ï¼Œå±•ç¤ºäº†é²æ£’æ€§èƒ½å’Œæ˜¾è‘—æ”¹è¿›ã€‚ |
| [^10] | [Efficient local linearity regularization to overcome catastrophic overfitting.](http://arxiv.org/abs/2401.11618) | æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºELLEçš„æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºé«˜æ•ˆåœ°å‡è½»å•æ­¥å¯¹æŠ—æ€§è®­ç»ƒä¸­çš„ç¾éš¾æ€§è¿‡æ‹Ÿåˆã€‚å®ƒèƒ½å¤Ÿä¿æŒæŸå¤±å‡½æ•°åœ¨è¾“å…¥ä¸Šçš„å±€éƒ¨çº¿æ€§æ€§ï¼Œä¸ä¼ ç»Ÿçš„æ­£åˆ™åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒELLEæ›´åŠ é«˜æ•ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤§å¯¹æŠ—æ€§æ‰°åŠ¨å’Œé•¿è®­ç»ƒè®¡åˆ’ç­‰å›°éš¾æƒ…å†µã€‚ |
| [^11] | [Understanding the Generalization Benefits of Late Learning Rate Decay.](http://arxiv.org/abs/2401.11600) | æœ¬æ–‡ç ”ç©¶äº†ä¸ºä»€ä¹ˆä½¿ç”¨å¤§å­¦ä¹ ç‡é•¿æ—¶é—´è®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥å®ç°æ›´å¥½çš„æ³›åŒ–ã€‚é€šè¿‡è§‚å¯Ÿè®­ç»ƒå’Œæµ‹è¯•æŸå¤±ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å‘ç°å¤§å­¦ä¹ ç‡ä¸‹çš„è®­ç»ƒè½¨è¿¹èƒ½å¤Ÿæ¥è¿‘æµ‹è¯•æŸå¤±çš„æœ€å°å€¼é™„è¿‘ã€‚åŸºäºè¿™ä¸ªå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸çœŸå®ç¥ç»ç½‘ç»œç±»ä¼¼çš„éçº¿æ€§æ¨¡å‹ï¼Œå¹¶è¯æ˜äº†ä½¿ç”¨å¤§å­¦ä¹ ç‡è¿›è¡Œå»¶é•¿é˜¶æ®µçš„è®­ç»ƒå¯ä»¥å®ç°æ›´æ¥è¿‘æœ€ä¼˜æ³›åŒ–çš„æ•ˆæœã€‚ |
| [^12] | [Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis.](http://arxiv.org/abs/2401.11565) | æœ¬æ–‡ç ”ç©¶äº†å…·æœ‰å™ªéŸ³ä¸Šä¸‹æ–‡çš„éšæœºèµŒè‡‚é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§Thompsoné‡‡æ ·ç®—æ³•ï¼Œé€šè¿‡è´å¶æ–¯æ¡†æ¶è¿›è¡Œåˆ†æï¼Œè¯æ˜äº†ç®—æ³•çš„è´å¶æ–¯åæ‚”ï¼Œå¹¶æ‰©å±•äº†é—®é¢˜åˆ°å»¶è¿Ÿè§‚å¯ŸçœŸå®ä¸Šä¸‹æ–‡çš„æƒ…å†µï¼Œå¹¶å®è¯äº†ç®—æ³•çš„æ€§èƒ½ã€‚ |
| [^13] | [Enhancing selectivity using Wasserstein distance based reweighing.](http://arxiv.org/abs/2401.11562) | æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒçš„ç®—æ³•ï¼Œåœ¨æ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†ç®—æ³•å¯ä»¥è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œä¸”ç®—æ³•ç®€å•å¯æ‰©å±•ã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºåº”ç”¨å®ä¾‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚ |
| [^14] | [MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning.](http://arxiv.org/abs/2401.11380) | MoMAæå‡ºäº†ä¸€ç§æ¨¡å‹ä¸ºåŸºç¡€çš„é•œåƒä¸Šå‡ç®—æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ— é™åˆ¶çš„ç­–ç•¥ç±»åˆ«å’Œä¸€èˆ¬å‡½æ•°é€¼è¿‘æ¥å®ç°ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå……åˆ†åˆ©ç”¨äº†æ¨¡å‹ä¸ºåŸºç¡€æ–¹æ³•çš„ä¼˜åŠ¿ã€‚ |
| [^15] | [Quantum Machine Learning: from NISQ to Fault Tolerance.](http://arxiv.org/abs/2401.11351) | æœ¬æ–‡æä¾›äº†å¯¹é‡å­æœºå™¨å­¦ä¹ é¢†åŸŸçš„å…¨é¢å›é¡¾ï¼Œæ¶µç›–äº†åœ¨NISQæŠ€æœ¯å’Œå®¹é”™é‡å­è®¡ç®—ç¡¬ä»¶ä¸Šä½¿ç”¨çš„æŠ€æœ¯å’Œç®—æ³•ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†ä¸é‡å­æœºå™¨å­¦ä¹ ç›¸å…³çš„åŸºæœ¬æ¦‚å¿µå’Œç»Ÿè®¡å­¦ä¹ ç†è®ºã€‚ |
| [^16] | [Estimating heterogeneous treatment effect from survival outcomes via (orthogonal) censoring unbiased learning.](http://arxiv.org/abs/2401.11263) | è¯¥è®ºæ–‡å¼€å‘äº†ä¸€ç§é€‚ç”¨äºå…·æœ‰å’Œæ²¡æœ‰ç«äº‰é£é™©çš„ç”Ÿå­˜ç»“æœçš„æˆªå°¾æ— åå˜æ¢æ–¹æ³•ï¼Œå¯ä»¥ä¼°è®¡å¼‚è´¨æ²»ç–—æ•ˆåº”ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºæ›´å¤šæœ€å…ˆè¿›çš„é€‚ç”¨äºè¢«æˆªå°¾ç»“æœçš„HTEå­¦ä¹ æ–¹æ³•ï¼Œå¹¶æä¾›äº†é™åˆ¶æœ‰é™æ ·æœ¬è¿‡åº¦é£é™©çš„æ–¹æ³•ã€‚ |
| [^17] | [AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking.](http://arxiv.org/abs/2401.11250) | AFS-BMé€šè¿‡è”åˆä¼˜åŒ–å®ç°äº†è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹å‡†ç¡®æ€§å¹¶å‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚ |
| [^18] | [Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable.](http://arxiv.org/abs/2401.11130) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å·¥å…·å˜é‡æ³•è¯†åˆ«å’Œä¼°è®¡è¿ç»­å¤„ç†çš„å› æœæ•ˆåº”å¼‚è´¨æ€§çš„æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†ä¸‰ç±»ç›¸åº”çš„ä¼°è®¡å™¨ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†ç»Ÿè®¡æ€§è´¨åˆ†æã€‚ |
| [^19] | [Efficient Data Shapley for Weighted Nearest Neighbor Algorithms.](http://arxiv.org/abs/2401.11103) | æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§£å†³åŠ æƒKæœ€è¿‘é‚»ç®—æ³•é«˜æ•ˆè®¡ç®—Data Shapleyå€¼çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶åœ¨æ•°æ®è´¨é‡åˆ¤åˆ«æ–¹é¢ä¼˜äºæœªåŠ æƒç‰ˆæœ¬ã€‚ |
| [^20] | [Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions.](http://arxiv.org/abs/2401.11081) | æœ¬æ–‡ç ”ç©¶äº†ä»èšåˆå“åº”ä¸­å­¦ä¹ çš„ä¸¤ç§æŸå¤±å‡½æ•°ï¼šåŒ…çº§åˆ«æŸå¤±å’Œå®ä¾‹çº§åˆ«æŸå¤±ï¼Œå¹¶å‘ç°å®ä¾‹çº§åˆ«æŸå¤±å¯ä»¥è¢«è§†ä¸ºåŒ…çº§åˆ«æŸå¤±çš„æ­£åˆ™åŒ–å½¢å¼ã€‚ |
| [^21] | [Provably Scalable Black-Box Variational Inference with Structured Variational Families.](http://arxiv.org/abs/2401.10989) | æœ¬æ–‡ç ”ç©¶äº†å‡å€¼åœºå˜åˆ†æ—å’Œæ»¡ç§©å˜åˆ†æ—ä¹‹é—´çš„ç†è®ºä¸­é—´åœ°å¸¦ï¼šç»“æ„åŒ–å˜åˆ†æ—ï¼Œå¹¶é€šè¿‡ç†è®ºè¯æ˜ç»“æ„åŒ–å˜åˆ†æ—å¯ä»¥åœ¨è¿­ä»£å¤æ‚æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œç¼©æ”¾æ•ˆæœæ›´å¥½ã€‚ |
| [^22] | [Debiasing and a local analysis for population clustering using semidefinite programming.](http://arxiv.org/abs/2401.10927) | æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨åŠæ­£å®šè§„åˆ’è¿›è¡Œäººç¾¤èšç±»çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†è®¡ç®—é«˜æ•ˆçš„ç®—æ³•ã€‚è¿™äº›ç®—æ³•å¯ä»¥æ ¹æ®å°æ ·æœ¬æ•°æ®çš„åŸå§‹ç§ç¾¤å°†æ•°æ®åˆ†ä¸ºä¸¤ç»„ï¼Œé€‚ç”¨äºç§ç¾¤ä¹‹é—´å·®å¼‚è¾ƒå°çš„æƒ…å†µã€‚ |
| [^23] | [Online estimation of the inverse of the Hessian for stochastic optimization with application to universal stochastic Newton algorithms.](http://arxiv.org/abs/2401.10923) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿ä¼°è®¡HessiançŸ©é˜µé€†çš„æ–¹æ³•ï¼Œåˆ©ç”¨Robbins-Monroè¿‡ç¨‹ï¼Œèƒ½å¤Ÿ drastical reducescomputational complexity,å¹¶å‘å±•äº†é€šç”¨çš„éšæœºç‰›é¡¿ç®—æ³•ï¼Œç ”ç©¶äº†æ‰€ææ–¹æ³•çš„æ¸è¿›æ•ˆç‡ã€‚ |
| [^24] | [Early alignment in two-layer networks training is a two-edged sword.](http://arxiv.org/abs/2401.10791) | æœ¬æ–‡ç ”ç©¶äº†ä¸¤å±‚ç½‘ç»œè®­ç»ƒä¸­çš„æ—©æœŸå¯¹é½ç°è±¡ï¼Œå‘ç°åœ¨å°åˆå§‹åŒ–å’Œä¸€ä¸ªéšè—çš„ReLUå±‚ç½‘ç»œä¸­ï¼Œç¥ç»å…ƒä¼šåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå‘å…³é”®æ–¹å‘è¿›è¡Œå¯¹é½ï¼Œå¯¼è‡´ç½‘ç»œç¨€ç–è¡¨ç¤ºä»¥åŠæ¢¯åº¦æµåœ¨æ”¶æ•›æ—¶çš„éšå«åå¥½ã€‚ç„¶è€Œï¼Œè¿™ç§ç¨€ç–è¯±å¯¼çš„å¯¹é½ä¹Ÿä½¿å¾—è®­ç»ƒç›®æ ‡çš„æœ€å°åŒ–å˜å¾—å›°éš¾ã€‚ |
| [^25] | [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images.](http://arxiv.org/abs/2401.08865) | æœ¬æ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒé¢†åŸŸå­¦ä¹ æ—¶çš„å·®å¼‚ï¼Œæå‡ºäº†ä¸€ä¸ªä¸è®­ç»ƒé›†ç»´åº¦æœ‰å…³çš„æ³›åŒ–ç¼©æ”¾å®šå¾‹ï¼Œå¹¶è®¤ä¸ºåŒ»å­¦å›¾åƒæ•°æ®é›†æ›´é«˜çš„å›ºæœ‰â€œæ ‡ç­¾é”åº¦â€å¯èƒ½æ˜¯ä¸¤ä¸ªé¢†åŸŸä¹‹é—´æ˜¾è‘—å·®å¼‚çš„éƒ¨åˆ†åŸå› ã€‚ |
| [^26] | [Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach.](http://arxiv.org/abs/2312.13152) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹æ¥å»ºæ¨¡æ—¶é—´åºåˆ—çš„å˜ç‚¹æ£€æµ‹ç®—æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå­¦ä¹ æ¯ä¸ªå˜ç‚¹å¯¹åº”çš„ç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹çš„å‚æ•°ï¼Œå¹¶é€šè¿‡GANåˆ¤åˆ«å™¨çš„è¾“å‡ºæ£€æµ‹å˜ç‚¹ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ |
| [^27] | [Towards Optimal Statistical Watermarking.](http://arxiv.org/abs/2312.07930) | è¿½æ±‚æœ€ä¼˜ç»Ÿè®¡æ°´å°æŠ€æœ¯ã€‚é€šè¿‡å°†ç»Ÿè®¡æ°´å°æŠ€æœ¯è§†ä¸ºå‡è®¾æ£€éªŒé—®é¢˜å¹¶å¼•å…¥ä¼ªéšæœºç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬å®ç°äº†è¾“å‡ºä»¤ç‰Œå’Œæ‹’ç»åŒºåŸŸçš„è€¦åˆï¼Œå®ç°äº†ç¬¬ä¸€ç±»é”™è¯¯å’Œç¬¬äºŒç±»é”™è¯¯ä¹‹é—´çš„éå¹³å‡¡æƒè¡¡ï¼ŒåŒæ—¶æå‡ºäº†æœ€ç»Ÿä¸€æœ€æœ‰åŠ›çš„æ°´å°å’Œæœ€å°åŒ–ç¬¬äºŒç±»é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç‹¬ç«‹åŒåˆ†å¸ƒä»¤ç‰Œæ•°é‡çš„ä¸Šä¸‹ç•Œï¼Œçªæ˜¾äº†æ”¹è¿›çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†é²æ£’æ€§æ°´å°é—®é¢˜ã€‚ |
| [^28] | [Optimal Multi-Distribution Learning.](http://arxiv.org/abs/2312.05134) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æœ€ä¼˜åŒ–å¤šåˆ†å¸ƒå­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é‡‡æ ·æ¥å®ç°æ•°æ®é«˜æ•ˆçš„å­¦ä¹ ã€‚é’ˆå¯¹Vapnik-Chervonenkis (VC)ç»´æ•°ä¸ºdçš„å‡è®¾ç±»ï¼Œç®—æ³•å¯ä»¥ç”Ÿæˆä¸€ä¸ªÎµ-æœ€ä¼˜éšæœºå‡è®¾ï¼Œå¹¶ä¸”æ ·æœ¬å¤æ‚åº¦ä¸æœ€ä½³ä¸‹ç•Œä¿æŒä¸€è‡´ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•çš„æ€æƒ³å’Œç†è®ºè¿˜è¢«è¿›ä¸€æ­¥æ‰©å±•ä»¥é€‚åº”Rademacherç±»ã€‚æœ€ç»ˆæå‡ºçš„ç®—æ³•æ˜¯å¥¥æ‹‰å…‹å°”é«˜æ•ˆçš„ï¼Œä»…è®¿é—®å‡è®¾ç±»ã€‚ |
| [^29] | [On the Nystrom Approximation for Preconditioning in Kernel Machines.](http://arxiv.org/abs/2312.03311) | æœ¬æ–‡åˆ†æäº†æ ¸æœºå™¨é¢„å¤„ç†ä¸­ä½¿ç”¨Nystromé€¼è¿‘çš„æƒè¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¯¹æ•°å¤§å°çš„æ ·æœ¬èƒ½å¤Ÿè®©Nystromé€¼è¿‘çš„é¢„å¤„ç†å™¨å‡ ä¹ä¸æ¢¯åº¦ä¸‹é™åŒæ ·æœ‰æ•ˆåœ°åŠ é€Ÿã€‚ |
| [^30] | [Annotation Sensitivity: Training Data Collection Methods Affect Model Performance.](http://arxiv.org/abs/2311.14212) | è¯¥ç ”ç©¶å‘ç°è®­ç»ƒæ•°æ®æ”¶é›†æ–¹æ³•å¯¹æ³¨é‡Šæœ¬èº«å’Œä¸‹æ¸¸æ¨¡å‹æ€§èƒ½äº§ç”Ÿå½±å“ã€‚åœ¨å¯¹ä»‡æ¨è¨€è®ºå’Œå†’çŠ¯æ€§è¯­è¨€è¿›è¡Œæ³¨é‡Šæ”¶é›†çš„å®éªŒä¸­ï¼Œå‘ç°æ³¨é‡Šå·¥å…·çš„è®¾è®¡é€‰æ‹©ä¼šå¯¹æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿæ˜æ˜¾å·®å¼‚ã€‚ |
| [^31] | [On the Foundation of Distributionally Robust Reinforcement Learning.](http://arxiv.org/abs/2311.09018) | è¯¥è®ºæ–‡ä¸ºåˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºç¡€åšå‡ºäº†è´¡çŒ®ï¼Œé€šè¿‡ä¸€ä¸ªç»¼åˆçš„å»ºæ¨¡æ¡†æ¶ï¼Œå†³ç­–è€…åœ¨æœ€åæƒ…å†µä¸‹çš„åˆ†å¸ƒè½¬å˜ä¸‹é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶è€ƒè™‘äº†å„ç§å»ºæ¨¡å±æ€§å’Œå¯¹æ‰‹å¼•èµ·çš„è½¬å˜çš„çµæ´»æ€§ã€‚ |
| [^32] | [Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures.](http://arxiv.org/abs/2311.03242) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç±»ä¼¼ResNetçš„ç¥ç»ç½‘ç»œæ¶æ„æ¥è¿‘ä¼¼Langevin Monte Carloç®—æ³•ï¼Œé€šè¿‡å°†æ¥è‡ªç®€å•å‚è€ƒåˆ†å¸ƒçš„æ ·æœ¬æ˜ å°„åˆ°ç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬ä¸­æ¥è¿›è¡Œé‡‡æ ·ï¼Œå…·æœ‰è¾ƒå¥½çš„é€¼è¿‘é€Ÿåº¦å’Œè¡¨è¾¾æ€§ã€‚ |
| [^33] | [Generator Identification for Linear SDEs with Additive and Multiplicative Noise.](http://arxiv.org/abs/2310.19491) | æœ¬æ–‡ä»‹ç»äº†ä»å…·æœ‰ç»™å®šåˆå§‹çŠ¶æ€çš„è§£è¿‡ç¨‹çš„åˆ†å¸ƒä¸­è¯†åˆ«çº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„å‘ç”Ÿå™¨çš„æ¡ä»¶ï¼Œå¹¶ä¸”æä¾›äº†å¯¹äºå…·æœ‰åŠ æ€§å’Œä¹˜æ€§å™ªå£°çš„SDEçš„è¯†åˆ«æ¡ä»¶ã€‚ |
| [^34] | [Learning an Inventory Control Policy with General Inventory Arrival Dynamics.](http://arxiv.org/abs/2310.17168) | æœ¬æ–‡è§£å†³äº†å­¦ä¹ å…·æœ‰ä¸€èˆ¬åº“å­˜åˆ°è´§åŠ¨æ€ä¸‹çš„åº“å­˜æ§åˆ¶ç­–ç•¥çš„é—®é¢˜ï¼ŒåŒæ—¶å…è®¸ä¿®æ”¹è®¢è´­æ•°é‡ä»¥æ»¡è¶³ä¾›åº”å•†çš„é™åˆ¶ï¼Œå¹¶å°†å‘¨æœŸæ€§å®¡æ ¸åº“å­˜æ§åˆ¶é—®é¢˜å®šä¹‰ä¸ºå¤–éƒ¨å†³ç­–è¿‡ç¨‹ã€‚ |
| [^35] | [Learning bounded-degree polytrees with known skeleton.](http://arxiv.org/abs/2310.06333) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆå­¦ä¹ å·²çŸ¥éª¨æ¶çš„æœ‰ç•Œåº¦å¤šæ ‘çš„ç®—æ³•ï¼Œå¹¶ç»™å‡ºäº†åœ¨å¤šé¡¹å¼æ—¶é—´å’Œæ ·æœ¬å¤æ‚åº¦å†…çš„æœ‰é™æ ·æœ¬ä¿è¯ã€‚è¿™å¯¹äºå¤æ‚æ¦‚ç‡åˆ†å¸ƒçš„å­¦ä¹ å…·æœ‰é‡è¦æ„ä¹‰ã€‚ |
| [^36] | [A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling.](http://arxiv.org/abs/2310.03298) | æå‡ºäº†ä¸€ç§åŸºäºæ½œå˜é‡çš„æ–¹æ³•ï¼Œç”¨äºéå±‚æ¬¡åŒ–å¤šä¿çœŸåº¦è‡ªé€‚åº”é‡‡æ ·ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ä¸åŒä¿çœŸåº¦æ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ä»¥æ›´é«˜æ•ˆåœ°æ¢ç´¢å’Œåˆ©ç”¨è®¾è®¡ç©ºé—´ã€‚ |
| [^37] | [On the different regimes of Stochastic Gradient Descent.](http://arxiv.org/abs/2309.10688) | è¿™é¡¹ç ”ç©¶è§£å†³äº†å¯¹äºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸­ä¸åŒæ¨¡å¼çš„è¿½è¸ªå’Œç†è§£çš„é—®é¢˜ï¼Œæä¾›äº†ä¸€ä¸ªç›¸ä½å›¾æ¥åŒºåˆ†å™ªå£°ä¸»å¯¼çš„SGDå’Œå¤§æ­¥éª¤ä¸»å¯¼çš„SGDã€‚ |
| [^38] | [Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression.](http://arxiv.org/abs/2309.05030) | æœ¬æ–‡æå‡ºäº†å»æ®–æ°‘åŒ–äººå·¥æ™ºèƒ½å¯¹é½çš„ä¸‰ä¸ªå»ºè®®ï¼šæ”¹å˜åŸºæœ¬é“å¾·å“²å­¦ä¸ºè¾¾å°”ç›å“²å­¦ï¼Œå…è®¸å¤šå…ƒä¸»ä¹‰çš„è®ºè¯ä¼ ç»Ÿå­˜åœ¨äºå¯¹é½æŠ€æœ¯ä¸­ï¼Œä»¥åŠå°†ä»·å€¼è®¤è¯†è®ºæ‰©å±•åˆ°è¶…è¶Šè‡ªç„¶è¯­è¨€ä¸­çš„æŒ‡ä»¤ã€‚ |
| [^39] | [Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction.](http://arxiv.org/abs/2308.09647) | è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMC-CPçš„æ–°å‹æ··åˆä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å°†è‡ªé€‚åº”è’™ç‰¹å¡æ´›dropoutæ–¹æ³•ä¸åˆè§„é¢„æµ‹ç›¸ç»“åˆï¼Œå®ç°äº†èŠ‚çœèµ„æºå’Œäº§ç”Ÿé²æ£’é¢„æµ‹é›†/åŒºé—´çš„ç›®æ ‡ã€‚å®éªŒè¯æ˜MC-CPåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ç›¸æ¯”å…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰æ˜¾è‘—æå‡ |
| [^40] | [Multiclass Online Learnability under Bandit Feedback.](http://arxiv.org/abs/2308.04620) | Banditåé¦ˆä¸‹çš„åœ¨çº¿å¤šç±»å­¦ä¹ çš„å…³é”®åœ¨äºBandit Littlestoneç»´åº¦çš„æœ‰é™æ€§ï¼Œæ— è®ºæ ‡ç­¾ç©ºé—´æ˜¯å¦æ— ç•Œã€‚ |
| [^41] | [Logarithmic Bayes Regret Bounds.](http://arxiv.org/abs/2306.09136) | è¯¥è®ºæ–‡æå‡ºäº†å¯¹äºè´å¶æ–¯èµŒåšæœºçš„é¦–ä¸ªæœ‰é™æ—¶é—´å¯¹æ•°é—æ†¾è¾¹ç•Œï¼Œå¹¶ç”¨äºé«˜æ–¯å’Œçº¿æ€§èµŒåšæœºï¼Œä»è€Œé˜æ˜äº†è´å¶æ–¯è®¾ç½®ä¸­å…ˆéªŒä»·å€¼ä»¥åŠå¯¹$\tilde{O}(\sqrt{n})$ç•Œé™çš„æ”¹å–„ã€‚ |
| [^42] | [Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders.](http://arxiv.org/abs/2306.05023) | æœ¬æ–‡ç ”ç©¶äº†é«˜åº¦ç›¸ä¼¼çš„å˜åˆ†åéªŒåˆ†å¸ƒå’Œå…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„åéªŒå´©æºƒç°è±¡ï¼Œç‰¹åˆ«åœ°ï¼Œé€šè¿‡å¯¹çº¿æ€§æ¡ä»¶VAEå’Œåˆ†å±‚VAEè¿›è¡Œåˆ†æï¼Œè¯æ˜äº†è¿™ç§ç°è±¡æ˜¯ç”±äºæ½œåœ¨å˜é‡å±‚æ¬¡å…³ç³»ä¸æ¸…æ™°è€Œå¼•èµ·çš„ã€‚ |
| [^43] | [Data-Driven Regret Balancing for Online Model Selection in Bandits.](http://arxiv.org/abs/2306.02869) | è®ºæ–‡è®¨è®ºåœ¨å…·æœ‰èµŒåšåé¦ˆçš„éšæœºç¯å¢ƒä¸­è¿›è¡Œé€‰æ‹©ï¼Œæå‡ºäº†ä¸¤ç§åŸºäºæ•°æ®çš„æ¨¡å‹é€‰æ‹©ç®—æ³•ï¼Œå¹¶è¯æ˜äº†å…¶ä¿è¯ã€‚é€šè¿‡åˆ©ç”¨å®é™…é—æ†¾ï¼Œè¿™äº›ç®—æ³•åœ¨å®é™…ä¸­å–å¾—äº†å¥½æ•ˆæœã€‚ |
| [^44] | [Better Batch for Deep Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.17028) | è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨ mini-batch ä¸­æ˜¾å¼åœ°å­¦ä¹ è¯¯å·®çš„åºåˆ—ç›¸å…³æ€§ï¼Œæ¥æé«˜æ·±åº¦æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚ |
| [^45] | [Theoretical Analysis of Inductive Biases in Deep Convolutional Networks.](http://arxiv.org/abs/2305.08404) | æœ¬æ–‡ç ”ç©¶æ·±å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„å½’çº³åç½®ï¼Œè¯æ˜äº†$\mathcal{O}(\log d)$çš„æ·±åº¦å°±è¶³ä»¥å®ç°æ™®é€‚æ€§ï¼Œç”¨CNNå­¦ä¹ ç¨€ç–å‡½æ•°åªéœ€è¦$\tilde{\mathcal{O}}(\log^2d)$ä¸ªæ ·æœ¬ã€‚åŒæ—¶ï¼Œé€šè¿‡å±€éƒ¨è¿æ¥ç½‘ç»œï¼ˆLCNï¼‰åˆ†æäº†æƒé‡å…±äº«å’Œå±€éƒ¨æ€§çš„å½’çº³åç½®çš„åŒºåˆ«ï¼Œå¾—å‡ºäº†å®ƒä»¬åœ¨è¡¨ç¤ºéœ€è¦æœ‰é™å¹³ç§»ç­‰å˜å’Œé«˜æ–¹å‘é€‰æ‹©æ€§çš„å‡½æ•°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ |
| [^46] | [Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm.](http://arxiv.org/abs/2303.07287) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æœ€å¤§åŒ–ä¸€ç³»åˆ—å½’ä¸€åŒ–çŸ©æ¥ä½¿ç”¨å­é«˜æ–¯å†…åœ¨çŸ©èŒƒå®ç°ç´§å‡‘çš„éæ¸è¿›æ¨æ–­çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¯¼è‡´æ›´ç´§çš„Hoeffdingå­é«˜æ–¯æµ“åº¦ä¸ç­‰å¼ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å­é«˜æ–¯å›¾æ£€æŸ¥å…·æœ‰æœ‰é™æ ·æœ¬å¤§å°çš„å­é«˜æ–¯æ•°æ®ã€‚ |
| [^47] | [Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities.](http://arxiv.org/abs/2211.08594) | OPAAæ˜¯ä¸€ç§åŠŸèƒ½åˆ†ææ–¹æ³•çš„ç®—æ³•ï¼Œé€šè¿‡æ‰¾åˆ°å¹³æ»‘çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ä¼°è®¡å€¼ã€è®¡ç®—å½’ä¸€åŒ–æƒé‡çš„ä¼°è®¡å€¼ï¼Œå¹¶ä½¿ç”¨ç‰¹æ®Šçš„å‡½æ•°ç©ºé—´è½¬æ¢æ¥ä¼°è®¡è¯æ®ï¼Œå®ç°äº†å¹¶è¡Œè®¡ç®—çš„ä¸€æ¬¡é€šè¿‡ã€‚å®ƒé€‚ç”¨äºä¼°è®¡æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œå°¤å…¶åœ¨è´å¶æ–¯é—®é¢˜ä¸­ä¼°è®¡å½’ä¸€åŒ–æƒé‡ã€‚ |
| [^48] | [Transfer learning with affine model transformation.](http://arxiv.org/abs/2210.09745) | æœ¬æ–‡æå‡ºäº†ä¸€ç§å«åšä»¿å°„æ¨¡å‹è½¬ç§»çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æœŸæœ›å¹³æ–¹æŸå¤±ï¼Œå¯ä»¥é€‚åº”å„ç§ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºç¥ç»ç‰¹å¾æå–å™¨çš„æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªæ–¹æ³•ä¹Ÿç»™å‡ºäº†ç†è®ºä¸Šçš„è§£é‡Šã€‚ |
| [^49] | [The Manifold Scattering Transform for High-Dimensional Point Cloud Data.](http://arxiv.org/abs/2206.10078) | å¤šæ ·æ€§æ•£å°„å˜æ¢æ˜¯ä¸€ç§ç”¨äºé«˜ç»´ç‚¹äº‘æ•°æ®çš„æ·±åº¦ç‰¹å¾æå–å™¨ï¼Œåœ¨å®ç°ä¸Šé‡‡ç”¨äº†æ‰©æ•£æ˜ å°„ç†è®ºï¼Œæœ‰æ•ˆç”¨äºä¿¡å·åˆ†ç±»å’Œæµå½¢åˆ†ç±»ä»»åŠ¡ã€‚ |
| [^50] | [Towards Size-Independent Generalization Bounds for Deep Operator Nets.](http://arxiv.org/abs/2205.11359) | æœ¬è®ºæ–‡ç ”ç©¶äº†æ·±åº¦æ“ä½œå™¨ç½‘ç»œçš„æ³›åŒ–ç•Œé™é—®é¢˜ï¼Œåœ¨ä¸€ç±»DeepONetsä¸­è¯æ˜äº†å®ƒä»¬çš„Rademacherå¤æ‚åº¦çš„ç•Œé™ä¸ä¼šéšç½‘ç»œå®½åº¦æ‰©å±•è€Œæ˜ç¡®å˜åŒ–ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªç»“æœå±•ç¤ºäº†å¦‚ä½•é€‰æ‹©HuberæŸå¤±æ¥è·å¾—ä¸æ˜ç¡®ä¾èµ–äºç½‘ç»œå¤§å°çš„æ³›åŒ–è¯¯å·®ç•Œé™ã€‚ |
| [^51] | [Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity.](http://arxiv.org/abs/2204.07526) | æœ¬æ–‡é€šè¿‡é€šä¿¡å¤æ‚åº¦æ¨å¯¼å‡ºäº†å¯¹äºå†…å­˜å—é™ç®—æ³•åœ¨å¼ é‡ä¸»æˆåˆ†åˆ†æä¸­çš„è®¡ç®—ä¸‹ç•Œï¼Œå¹¶ä¸”æŒ‡å®šäº†è§£å†³è¯¥é—®é¢˜çš„ç®—æ³•å¿…é¡»åœ¨æ•°æ®æ ·æœ¬ç»è¿‡æ¬¡æ•°ã€æ ·æœ¬å¤§å°å’Œæ‰€éœ€å†…å­˜ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è¿™äº›ä¸‹ç•Œæš—ç¤ºäº†è®¸å¤šå¸¸ç”¨ç®—æ³•åœ¨æ ·æœ¬å¤§å°ä¸å¤Ÿå¤§æ—¶éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°ã€‚ |
| [^52] | [The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models.](http://arxiv.org/abs/2203.00144) | æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ConcordanceæŒ‡æ•°åˆ†è§£æˆä¸¤ä¸ªéƒ¨åˆ†çš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°ç”Ÿå­˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥åˆ†è§£æ–¹æ³•å¯ä»¥è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†æï¼Œæ­ç¤ºä¸åŒé¢„æµ‹æ–¹æ³•ä¹‹é—´çš„ä¼˜åŠ£ã€‚å®éªŒè¯æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨äº†è§‚æµ‹äº‹ä»¶ã€‚ |
| [^53] | [High-dimensional Inference and FDR Control for Simulated Markov Random Fields.](http://arxiv.org/abs/2202.05612) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨é«˜ç»´èƒŒæ™¯ä¸‹è¿›è¡Œæ¨¡æ‹Ÿé©¬å°”å¯å¤«éšæœºåœºç»Ÿè®¡æ¨æ–­çš„æ–¹æ³•ï¼Œå®ç°äº†ä¸€è‡´æ€§ï¼Œå¹¶æ„å»ºäº†ä¸¤ç§è¯¯å‘ç°ç‡æ§åˆ¶ç¨‹åºã€‚ |
| [^54] | [Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series.](http://arxiv.org/abs/2006.05259) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ—¶é—´åºåˆ—å›ºæœ‰å¯¹ç§°æ€§æ„å»ºçš„å°æ³¢ç½‘ç»œï¼Œå…¶è¡¨ç°å‡ºåµŒå¥—çš„éçº¿æ€§å°æ³¢æ ·çš„æ—¶é¢‘å˜æ¢ï¼Œå®éªŒè¯æ˜å…¶åœ¨åŸå§‹æ³¢å½¢ä¸Šä¼˜äºä¼ ç»Ÿçš„CNNã€‚ |
| [^55] | [Fast approximations in the homogeneous Ising model for use in scene analysis.](http://arxiv.org/abs/1712.02195) | æœ¬æ–‡æä¾›äº†ä¸€ç§å¿«é€Ÿè¿‘ä¼¼è®¡ç®—åŒè´¨ Ising æ¨¡å‹ä¸­é‡è¦é‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°åœ¨æ¨¡æ‹Ÿç ”ç©¶ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¯ç”¨äºåœºæ™¯åˆ†æã€‚ |

# è¯¦ç»†

[^1]: ç”¨äºå‡è½»åˆ†å¸ƒè½¬å˜çš„é”™è¯¯å›å½’çš„æ–¹æ³•åŠåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨

    Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning. (arXiv:2401.12216v1 [stat.ML])

    [http://arxiv.org/abs/2401.12216](http://arxiv.org/abs/2401.12216)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨æ¨¡å‹è§„èŒƒé”™è¯¯çš„æ¡ä»¶ä¸‹ï¼Œç”±äºåˆ†å¸ƒè½¬å˜å¯¼è‡´çš„é”™è¯¯æ”¾å¤§é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

    

    æœºå™¨å­¦ä¹ åº”ç”¨ä¸­æ™®éå­˜åœ¨çš„ä¸€ä¸ªç°è±¡æ˜¯åˆ†å¸ƒè½¬å˜ï¼ŒæŒ‡çš„æ˜¯è®­ç»ƒå’Œéƒ¨ç½²æ¡ä»¶ä¹‹é—´çš„å·®å¼‚ã€‚ç”±äºåˆ†å¸ƒè½¬å˜é€šå¸¸å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› æ­¤äººä»¬ä¸€ç›´è‡´åŠ›äºç®—æ³•å¹²é¢„ä»¥å‡è½»è¿™äº›ä¸åˆ©å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨æ¨¡å‹è§„èŒƒé”™è¯¯çš„æƒ…å†µä¸‹åˆ†å¸ƒè½¬å˜çš„å½±å“ï¼Œå…·ä½“å…³æ³¨Lâˆ-é”™è¯¯å›å½’å’Œå¯¹æŠ—æ€§åå˜é‡è½¬å˜ï¼Œå…¶ä¸­å›å½’ç›®æ ‡ä¿æŒä¸å˜ï¼Œè€Œåå˜é‡åˆ†å¸ƒä»»æ„å˜åŒ–ã€‚æˆ‘ä»¬å‘ç°ç»éªŒé£é™©æœ€å°åŒ–æˆ–æ ‡å‡†æœ€å°äºŒä¹˜å›å½’å¯èƒ½å¯¼è‡´ä¸å¯å–çš„é”™è¯¯æ”¾å¤§ï¼Œå…¶ä¸­ç”±äºè§„èŒƒé”™è¯¯è€Œäº§ç”Ÿçš„è¯¯å·®è¢«è®­ç»ƒå’Œæµ‹è¯•åˆ†å¸ƒä¹‹é—´çš„å¯†åº¦æ¯”æ”¾å¤§ã€‚ä½œä¸ºæˆ‘ä»¬çš„ä¸»è¦ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”å—é²æ£’ä¼˜åŒ–æŠ€æœ¯å¯å‘â€”â€”ä»¥é¿å…è¿™ç§ä¸å¯å–çš„é”™è¯¯æ”¾å¤§ç°è±¡ã€‚

    A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undes
    
[^2]: ç”¨æ¢¯åº¦æ¥åå‡»ç»´åº¦ï¼šéšæœºå‡¸ä¼˜åŒ–ä¸­æ¢¯åº¦æ–¹æ³•çš„æ³›åŒ–ç ”ç©¶

    The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization. (arXiv:2401.12058v1 [cs.LG])

    [http://arxiv.org/abs/2401.12058](http://arxiv.org/abs/2401.12058)

    æœ¬ç ”ç©¶æ¢è®¨äº†éšæœºå‡¸ä¼˜åŒ–ä¸­æ¢¯åº¦æ–¹æ³•çš„æ³›åŒ–æ€§èƒ½ä»¥åŠç»´åº¦ä¾èµ–æ€§ã€‚æˆ‘ä»¬å‘ç°æ ‡å‡†çš„å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨ç»´åº¦$d = O(n^2)$æ—¶éœ€è¦è‡³å°‘$\Omega(\sqrt{d})$ä¸ªè®­ç»ƒæ ·æœ¬æ‰èƒ½è¾¾åˆ°éå¹³å‡¡çš„æµ‹è¯•è¯¯å·®ã€‚è€Œå¯¹äºæ ‡å‡†çš„ä¸€ééšæœºæ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼ŒåŒæ ·çš„ç»´åº¦ä¸‹ç•Œä¹Ÿé€‚ç”¨ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†æ¢¯åº¦æ–¹æ³•åœ¨åŸºç¡€éšæœºå‡¸ä¼˜åŒ–è®¾ç½®ä¸­çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶ç€é‡å…³æ³¨å…¶ç»´åº¦ä¾èµ–æ€§ã€‚é¦–å…ˆï¼Œé’ˆå¯¹å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ï¼Œæˆ‘ä»¬æ„é€ äº†ä¸€ä¸ªåœ¨ç»´åº¦$d=O(n^2)$ä¸‹çš„å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ç»è¿‡è°ƒæ•´ä»¥è¾¾åˆ°ç»éªŒé£é™©çš„æœ€ä½³æ€§èƒ½çš„æ ‡å‡†GDï¼ˆä½¿ç”¨$n$ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼‰ä»¥å¸¸æ•°æ¦‚ç‡æ”¶æ•›åˆ°ä¸€ä¸ªè¿‘ä¼¼çš„ç»éªŒé£é™©æœ€å°åŒ–å™¨ï¼Œå…¶äººå£è¿‡å‰©é£é™©ä¸º$\Omega(1)$ã€‚æˆ‘ä»¬çš„ç•Œé™å¯ä»¥è½¬åŒ–ä¸ºæ ‡å‡†GDéœ€è¦$\Omega(\sqrt{d})$ä¸ªè®­ç»ƒæ ·æœ¬æ‰èƒ½è¾¾åˆ°éå¹³å‡¡çš„æµ‹è¯•è¯¯å·®ï¼Œä»è€Œå›ç­”äº†Feldmanï¼ˆ2016ï¼‰å’ŒAmirã€Korenã€Livniï¼ˆ2021bï¼‰æå‡ºçš„ä¸€ä¸ªå¼€æ”¾é—®é¢˜ï¼Œå¹¶è¡¨æ˜éå¹³å‡¡çš„ç»´åº¦ä¾èµ–æ€§æ˜¯ä¸å¯é¿å…çš„ã€‚æ­¤å¤–ï¼Œå¯¹äºæ ‡å‡†çš„ä¸€ééšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œæˆ‘ä»¬å‘ç°åŒæ ·çš„æ„é€ æŠ€æœ¯å¯ä»¥æä¾›ç±»ä¼¼çš„$\Omega(\sqrt{d})$çš„ä¸‹ç•Œã€‚

    We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\Omega(1)$ population excess risk. Our bound translates to a lower bound of $\Omega (\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\Omega(\sqrt{d})$ lo
    
[^3]: å°†ç»Ÿè®¡æ˜¾è‘—æ€§å’Œåˆ¤åˆ«èƒ½åŠ›èå…¥æ¨¡å¼å‘ç°ä¸­

    Integrating Statistical Significance and Discriminative Power in Pattern Discovery. (arXiv:2401.12000v1 [cs.LG])

    [http://arxiv.org/abs/2401.12000](http://arxiv.org/abs/2401.12000)

    æœ¬è®ºæ–‡å°†ç»Ÿè®¡æ˜¾è‘—æ€§å’Œåˆ¤åˆ«èƒ½åŠ›èå…¥æ¨¡å¼å‘ç°ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–¹æ³•æ¥åŒæ—¶æ»¡è¶³æ¨¡å¼è´¨é‡å’Œç»Ÿè®¡æ ‡å‡†ï¼Œåœ¨ä¸‰å…ƒèšç±»ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶æœ‰æ•ˆæ€§ã€‚

    

    æ¨¡å¼å‘ç°åœ¨å¤šä¸ªé¢†åŸŸçš„æè¿°æ€§å’Œé¢„æµ‹æ€§ä»»åŠ¡ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚å¯æ“ä½œçš„æ¨¡å¼å¿…é¡»æ»¡è¶³ä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ ‡å‡†ï¼Œå¹¶ä¸”åœ¨ç›®æ ‡å˜é‡å­˜åœ¨æ—¶è¿›ä¸€æ­¥å…·æœ‰åˆ¤åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£å†³äº†åœ¨ç°æœ‰ç®—æ³•ä¸­å°†ç»Ÿè®¡æ˜¾è‘—æ€§å’Œåˆ¤åˆ«èƒ½åŠ›æ ‡å‡†èå…¥æ¨¡å¼å‘ç°çš„å°šæœªæ·±å…¥ç ”ç©¶çš„é¢†åŸŸï¼ŒåŒæ—¶ä¿æŒæ¨¡å¼è´¨é‡ã€‚æˆ‘ä»¬è¿˜è§£å†³äº†ä¸€äº›ç®—æ³•å¼•å…¥çš„æ¨¡å¼è´¨é‡é˜ˆå€¼å¦‚ä½•è°ƒæ•´ä»¥é€‚åº”è¿™äº›é¢å¤–æ ‡å‡†çš„é—®é¢˜ã€‚ä¸ºäº†æµ‹è¯•è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸‰å…ƒèšç±»ä»»åŠ¡ä½œä¸ºæ¨¡å¼å‘ç°æ¡ˆä¾‹ï¼Œå¹¶æ‰©å±•äº†ä¸¤ä¸ªè‘—åçš„è´ªå©ªå’Œå¤šç›®æ ‡ä¼˜åŒ–ä¸‰å…ƒèšç±»ç®—æ³•ï¼Œå³Î´-Trimaxå’ŒTriGenï¼Œå®ƒä»¬ä½¿ç”¨äº†å„ç§æ¨¡å¼è´¨é‡æ ‡å‡†ï¼Œä¾‹å¦‚å‡æ–¹æ®‹å·®ï¼ˆMSRï¼‰ã€æœ€å°äºŒä¹˜çº¿ï¼ˆLSLï¼‰å’Œå¤šæ–œç‡æµ‹é‡ï¼ˆMSLï¼‰ã€‚ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶çš„ç»“æœæ˜¾ç¤º

    Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show 
    
[^4]: äº¤å‰éªŒè¯åˆè§„é£é™©æ§åˆ¶

    Cross-Validation Conformal Risk Control. (arXiv:2401.11974v1 [cs.LG])

    [http://arxiv.org/abs/2401.11974](http://arxiv.org/abs/2401.11974)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰éªŒè¯çš„æ–°å‹åˆè§„é£é™©æ§åˆ¶æ–¹æ³•(CV-CRC)ï¼Œå®ƒæ‰©å±•äº†ä¸€è‡´æ€§é¢„æµ‹çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæ§åˆ¶æ›´å¹¿æ³›çš„é£é™©å‡½æ•°ï¼Œå¹¶åœ¨é¢„æµ‹å™¨é›†åˆçš„å¹³å‡é£é™©ä¸Šæä¾›äº†ç†è®ºä¿è¯ã€‚

    

    åˆè§„é£é™©æ§åˆ¶ï¼ˆCRCï¼‰æ˜¯ä¸€ç§æœ€è¿‘æå‡ºçš„æŠ€æœ¯ï¼Œå®ƒåº”ç”¨äºä¼ ç»Ÿçš„ç‚¹é¢„æµ‹å™¨ä¸Šï¼Œä»¥æä¾›æ ¡å‡†ä¿è¯ã€‚åœ¨CRCä¸­æ¨å¹¿ä¸€è‡´æ€§é¢„æµ‹ï¼ˆCPï¼‰ï¼Œé€šè¿‡ä»ç‚¹é¢„æµ‹å™¨ä¸­æå–ä¸€ä¸ªé¢„æµ‹å™¨é›†åˆæ¥æ§åˆ¶é£é™©å‡½æ•°ï¼ˆå¦‚è¯¯è¦†ç›–æ¦‚ç‡æˆ–é”™è¯¯è´Ÿä¾‹ç‡ï¼‰ï¼Œä»è€Œç¡®ä¿æ ¡å‡†æ€§ã€‚åŸå§‹çš„CRCéœ€è¦å°†å¯ç”¨æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€‚å½“æ•°æ®å¯ç”¨æ€§æœ‰é™æ—¶ï¼Œè¿™å¯èƒ½å¯¼è‡´é¢„æµ‹å™¨é›†åˆæ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºäº¤å‰éªŒè¯è€Œä¸æ˜¯åŸå§‹CRCçš„æ–°å‹CRCæ–¹æ³•ã€‚æ‰€æå‡ºçš„äº¤å‰éªŒè¯CRCï¼ˆCV-CRCï¼‰å°†CPçš„ä¸€ç§ç‰ˆæœ¬æ‰©å±•åˆ°CRCï¼Œå¯ä»¥æ§åˆ¶æ›´å¹¿æ³›çš„é£é™©å‡½æ•°ã€‚CV-CRCè¢«è¯æ˜åœ¨é¢„æµ‹å™¨é›†åˆçš„å¹³å‡é£é™©ä¸Šå…·æœ‰ç†è®ºä¿è¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ•°å€¼å®éªŒè¯æ˜CV-CRCåœ¨å®è·µä¸­çš„æœ‰æ•ˆæ€§ã€‚

    Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical exper
    
[^5]: RUMBoostï¼šæ¢¯åº¦æå‡çš„éšæœºæ•ˆç”¨æ¨¡å‹

    RUMBoost: Gradient Boosted Random Utility Models. (arXiv:2401.11954v1 [cs.LG])

    [http://arxiv.org/abs/2401.11954](http://arxiv.org/abs/2401.11954)

    RUMBoostæ¨¡å‹å°†éšæœºæ•ˆç”¨æ¨¡å‹ï¼ˆRUMsï¼‰çš„è§£é‡Šæ€§å’Œè¡Œä¸ºé²æ£’æ€§ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ç›¸ç»“åˆï¼Œé€šè¿‡ä½¿ç”¨æ¢¯åº¦æå‡å›å½’æ ‘çš„é›†åˆæ¥è·å¾—éçº¿æ€§æ•ˆç”¨å‡½æ•°çš„å®Œæ•´å‡½æ•°å½¢å¼ï¼Œå®ç°äº†å¯¹è¾“å…¥å˜é‡çš„ä»»ä½•å¯èƒ½ç»„åˆè¿›è¡Œå¸¸æ•°æ’è¡¥ã€‚

    

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç¦»æ•£é€‰æ‹©å»ºæ¨¡æ–¹æ³•ï¼ŒRUMBoostæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†éšæœºæ•ˆç”¨æ¨¡å‹ï¼ˆRUMsï¼‰çš„å¯è§£é‡Šæ€§å’Œè¡Œä¸ºé²æ£’æ€§ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œé¢„æµ‹èƒ½åŠ›ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡ç”¨æ¢¯åº¦æå‡å›å½’æ ‘çš„é›†åˆæ›¿æ¢RUMçš„æ•ˆç”¨å‡½æ•°ä¸­çš„çº¿æ€§å‚æ•°æ¥è·å¾—éçº¿æ€§æ•ˆç”¨å‡½æ•°çš„å®Œæ•´å‡½æ•°å½¢å¼ã€‚è¿™ä½¿å¾—å¯ä»¥ç›´æ¥ä»æ•°æ®ä¸­ä¸ºæ‰€æœ‰å¤‡é€‰æ–¹æ¡ˆçš„æ•ˆç”¨å€¼è¿›è¡Œåˆ†æ®µå¸¸æ•°æ’è¡¥ï¼Œä»¥é€‚åº”ä»»ä½•å¯èƒ½çš„è¾“å…¥å˜é‡ç»„åˆã€‚æˆ‘ä»¬å¯¹é›†åˆå¼•å…¥äº†é¢å¤–çš„çº¦æŸæ¡ä»¶ï¼Œä»¥ç¡®ä¿æ•ˆç”¨å‡½æ•°å…·æœ‰ä¸‰ä¸ªå…³é”®ç‰¹å¾ï¼šï¼ˆiï¼‰æ¯ä¸ªå¤‡é€‰æ–¹æ¡ˆçš„æ•ˆç”¨ä»…ä¾èµ–äºè¯¥å¤‡é€‰æ–¹æ¡ˆçš„å±æ€§ï¼Œï¼ˆiiï¼‰è¾¹é™…æ•ˆç”¨å•è°ƒæ€§ï¼Œä»¥åŠï¼ˆiiiï¼‰å†…åœ¨å¯è§£é‡Šçš„å‡½æ•°å½¢å¼ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ•´ä¸ªè¾“å…¥ç©ºé—´ä¸­çš„å“åº”éƒ½æ˜¯å·²çŸ¥çš„ã€‚

    This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods. We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees. This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. Fur
    
[^6]: é€šè¿‡åˆ†è§£æ¢¯åº¦ä¸‹é™å®ç°ä½èƒçŠ¶ç§©å¼ é‡æ¢å¤

    Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent. (arXiv:2401.11940v1 [cs.LG])

    [http://arxiv.org/abs/2401.11940](http://arxiv.org/abs/2401.11940)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†è§£æ¢¯åº¦ä¸‹é™æ–¹æ³•è§£å†³ä½èƒçŠ¶ç§©å¼ é‡æ¢å¤é—®é¢˜çš„é«˜æ•ˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†å¤§å¼ é‡åˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒå°çš„å› å­å¼ é‡ï¼Œåœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œå­˜å‚¨éœ€æ±‚çš„åŒæ—¶ï¼Œç¡®ä¿äº†æ”¶æ•›æ€§ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†ä»å°‘é‡è¢«ç ´åçš„çº¿æ€§æµ‹é‡ä¸­æ¢å¤å…·æœ‰ä½èƒçŠ¶ç§©ç»“æ„çš„å¼ é‡çš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦è®¡ç®—å¼ é‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆt-SVDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è®¡ç®—å¯†é›†çš„è¿‡ç¨‹ï¼Œä½¿å®ƒä»¬éš¾ä»¥å¤„ç†å¤§è§„æ¨¡å¼ é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç±»ä¼¼äºBurer-Monteiroï¼ˆBMï¼‰æ–¹æ³•çš„åˆ†è§£è¿‡ç¨‹çš„é«˜æ•ˆä½èƒçŠ¶ç§©å¼ é‡æ¢å¤æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„åŸºæœ¬æ–¹æ³•æ¶‰åŠå°†ä¸€ä¸ªå¤§å¼ é‡åˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒå°çš„å› å­å¼ é‡ï¼Œç„¶åé€šè¿‡åˆ†è§£æ¢¯åº¦ä¸‹é™ï¼ˆFGDï¼‰æ¥è§£å†³é—®é¢˜ã€‚è¯¥ç­–ç•¥æ¶ˆé™¤äº†t-SVDè®¡ç®—çš„éœ€è¦ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—æˆæœ¬å’Œå­˜å‚¨éœ€æ±‚ã€‚æˆ‘ä»¬æä¾›äº†ä¸¥æ ¼çš„ç†è®ºåˆ†æï¼Œä»¥ä¿è¯FGDåœ¨æ— å™ªå£°å’Œæœ‰å™ªå£°æƒ…å†µä¸‹çš„æ”¶æ•›æ€§ã€‚

    This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it 
    
[^7]: å¼‚è´¨æ€§éšæœºå¯¹ç…§è¯•éªŒä¸­æ—¶é—´åˆ°äº‹ä»¶ç»“æœçš„äºšç»„åˆ†ææ–¹æ³•

    Subgroup analysis methods for time-to-event outcomes in heterogeneous randomized controlled trials. (arXiv:2401.11842v1 [stat.ME])

    [http://arxiv.org/abs/2401.11842](http://arxiv.org/abs/2401.11842)

    æœ¬è®ºæ–‡è¯„ä¼°äº†å¤šç§æ—¶é—´åˆ°äº‹ä»¶ç»“æœçš„äºšç»„åˆ†æç®—æ³•ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥æ¢ç´¢ä¸åŒçš„å¼‚è´¨æ€§æƒ…æ™¯ã€‚

    

    éæ˜¾è‘—çš„éšæœºå¯¹ç…§è¯•éªŒå¯èƒ½éšè—äº†å¯¹å®éªŒæ€§è¯ç‰©æœ‰è‰¯å¥½ååº”çš„äºšç»„ï¼Œä»è€Œé˜»ç¢äº†åç»­çš„å‘å±•ã€‚é‰´å®šè¿™ç§å¼‚è´¨æ€§æ²»ç–—æ•ˆåº”å¯¹äºç²¾å‡†åŒ»å­¦è‡³å…³é‡è¦ï¼Œä¸ºæ­¤å·²ç»å¼€å‘å‡ºè®¸å¤šäº‹ååˆ†ææ–¹æ³•ã€‚è™½ç„¶å·²ç»è¿›è¡Œäº†å‡ ä¸ªåŸºå‡†æµ‹è¯•æ¥é‰´å®šè¿™äº›æ–¹æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå°¤å…¶æ˜¯å¯¹äºäºŒè¿›åˆ¶å’Œè¿ç»­ç»ˆç‚¹ï¼Œä½†æ˜¯å¯¹äºæ—¶é—´åˆ°äº‹ä»¶ç»ˆç‚¹çš„äºšç»„åˆ†æç¼ºä¹ç±»ä¼¼çš„ç³»ç»Ÿå®è¯è¯„ä¼°ã€‚æœ¬å·¥ä½œæ—¨åœ¨é€šè¿‡è¯„ä¼°å‡ ç§æ—¶é—´åˆ°äº‹ä»¶ç»“æœçš„äºšç»„åˆ†æç®—æ³•æ¥å¡«è¡¥è¿™ä¸ªç©ºç™½ï¼Œé€šè¿‡ä¸‰ä¸ªä¸åŒçš„ç ”ç©¶é—®é¢˜ï¼šæ˜¯å¦å­˜åœ¨å¼‚è´¨æ€§ï¼Ÿä»€ä¹ˆç”Ÿç‰©æ ‡å¿—ç‰©æ˜¯å¯¼è‡´è¿™ç§å¼‚è´¨æ€§çš„åŸå› ï¼Ÿè°æ˜¯å¯¹æ²»ç–—æœ‰è‰¯å¥½ååº”çš„äººï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆæˆå’ŒåŠåˆæˆæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿äººä»¬èƒ½å¤Ÿæ¢ç´¢å¹¿æ³›çš„å¼‚è´¨æ€§æƒ…æ™¯ã€‚

    Non-significant randomized control trials can hide subgroups of good responders to experimental drugs, thus hindering subsequent development. Identifying such heterogeneous treatment effects is key for precision medicine and many post-hoc analysis methods have been developed for that purpose. While several benchmarks have been carried out to identify the strengths and weaknesses of these methods, notably for binary and continuous endpoints, similar systematic empirical evaluation of subgroup analysis for time-to-event endpoints are lacking. This work aims to fill this gap by evaluating several subgroup analysis algorithms in the context of time-to-event outcomes, by means of three different research questions: Is there heterogeneity? What are the biomarkers responsible for such heterogeneity? Who are the good responders to treatment? In this context, we propose a new synthetic and semi-synthetic data generation process that allows one to explore a wide range of heterogeneity scenarios 
    
[^8]: ä½¿ç”¨æ¬ é˜»å°¼ Langevin Monte Carlo åŠ é€Ÿè¿‘ä¼¼ Thompson é‡‡æ ·

    Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])

    [http://arxiv.org/abs/2401.11665](http://arxiv.org/abs/2401.11665)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¬ é˜»å°¼ Langevin Monte Carlo åŠ é€Ÿçš„è¿‘ä¼¼ Thompson é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å®šåŠ¿å‡½æ•°çš„è®¾è®¡æ”¹å–„äº†é«˜ç»´é—®é¢˜ä¸­çš„æ ·æœ¬å¤æ‚åº¦ï¼Œå¹¶åœ¨é«˜ç»´èµŒåšæœºé—®é¢˜ä¸­è¿›è¡Œäº†éªŒè¯ã€‚

    

    ä½¿ç”¨æ¬ é˜»å°¼ Langevin Monte Carlo çš„è¿‘ä¼¼ Thompson é‡‡æ ·æ–¹æ³•æ‰©å±•äº†å…¶é€‚ç”¨èŒƒå›´ï¼Œä»é«˜æ–¯åéªŒé‡‡æ ·æ‰©å±•åˆ°æ›´ä¸€èˆ¬çš„å¹³æ»‘åéªŒã€‚ç„¶è€Œï¼Œåœ¨é«˜ç»´é—®é¢˜ä¸­è¦æ±‚é«˜å‡†ç¡®æ€§æ—¶ï¼Œä»ç„¶é¢ä¸´å¯æ‰©å±•æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿‘ä¼¼ Thompson é‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨æ¬ é˜»å°¼ Langevin Monte Carloï¼Œåè€…æ˜¯æ¨¡æ‹Ÿé«˜ç»´åéªŒçš„é€šç”¨å·¥å…·ã€‚åŸºäºæ ‡å‡†çš„å¹³æ»‘æ€§å’Œå¯¹æ•°å‡¹æ€§æ¡ä»¶ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨ç‰¹å®šåŠ¿å‡½æ•°çš„åŠ é€ŸåéªŒé›†ä¸­å’Œé‡‡æ ·ã€‚è¯¥è®¾è®¡æ”¹è¿›äº†å®ç°å¯¹æ•°é—æ†¾çš„æ ·æœ¬å¤æ‚åº¦ï¼Œä»$\mathcal{\tilde O}(d)$æ”¹è¿›åˆ°$\mathcal{\tilde O}(\sqrt{d})$ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åˆæˆå®éªŒåœ¨é«˜ç»´èµŒåšæœºé—®é¢˜ä¸­ç»éªŒéªŒè¯äº†æˆ‘ä»¬ç®—æ³•çš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚

    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
    
[^9]: é€šè¿‡æ–¹å·®é™ä½çš„è‰å›¾è¿›è¡Œéå‚æ•°ä¼°è®¡

    Nonparametric Estimation via Variance-Reduced Sketching. (arXiv:2401.11646v1 [stat.ML])

    [http://arxiv.org/abs/2401.11646](http://arxiv.org/abs/2401.11646)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVariance-Reduced Sketchingçš„æ¡†æ¶ï¼Œç”¨äºåœ¨é«˜ç»´åº¦ä¸­ä¼°è®¡å¯†åº¦å‡½æ•°å’Œéå‚æ•°å›å½’å‡½æ•°ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å‡½æ•°æ¦‚å¿µåŒ–ä¸ºçŸ©é˜µï¼Œå¹¶é‡‡ç”¨è‰å›¾æŠ€æœ¯æ¥é™ä½ç»´åº¦ç¾éš¾å¼•èµ·çš„æ–¹å·®ï¼Œå±•ç¤ºäº†é²æ£’æ€§èƒ½å’Œæ˜¾è‘—æ”¹è¿›ã€‚

    

    éå‚æ•°æ¨¡å‹åœ¨å„ä¸ªç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸä¸­å¤‡å—å…³æ³¨ã€‚ç»å…¸çš„æ ¸æ–¹æ³•åœ¨ä½ç»´æƒ…å†µä¸‹å…·æœ‰æ•°å€¼ç¨³å®šæ€§å’Œç»Ÿè®¡å¯é æ€§ï¼Œä½†åœ¨é«˜ç»´æƒ…å†µä¸‹ç”±äºç»´åº¦ç¾éš¾å˜å¾—ä¸å¤Ÿé€‚ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºVariance-Reduced Sketchingï¼ˆVRSï¼‰çš„æ–°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåœ¨é™ä½ç»´åº¦ç¾éš¾çš„åŒæ—¶åœ¨é«˜ç»´åº¦ä¸­ä¼°è®¡å¯†åº¦å‡½æ•°å’Œéå‚æ•°å›å½’å‡½æ•°ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†å¤šå˜é‡å‡½æ•°æ¦‚å¿µåŒ–ä¸ºæ— é™å¤§å°çš„çŸ©é˜µï¼Œå¹¶å€Ÿé‰´äº†æ•°å€¼çº¿æ€§ä»£æ•°æ–‡çŒ®ä¸­çš„ä¸€ç§æ–°çš„è‰å›¾æŠ€æœ¯æ¥é™ä½ä¼°è®¡é—®é¢˜ä¸­çš„æ–¹å·®ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—çš„æ¨¡æ‹Ÿå®éªŒå’ŒçœŸå®æ•°æ®åº”ç”¨å±•ç¤ºäº†VRSçš„é²æ£’æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®¸å¤šå¯†åº¦ä¼°è®¡é—®é¢˜ä¸­ï¼ŒVRSç›¸è¾ƒäºç°æœ‰çš„ç¥ç»ç½‘ç»œä¼°è®¡å™¨å’Œç»å…¸çš„æ ¸æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚

    Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density 
    
[^10]: ç”¨äºå…‹æœç¾éš¾æ€§è¿‡æ‹Ÿåˆçš„é«˜æ•ˆæœ¬åœ°çº¿æ€§æ­£åˆ™åŒ–

    Efficient local linearity regularization to overcome catastrophic overfitting. (arXiv:2401.11618v1 [cs.LG])

    [http://arxiv.org/abs/2401.11618](http://arxiv.org/abs/2401.11618)

    æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºELLEçš„æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºé«˜æ•ˆåœ°å‡è½»å•æ­¥å¯¹æŠ—æ€§è®­ç»ƒä¸­çš„ç¾éš¾æ€§è¿‡æ‹Ÿåˆã€‚å®ƒèƒ½å¤Ÿä¿æŒæŸå¤±å‡½æ•°åœ¨è¾“å…¥ä¸Šçš„å±€éƒ¨çº¿æ€§æ€§ï¼Œä¸ä¼ ç»Ÿçš„æ­£åˆ™åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒELLEæ›´åŠ é«˜æ•ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤§å¯¹æŠ—æ€§æ‰°åŠ¨å’Œé•¿è®­ç»ƒè®¡åˆ’ç­‰å›°éš¾æƒ…å†µã€‚

    

    å•æ­¥å¯¹æŠ—æ€§è®­ç»ƒä¸­çš„ç¾éš¾æ€§è¿‡æ‹Ÿåˆ (CO) å¯¼è‡´å¯¹æŠ—æ€§æµ‹è¯•å‡†ç¡®ç‡çªç„¶ä¸‹é™ï¼ˆç”šè‡³é™è‡³0%ï¼‰ã€‚å¯¹äºä½¿ç”¨å¤šæ­¥å¯¹æŠ—æ€§è®­ç»ƒè®­ç»ƒçš„æ¨¡å‹ï¼Œå·²è§‚å¯Ÿåˆ°æŸå¤±å‡½æ•°åœ¨è¾“å…¥ä¸Šå…·æœ‰å±€éƒ¨çº¿æ€§æ€§ï¼Œä½†è¿™ç§ç‰¹æ€§åœ¨å•æ­¥å¯¹æŠ—æ€§è®­ç»ƒä¸­ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³å•æ­¥å¯¹æŠ—æ€§è®­ç»ƒä¸­çš„COé—®é¢˜ï¼Œæå‡ºäº†å‡ ç§é€šè¿‡æ­£åˆ™åŒ–æ¥å¼ºåˆ¶æŸå¤±å‡½æ•°å±€éƒ¨çº¿æ€§æ€§çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºåŒé‡åå‘ä¼ æ’­ï¼Œè¿™äº›æ­£åˆ™åŒ–é¡¹ä¼šæ˜¾è‘—å‡æ…¢è®­ç»ƒé€Ÿåº¦ã€‚ä¸ä¹‹ç›¸åï¼Œåœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ç§ç§°ä¸ºELLEçš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥åœ¨ç»å…¸å¯¹æŠ—æ€§è®­ç»ƒè¯„ä¼°ä¸­æœ‰æ•ˆä¸”é«˜æ•ˆåœ°å‡è½»COé—®é¢˜ï¼Œåœ¨ä¸€äº›æ›´å›°éš¾çš„æƒ…å†µä¸‹ä¹Ÿèƒ½èµ·ä½œç”¨ï¼Œä¾‹å¦‚å¤§å¯¹æŠ—æ€§æ‰°åŠ¨å’Œé•¿è®­ç»ƒè®¡åˆ’ã€‚æˆ‘ä»¬çš„æ­£åˆ™åŒ–é¡¹åœ¨ç†è®ºä¸Šä¸æŸå¤±å‡½æ•°çš„æ›²ç‡æœ‰è”ç³»ï¼Œå¹¶ä¸”é€šè¿‡é¿å…åŒé‡åå‘ä¼ æ’­è€Œå…·æœ‰æ¯”å…ˆå‰æ–¹æ³•æ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å½»åº•çš„å®éªŒç ”ç©¶...

    Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental 
    
[^11]: ç†è§£åæœŸå­¦ä¹ ç‡è¡°å‡çš„æ³›åŒ–ä¼˜åŠ¿

    Understanding the Generalization Benefits of Late Learning Rate Decay. (arXiv:2401.11600v1 [cs.LG])

    [http://arxiv.org/abs/2401.11600](http://arxiv.org/abs/2401.11600)

    æœ¬æ–‡ç ”ç©¶äº†ä¸ºä»€ä¹ˆä½¿ç”¨å¤§å­¦ä¹ ç‡é•¿æ—¶é—´è®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥å®ç°æ›´å¥½çš„æ³›åŒ–ã€‚é€šè¿‡è§‚å¯Ÿè®­ç»ƒå’Œæµ‹è¯•æŸå¤±ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å‘ç°å¤§å­¦ä¹ ç‡ä¸‹çš„è®­ç»ƒè½¨è¿¹èƒ½å¤Ÿæ¥è¿‘æµ‹è¯•æŸå¤±çš„æœ€å°å€¼é™„è¿‘ã€‚åŸºäºè¿™ä¸ªå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸çœŸå®ç¥ç»ç½‘ç»œç±»ä¼¼çš„éçº¿æ€§æ¨¡å‹ï¼Œå¹¶è¯æ˜äº†ä½¿ç”¨å¤§å­¦ä¹ ç‡è¿›è¡Œå»¶é•¿é˜¶æ®µçš„è®­ç»ƒå¯ä»¥å®ç°æ›´æ¥è¿‘æœ€ä¼˜æ³›åŒ–çš„æ•ˆæœã€‚

    

    ä¸ºä»€ä¹ˆç”¨å¤§å­¦ä¹ ç‡é•¿æ—¶é—´è®­ç»ƒçš„ç¥ç»ç½‘ç»œé€šå¸¸èƒ½å¤Ÿå®ç°æ›´å¥½çš„æ³›åŒ–å‘¢ï¼Ÿæœ¬æ–‡é€šè¿‡ç ”ç©¶ç¥ç»ç½‘ç»œä¸­è®­ç»ƒæŸå¤±å’Œæµ‹è¯•æŸå¤±ä¹‹é—´çš„å…³ç³»æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚é€šè¿‡å¯è§†åŒ–è¿™äº›æŸå¤±æƒ…å†µï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨å¤§å­¦ä¹ ç‡ä¸‹çš„è®­ç»ƒè½¨è¿¹ä¼šé€šè¿‡è®­ç»ƒæŸå¤±çš„æå°å€¼æµå½¢ï¼Œæœ€ç»ˆæ¥è¿‘äºæµ‹è¯•æŸå¤±çš„æå°å€¼é™„è¿‘ã€‚åœ¨è¿™ä¸ªå‘ç°çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéçº¿æ€§æ¨¡å‹ï¼Œå…¶æŸå¤±æ™¯è§‚ä¸çœŸå®ç¥ç»ç½‘ç»œè§‚å¯Ÿåˆ°çš„ç›¸ä¼¼ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šä½¿ç”¨SGDç ”ç©¶è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨å¤§å­¦ä¹ ç‡ä¸‹çš„å»¶é•¿é˜¶æ®µå¯ä»¥å°†æˆ‘ä»¬çš„æ¨¡å‹å¼•å¯¼å‘è®­ç»ƒæŸå¤±çš„æœ€å°èŒƒæ•°è§£ï¼Œè¿™å¯èƒ½å®ç°æ¥è¿‘æœ€ä¼˜çš„æ³›åŒ–ï¼Œä»è€Œè¯å®äº†åæœŸå­¦ä¹ ç‡è¡°å‡çš„ç»éªŒä¼˜åŠ¿ã€‚

    Why do neural networks trained with large learning rates for a longer time often lead to better generalization? In this paper, we delve into this question by examining the relation between training and testing loss in neural networks. Through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. Motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. Upon investigating the training process using SGD on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.
    
[^12]: Thompsoné‡‡æ ·ç”¨äºå…·æœ‰å™ªéŸ³ä¸Šä¸‹æ–‡çš„éšæœºèµŒè‡‚é—®é¢˜çš„ä¿¡æ¯è®ºæ€§åæ‚”åˆ†æ

    Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis. (arXiv:2401.11565v1 [cs.LG])

    [http://arxiv.org/abs/2401.11565](http://arxiv.org/abs/2401.11565)

    æœ¬æ–‡ç ”ç©¶äº†å…·æœ‰å™ªéŸ³ä¸Šä¸‹æ–‡çš„éšæœºèµŒè‡‚é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§Thompsoné‡‡æ ·ç®—æ³•ï¼Œé€šè¿‡è´å¶æ–¯æ¡†æ¶è¿›è¡Œåˆ†æï¼Œè¯æ˜äº†ç®—æ³•çš„è´å¶æ–¯åæ‚”ï¼Œå¹¶æ‰©å±•äº†é—®é¢˜åˆ°å»¶è¿Ÿè§‚å¯ŸçœŸå®ä¸Šä¸‹æ–‡çš„æƒ…å†µï¼Œå¹¶å®è¯äº†ç®—æ³•çš„æ€§èƒ½ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§éšæœºä¸Šä¸‹æ–‡çº¿æ€§èµŒè‡‚é—®é¢˜ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡ä¸€ä¸ªæœªçŸ¥å™ªå£°å‚æ•°çš„å™ªå£°ä¿¡é“è§‚å¯Ÿåˆ°çœŸå®ä¸Šä¸‹æ–‡çš„å™ªå£°ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªåŠ¨ä½œç­–ç•¥ï¼Œå¯ä»¥è¿‘ä¼¼äºå…·æœ‰å¥–åŠ±æ¨¡å‹ã€å™ªå£°å‚æ•°å’Œä»è§‚å¯Ÿåˆ°çš„å™ªå£°ä¸Šä¸‹æ–‡ä¸­çœŸå®ä¸Šä¸‹æ–‡çš„é¢„æµ‹åˆ†å¸ƒçš„oracleçš„åŠ¨ä½œç­–ç•¥ã€‚åœ¨è´å¶æ–¯æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é’ˆå¯¹å…·æœ‰é«˜æ–¯ä¸Šä¸‹æ–‡å™ªå£°çš„é«˜æ–¯èµŒè‡‚çš„Thompsoné‡‡æ ·ç®—æ³•ã€‚é‡‡ç”¨ä¿¡æ¯è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•ç›¸å¯¹äºoracleçš„åŠ¨ä½œç­–ç•¥çš„è´å¶æ–¯åæ‚”ã€‚æˆ‘ä»¬è¿˜å°†è¿™ä¸ªé—®é¢˜æ‰©å±•åˆ°äº†ä»£ç†åœ¨æ¥æ”¶åˆ°å¥–åŠ±åå»¶è¿Ÿè§‚å¯Ÿåˆ°çœŸå®ä¸Šä¸‹æ–‡çš„æƒ…å†µï¼Œå¹¶å±•ç¤ºäº†å»¶è¿ŸçœŸå®ä¸Šä¸‹æ–‡å¯¼è‡´æ›´ä½çš„è´å¶æ–¯åæ‚”ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä¸åŸºçº¿ç®—æ³•çš„æ¯”è¾ƒå®è¯åœ°å±•ç¤ºäº†æ‰€æå‡ºç®—æ³•çš„æ€§èƒ½ã€‚

    We explore a stochastic contextual linear bandit problem where the agent observes a noisy, corrupted version of the true context through a noise channel with an unknown noise parameter. Our objective is to design an action policy that can approximate" that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context. In a Bayesian framework, we introduce a Thompson sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting an information-theoretic analysis, we demonstrate the Bayesian regret of our algorithm concerning the oracle's action policy. We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret. Finally, we empirically demonstrate the performance of the proposed algorithms against baselines.
    
[^13]: ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒä»¥å¢å¼ºé€‰æ‹©æ€§

    Enhancing selectivity using Wasserstein distance based reweighing. (arXiv:2401.11562v1 [stat.ML])

    [http://arxiv.org/abs/2401.11562](http://arxiv.org/abs/2401.11562)

    æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒçš„ç®—æ³•ï¼Œåœ¨æ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†ç®—æ³•å¯ä»¥è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œä¸”ç®—æ³•ç®€å•å¯æ‰©å±•ã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºåº”ç”¨å®ä¾‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚

    

    ç»™å®šä¸¤ä¸ªæ ‡è®°æ•°æ®é›†ğ’®å’Œğ’¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„è´ªå©ªç®—æ³•æ¥å¯¹æŸå¤±å‡½æ•°è¿›è¡ŒåŠ æƒï¼Œä½¿å¾—åœ¨ğ’®ä¸Šè®­ç»ƒå¾—åˆ°çš„ç¥ç»ç½‘ç»œæƒé‡çš„æé™åˆ†å¸ƒé€¼è¿‘åœ¨ğ’¯ä¸Šè®­ç»ƒå¾—åˆ°çš„æé™åˆ†å¸ƒã€‚åœ¨ç†è®ºæ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“è¾“å…¥æ•°æ®é›†çš„åº¦é‡ç†µæœ‰ç•Œæ—¶ï¼Œæˆ‘ä»¬çš„è´ªå©ªç®—æ³•è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œå³ç½‘ç»œæƒé‡çš„ä¸¤ä¸ªä¸å˜åˆ†å¸ƒåœ¨æ€»å˜å·®è·ç¦»ä¸Šå¯ä»¥è¯æ˜æ¥è¿‘ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ç®€å•å¯æ‰©å±•ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜è¯æ˜äº†ç®—æ³•çš„æ•ˆç‡ä¸Šç•Œã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»ä»¥è¿›è¡Œï¼ˆè½¯ï¼‰å¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºä¸€ä¸ªåŠ¨æœºåº”ç”¨ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹MNK2ï¼ˆä¸€ç§ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶ï¼‰å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚

    Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1
    
[^14]: MoMA: æ¨¡å‹ä¸ºåŸºç¡€çš„é•œåƒä¸Šå‡ç®—æ³•çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ 

    MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning. (arXiv:2401.11380v1 [cs.LG])

    [http://arxiv.org/abs/2401.11380](http://arxiv.org/abs/2401.11380)

    MoMAæå‡ºäº†ä¸€ç§æ¨¡å‹ä¸ºåŸºç¡€çš„é•œåƒä¸Šå‡ç®—æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ— é™åˆ¶çš„ç­–ç•¥ç±»åˆ«å’Œä¸€èˆ¬å‡½æ•°é€¼è¿‘æ¥å®ç°ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå……åˆ†åˆ©ç”¨äº†æ¨¡å‹ä¸ºåŸºç¡€æ–¹æ³•çš„ä¼˜åŠ¿ã€‚

    

    æ¨¡å‹ä¸ºåŸºç¡€çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®¸å¤šå†³ç­–é—®é¢˜ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¾—ç›Šäºå®ƒä»¬çš„æ ·æœ¬æ•ˆç‡å’Œé€šç”¨æ€§ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†ç°æœ‰çš„æ¨¡å‹ä¸ºåŸºç¡€çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¦ä¹ˆä¾§é‡äºç†è®ºç ”ç©¶è€Œæ²¡æœ‰å¼€å‘å®é™…ç®—æ³•ï¼Œè¦ä¹ˆä¾èµ–äºå—é™çš„å‚æ•°åŒ–ç­–ç•¥ç©ºé—´ï¼Œå› æ­¤æ²¡æœ‰å……åˆ†åˆ©ç”¨æ¨¡å‹ä¸ºåŸºç¡€æ–¹æ³•å›ºæœ‰çš„æ— é™åˆ¶ç­–ç•¥ç©ºé—´çš„ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†MoMAï¼Œä¸€ä¸ªåœ¨ç¦»çº¿æ•°æ®éƒ¨åˆ†è¦†ç›–ä¸‹ä½¿ç”¨ä¸€èˆ¬å‡½æ•°é€¼è¿‘çš„æ¨¡å‹ä¸ºåŸºç¡€çš„é•œåƒä¸Šå‡ç®—æ³•ã€‚MoMAé€šè¿‡é‡‡ç”¨æ— é™åˆ¶çš„ç­–ç•¥ç±»åˆ«ï¼ŒåŒºåˆ«äºç°æœ‰æ–‡çŒ®ã€‚åœ¨æ¯ä¸ªè¿­ä»£ä¸­ï¼ŒMoMAåœ¨ç­–ç•¥è¯„ä¼°æ­¥éª¤ä¸­é€šè¿‡åœ¨è¿‡æ¸¡æ¨¡å‹çš„ç½®ä¿¡åŒºé—´å†…è¿›è¡Œæœ€å°åŒ–è¿‡ç¨‹æ¥ä¿å®ˆåœ°ä¼°è®¡å€¼å‡½æ•°ï¼Œç„¶åä½¿ç”¨ä¸€èˆ¬å‡½æ•°é€¼è¿‘æ¥æ›´æ–°ç­–ç•¥ï¼Œè€Œä¸æ˜¯å¸¸ç”¨çš„æ–¹æ³•ã€‚

    Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-use
    
[^15]: é‡å­æœºå™¨å­¦ä¹ ï¼šä»NISQåˆ°å®¹é”™ã€‚

    Quantum Machine Learning: from NISQ to Fault Tolerance. (arXiv:2401.11351v1 [quant-ph])

    [http://arxiv.org/abs/2401.11351](http://arxiv.org/abs/2401.11351)

    æœ¬æ–‡æä¾›äº†å¯¹é‡å­æœºå™¨å­¦ä¹ é¢†åŸŸçš„å…¨é¢å›é¡¾ï¼Œæ¶µç›–äº†åœ¨NISQæŠ€æœ¯å’Œå®¹é”™é‡å­è®¡ç®—ç¡¬ä»¶ä¸Šä½¿ç”¨çš„æŠ€æœ¯å’Œç®—æ³•ï¼Œå¹¶æ·±å…¥è®¨è®ºäº†ä¸é‡å­æœºå™¨å­¦ä¹ ç›¸å…³çš„åŸºæœ¬æ¦‚å¿µå’Œç»Ÿè®¡å­¦ä¹ ç†è®ºã€‚

    

    é‡å­æœºå™¨å­¦ä¹ æ˜¯åœ¨é‡å­è®¾å¤‡ä¸Šè¿è¡Œæœºå™¨å­¦ä¹ ç®—æ³•çš„è¿‡ç¨‹ï¼Œåœ¨å­¦æœ¯ç•Œå’Œå•†ä¸šç•Œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡å¯¹é‡å­æœºå™¨å­¦ä¹ é¢†åŸŸæ¶Œç°çš„å„ç§æ¦‚å¿µè¿›è¡Œäº†å…¨é¢è€Œå…¬æ­£çš„å›é¡¾ã€‚è¿™åŒ…æ‹¬åœ¨å™ªå£°ä¸­é—´å°ºåº¦é‡å­ï¼ˆNISQï¼‰æŠ€æœ¯ä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼Œä»¥åŠä¸å®¹é”™é‡å­è®¡ç®—ç¡¬ä»¶å…¼å®¹çš„ç®—æ³•æ–¹æ³•ã€‚æˆ‘ä»¬çš„å›é¡¾æ¶µç›–äº†ä¸é‡å­æœºå™¨å­¦ä¹ ç›¸å…³çš„åŸºæœ¬æ¦‚å¿µã€ç®—æ³•å’Œç»Ÿè®¡å­¦ä¹ ç†è®ºã€‚

    Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.
    
[^16]: é€šè¿‡ï¼ˆæ­£äº¤ï¼‰å®Œå…¨æ— åçš„æˆªå°¾å­¦ä¹ æ¥ä¼°è®¡ç”Ÿå­˜ç»“æœçš„å¼‚è´¨æ²»ç–—æ•ˆåº”

    Estimating heterogeneous treatment effect from survival outcomes via (orthogonal) censoring unbiased learning. (arXiv:2401.11263v1 [stat.ME])

    [http://arxiv.org/abs/2401.11263](http://arxiv.org/abs/2401.11263)

    è¯¥è®ºæ–‡å¼€å‘äº†ä¸€ç§é€‚ç”¨äºå…·æœ‰å’Œæ²¡æœ‰ç«äº‰é£é™©çš„ç”Ÿå­˜ç»“æœçš„æˆªå°¾æ— åå˜æ¢æ–¹æ³•ï¼Œå¯ä»¥ä¼°è®¡å¼‚è´¨æ²»ç–—æ•ˆåº”ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºæ›´å¤šæœ€å…ˆè¿›çš„é€‚ç”¨äºè¢«æˆªå°¾ç»“æœçš„HTEå­¦ä¹ æ–¹æ³•ï¼Œå¹¶æä¾›äº†é™åˆ¶æœ‰é™æ ·æœ¬è¿‡åº¦é£é™©çš„æ–¹æ³•ã€‚

    

    ä»è§‚å¯Ÿæ•°æ®ä¸­ä¼°è®¡å¼‚è´¨æ²»ç–—æ•ˆåº”ï¼ˆHTEï¼‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¿ç»­æˆ–äºŒå…ƒç»“æœä¸Šï¼Œè¾ƒå°‘å…³æ³¨ç”Ÿå­˜ç»“æœï¼Œå‡ ä¹æ²¡æœ‰å…³æ³¨ç«äº‰é£é™©æƒ…æ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†é€‚ç”¨äºå…·æœ‰å’Œæ²¡æœ‰ç«äº‰é£é™©çš„ç”Ÿå­˜ç»“æœçš„æˆªå°¾æ— åå˜æ¢ï¼ˆCUTsï¼‰ã€‚ä½¿ç”¨è¿™äº›CUTså°†æ—¶é—´åˆ°äº‹ä»¶ç»“æœè½¬æ¢åï¼Œå¯¹è¿ç»­ç»“æœçš„HTEå­¦ä¹ æ–¹æ³•çš„ç›´æ¥åº”ç”¨å¯ä»¥äº§ç”Ÿä¸€è‡´ä¼°è®¡çš„å¼‚è´¨ç´¯ç§¯å‘ç”Ÿç‡æ•ˆåº”ã€æ€»æ•ˆåº”å’Œå¯åˆ†ç¦»ç›´æ¥æ•ˆåº”ã€‚æˆ‘ä»¬çš„CUTså¯ä»¥ä½¿ç”¨æ¯”ä»¥å‰æ›´å¤šçš„æœ€å…ˆè¿›çš„é€‚ç”¨äºè¢«æˆªå°¾ç»“æœçš„HTEå­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç«äº‰é£é™©æƒ…æ™¯ä¸‹ã€‚æˆ‘ä»¬æä¾›äº†é€šç”¨çš„æ— æ¨¡å‹å­¦ä¹ ç‰¹å®šoracleä¸ç­‰å¼æ¥é™åˆ¶æœ‰é™æ ·æœ¬çš„è¿‡åº¦é£é™©ã€‚oracleæ•ˆç‡ç»“æœå–å†³äºä¸€ä¸ªoracleé€‰æ‹©å™¨å’Œä»æ‰€æœ‰æ­¥éª¤ä¸­ä¼°è®¡çš„å¹²æ‰°å‡½æ•°ã€‚

    Methods for estimating heterogeneous treatment effects (HTE) from observational data have largely focused on continuous or binary outcomes, with less attention paid to survival outcomes and almost none to settings with competing risks. In this work, we develop censoring unbiased transformations (CUTs) for survival outcomes both with and without competing risks.After converting time-to-event outcomes using these CUTs, direct application of HTE learners for continuous outcomes yields consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. Our CUTs enable application of a much larger set of state of the art HTE learners for censored outcomes than had previously been available, especially in competing risks settings. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps invol
    
[^17]: AFS-BM:é€šè¿‡è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©å’ŒäºŒå€¼å±è”½å¢å¼ºæ¨¡å‹æ€§èƒ½

    AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking. (arXiv:2401.11250v1 [cs.LG])

    [http://arxiv.org/abs/2401.11250](http://arxiv.org/abs/2401.11250)

    AFS-BMé€šè¿‡è”åˆä¼˜åŒ–å®ç°äº†è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹å‡†ç¡®æ€§å¹¶å‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ç‰¹å¾é€‰æ‹©çš„é—®é¢˜ï¼Œè¿™æ˜¯è¯¥é¢†åŸŸä¸­æœ€å…³é”®çš„ä¸»é¢˜ä¹‹ä¸€ã€‚å°½ç®¡å­˜åœ¨è®¸å¤šç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•é¢ä¸´å¯æ‰©å±•æ€§ã€å¤„ç†é«˜ç»´æ•°æ®ã€å¤„ç†ç›¸å…³ç‰¹å¾ã€é€‚åº”å¯å˜ç‰¹å¾é‡è¦æ€§å’Œæ•´åˆé¢†åŸŸçŸ¥è¯†ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©å’ŒäºŒå€¼å±è”½â€(AFS-BM)ã€‚AFS-BMé€šè¿‡è”åˆä¼˜åŒ–æ¥åŒæ—¶è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹è®­ç»ƒã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡è”åˆä¼˜åŒ–å’ŒäºŒå€¼å±è”½ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŒç»­è°ƒæ•´ç‰¹å¾é›†å’Œæ¨¡å‹å‚æ•°ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—çš„å®éªŒè¯æ˜ï¼Œå°†AFS-BMä¸å·²æœ‰çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚

    We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods usin
    
[^18]: é€šè¿‡å·¥å…·å˜é‡æ³•è¯†åˆ«å’Œä¼°è®¡æ¡ä»¶å¹³å‡åå› æœæ•ˆåº”

    Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])

    [http://arxiv.org/abs/2401.11130](http://arxiv.org/abs/2401.11130)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å·¥å…·å˜é‡æ³•è¯†åˆ«å’Œä¼°è®¡è¿ç»­å¤„ç†çš„å› æœæ•ˆåº”å¼‚è´¨æ€§çš„æ–¹æ³•ï¼Œå¹¶å¼€å‘äº†ä¸‰ç±»ç›¸åº”çš„ä¼°è®¡å™¨ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†ç»Ÿè®¡æ€§è´¨åˆ†æã€‚

    

    è¿‘å¹´æ¥ï¼Œå¯¹äºä¼°è®¡å¼‚è´¨å› æœæ•ˆåº”çš„å…´è¶£æ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡å¼•å…¥äº†æ¡ä»¶å¹³å‡åå› æœæ•ˆåº”ï¼ˆCAPCEï¼‰ï¼Œä»¥æ­ç¤ºè¿ç»­å¤„ç†çš„å› æœæ•ˆåº”çš„å¼‚è´¨æ€§ã€‚æˆ‘ä»¬ç»™å‡ºäº†åœ¨å·¥å…·å˜é‡è®¾ç½®ä¸‹è¯†åˆ«CAPCEçš„æ¡ä»¶ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸‰ç±»CAPCEä¼°è®¡å™¨ï¼šç­›é€‰ã€å‚æ•°åŒ–å’Œå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰-åŸºç¡€ï¼Œåˆ†æäº†å®ƒä»¬çš„ç»Ÿè®¡æ€§è´¨ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆå’ŒçœŸå®æ•°æ®å¯¹æå‡ºçš„CAPCEä¼°è®¡å™¨è¿›è¡Œäº†è¯´æ˜ã€‚

    There has been considerable recent interest in estimating heterogeneous causal effects. In this paper, we introduce conditional average partial causal effects (CAPCE) to reveal the heterogeneity of causal effects with continuous treatment. We provide conditions for identifying CAPCE in an instrumental variable setting. We develop three families of CAPCE estimators: sieve, parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze their statistical properties. We illustrate the proposed CAPCE estimators on synthetic and real-world data.
    
[^19]: åŠ æƒæœ€è¿‘é‚»ç®—æ³•çš„é«˜æ•ˆæ•°æ®Shapleyå€¼è®¡ç®—æ–¹æ³•

    Efficient Data Shapley for Weighted Nearest Neighbor Algorithms. (arXiv:2401.11103v1 [cs.DS])

    [http://arxiv.org/abs/2401.11103](http://arxiv.org/abs/2401.11103)

    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è§£å†³åŠ æƒKæœ€è¿‘é‚»ç®—æ³•é«˜æ•ˆè®¡ç®—Data Shapleyå€¼çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶åœ¨æ•°æ®è´¨é‡åˆ¤åˆ«æ–¹é¢ä¼˜äºæœªåŠ æƒç‰ˆæœ¬ã€‚

    

    æœ¬æ–‡æ—¨åœ¨è§£å†³æ•°æ®è¯„ä¼°æ–‡çŒ®ä¸­å…³äºåŠ æƒKæœ€è¿‘é‚»ç®—æ³•çš„æ•°æ®Shapleyå€¼ï¼ˆWKNN-Shapleyï¼‰é«˜æ•ˆè®¡ç®—çš„é—®é¢˜ã€‚é€šè¿‡å°†ç¡¬æ ‡ç­¾KNNçš„å‡†ç¡®åº¦ä¸ç¦»æ•£æƒé‡è§†ä¸ºæ•ˆç”¨å‡½æ•°ï¼Œæˆ‘ä»¬å°†WKNN-Shapleyå€¼çš„è®¡ç®—è½¬åŒ–ä¸ºä¸€ä¸ªè®¡æ•°é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªäºŒæ¬¡æ—¶é—´å¤æ‚åº¦çš„ç®—æ³•ï¼Œä»è€Œå®ç°äº†å¯¹ç°æœ‰æ–‡çŒ®ä¸­O(N^K)çš„æœ€ä½³ç»“æœçš„æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç¡®å®šæ€§è¿‘ä¼¼ç®—æ³•ï¼Œè¿›ä¸€æ­¥æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†Shapleyå€¼çš„å…³é”®å…¬å¹³æ€§è´¨ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†WKNN-Shapleyå€¼çš„è®¡ç®—æ•ˆç‡ä»¥åŠä¸æœªåŠ æƒç‰ˆæœ¬ç›¸æ¯”åœ¨åˆ¤æ–­æ•°æ®è´¨é‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚

    This work aims to address an open problem in data valuation literature concerning the efficient computation of Data Shapley for weighted $K$ nearest neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label KNN with discretized weights as the utility function, we reframe the computation of WKNN-Shapley into a counting problem and introduce a quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the best result from existing literature. We develop a deterministic approximation algorithm that further improves computational efficiency while maintaining the key fairness properties of the Shapley value. Through extensive experiments, we demonstrate WKNN-Shapley's computational efficiency and its superior performance in discerning data quality compared to its unweighted counterpart.
    
[^20]: ä»èšåˆå“åº”ä¸­å­¦ä¹ ï¼šå®ä¾‹çº§åˆ«ä¸åŒ…çº§åˆ«çš„æŸå¤±å‡½æ•°æ¯”è¾ƒ

    Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions. (arXiv:2401.11081v1 [cs.LG])

    [http://arxiv.org/abs/2401.11081](http://arxiv.org/abs/2401.11081)

    æœ¬æ–‡ç ”ç©¶äº†ä»èšåˆå“åº”ä¸­å­¦ä¹ çš„ä¸¤ç§æŸå¤±å‡½æ•°ï¼šåŒ…çº§åˆ«æŸå¤±å’Œå®ä¾‹çº§åˆ«æŸå¤±ï¼Œå¹¶å‘ç°å®ä¾‹çº§åˆ«æŸå¤±å¯ä»¥è¢«è§†ä¸ºåŒ…çº§åˆ«æŸå¤±çš„æ­£åˆ™åŒ–å½¢å¼ã€‚

    

    ç”±äºéšç§é—®é¢˜çš„å¢åŠ ï¼Œåœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œè®­ç»ƒæ•°æ®åœ¨ä¸å­¦ä¹ è€…å…±äº«ä¹‹å‰ä¼šè¢«èšåˆèµ·æ¥ï¼Œä»¥ä¿æŠ¤ç”¨æˆ·æ•æ„Ÿå“åº”çš„éšç§ã€‚åœ¨èšåˆå­¦ä¹ æ¡†æ¶ä¸­ï¼Œæ•°æ®é›†è¢«åˆ†ç»„æˆæ ·æœ¬çš„åŒ…ï¼Œæ¯ä¸ªåŒ…åªæä¾›ä¸€ä¸ªèšåˆå“åº”ï¼Œæä¾›äº†è¯¥åŒ…ä¸­ä¸ªä½“å“åº”çš„æ‘˜è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†ä»èšåˆå“åº”ä¸­å­¦ä¹ çš„ä¸¤ç§è‡ªç„¶æŸå¤±å‡½æ•°ï¼šåŒ…çº§åˆ«æŸå¤±å’Œå®ä¾‹çº§åˆ«æŸå¤±ã€‚åœ¨å‰è€…ä¸­ï¼Œæ¨¡å‹é€šè¿‡æœ€å°åŒ–èšåˆå“åº”ä¸èšåˆæ¨¡å‹é¢„æµ‹ä¹‹é—´çš„æŸå¤±æ¥å­¦ä¹ ï¼Œè€Œåœ¨åè€…ä¸­ï¼Œæ¨¡å‹æ—¨åœ¨å°†ä¸ªä½“é¢„æµ‹ä¸èšåˆå“åº”æ‹Ÿåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜å®ä¾‹çº§åˆ«æŸå¤±å¯ä»¥è¢«è§†ä¸ºåŒ…çº§åˆ«æŸå¤±çš„æ­£åˆ™åŒ–å½¢å¼ã€‚è¿™ä¸ªè§‚å¯Ÿè®©æˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒè¿™ä¸¤ç§æ–¹æ³•å…³äºæ‰€å¾—ä¼°è®¡å€¼çš„åå·®å’Œæ–¹å·®ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ’å€¼ã€‚

    Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpol
    
[^21]: å…·æœ‰ç»“æ„åŒ–å˜åˆ†æ—çš„å¯è¯ä¼¸ç¼©æ€§é»‘ç›’å˜åˆ†æ¨æ–­

    Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])

    [http://arxiv.org/abs/2401.10989](http://arxiv.org/abs/2401.10989)

    æœ¬æ–‡ç ”ç©¶äº†å‡å€¼åœºå˜åˆ†æ—å’Œæ»¡ç§©å˜åˆ†æ—ä¹‹é—´çš„ç†è®ºä¸­é—´åœ°å¸¦ï¼šç»“æ„åŒ–å˜åˆ†æ—ï¼Œå¹¶é€šè¿‡ç†è®ºè¯æ˜ç»“æ„åŒ–å˜åˆ†æ—å¯ä»¥åœ¨è¿­ä»£å¤æ‚æ€§ä¸Šè¡¨ç°æ›´å¥½ï¼Œç¼©æ”¾æ•ˆæœæ›´å¥½ã€‚

    

    å·²çŸ¥å…·æœ‰æ»¡ç§©åæ–¹å·®é€¼è¿‘çš„å˜åˆ†æ—åœ¨é»‘ç›’å˜åˆ†æ¨æ–­ä¸­è¡¨ç°ä¸ä½³ï¼Œæ— è®ºæ˜¯ä»å®è¯ä¸Šè¿˜æ˜¯ç†è®ºä¸Šã€‚äº‹å®ä¸Šï¼Œæœ€è¿‘å¯¹é»‘ç›’å˜åˆ†æ¨æ–­çš„è®¡ç®—å¤æ‚æ€§ç»“æœè¡¨æ˜ï¼Œä¸å‡å€¼åœºå˜åˆ†æ—ç›¸æ¯”ï¼Œæ»¡ç§©å˜åˆ†æ—åœ¨é—®é¢˜çš„ç»´åº¦ä¸Šæ‰©å±•å¾—å¾ˆå·®ã€‚è¿™å¯¹å…·æœ‰æœ¬åœ°å˜é‡çš„åˆ†å±‚è´å¶æ–¯æ¨¡å‹å°¤ä¸ºå…³é”®ï¼Œå®ƒä»¬çš„ç»´åº¦éšç€æ•°æ®é›†çš„å¤§å°è€Œå¢åŠ ã€‚å› æ­¤ï¼Œè¿­ä»£å¤æ‚æ€§å¯¹æ•°æ®é›†å¤§å°Nå­˜åœ¨æ˜ç¡®çš„O(N^2)ä¾èµ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å€¼åœºå˜åˆ†æ—å’Œæ»¡ç§©å˜åˆ†æ—ä¹‹é—´çš„ç†è®ºä¸­é—´åœ°å¸¦ï¼šç»“æ„åŒ–å˜åˆ†æ—ã€‚æˆ‘ä»¬ä¸¥æ ¼è¯æ˜äº†æŸäº›å°ºåº¦çŸ©é˜µç»“æ„å¯ä»¥å®ç°æ›´å¥½çš„è¿­ä»£å¤æ‚æ€§O(N)ï¼Œä»è€Œä¸Nçš„ç¼©æ”¾æ›´å¥½åœ°åŒ¹é…ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸­éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœ

    Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
    
[^22]: ä½¿ç”¨åŠæ­£å®šè§„åˆ’çš„å»åå’Œå±€éƒ¨åˆ†æè¿›è¡Œäººç¾¤èšç±»

    Debiasing and a local analysis for population clustering using semidefinite programming. (arXiv:2401.10927v1 [stat.ML])

    [http://arxiv.org/abs/2401.10927](http://arxiv.org/abs/2401.10927)

    æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨åŠæ­£å®šè§„åˆ’è¿›è¡Œäººç¾¤èšç±»çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†è®¡ç®—é«˜æ•ˆçš„ç®—æ³•ã€‚è¿™äº›ç®—æ³•å¯ä»¥æ ¹æ®å°æ ·æœ¬æ•°æ®çš„åŸå§‹ç§ç¾¤å°†æ•°æ®åˆ†ä¸ºä¸¤ç»„ï¼Œé€‚ç”¨äºç§ç¾¤ä¹‹é—´å·®å¼‚è¾ƒå°çš„æƒ…å†µã€‚

    

    æœ¬æ–‡è€ƒè™‘äº†ä»æ··åˆçš„2ä¸ªæ¬¡é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–çš„å°æ•°æ®æ ·æœ¬çš„åˆ†åŒºé—®é¢˜ã€‚æˆ‘ä»¬åˆ†æäº†åŒä¸€ä½œè€…æå‡ºçš„è®¡ç®—é«˜æ•ˆçš„ç®—æ³•ï¼Œå°†æ•°æ®æ ¹æ®å…¶åŸå§‹ç§ç¾¤å¤§è‡´åˆ†ä¸ºä¸¤ç»„ï¼Œç»™å®šä¸€ä¸ªå°æ ·æœ¬ã€‚æœ¬æ–‡çš„ç ”ç©¶åŠ¨æœºæ˜¯å°†ä¸ªä½“æ ¹æ®å…¶åŸå§‹ç§ç¾¤ä½¿ç”¨pä¸ªæ ‡è®°è¿›è¡Œèšç±»ï¼Œå½“ä»»æ„ä¸¤ä¸ªç§ç¾¤ä¹‹é—´çš„å·®å¼‚å¾ˆå°æ—¶ã€‚æˆ‘ä»¬åŸºäºæ•´æ•°äºŒæ¬¡è§„åˆ’çš„åŠæ­£å®šæ¾å¼›å½¢å¼æ„å»ºï¼Œè¯¥è§„åˆ’é—®é¢˜æœ¬è´¨ä¸Šæ˜¯åœ¨ä¸€ä¸ªå›¾ä¸Šæ‰¾åˆ°æœ€å¤§å‰²ï¼Œå…¶ä¸­å‰²ä¸­çš„è¾¹æƒé‡è¡¨ç¤ºåŸºäºå®ƒä»¬çš„pä¸ªç‰¹å¾çš„ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ä¸ç›¸ä¼¼åº¦å¾—åˆ†ã€‚æˆ‘ä»¬ç”¨Î”^2:=pÎ³æ¥è¡¨ç¤ºä¸¤ä¸ªä¸­å¿ƒï¼ˆå‡å€¼å‘é‡ï¼‰ä¹‹é—´çš„â„“_2^2è·ç¦»ï¼Œå³Î¼^(1), Î¼^(2)âˆˆâ„^pã€‚ç›®æ ‡æ˜¯åœ¨äº¤æ¢ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´æä¾›å…¨é¢çš„æƒè¡¡ã€‚

    In this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular, we analyze computational efficient algorithms proposed by the same author, to partition data into two groups approximately according to their population of origin given a small sample. This work is motivated by the application of clustering individuals according to their population of origin using $p$ markers, when the divergence between any two of the populations is small. We build upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$ distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$ $\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
    
[^23]: åœ¨å…·æœ‰é€šç”¨éšæœºç‰›é¡¿ç®—æ³•åº”ç”¨äºéšæœºä¼˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå…³äºHessiançŸ©é˜µçš„é€†çš„åœ¨çº¿ä¼°è®¡

    Online estimation of the inverse of the Hessian for stochastic optimization with application to universal stochastic Newton algorithms. (arXiv:2401.10923v1 [math.OC])

    [http://arxiv.org/abs/2401.10923](http://arxiv.org/abs/2401.10923)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿ä¼°è®¡HessiançŸ©é˜µé€†çš„æ–¹æ³•ï¼Œåˆ©ç”¨Robbins-Monroè¿‡ç¨‹ï¼Œèƒ½å¤Ÿ drastical reducescomputational complexity,å¹¶å‘å±•äº†é€šç”¨çš„éšæœºç‰›é¡¿ç®—æ³•ï¼Œç ”ç©¶äº†æ‰€ææ–¹æ³•çš„æ¸è¿›æ•ˆç‡ã€‚

    

    æœ¬æ–‡é’ˆå¯¹ä»¥æœŸæœ›å½¢å¼è¡¨ç¤ºçš„å‡¸å‡½æ•°çš„æ¬¡ä¼˜éšæœºä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç›´æ¥é€’å½’ä¼°è®¡é€†HessiançŸ©é˜µçš„æŠ€æœ¯ï¼Œé‡‡ç”¨äº†Robbins-Monroè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤§å¹…é™ä½è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶ä¸”å…è®¸å¼€å‘é€šç”¨çš„éšæœºç‰›é¡¿æ–¹æ³•ï¼Œå¹¶ç ”ç©¶æ‰€ææ–¹æ³•çš„æ¸è¿‘æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œæ‰©å±•äº†åœ¨éšæœºä¼˜åŒ–ä¸­äºŒé˜¶ç®—æ³•çš„åº”ç”¨èŒƒå›´ã€‚

    This paper addresses second-order stochastic optimization for estimating the minimizer of a convex function written as an expectation. A direct recursive estimation technique for the inverse Hessian matrix using a Robbins-Monro procedure is introduced. This approach enables to drastically reduces computational complexity. Above all, it allows to develop universal stochastic Newton methods and investigate the asymptotic efficiency of the proposed approach. This work so expands the application scope of secondorder algorithms in stochastic optimization.
    
[^24]: ä¸¤å±‚ç½‘ç»œè®­ç»ƒä¸­çš„æ—©æœŸå¯¹é½æ˜¯ä¸€æŠŠåŒåˆƒå‰‘

    Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])

    [http://arxiv.org/abs/2401.10791](http://arxiv.org/abs/2401.10791)

    æœ¬æ–‡ç ”ç©¶äº†ä¸¤å±‚ç½‘ç»œè®­ç»ƒä¸­çš„æ—©æœŸå¯¹é½ç°è±¡ï¼Œå‘ç°åœ¨å°åˆå§‹åŒ–å’Œä¸€ä¸ªéšè—çš„ReLUå±‚ç½‘ç»œä¸­ï¼Œç¥ç»å…ƒä¼šåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µå‘å…³é”®æ–¹å‘è¿›è¡Œå¯¹é½ï¼Œå¯¼è‡´ç½‘ç»œç¨€ç–è¡¨ç¤ºä»¥åŠæ¢¯åº¦æµåœ¨æ”¶æ•›æ—¶çš„éšå«åå¥½ã€‚ç„¶è€Œï¼Œè¿™ç§ç¨€ç–è¯±å¯¼çš„å¯¹é½ä¹Ÿä½¿å¾—è®­ç»ƒç›®æ ‡çš„æœ€å°åŒ–å˜å¾—å›°éš¾ã€‚

    

    ä½¿ç”¨ä¸€é˜¶ä¼˜åŒ–æ–¹æ³•è®­ç»ƒç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ æˆåŠŸçš„æ ¸å¿ƒã€‚åˆå§‹åŒ–çš„è§„æ¨¡æ˜¯ä¸€ä¸ªå…³é”®å› ç´ ï¼Œå› ä¸ºå°çš„åˆå§‹åŒ–é€šå¸¸ä¸ç‰¹å¾å­¦ä¹ æ¨¡å¼ç›¸å…³ï¼Œåœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæ¢¯åº¦ä¸‹é™å¯¹ç®€å•è§£éšå«åå¥½ã€‚æœ¬æ–‡æä¾›äº†æ—©æœŸå¯¹é½é˜¶æ®µçš„æ™®éå’Œé‡åŒ–æè¿°ï¼Œæœ€åˆç”±Maennelç­‰äººæå‡ºã€‚å¯¹äºå°åˆå§‹åŒ–å’Œä¸€ä¸ªéšè—çš„ReLUå±‚ç½‘ç»œï¼Œè®­ç»ƒåŠ¨æ€çš„æ—©æœŸé˜¶æ®µå¯¼è‡´ç¥ç»å…ƒå‘å…³é”®æ–¹å‘è¿›è¡Œå¯¹é½ã€‚è¿™ç§å¯¹é½å¼•å‘äº†ç½‘ç»œçš„ç¨€ç–è¡¨ç¤ºï¼Œè¿™ä¸æ¢¯åº¦æµåœ¨æ”¶æ•›æ—¶çš„éšå«åå¥½ç›´æ¥ç›¸å…³ã€‚ç„¶è€Œï¼Œè¿™ç§ç¨€ç–è¯±å¯¼çš„å¯¹é½æ˜¯ä»¥åœ¨æœ€å°åŒ–è®­ç»ƒç›®æ ‡æ–¹é¢é‡åˆ°å›°éš¾ä¸ºä»£ä»·çš„ï¼šæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç®€å•çš„æ•°æ®ç¤ºä¾‹ï¼Œå…¶ä¸­è¶…å‚æ•°ç½‘ç»œæ— æ³•æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼ã€‚

    Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
    
[^25]: Intrinsic Dataset Propertieså¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“ï¼šæ­ç¤ºè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„å­¦ä¹ å·®å¼‚

    The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])

    [http://arxiv.org/abs/2401.08865](http://arxiv.org/abs/2401.08865)

    æœ¬æ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒé¢†åŸŸå­¦ä¹ æ—¶çš„å·®å¼‚ï¼Œæå‡ºäº†ä¸€ä¸ªä¸è®­ç»ƒé›†ç»´åº¦æœ‰å…³çš„æ³›åŒ–ç¼©æ”¾å®šå¾‹ï¼Œå¹¶è®¤ä¸ºåŒ»å­¦å›¾åƒæ•°æ®é›†æ›´é«˜çš„å›ºæœ‰â€œæ ‡ç­¾é”åº¦â€å¯èƒ½æ˜¯ä¸¤ä¸ªé¢†åŸŸä¹‹é—´æ˜¾è‘—å·®å¼‚çš„éƒ¨åˆ†åŸå› ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œåœ¨ä¸åŒå›¾åƒé¢†åŸŸå­¦ä¹ æ—¶çš„å·®å¼‚ï¼Œè¿™åœ¨ä»è‡ªç„¶å›¾åƒåˆ°å…¶ä»–ä¸“é—¨é¢†åŸŸï¼ˆå¦‚åŒ»å­¦å›¾åƒï¼‰é‡‡ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯æ—¶é€šå¸¸è¢«å¿½è§†ã€‚æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒé›†çš„å›ºæœ‰ç»´åº¦($d_{data}$)ä¸ç½‘ç»œçš„æ³›åŒ–é”™è¯¯ä¸€èˆ¬ä¼šå¢åŠ ã€‚ç„¶è€Œï¼ŒåŒ»å­¦ï¼ˆæ”¾å°„å­¦ï¼‰å’Œè‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„è¿™ç§å…³ç³»çš„é™¡å³­ç¨‹åº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”æ— ç°æœ‰çš„ç†è®ºè§£é‡Šã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹å¹¶ç»éªŒè¯ä¸€ä¸ªä¸$d_{data}$ç›¸å…³çš„æ³›åŒ–ç¼©æ”¾å®šå¾‹æ¥è§£å†³è¿™ä¸ªçŸ¥è¯†ç©ºç™½ï¼Œå¹¶æå‡ºè€ƒè™‘åˆ°åŒ»å­¦å›¾åƒæ•°æ®é›†æ›´é«˜çš„å›ºæœ‰â€œæ ‡ç­¾é”åº¦â€($K_F$)è¿™ä¸€åº¦é‡æŒ‡æ ‡å¯ä»¥éƒ¨åˆ†è§£é‡Šè¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„æ˜¾è‘—ç¼©æ”¾å·®å¼‚ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆ©ç”¨æµ‹é‡è¿™ä¸€æŒ‡æ ‡å¯ä»¥æä¾›çš„é¢å¤–å¥½å¤„ã€‚

    This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
    
[^26]: å¸¦æœ‰å˜ç‚¹çš„ç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹ï¼šä¸€ç§ç”Ÿæˆå¯¹æŠ—æ–¹æ³•

    Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach. (arXiv:2312.13152v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13152](http://arxiv.org/abs/2312.13152)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹æ¥å»ºæ¨¡æ—¶é—´åºåˆ—çš„å˜ç‚¹æ£€æµ‹ç®—æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå­¦ä¹ æ¯ä¸ªå˜ç‚¹å¯¹åº”çš„ç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹çš„å‚æ•°ï¼Œå¹¶é€šè¿‡GANåˆ¤åˆ«å™¨çš„è¾“å‡ºæ£€æµ‹å˜ç‚¹ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚

    

    éšæœºå¾®åˆ†æ–¹ç¨‹å¹¿æ³›ç”¨äºå»ºæ¨¡çœŸå®ä¸–ç•Œçš„éšæœºç°è±¡ã€‚ç°æœ‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ—¶é—´åºåˆ—ç”±å•ä¸ªéšæœºå¾®åˆ†æ–¹ç¨‹å»ºæ¨¡çš„æƒ…å†µï¼Œè¿™å¯¹äºå…·æœ‰åˆ†å¸ƒåç§»çš„æ—¶é—´åºåˆ—å»ºæ¨¡å¯èƒ½æ˜¯å±€é™çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æ—¶é—´åºåˆ—å»ºæ¨¡ä¸ºç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹çš„å˜ç‚¹æ£€æµ‹ç®—æ³•ã€‚ç»™å®šä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è”åˆå­¦ä¹ æœªçŸ¥çš„å˜ç‚¹å’Œä¸æ¯ä¸ªå˜ç‚¹å¯¹åº”çš„ä¸åŒç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹æ¨¡å‹çš„å‚æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œç¥ç»éšæœºå¾®åˆ†æ–¹ç¨‹åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¡†æ¶ä¸‹è¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä¸”å˜ç‚¹æ˜¯æ ¹æ®GANåˆ¤åˆ«å™¨çš„è¾“å‡ºåœ¨å‰å‘ä¼ é€’ä¸­è¢«æ£€æµ‹åˆ°ã€‚åœ¨æ‰€æå‡ºçš„ç®—æ³•çš„æ¯ä¸€æ­¥ä¸­ï¼Œå˜ç‚¹å’ŒSDEæ¨¡å‹å‚æ•°ä»¥äº¤æ›¿çš„æ–¹å¼è¿›è¡Œæ›´æ–°ã€‚é€šè¿‡å¯¹åˆæˆå’ŒçœŸå®æ•°æ®é›†çš„æ•°å€¼ç»“æœè¿›è¡ŒéªŒè¯ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„ç®—æ³•ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½æ¯”è¾ƒã€‚

    Stochastic differential equations (SDEs) have been widely used to model real world random phenomena. Existing works mainly focus on the case where the time series is modeled by a single SDE, which might be restrictive for modeling time series with distributional shift. In this work, we propose a change point detection algorithm for time series modeled as neural SDEs. Given a time series dataset, the proposed method jointly learns the unknown change points and the parameters of distinct neural SDE models corresponding to each change point. Specifically, the SDEs are learned under the framework of generative adversarial networks (GANs) and the change points are detected based on the output of the GAN discriminator in a forward pass. At each step of the proposed algorithm, the change points and the SDE model parameters are updated in an alternating fashion. Numerical results on both synthetic and real datasets are provided to validate the performance of our algorithm in comparison to clas
    
[^27]: è¿½æ±‚æœ€ä¼˜ç»Ÿè®¡æ°´å°æŠ€æœ¯

    Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07930](http://arxiv.org/abs/2312.07930)

    è¿½æ±‚æœ€ä¼˜ç»Ÿè®¡æ°´å°æŠ€æœ¯ã€‚é€šè¿‡å°†ç»Ÿè®¡æ°´å°æŠ€æœ¯è§†ä¸ºå‡è®¾æ£€éªŒé—®é¢˜å¹¶å¼•å…¥ä¼ªéšæœºç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬å®ç°äº†è¾“å‡ºä»¤ç‰Œå’Œæ‹’ç»åŒºåŸŸçš„è€¦åˆï¼Œå®ç°äº†ç¬¬ä¸€ç±»é”™è¯¯å’Œç¬¬äºŒç±»é”™è¯¯ä¹‹é—´çš„éå¹³å‡¡æƒè¡¡ï¼ŒåŒæ—¶æå‡ºäº†æœ€ç»Ÿä¸€æœ€æœ‰åŠ›çš„æ°´å°å’Œæœ€å°åŒ–ç¬¬äºŒç±»é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç‹¬ç«‹åŒåˆ†å¸ƒä»¤ç‰Œæ•°é‡çš„ä¸Šä¸‹ç•Œï¼Œçªæ˜¾äº†æ”¹è¿›çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†é²æ£’æ€§æ°´å°é—®é¢˜ã€‚

    

    æˆ‘ä»¬å°†ç»Ÿè®¡æ°´å°æŠ€æœ¯ä½œä¸ºä¸€ä¸ªå‡è®¾æ£€éªŒé—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ³›åŒ–äº†æ‰€æœ‰ä¹‹å‰ç»Ÿè®¡æ°´å°æ–¹æ³•çš„é€šç”¨æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®æ˜¯é€šè¿‡å®è·µä¸­çš„ä¼ªéšæœºç”Ÿæˆå™¨å®ç°è¾“å‡ºä»¤ç‰Œå’Œæ‹’ç»åŒºåŸŸçš„è€¦åˆï¼Œä»è€Œå…è®¸åœ¨ç¬¬ä¸€ç±»é”™è¯¯å’Œç¬¬äºŒç±»é”™è¯¯ä¹‹é—´è¿›è¡Œéå¹³å‡¡çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨ä¸€èˆ¬çš„å‡è®¾æ£€éªŒç¯å¢ƒä¸‹è¡¨å¾äº†æœ€ç»Ÿä¸€æœ€æœ‰åŠ›çš„æ°´å°ä»¥åŠåœ¨æ¨¡å‹æ— å…³çš„ç¯å¢ƒä¸­æœ€å°åŒ–ç¬¬äºŒç±»é”™è¯¯ã€‚åœ¨è¾“å‡ºæ˜¯$n$ä¸ªä»¤ç‰Œçš„å¸¸è§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯¹éœ€è¦ä¿è¯å°çš„ç¬¬ä¸€ç±»å’Œç¬¬äºŒç±»é”™è¯¯çš„ç‹¬ç«‹åŒåˆ†å¸ƒä»¤ç‰Œæ•°é‡å»ºç«‹äº†è¿‘ä¹åŒ¹é…çš„ä¸Šä¸‹ç•Œã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸­çš„$ h ^ {-2} $é€Ÿç‡ç›¸æ¯”ï¼Œæˆ‘ä»¬ç›¸å¯¹äºæ¯ä¸ªä»¤ç‰Œçš„å¹³å‡ç†µ$h$çš„é€Ÿç‡ä¸º$ \Theta(h ^ {-1} \log (1/h)) $ï¼Œçªæ˜¾äº†æ”¹è¿›çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é²æ£’æ€§æ°´å°é—®é¢˜ï¼Œå…¶ä¸­ç”¨æˆ·éƒ½æ˜¯...

    We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
    
[^28]: æœ€ä¼˜åŒ–å¤šåˆ†å¸ƒå­¦ä¹ 

    Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05134](http://arxiv.org/abs/2312.05134)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æœ€ä¼˜åŒ–å¤šåˆ†å¸ƒå­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é‡‡æ ·æ¥å®ç°æ•°æ®é«˜æ•ˆçš„å­¦ä¹ ã€‚é’ˆå¯¹Vapnik-Chervonenkis (VC)ç»´æ•°ä¸ºdçš„å‡è®¾ç±»ï¼Œç®—æ³•å¯ä»¥ç”Ÿæˆä¸€ä¸ªÎµ-æœ€ä¼˜éšæœºå‡è®¾ï¼Œå¹¶ä¸”æ ·æœ¬å¤æ‚åº¦ä¸æœ€ä½³ä¸‹ç•Œä¿æŒä¸€è‡´ã€‚åŒæ—¶ï¼Œè¯¥ç®—æ³•çš„æ€æƒ³å’Œç†è®ºè¿˜è¢«è¿›ä¸€æ­¥æ‰©å±•ä»¥é€‚åº”Rademacherç±»ã€‚æœ€ç»ˆæå‡ºçš„ç®—æ³•æ˜¯å¥¥æ‹‰å…‹å°”é«˜æ•ˆçš„ï¼Œä»…è®¿é—®å‡è®¾ç±»ã€‚

    

    å¤šåˆ†å¸ƒå­¦ä¹ ï¼ˆMDLï¼‰æ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå…±äº«æ¨¡å‹ï¼Œä½¿å¾—åœ¨kä¸ªä¸åŒçš„æ•°æ®åˆ†å¸ƒä¸‹ï¼Œæœ€å°åŒ–æœ€åæƒ…å†µé£é™©ï¼Œå·²æˆä¸ºé€‚åº”å¥å£®æ€§ã€å…¬å¹³æ€§ã€å¤šç»„åˆä½œç­‰éœ€æ±‚çš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ç°æ•°æ®é«˜æ•ˆçš„MDLéœ€è¦åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·ï¼Œä¹Ÿç§°ä¸ºæŒ‰éœ€é‡‡æ ·ã€‚ç„¶è€Œï¼Œæœ€ä¼˜æ ·æœ¬å¤æ‚åº¦çš„ä¸Šä¸‹ç•Œä¹‹é—´å­˜åœ¨è¾ƒå¤§å·®è·ã€‚é’ˆå¯¹Vapnik-Chervonenkisï¼ˆVCï¼‰ç»´æ•°ä¸ºdçš„å‡è®¾ç±»ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®—æ³•ï¼Œå¯ç”Ÿæˆä¸€ä¸ªÎµ-æœ€ä¼˜éšæœºå‡è®¾ï¼Œå…¶æ ·æœ¬å¤æ‚åº¦æ¥è¿‘äºï¼ˆd+kï¼‰/Îµ^2ï¼ˆåœ¨æŸäº›å¯¹æ•°å› å­ä¸­ï¼‰ï¼Œä¸å·²çŸ¥çš„æœ€ä½³ä¸‹ç•ŒåŒ¹é…ã€‚æˆ‘ä»¬çš„ç®—æ³•æ€æƒ³å’Œç†è®ºè¢«è¿›ä¸€æ­¥æ‰©å±•ï¼Œä»¥é€‚åº”Rademacherç±»ã€‚æå‡ºçš„ç®—æ³•æ˜¯å¥¥æ‹‰å…‹å°”é«˜æ•ˆçš„ï¼Œä»…ä»…è®¿é—®å‡è®¾ç±»

    Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
    
[^29]: å¯¹äºæ ¸æœºå™¨åœ¨é¢„å¤„ç†ä¸­çš„Nystromé€¼è¿‘

    On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2312.03311](http://arxiv.org/abs/2312.03311)

    æœ¬æ–‡åˆ†æäº†æ ¸æœºå™¨é¢„å¤„ç†ä¸­ä½¿ç”¨Nystromé€¼è¿‘çš„æƒè¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¯¹æ•°å¤§å°çš„æ ·æœ¬èƒ½å¤Ÿè®©Nystromé€¼è¿‘çš„é¢„å¤„ç†å™¨å‡ ä¹ä¸æ¢¯åº¦ä¸‹é™åŒæ ·æœ‰æ•ˆåœ°åŠ é€Ÿã€‚

    

    æ ¸æ–¹æ³•æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ç±»æµè¡Œçš„éçº¿æ€§é¢„æµ‹æ¨¡å‹ã€‚å­¦ä¹ æ ¸æ¨¡å‹çš„å¯æ‰©å±•ç®—æ³•éœ€è¦å…·æœ‰è¿­ä»£æ€§è´¨ï¼Œä½†ç”±äºç³Ÿç³•çš„æ¡ä»¶ï¼Œæ”¶æ•›å¯èƒ½å¾ˆæ…¢ã€‚è°±é¢„å¤„ç†æ˜¯åŠ å¿«è®­ç»ƒæ ¸æ¨¡å‹è¿­ä»£ç®—æ³•æ”¶æ•›é€Ÿåº¦çš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œè®¡ç®—å’Œå­˜å‚¨è°±é¢„å¤„ç†å™¨å¯èƒ½ä»£ä»·é«˜æ˜‚ï¼Œä¼šå¯¼è‡´å¤§é‡çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼Œé™åˆ¶äº†æ ¸æ–¹æ³•åœ¨å¤§å‹æ•°æ®é›†é—®é¢˜ä¸Šçš„åº”ç”¨ã€‚Nystromé€¼è¿‘çš„è°±é¢„å¤„ç†å™¨é€šå¸¸æ›´ä¾¿å®œå’Œæ›´å®¹æ˜“è®¡ç®—å’Œå­˜å‚¨ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†æˆåŠŸã€‚æœ¬æ–‡åˆ†æäº†ä½¿ç”¨è¿™ç§é€¼è¿‘é¢„å¤„ç†å™¨çš„æƒè¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¡¨æ˜ä¸æ•°æ®é›†å¤§å°ç›¸å…³çš„å¯¹æ•°æ ·æœ¬æ•°é‡èƒ½å¤Ÿè®©åŸºäºNystromé€¼è¿‘çš„é¢„å¤„ç†å™¨å‡ ä¹ä¸æ¢¯åº¦ä¸‹é™åŒæ ·æœ‰æ•ˆåœ°åŠ é€Ÿã€‚

    Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as
    
[^30]: æ³¨é‡Šæ•æ„Ÿæ€§ï¼šè®­ç»ƒæ•°æ®æ”¶é›†æ–¹æ³•å½±å“æ¨¡å‹æ€§èƒ½

    Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2311.14212](http://arxiv.org/abs/2311.14212)

    è¯¥ç ”ç©¶å‘ç°è®­ç»ƒæ•°æ®æ”¶é›†æ–¹æ³•å¯¹æ³¨é‡Šæœ¬èº«å’Œä¸‹æ¸¸æ¨¡å‹æ€§èƒ½äº§ç”Ÿå½±å“ã€‚åœ¨å¯¹ä»‡æ¨è¨€è®ºå’Œå†’çŠ¯æ€§è¯­è¨€è¿›è¡Œæ³¨é‡Šæ”¶é›†çš„å®éªŒä¸­ï¼Œå‘ç°æ³¨é‡Šå·¥å…·çš„è®¾è®¡é€‰æ‹©ä¼šå¯¹æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿæ˜æ˜¾å·®å¼‚ã€‚

    

    å½“è®­ç»ƒæ•°æ®ç”±äººå·¥æ³¨é‡Šè€…æ”¶é›†æ—¶ï¼Œæ³¨é‡Šå·¥å…·çš„è®¾è®¡ã€ç»™äºˆæ³¨é‡Šè€…çš„æŒ‡ç¤ºã€æ³¨é‡Šè€…çš„ç‰¹å¾ä»¥åŠä»–ä»¬ä¹‹é—´çš„äº’åŠ¨éƒ½å¯èƒ½å¯¹è®­ç»ƒæ•°æ®äº§ç”Ÿå½±å“ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†åˆ›å»ºæ³¨é‡Šå·¥å…·æ—¶çš„è®¾è®¡é€‰æ‹©ä¹Ÿä¼šå½±å“åŸºäºå¾—åˆ°çš„æ³¨é‡Šè®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†"æ³¨é‡Šæ•æ„Ÿæ€§"è¿™ä¸ªæœ¯è¯­ï¼Œç”¨æ¥æŒ‡ä»£æ³¨é‡Šæ•°æ®æ”¶é›†æ–¹æ³•å¯¹æ³¨é‡Šæœ¬èº«ä»¥åŠä¸‹æ¸¸æ¨¡å‹æ€§èƒ½å’Œé¢„æµ‹çš„å½±å“ã€‚æˆ‘ä»¬åœ¨äº”ç§å®éªŒæ¡ä»¶ä¸‹å¯¹ä»‡æ¨è¨€è®ºå’Œå†’çŠ¯æ€§è¯­è¨€è¿›è¡Œæ³¨é‡Šæ”¶é›†ï¼Œéšæœºå°†æ³¨é‡Šè€…åˆ†é…åˆ°ä¸åŒæ¡ä»¶ä¸‹ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªå¾—åˆ°çš„äº”ä¸ªæ•°æ®é›†ä¸Šå¯¹BERTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ¯ä¸ªæ¡ä»¶çš„ä¿ç•™éƒ¨åˆ†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°åœ¨ä»¥ä¸‹æ–¹é¢æ¡ä»¶ä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼š1ï¼‰ä»‡æ¨è¨€è®º/å†’çŠ¯æ€§è¯­è¨€æ³¨é‡Šçš„æ¯”ä¾‹ï¼Œ2ï¼‰æ¨¡å‹æ€§èƒ½ã€‚

    When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
    
[^31]: å…³äºåˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€

    On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.09018](http://arxiv.org/abs/2311.09018)

    è¯¥è®ºæ–‡ä¸ºåˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºç¡€åšå‡ºäº†è´¡çŒ®ï¼Œé€šè¿‡ä¸€ä¸ªç»¼åˆçš„å»ºæ¨¡æ¡†æ¶ï¼Œå†³ç­–è€…åœ¨æœ€åæƒ…å†µä¸‹çš„åˆ†å¸ƒè½¬å˜ä¸‹é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶è€ƒè™‘äº†å„ç§å»ºæ¨¡å±æ€§å’Œå¯¹æ‰‹å¼•èµ·çš„è½¬å˜çš„çµæ´»æ€§ã€‚

    

    å‡ºäºå¯¹åœ¨è®­ç»ƒå’Œéƒ¨ç½²ä¹‹é—´ç¯å¢ƒå˜åŒ–æ—¶é²æ£’ç­–ç•¥çš„éœ€æ±‚ï¼Œæˆ‘ä»¬ä¸ºåˆ†å¸ƒé²æ£’å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºç¡€åšå‡ºäº†è´¡çŒ®ã€‚é€šè¿‡ä¸€ä¸ªä»¥åˆ†å¸ƒé²æ£’é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆDRMDPsï¼‰ä¸ºä¸­å¿ƒçš„ç»¼åˆå»ºæ¨¡æ¡†æ¶ï¼Œæˆ‘ä»¬ä½¿å†³ç­–è€…åœ¨ä¸€ä¸ªç”±å¯¹æ‰‹æ“çºµçš„æœ€åæƒ…å†µåˆ†å¸ƒè½¬å˜ä¸‹é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚é€šè¿‡ç»Ÿä¸€å’Œæ‰©å±•ç°æœ‰çš„è¡¨è¿°ï¼Œæˆ‘ä»¬ä¸¥æ ¼æ„å»ºäº†é€‚ç”¨äºå†³ç­–è€…å’Œå¯¹æ‰‹çš„å„ç§å»ºæ¨¡å±æ€§çš„DRMDPsï¼ŒåŒ…æ‹¬é€‚åº”æ€§ç²’åº¦ã€æ¢ç´¢å†å²ä¾èµ–æ€§ã€é©¬å°”ç§‘å¤«å’Œé©¬å°”ç§‘å¤«æ—¶é—´é½æ¬¡çš„å†³ç­–è€…å’Œå¯¹æ‰‹åŠ¨æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å¯¹æ‰‹å¼•èµ·çš„è½¬å˜çš„çµæ´»æ€§ï¼Œç ”ç©¶äº†SAå’ŒS-çŸ©å½¢æ€§ã€‚åœ¨è¿™ä¸ªDRMDPæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å®ç°é²æ£’æ€§æ‰€éœ€çš„æ¡ä»¶ã€‚

    Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the e
    
[^32]: ä½¿ç”¨ç±»ä¼¼ResNetçš„ç¥ç»ç½‘ç»œæ¶æ„è¿‘ä¼¼Langevin Monte Carlo

    Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03242](http://arxiv.org/abs/2311.03242)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç±»ä¼¼ResNetçš„ç¥ç»ç½‘ç»œæ¶æ„æ¥è¿‘ä¼¼Langevin Monte Carloç®—æ³•ï¼Œé€šè¿‡å°†æ¥è‡ªç®€å•å‚è€ƒåˆ†å¸ƒçš„æ ·æœ¬æ˜ å°„åˆ°ç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬ä¸­æ¥è¿›è¡Œé‡‡æ ·ï¼Œå…·æœ‰è¾ƒå¥½çš„é€¼è¿‘é€Ÿåº¦å’Œè¡¨è¾¾æ€§ã€‚

    

    æˆ‘ä»¬é€šè¿‡æ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå°†æ¥è‡ªç®€å•å‚è€ƒåˆ†å¸ƒï¼ˆå¦‚æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰çš„æ ·æœ¬æ˜ å°„åˆ°ç›®æ ‡åˆ†å¸ƒçš„æ ·æœ¬ä¸­ï¼Œä»è€Œä»ç»™å®šçš„ç›®æ ‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å—Langevin Monte Carlo (LMC)ç®—æ³•å¯å‘çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚åŸºäºLMCæ‰°åŠ¨ç»“æœï¼Œåœ¨Wasserstein-2è·ç¦»ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¶æ„å¯¹äºå¹³æ»‘çš„å¯¹æ•°å‡¹ç›®æ ‡åˆ†å¸ƒçš„é€¼è¿‘é€Ÿåº¦ã€‚åˆ†æä¸¥é‡ä¾èµ–äºæ‰°åŠ¨LMCè¿‡ç¨‹çš„ä¸­é—´åº¦é‡çš„äºšé«˜æ–¯æ€§æ¦‚å¿µã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒæ‰°åŠ¨å‡è®¾æ¨å¯¼å‡ºäº†ä¸­é—´æ–¹å·®ä»£ç†çš„å¢é•¿ç•Œé™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼äºæ·±åº¦æ®‹å·®ç¥ç»ç½‘ç»œçš„æ¶æ„ï¼Œå¹¶æ¨å¯¼å‡ºäº†è¿‘ä¼¼æ ·æœ¬ä¸ç›®æ ‡åˆ†å¸ƒæ˜ å°„çš„è¡¨è¾¾æ€§ç»“æœã€‚

    We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
    
[^33]: å…·æœ‰åŠ æ€§å’Œä¹˜æ€§å™ªå£°çš„çº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹çš„å‘ç”Ÿå™¨è¯†åˆ«

    Generator Identification for Linear SDEs with Additive and Multiplicative Noise. (arXiv:2310.19491v1 [math.ST])

    [http://arxiv.org/abs/2310.19491](http://arxiv.org/abs/2310.19491)

    æœ¬æ–‡ä»‹ç»äº†ä»å…·æœ‰ç»™å®šåˆå§‹çŠ¶æ€çš„è§£è¿‡ç¨‹çš„åˆ†å¸ƒä¸­è¯†åˆ«çº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„å‘ç”Ÿå™¨çš„æ¡ä»¶ï¼Œå¹¶ä¸”æä¾›äº†å¯¹äºå…·æœ‰åŠ æ€§å’Œä¹˜æ€§å™ªå£°çš„SDEçš„è¯†åˆ«æ¡ä»¶ã€‚

    

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å…·æœ‰ç»™å®šå›ºå®šåˆå§‹çŠ¶æ€çš„è§£è¿‡ç¨‹çš„åˆ†å¸ƒä¸­è¯†åˆ«çº¿æ€§éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„å‘ç”Ÿå™¨çš„æ¡ä»¶ã€‚è¿™äº›å¯è¯†åˆ«æ€§æ¡ä»¶åœ¨ä½¿ç”¨çº¿æ€§SDEè¿›è¡Œå› æœæ¨æ–­æ—¶è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä½¿å¾—å¯ä»¥ä»å…¶è§‚æµ‹åˆ†å¸ƒä¸­è¯†åˆ«å‡ºå¹²é¢„åçš„åˆ†å¸ƒã€‚æˆ‘ä»¬å…·ä½“æ¨å¯¼å‡ºäº†è¯†åˆ«å…·æœ‰åŠ æ€§å™ªå£°çš„çº¿æ€§SDEçš„å‘ç”Ÿå™¨çš„å……åˆ†å¿…è¦æ¡ä»¶ï¼Œä»¥åŠè¯†åˆ«å…·æœ‰ä¹˜æ€§å™ªå£°çš„çº¿æ€§SDEçš„å‘ç”Ÿå™¨çš„å……åˆ†æ¡ä»¶ã€‚æˆ‘ä»¬è¯æ˜äº†å¯¹äºä¸¤ç§ç±»å‹çš„SDEï¼Œå¾—åˆ°çš„æ¡ä»¶æ˜¯ä¸€èˆ¬æ€§çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹å¾—åˆ°çš„å¯è¯†åˆ«æ€§æ¡ä»¶çš„å‡ ä½•è§£é‡Šï¼Œä»¥å¢å¼ºå¯¹å…¶çš„ç†è§£ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—çš„æ¨¡æ‹Ÿå®éªŒï¼Œè¿™äº›å®éªŒæ”¯æŒå¹¶è¯å®äº†æˆ‘ä»¬æ‰€å¾—åˆ°çš„ç»“æœã€‚

    In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
    
[^34]: å­¦ä¹ å¤„ç†å…·æœ‰ä¸€èˆ¬åº“å­˜åˆ°è´§åŠ¨æ€çš„åº“å­˜æ§åˆ¶ç­–ç•¥

    Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])

    [http://arxiv.org/abs/2310.17168](http://arxiv.org/abs/2310.17168)

    æœ¬æ–‡è§£å†³äº†å­¦ä¹ å…·æœ‰ä¸€èˆ¬åº“å­˜åˆ°è´§åŠ¨æ€ä¸‹çš„åº“å­˜æ§åˆ¶ç­–ç•¥çš„é—®é¢˜ï¼ŒåŒæ—¶å…è®¸ä¿®æ”¹è®¢è´­æ•°é‡ä»¥æ»¡è¶³ä¾›åº”å•†çš„é™åˆ¶ï¼Œå¹¶å°†å‘¨æœŸæ€§å®¡æ ¸åº“å­˜æ§åˆ¶é—®é¢˜å®šä¹‰ä¸ºå¤–éƒ¨å†³ç­–è¿‡ç¨‹ã€‚

    

    æœ¬æ–‡åœ¨é¢å¯¹ä¸€èˆ¬åˆ°è´§åŠ¨æ€çš„æƒ…å†µä¸‹è§£å†³äº†å­¦ä¹ å’Œå›æµ‹åº“å­˜æ§åˆ¶ç­–ç•¥çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºæ•°é‡éšæ—¶é—´åˆ°è´§æ¨¡å‹ï¼ˆQOTï¼‰ã€‚åœ¨å®é™…ä¾›åº”é“¾ä¸­ï¼Œæˆ‘ä»¬è¿˜å…è®¸ä¿®æ”¹è®¢è´­æ•°é‡ä»¥æ»¡è¶³ä¾›åº”å•†çš„é™åˆ¶ï¼Œä¾‹å¦‚è®¢è´­æœ€ä½æ•°é‡å’Œæ‰¹æ¬¡å¤§å°çº¦æŸã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡å¤„ç†ä»»æ„åˆ°è´§åŠ¨æ€æˆ–ä»»æ„åç»­å¤„ç†çš„è®¢è´­æ•°é‡çš„ç ”ç©¶ã€‚åœ¨æœ€è¿‘çš„å·¥ä½œï¼ˆMadekaç­‰ï¼Œ2022ï¼‰çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åŒæ ·å°†å‘¨æœŸæ€§å®¡æ ¸åº“å­˜æ§åˆ¶é—®é¢˜å®šä¹‰ä¸ºå¤–éƒ¨å†³ç­–è¿‡ç¨‹ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†çŠ¶æ€ä¸å—ä»£ç†çš„æ§åˆ¶ã€‚Madekaç­‰äººï¼ˆ2022ï¼‰å±•ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ä¸ªæ¨¡æ‹Ÿå™¨æ¥å›æ”¾å†å²æ•°æ®ä»¥è§£å†³è¿™ç±»é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªæ·±åº¦ç”Ÿæˆæ¨¡å‹çº³å…¥åˆ°è´§è¿‡ç¨‹çš„å†å²å›æ”¾ä¸­ã€‚

    In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al. (2022) show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulat
    
[^35]: å­¦ä¹ å…·æœ‰å·²çŸ¥éª¨æ¶çš„æœ‰ç•Œåº¦å¤šæ ‘

    Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])

    [http://arxiv.org/abs/2310.06333](http://arxiv.org/abs/2310.06333)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆå­¦ä¹ å·²çŸ¥éª¨æ¶çš„æœ‰ç•Œåº¦å¤šæ ‘çš„ç®—æ³•ï¼Œå¹¶ç»™å‡ºäº†åœ¨å¤šé¡¹å¼æ—¶é—´å’Œæ ·æœ¬å¤æ‚åº¦å†…çš„æœ‰é™æ ·æœ¬ä¿è¯ã€‚è¿™å¯¹äºå¤æ‚æ¦‚ç‡åˆ†å¸ƒçš„å­¦ä¹ å…·æœ‰é‡è¦æ„ä¹‰ã€‚

    

    æˆ‘ä»¬ä¸ºé«˜ç»´æ¦‚ç‡åˆ†å¸ƒçš„ä¸€ç±»ä¸°å¯Œçš„å¤šæ ‘ï¼ˆpolytreesï¼‰â€”â€”æœ‰ç•Œåº¦å¤šæ ‘ï¼Œå»ºç«‹äº†é«˜æ•ˆé€‚å½“å­¦ä¹ çš„æœ‰é™æ ·æœ¬ä¿è¯ã€‚æœ‰ç•Œåº¦å¤šæ ‘æ˜¯è´å¶æ–¯ç½‘ç»œçš„å­ç±»ï¼Œè´å¶æ–¯ç½‘ç»œæ˜¯ä¸€ç§å¹¿æ³›ç ”ç©¶çš„å›¾æ¨¡å‹ç±»å‹ã€‚æœ€è¿‘ï¼ŒBhattacharyyaç­‰äººï¼ˆ2021ï¼‰é€šè¿‡æä¾›ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œåœ¨å·²çŸ¥æ— å‘å›¾ï¼ˆéª¨æ¶ï¼‰çš„æƒ…å†µä¸‹ï¼Œä¸º1-å¤šæ ‘æ¢å¤äº†æœ‰é™æ ·æœ¬ä¿è¯ã€‚æˆ‘ä»¬é€šè¿‡æ‰©å±•ä»–ä»¬çš„ç»“æœï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆç®—æ³•ï¼Œå¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å’Œæ ·æœ¬å¤æ‚åº¦å†…å­¦ä¹ ä»»ä½•æœ‰ç•Œåº¦çš„$d$-å¤šæ ‘ã€‚æˆ‘ä»¬å°†ç®—æ³•ä¸ä¿¡æ¯è®ºæ ·æœ¬å¤æ‚åº¦çš„ä¸‹ç•Œç»“åˆèµ·æ¥ï¼Œè¡¨æ˜å¯¹ç»´åº¦å’Œç›®æ ‡ç²¾åº¦å‚æ•°çš„ä¾èµ–å‡ ä¹æ˜¯ç´§è‡´çš„ã€‚

    We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
    
[^36]: ä¸€ç§ç”¨äºéå±‚æ¬¡åŒ–å¤šä¿çœŸåº¦è‡ªé€‚åº”é‡‡æ ·çš„æ½œå˜é‡æ–¹æ³•

    A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])

    [http://arxiv.org/abs/2310.03298](http://arxiv.org/abs/2310.03298)

    æå‡ºäº†ä¸€ç§åŸºäºæ½œå˜é‡çš„æ–¹æ³•ï¼Œç”¨äºéå±‚æ¬¡åŒ–å¤šä¿çœŸåº¦è‡ªé€‚åº”é‡‡æ ·ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ä¸åŒä¿çœŸåº¦æ¨¡å‹ä¹‹é—´çš„ç›¸å…³æ€§ä»¥æ›´é«˜æ•ˆåœ°æ¢ç´¢å’Œåˆ©ç”¨è®¾è®¡ç©ºé—´ã€‚

    

    å¤šä¿çœŸåº¦ï¼ˆMFï¼‰æ–¹æ³•åœ¨æé«˜æ›¿ä»£æ¨¡å‹å’Œè®¾è®¡ä¼˜åŒ–æ–¹é¢è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œé€šè¿‡æ•´åˆæ¥è‡ªä¸åŒä½ä¿çœŸåº¦ï¼ˆLFï¼‰æ¨¡å‹çš„æ•°æ®ã€‚å°½ç®¡å¤§å¤šæ•°ç°æœ‰çš„MFæ–¹æ³•å‡å®šäº†ä¸€ä¸ªå›ºå®šçš„æ•°æ®é›†ï¼Œä½†æ˜¯åŠ¨æ€åˆ†é…èµ„æºåœ¨ä¸åŒä¿çœŸåº¦æ¨¡å‹ä¹‹é—´å¯ä»¥å®ç°æ›´é«˜çš„æ¢ç´¢å’Œåˆ©ç”¨è®¾è®¡ç©ºé—´çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„MFæ–¹æ³•ä¾èµ–äºä¿çœŸåº¦çº§åˆ«çš„å±‚æ¬¡å‡è®¾ï¼Œæˆ–è€…æ— æ³•æ•æ‰å¤šä¸ªä¿çœŸåº¦çº§åˆ«ä¹‹é—´çš„ç›¸äº’å…³ç³»å¹¶åˆ©ç”¨å…¶æ¥é‡åŒ–æœªæ¥æ ·æœ¬çš„ä»·å€¼å’Œå¯¼èˆªè‡ªé€‚åº”é‡‡æ ·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªéšœç¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºä¸åŒä¿çœŸåº¦æ¨¡å‹çš„æ½œå˜é‡åµŒå…¥å’Œç›¸å…³çš„å…ˆéªŒ-åéªŒåˆ†æçš„æ¡†æ¶ï¼Œä»¥æ˜¾å¼åœ°åˆ©ç”¨å®ƒä»¬çš„ç›¸å…³æ€§è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæ¯ä¸ªå¡«å……é‡‡æ ·è¿­ä»£åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆæˆ‘ä»¬ç¡®å®šå…·æœ‰æœ€å¤§æ½œåŠ›å½±å“çš„ä½ç½®ã€‚

    Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
    
[^37]: å…³äºéšæœºæ¢¯åº¦ä¸‹é™çš„ä¸åŒæ¨¡å¼

    On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v1 [cs.LG])

    [http://arxiv.org/abs/2309.10688](http://arxiv.org/abs/2309.10688)

    è¿™é¡¹ç ”ç©¶è§£å†³äº†å¯¹äºéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸­ä¸åŒæ¨¡å¼çš„è¿½è¸ªå’Œç†è§£çš„é—®é¢˜ï¼Œæä¾›äº†ä¸€ä¸ªç›¸ä½å›¾æ¥åŒºåˆ†å™ªå£°ä¸»å¯¼çš„SGDå’Œå¤§æ­¥éª¤ä¸»å¯¼çš„SGDã€‚

    

    ç°ä»£æ·±åº¦ç½‘ç»œé€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è¿›è¡Œè®­ç»ƒï¼Œå…¶å…³é”®å‚æ•°æ˜¯æ¯ä¸ªæ­¥éª¤è€ƒè™‘çš„æ•°æ®é‡æˆ–æ‰¹é‡å¤§å°Bä»¥åŠæ­¥é•¿æˆ–å­¦ä¹ ç‡Î·ã€‚å¯¹äºå°çš„Bå’Œå¤§çš„Î·ï¼ŒSGDå¯¹åº”äºå‚æ•°çš„éšæœºæ¼”åŒ–ï¼Œå…¶å™ªå£°å¹…åº¦ç”±â€œæ¸©åº¦â€T=Î·/Bæ§åˆ¶ã€‚ç„¶è€Œå½“æ‰¹é‡å¤§å°Bâ‰¥B^*è¶³å¤Ÿå¤§æ—¶ï¼Œè¿™ç§æè¿°è¢«è§‚å¯Ÿåˆ°å¤±æ•ˆï¼Œæˆ–è€…åœ¨æ¸©åº¦è¶³å¤Ÿå°æ—¶ç®€åŒ–ä¸ºæ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ã€‚ç†è§£è¿™äº›äº¤å‰å‘ç”Ÿçš„ä½ç½®ä»ç„¶æ˜¯ä¸€ä¸ªä¸­å¿ƒæŒ‘æˆ˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œåœ¨ä¸€ä¸ªæ•™å¸ˆ-å­¦ç”Ÿæ„ŸçŸ¥å™¨åˆ†ç±»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„å…³é”®é¢„æµ‹ä»é€‚ç”¨äºæ·±åº¦ç½‘ç»œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨B-Î·å¹³é¢ä¸Šè·å¾—äº†ä¸€ä¸ªç›¸ä½å›¾ï¼Œå°†ä¸‰ä¸ªåŠ¨æ€é˜¶æ®µåˆ†å¼€ï¼šï¼ˆiï¼‰å—æ¸©åº¦æ§åˆ¶çš„å™ªå£°ä¸»å¯¼çš„SGDï¼Œï¼ˆiiï¼‰å¤§æ­¥éª¤ä¸»å¯¼çš„SGDå’Œ

    Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and
    
[^38]: å»æ®–æ°‘åŒ–çš„äººå·¥æ™ºèƒ½å¯¹é½ï¼šå¨è‰²è¾¾å°”ç›ã€è®ºè¯å’Œè‰ºæœ¯è¡¨è¾¾

    Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])

    [http://arxiv.org/abs/2309.05030](http://arxiv.org/abs/2309.05030)

    æœ¬æ–‡æå‡ºäº†å»æ®–æ°‘åŒ–äººå·¥æ™ºèƒ½å¯¹é½çš„ä¸‰ä¸ªå»ºè®®ï¼šæ”¹å˜åŸºæœ¬é“å¾·å“²å­¦ä¸ºè¾¾å°”ç›å“²å­¦ï¼Œå…è®¸å¤šå…ƒä¸»ä¹‰çš„è®ºè¯ä¼ ç»Ÿå­˜åœ¨äºå¯¹é½æŠ€æœ¯ä¸­ï¼Œä»¥åŠå°†ä»·å€¼è®¤è¯†è®ºæ‰©å±•åˆ°è¶…è¶Šè‡ªç„¶è¯­è¨€ä¸­çš„æŒ‡ä»¤ã€‚

    

    å…ˆå‰çš„ç ”ç©¶å·²ç»é˜æ˜äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¼€å‘å’Œéƒ¨ç½²çš„æ®–æ°‘æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾ˆå°‘æ¶‰åŠåˆ°å¯¹é½ï¼šå³åŸºäºç»†è‡´çš„äººç±»åé¦ˆï¼Œè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡Œä¸ºä¸æœŸæœ›å€¼ä¸€è‡´ã€‚é™¤äº†å…¶ä»–å®è·µï¼Œæ®–æ°‘ä¸»ä¹‰è¿˜æœ‰ä¸€éƒ¨åˆ†æ˜¯æ”¹å˜è¢«æ®–æ°‘æ°‘æ—çš„ä¿¡ä»°å’Œä»·å€¼è§‚çš„å†å²ï¼›è€Œå½“å‰çš„LLMå¯¹é½å®è·µæ­£æ˜¯è¿™ä¸€å†å²çš„å¤åˆ¶ã€‚æˆ‘ä»¬å»ºè®®é€šè¿‡ä¸‰ä¸ªæè®®å¯¹AIå¯¹é½è¿›è¡Œå»æ®–æ°‘åŒ–ï¼šï¼ˆaï¼‰å°†åŸºæœ¬é“å¾·å“²å­¦ä»è¥¿æ–¹å“²å­¦è½¬å˜ä¸ºè¾¾å°”ç›å“²å­¦ï¼Œï¼ˆbï¼‰åœ¨å¯¹é½æŠ€æœ¯ä¸­å…è®¸è®ºè¯å’Œå¤šå…ƒä¸»ä¹‰çš„ä¼ ç»Ÿï¼Œä»¥åŠï¼ˆcï¼‰å°†ä»·å€¼çš„è®¤è¯†è®ºæ‰©å±•åˆ°è¶…è¶Šè‡ªç„¶è¯­è¨€ä¸­çš„æŒ‡ä»¤æˆ–å‘½ä»¤ã€‚

    Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
    
[^39]: ä½¿ç”¨åˆè§„çš„è’™ç‰¹å¡æ´›é¢„æµ‹å®ç°é²æ£’çš„ä¸ç¡®å®šæ€§é‡åŒ–

    Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction. (arXiv:2308.09647v1 [cs.LG])

    [http://arxiv.org/abs/2308.09647](http://arxiv.org/abs/2308.09647)

    è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMC-CPçš„æ–°å‹æ··åˆä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å°†è‡ªé€‚åº”è’™ç‰¹å¡æ´›dropoutæ–¹æ³•ä¸åˆè§„é¢„æµ‹ç›¸ç»“åˆï¼Œå®ç°äº†èŠ‚çœèµ„æºå’Œäº§ç”Ÿé²æ£’é¢„æµ‹é›†/åŒºé—´çš„ç›®æ ‡ã€‚å®éªŒè¯æ˜MC-CPåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ç›¸æ¯”å…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰æ˜¾è‘—æå‡

    

    åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­éƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ä»ç„¶æ˜¯ä¸€é¡¹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¯¹è¿™äº›æ¨¡å‹çš„å¯é è¿è¡Œæä¾›ä¿è¯ã€‚ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ–¹æ³•ä¼°è®¡æ¯ä¸ªé¢„æµ‹çš„æ¨¡å‹ç½®ä¿¡åº¦ï¼Œé€šè¿‡è€ƒè™‘éšæœºæ€§å’Œæ¨¡å‹é”™è¯¯è§„èŒƒåŒ–çš„å½±å“æ¥æŒ‡å¯¼å†³ç­–ã€‚å°½ç®¡æœ€å…ˆè¿›çš„UQæ–¹æ³•å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨è®¡ç®—ä¸Šè¦ä¹ˆéå¸¸æ˜‚è´µï¼Œè¦ä¹ˆäº§ç”Ÿä¿å®ˆçš„é¢„æµ‹é›†/åŒºé—´ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ··åˆUQæ–¹æ³•MC-CPï¼Œå®ƒå°†ä¸€ç§æ–°çš„è‡ªé€‚åº”è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰dropoutæ–¹æ³•ä¸åˆè§„é¢„æµ‹ï¼ˆCPï¼‰ç›¸ç»“åˆã€‚MC-CPåœ¨è¿è¡Œæ—¶è‡ªé€‚åº”è°ƒèŠ‚ä¼ ç»Ÿçš„MC dropoutä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºï¼Œä½¿å¾—é¢„æµ‹å¯ä»¥è¢«CPä½¿ç”¨ï¼Œå¾—åˆ°é²æ£’çš„é¢„æµ‹é›†/åŒºé—´ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†MC-CPç›¸æ¯”MC dropoutã€RAPSå’ŒCQRç­‰å…ˆè¿›çš„UQæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æ”¹å–„åˆ†ç±»æ€§èƒ½

    Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classificati
    
[^40]: å¤šç±»åœ¨çº¿å­¦ä¹ åœ¨Banditåé¦ˆä¸‹çš„ç ”ç©¶

    Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])

    [http://arxiv.org/abs/2308.04620](http://arxiv.org/abs/2308.04620)

    Banditåé¦ˆä¸‹çš„åœ¨çº¿å¤šç±»å­¦ä¹ çš„å…³é”®åœ¨äºBandit Littlestoneç»´åº¦çš„æœ‰é™æ€§ï¼Œæ— è®ºæ ‡ç­¾ç©ºé—´æ˜¯å¦æ— ç•Œã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†åœ¨Banditåé¦ˆä¸‹çš„å¤šç±»åœ¨çº¿åˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬æ‰©å±•äº†(daniely2013price)çš„ç»“æœï¼Œé€šè¿‡å±•ç¤ºBandit Littlestoneç»´åº¦çš„æœ‰é™æ€§æ˜¯å¤šç±»åœ¨çº¿å­¦ä¹ çš„å¿…è¦ä¸”å……åˆ†æ¡ä»¶ï¼Œå³ä½¿æ ‡ç­¾ç©ºé—´æ˜¯æ— ç•Œçš„ã€‚æˆ‘ä»¬çš„ç»“æœè¡¥å……äº†(hanneke2023multiclass)çš„æœ€è¿‘å·¥ä½œï¼Œä»–ä»¬åœ¨æ ‡ç­¾ç©ºé—´æ— ç•Œçš„å…¨ä¿¡æ¯è®¾ç½®ä¸­ï¼Œå±•ç¤ºäº†Littlestoneç»´åº¦åˆ»ç”»äº†åœ¨çº¿å¤šç±»å­¦ä¹ çš„èƒ½åŠ›ã€‚

    We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
    
[^41]: å¯¹æ•°è´å¶æ–¯é—æ†¾è¾¹ç•Œ

    Logarithmic Bayes Regret Bounds. (arXiv:2306.09136v1 [cs.LG])

    [http://arxiv.org/abs/2306.09136](http://arxiv.org/abs/2306.09136)

    è¯¥è®ºæ–‡æå‡ºäº†å¯¹äºè´å¶æ–¯èµŒåšæœºçš„é¦–ä¸ªæœ‰é™æ—¶é—´å¯¹æ•°é—æ†¾è¾¹ç•Œï¼Œå¹¶ç”¨äºé«˜æ–¯å’Œçº¿æ€§èµŒåšæœºï¼Œä»è€Œé˜æ˜äº†è´å¶æ–¯è®¾ç½®ä¸­å…ˆéªŒä»·å€¼ä»¥åŠå¯¹$\tilde{O}(\sqrt{n})$ç•Œé™çš„æ”¹å–„ã€‚

    

    æˆ‘ä»¬ä¸ºè´å¶æ–¯èµŒåšæœºå¯¼å‡ºäº†é¦–ä¸ªæœ‰é™æ—¶é—´å¯¹æ•°é—æ†¾è¾¹ç•Œã€‚å¯¹äºé«˜æ–¯èµŒåšæœºï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ª$O(c_h \log^2 n)$çš„è¾¹ç•Œï¼Œå…¶ä¸­$c_h$æ˜¯ä¸å…ˆéªŒç›¸å…³çš„å¸¸é‡ã€‚è¿™ä¸Laiï¼ˆ1987ï¼‰çš„æ¸è¿‘ä¸‹é™ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„è¯æ˜ä¸å…ˆå‰çš„å·¥ä½œæœ‰æ‰€ä¸åŒï¼Œä¸”ç®€å•ä¸”æ™®éã€‚ä¸ºäº†æ˜¾ç¤ºä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æŠ€æœ¯åº”ç”¨äºçº¿æ€§èµŒåšæœºã€‚æˆ‘ä»¬çš„ç•Œé™é˜æ˜äº†è´å¶æ–¯è®¾ç½®ä¸­å…ˆéªŒçš„ä»·å€¼ï¼Œæ—¢å¯ä»¥ä½œä¸ºç›®æ ‡ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºä¼ é€’ç»™å­¦ä¹ è€…çš„é™„åŠ ä¿¡æ¯ã€‚å®ƒä»¬æ˜¾ç€æ”¹å–„äº†ç°æœ‰çš„$\tilde{O}(\sqrt{n})$ç•Œé™ï¼Œå°½ç®¡å­˜åœ¨ä¸‹é™ï¼Œä½†å·²æˆä¸ºæ–‡çŒ®ä¸­çš„æ ‡å‡†ã€‚

    We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\tilde{O}(\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.
    
[^42]: çº¿æ€§æ¡ä»¶VAEå’Œåˆ†å±‚VAEä¸­çš„åéªŒå´©æºƒç°è±¡

    Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])

    [http://arxiv.org/abs/2306.05023](http://arxiv.org/abs/2306.05023)

    æœ¬æ–‡ç ”ç©¶äº†é«˜åº¦ç›¸ä¼¼çš„å˜åˆ†åéªŒåˆ†å¸ƒå’Œå…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„åéªŒå´©æºƒç°è±¡ï¼Œç‰¹åˆ«åœ°ï¼Œé€šè¿‡å¯¹çº¿æ€§æ¡ä»¶VAEå’Œåˆ†å±‚VAEè¿›è¡Œåˆ†æï¼Œè¯æ˜äº†è¿™ç§ç°è±¡æ˜¯ç”±äºæ½œåœ¨å˜é‡å±‚æ¬¡å…³ç³»ä¸æ¸…æ™°è€Œå¼•èµ·çš„ã€‚

    

    åœ¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸­ï¼ŒåéªŒå´©æºƒç°è±¡æŒ‡çš„æ˜¯å˜åˆ†åéªŒåˆ†å¸ƒä¸å…ˆéªŒåˆ†å¸ƒçš„ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œå¯¼è‡´ç¼–ç å™¨æå–çš„æ½œåœ¨å˜é‡ä¿å­˜çš„è¾“å…¥æ•°æ®ä¿¡æ¯è¾ƒå°‘ï¼Œæ— æ³•ä¸ºè§£ç å™¨çš„æ•°æ®é‡å»ºè¿‡ç¨‹äº§ç”Ÿæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚å°½ç®¡è¯¥ç°è±¡ä¸€ç›´æ˜¯VAEsæ€§èƒ½çš„ç ”ç©¶çƒ­ç‚¹ï¼Œä½†æ˜¯å¯¹äºåéªŒå´©æºƒçš„ç†è®ºå´ç›¸å¯¹è–„å¼±ï¼Œç‰¹åˆ«æ˜¯åœ¨éæ ‡å‡†çš„VAEsä¸­ã€‚æœ¬æ–‡é€šè¿‡å¯¹ä¸¤ç±»é‡è¦è€Œå¸¸è§åˆè¾ƒå°‘ç ”ç©¶çš„VAEsè¿›è¡Œéå¹³å‡¡çš„ç†è®ºåˆ†æï¼Œå³å…·æœ‰ä¸¤ä¸ªæ½œåœ¨å˜é‡å±‚æ¬¡çš„çº¿æ€§æ¡ä»¶VAEå’Œåˆ†å±‚VAEï¼Œæå‡äº†å¯¹åéªŒå´©æºƒçš„ç†è®ºè®¤è¯†ï¼Œè¯æ˜äº†å…¶æˆå› ã€‚

    The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
    
[^43]: åŸºäºæ•°æ®é©±åŠ¨çš„é—æ†¾å¹³è¡¡åœ¨çº¿æ¨¡å‹é€‰æ‹©çš„ç ”ç©¶

    Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v1 [cs.LG])

    [http://arxiv.org/abs/2306.02869](http://arxiv.org/abs/2306.02869)

    è®ºæ–‡è®¨è®ºåœ¨å…·æœ‰èµŒåšåé¦ˆçš„éšæœºç¯å¢ƒä¸­è¿›è¡Œé€‰æ‹©ï¼Œæå‡ºäº†ä¸¤ç§åŸºäºæ•°æ®çš„æ¨¡å‹é€‰æ‹©ç®—æ³•ï¼Œå¹¶è¯æ˜äº†å…¶ä¿è¯ã€‚é€šè¿‡åˆ©ç”¨å®é™…é—æ†¾ï¼Œè¿™äº›ç®—æ³•åœ¨å®é™…ä¸­å–å¾—äº†å¥½æ•ˆæœã€‚

    

    æˆ‘ä»¬è€ƒè™‘åœ¨å…·æœ‰èµŒåšåé¦ˆçš„éšæœºç¯å¢ƒä¸­è¿›è¡Œé¡ºåºå†³ç­–æ¨¡å‹é€‰æ‹©ï¼Œå…¶ä¸­å…ƒå­¦ä¹ å™¨å¯ä»¥ä½¿ç”¨ä¸€ç»„åŸºæœ¬å­¦ä¹ å™¨ï¼Œå¹¶æ ¹æ®æ¯ä¸ªåŸºæœ¬å­¦ä¹ å™¨æ¨èçš„ç­–ç•¥åŠ¨æ€å†³ç­–ã€‚æˆ‘ä»¬é€šè¿‡é—æ†¾å¹³è¡¡æ¥æ‰§è¡Œæ¨¡å‹é€‰æ‹©ï¼Œä½†ä¸æ­¤ç›¸å…³çš„æœ€è¿‘æ–‡çŒ®ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å‡è®¾ä»»ä½•å…³äºåŸºæœ¬å­¦ä¹ å™¨çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¦‚å€™é€‰é—æ†¾ä¿è¯ï¼›ç›¸åï¼Œæˆ‘ä»¬ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼æ­ç¤ºè¿™äº›æ•°é‡ã€‚å› æ­¤ï¼Œå…ƒå­¦ä¹ å™¨èƒ½å¤Ÿåˆ©ç”¨æ¯ä¸ªåŸºæœ¬å­¦ä¹ å™¨åœ¨ç»™å®šçš„å­¦ä¹ ç¯å¢ƒä¸­äº§ç”Ÿçš„å®é™…é—æ†¾ï¼ˆè€Œä¸æ˜¯æœŸæœ›é—æ†¾ï¼‰ï¼Œå¹¶æŒ‘é€‰å‡ºæœ€ä½³çš„é—æ†¾ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªæ¨¡å‹é€‰æ‹©ç®—æ³•ï¼Œæ“ä½œæ›´ä¸ºé›„å¿ƒå‹ƒå‹ƒçš„é—æ†¾æ¦‚å¿µï¼Œå¹¶ä¸”é™¤äº†é€šè¿‡é—æ†¾å¹³è¡¡è¯æ˜æ¨¡å‹é€‰æ‹©ä¿è¯å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨å®éªŒä¸­å±•ç¤ºäº†å¤„ç†å®é™…é—æ†¾çš„ä»¤äººä¿¡æœçš„å®é™…ä¼˜åŠ¿ã€‚

    We consider model selection for sequential decision making in stochastic environments with bandit feedback, where a meta-learner has at its disposal a pool of base learners, and decides on the fly which action to take based on the policies recommended by each base learner. Model selection is performed by regret balancing but, unlike the recent literature on this subject, we do not assume any prior knowledge about the base learners like candidate regret guarantees; instead, we uncover these quantities in a data-driven manner. The meta-learner is therefore able to leverage the realized regret incurred by each base learner for the learning environment at hand (as opposed to the expected regret), and single out the best such regret. We design two model selection algorithms operating with this more ambitious notion of regret and, besides proving model selection guarantees via regret balancing, we experimentally demonstrate the compelling practical benefits of dealing with actual regrets ins
    
[^44]: æ·±åº¦æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹çš„æ›´å¥½Batchæ–¹æ³•

    Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])

    [http://arxiv.org/abs/2305.17028](http://arxiv.org/abs/2305.17028)

    è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨ mini-batch ä¸­æ˜¾å¼åœ°å­¦ä¹ è¯¯å·®çš„åºåˆ—ç›¸å…³æ€§ï¼Œæ¥æé«˜æ·±åº¦æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚

    

    æ·±åº¦æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹å› å…¶èƒ½å¤Ÿæä¾›æœ‰ä»·å€¼çš„ä¸ç¡®å®šæ€§é‡åŒ–è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ¨¡å‹è¿‡äºç®€å•åŒ–é—®é¢˜ï¼Œå‡è®¾è¯¯å·®è¿‡ç¨‹æ˜¯ä¸æ—¶é—´æ— å…³çš„ï¼Œä»è€Œå¿½ç•¥äº†è¯¯å·®è¿‡ç¨‹ä¸­çš„åºåˆ—ç›¸å…³æ€§ã€‚è¿™å¯èƒ½ä¼šé™ä½é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä½¿è¿™äº›æ¨¡å‹å¯¹å†³ç­–æ€§ä»»åŠ¡çš„æœ‰æ•ˆæ€§å‡å¼±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œå°†è¯¯å·®è‡ªç›¸å…³æ€§çº³å…¥è€ƒè™‘ï¼Œä»¥å¢å¼ºæ¦‚ç‡é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠæ„é€ ä¸€ä¸ªmini-batchï¼Œä½œä¸º$D$ä¸ªè¿ç»­æ—¶é—´åºåˆ—æ®µè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶æ˜¾å¼åœ°å­¦ä¹ ä¸€ä¸ªåæ–¹å·®çŸ©é˜µï¼Œè¦†ç›–äº†ç›¸é‚»æ—¶é—´æ­¥ä¹‹é—´çš„è¯¯å·®ç›¸å…³æ€§ã€‚ç”±æ­¤äº§ç”Ÿçš„åæ–¹å·®çŸ©é˜µå¯ç”¨äºæé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œå¢å¼ºä¸ç¡®å®šæ€§çš„é‡åŒ–ã€‚

    Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
    
[^45]: æ·±å·ç§¯ç¥ç»ç½‘ç»œä¸­å½’çº³åç½®çš„ç†è®ºåˆ†æ

    Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v1 [cs.LG])

    [http://arxiv.org/abs/2305.08404](http://arxiv.org/abs/2305.08404)

    æœ¬æ–‡ç ”ç©¶æ·±å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„å½’çº³åç½®ï¼Œè¯æ˜äº†$\mathcal{O}(\log d)$çš„æ·±åº¦å°±è¶³ä»¥å®ç°æ™®é€‚æ€§ï¼Œç”¨CNNå­¦ä¹ ç¨€ç–å‡½æ•°åªéœ€è¦$\tilde{\mathcal{O}}(\log^2d)$ä¸ªæ ·æœ¬ã€‚åŒæ—¶ï¼Œé€šè¿‡å±€éƒ¨è¿æ¥ç½‘ç»œï¼ˆLCNï¼‰åˆ†æäº†æƒé‡å…±äº«å’Œå±€éƒ¨æ€§çš„å½’çº³åç½®çš„åŒºåˆ«ï¼Œå¾—å‡ºäº†å®ƒä»¬åœ¨è¡¨ç¤ºéœ€è¦æœ‰é™å¹³ç§»ç­‰å˜å’Œé«˜æ–¹å‘é€‰æ‹©æ€§çš„å‡½æ•°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚

    

    æœ¬æ–‡ç ”ç©¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„å½’çº³åç½®ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯CNNåœ¨è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å¼‚å¸¸å‡ºè‰²çš„é‡è¦é©±åŠ¨å› ç´ ã€‚æˆ‘ä»¬é¦–å…ˆåˆ†æäº†CNNçš„æ™®é€‚æ€§ï¼Œå³é€¼è¿‘è¿ç»­å‡½æ•°çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†$\mathcal{O}(\log d)$çš„æ·±åº¦å°±è¶³ä»¥å®ç°æ™®é€‚æ€§ï¼Œå…¶ä¸­$d$æ˜¯è¾“å…¥ç»´åº¦ã€‚è¿™ç›¸æ¯”äºç°æœ‰ç»“æœéœ€è¦$\Omega(d)$çš„æ·±åº¦æ˜¯ä¸€é¡¹é‡å¤§æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†ç”¨CNNå­¦ä¹ ç¨€ç–å‡½æ•°åªéœ€è¦$\tilde{\mathcal{O}}(\log^2d)$ä¸ªæ ·æœ¬ï¼Œè¡¨æ˜æ·±åº¦CNNå¯ä»¥æœ‰æ•ˆåœ°æ•æ‰é•¿ç¨‹ç¨€ç–ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜åˆ†æäº†å…±äº«æƒé‡å’Œå±€éƒ¨æ€§çš„å½’çº³åç½®ï¼Œé€šè¿‡å¯¹ç§°æ€§å¾—å‡ºç»“è®ºã€‚ä¸ºäº†åŒºåˆ†è¿™ä¸¤ç§åè§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±€éƒ¨è¿æ¥ç½‘ç»œï¼ˆLCNï¼‰å¹¶è¯æ˜äº†å®ƒä»¬åœ¨è¡¨ç¤ºéœ€è¦æœ‰é™å¹³ç§»ç­‰å˜å’Œé«˜æ–¹å‘é€‰æ‹©æ€§çš„å‡½æ•°æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºæ·±CNNçš„æˆåŠŸæä¾›äº†ç†è®ºæ´å¯ŸåŠ›ï¼ŒåŒæ—¶æ›´å¥½åœ°ç†è§£äº†å®ƒä»¬çš„å±€é™æ€§ã€‚

    In this paper, we study the inductive biases in convolutional neural networks (CNNs), which are believed to be vital drivers behind CNNs' exceptional performance on vision-like tasks. We first analyze the universality of CNNs, i.e., the ability to approximate continuous functions. We prove that a depth of $\mathcal{O}(\log d)$ is sufficient for achieving universality, where $d$ is the input dimension. This is a significant improvement over existing results that required a depth of $\Omega(d)$. We also prove that learning sparse functions with CNNs needs only $\tilde{\mathcal{O}}(\log^2d)$ samples, indicating that deep CNNs can efficiently capture long-range sparse correlations. Note that all these are achieved through a novel combination of increased network depth and the utilization of multichanneling and downsampling.  Lastly, we study the inductive biases of weight sharing and locality through the lens of symmetry. To separate two biases, we introduce locally-connected networks (LCN
    
[^46]: é€šè¿‡å­é«˜æ–¯å†…åœ¨çŸ©èŒƒå®ç°ç´§å‡‘çš„éæ¸è¿›æ¨æ–­

    Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm. (arXiv:2303.07287v1 [stat.ML])

    [http://arxiv.org/abs/2303.07287](http://arxiv.org/abs/2303.07287)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æœ€å¤§åŒ–ä¸€ç³»åˆ—å½’ä¸€åŒ–çŸ©æ¥ä½¿ç”¨å­é«˜æ–¯å†…åœ¨çŸ©èŒƒå®ç°ç´§å‡‘çš„éæ¸è¿›æ¨æ–­çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¯¼è‡´æ›´ç´§çš„Hoeffdingå­é«˜æ–¯æµ“åº¦ä¸ç­‰å¼ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å­é«˜æ–¯å›¾æ£€æŸ¥å…·æœ‰æœ‰é™æ ·æœ¬å¤§å°çš„å­é«˜æ–¯æ•°æ®ã€‚

    This paper proposes a method of achieving tight non-asymptotic inference by using sub-Gaussian intrinsic moment norm through maximizing a series of normalized moments, which can lead to tighter Hoeffding's sub-Gaussian concentration inequalities and can be checked with sub-Gaussian plot for sub-Gaussian data with a finite sample size.

    åœ¨éæ¸è¿›ç»Ÿè®¡æ¨æ–­ä¸­ï¼Œå­é«˜æ–¯åˆ†å¸ƒçš„æ–¹å·®ç±»å‹å‚æ•°èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒåŸºäºç»éªŒçŸ©ç”Ÿæˆå‡½æ•°ï¼ˆMGFï¼‰çš„ç›´æ¥ä¼°è®¡è¿™äº›å‚æ•°æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºè®®é€šè¿‡æœ€å¤§åŒ–ä¸€ç³»åˆ—å½’ä¸€åŒ–çŸ©æ¥ä½¿ç”¨å­é«˜æ–¯å†…åœ¨çŸ©èŒƒ[Buldyginå’ŒKozachenkoï¼ˆ2000ï¼‰ï¼Œå®šç†1.3]ã€‚é‡è¦çš„æ˜¯ï¼Œæ¨èçš„èŒƒæ•°ä¸ä»…å¯ä»¥æ¢å¤ç›¸åº”MGFçš„æŒ‡æ•°çŸ©ç•Œé™ï¼Œè€Œä¸”è¿˜å¯ä»¥å¯¼è‡´æ›´ç´§çš„Hoeffdingå­é«˜æ–¯æµ“åº¦ä¸ç­‰å¼ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´è§‚çš„æ–¹æ³•ï¼Œé€šè¿‡å­é«˜æ–¯å›¾æ£€æŸ¥å…·æœ‰æœ‰é™æ ·æœ¬å¤§å°çš„å­é«˜æ–¯æ•°æ®ã€‚å¯ä»¥é€šè¿‡ç®€å•çš„æ’å…¥æ–¹æ³•é²æ£’åœ°ä¼°è®¡å†…åœ¨çŸ©èŒƒæ•°ã€‚æˆ‘ä»¬çš„ç†è®ºç»“æœåº”ç”¨äºéæ¸è¿›åˆ†æï¼ŒåŒ…æ‹¬å¤šè‡‚èµŒåšæœºã€‚

    In non-asymptotic statistical inferences, variance-type parameters of sub-Gaussian distributions play a crucial role. However, direct estimation of these parameters based on the empirical moment generating function (MGF) is infeasible. To this end, we recommend using a sub-Gaussian intrinsic moment norm [Buldygin and Kozachenko (2000), Theorem 1.3] through maximizing a series of normalized moments. Importantly, the recommended norm can not only recover the exponential moment bounds for the corresponding MGFs, but also lead to tighter Hoeffding's sub-Gaussian concentration inequalities. In practice, {\color{black} we propose an intuitive way of checking sub-Gaussian data with a finite sample size by the sub-Gaussian plot}. Intrinsic moment norm can be robustly estimated via a simple plug-in approach. Our theoretical results are applied to non-asymptotic analysis, including the multi-armed bandit.
    
[^47]: æ­£äº¤å¤šé¡¹å¼é€¼è¿‘ç®—æ³•ï¼ˆOPAAï¼‰ï¼šä¸€ç§ç”¨äºä¼°è®¡æ¦‚ç‡å¯†åº¦çš„åŠŸèƒ½åˆ†ææ–¹æ³•

    Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities. (arXiv:2211.08594v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08594](http://arxiv.org/abs/2211.08594)

    OPAAæ˜¯ä¸€ç§åŠŸèƒ½åˆ†ææ–¹æ³•çš„ç®—æ³•ï¼Œé€šè¿‡æ‰¾åˆ°å¹³æ»‘çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°ä¼°è®¡å€¼ã€è®¡ç®—å½’ä¸€åŒ–æƒé‡çš„ä¼°è®¡å€¼ï¼Œå¹¶ä½¿ç”¨ç‰¹æ®Šçš„å‡½æ•°ç©ºé—´è½¬æ¢æ¥ä¼°è®¡è¯æ®ï¼Œå®ç°äº†å¹¶è¡Œè®¡ç®—çš„ä¸€æ¬¡é€šè¿‡ã€‚å®ƒé€‚ç”¨äºä¼°è®¡æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œå°¤å…¶åœ¨è´å¶æ–¯é—®é¢˜ä¸­ä¼°è®¡å½’ä¸€åŒ–æƒé‡ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ­£äº¤å¤šé¡¹å¼é€¼è¿‘ç®—æ³•ï¼ˆOPAAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯å¹¶è¡ŒåŒ–çš„ç®—æ³•ï¼Œä½¿ç”¨åŠŸèƒ½åˆ†ææ–¹æ³•ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒï¼šé¦–å…ˆï¼Œå®ƒæ‰¾åˆ°äº†æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘å‡½æ•°ä¼°è®¡ï¼Œæ— è®ºå®ƒæ˜¯å¦å½’ä¸€åŒ–ï¼›å…¶æ¬¡ï¼Œç®—æ³•æä¾›äº†å½’ä¸€åŒ–æƒé‡çš„ä¼°è®¡ï¼›ç¬¬ä¸‰ï¼Œç®—æ³•æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ–¹æ¡ˆæ¥è®¡ç®—è¿™äº›ä¼°è®¡å€¼ã€‚OPAAçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†æ˜¯å°†è”åˆåˆ†å¸ƒçš„å¹³æ–¹æ ¹è½¬åŒ–ä¸ºæˆ‘ä»¬æ„é€ çš„ç‰¹æ®Šå‡½æ•°ç©ºé—´çš„ç‰¹æ®Šå˜æ¢ã€‚é€šè¿‡è¿™ä¸ªå˜æ¢ï¼Œè¯æ®ç­‰äºè½¬æ¢å‡½æ•°çš„$L^2$èŒƒæ•°çš„å¹³æ–¹ã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡è½¬æ¢ç³»æ•°çš„å¹³æ–¹å’Œæ¥ä¼°è®¡è¯æ®ã€‚è®¡ç®—å¯ä»¥å¹¶è¡ŒåŒ–å¹¶åœ¨ä¸€æ¬¡é€šè¿‡ä¸­å®Œæˆã€‚OPAAå¯ä»¥å¹¿æ³›åº”ç”¨äºæ¦‚ç‡å¯†åº¦å‡½æ•°çš„ä¼°è®¡ã€‚åœ¨è´å¶æ–¯é—®é¢˜ä¸­ï¼Œå®ƒå¯ä»¥ç”¨äºä¼°è®¡å½’ä¸€åŒ–æƒé‡ã€‚

    We present the new Orthogonal Polynomials Approximation Algorithm (OPAA), a parallelizable algorithm that estimates probability distributions using functional analytic approach: first, it finds a smooth functional estimate of the probability distribution, whether it is normalized or not; second, the algorithm provides an estimate of the normalizing weight; and third, the algorithm proposes a new computation scheme to compute such estimates.  A core component of OPAA is a special transform of the square root of the joint distribution into a special functional space of our construct. Through this transform, the evidence is equated with the $L^2$ norm of the transformed function, squared. Hence, the evidence can be estimated by the sum of squares of the transform coefficients. Computations can be parallelized and completed in one pass.  OPAA can be applied broadly to the estimation of probability density functions. In Bayesian problems, it can be applied to estimating the normalizing weig
    
[^48]: åˆ©ç”¨ä»¿å°„æ¨¡å‹å˜æ¢çš„è¿ç§»å­¦ä¹ 

    Transfer learning with affine model transformation. (arXiv:2210.09745v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09745](http://arxiv.org/abs/2210.09745)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§å«åšä»¿å°„æ¨¡å‹è½¬ç§»çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æœŸæœ›å¹³æ–¹æŸå¤±ï¼Œå¯ä»¥é€‚åº”å„ç§ä¸åŒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºç¥ç»ç‰¹å¾æå–å™¨çš„æ–¹æ³•ã€‚å¯¹äºè¿™ä¸ªæ–¹æ³•ä¹Ÿç»™å‡ºäº†ç†è®ºä¸Šçš„è§£é‡Šã€‚

    

    ç”±äºåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹èƒ½å¤Ÿæé«˜æœºå™¨å­¦ä¹ çš„é¢„æµ‹èƒ½åŠ›ï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ä¼ ç»Ÿä¸Šï¼Œä½¿ç”¨ç»™å®šçš„æºæ¨¡å‹é›†å’Œæ¥è‡ªç›®æ ‡é¢†åŸŸçš„æ•°æ®é›†ï¼Œé€šè¿‡ç»Ÿè®¡å­¦ä¹ æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ç›®æ ‡é¢†åŸŸï¼Œå­¦ä¹ é¢†åŸŸè½¬ç§»å’Œé¢†åŸŸç‰¹å®šå› ç´ ã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨å¹¿æ³›çš„å®é™…åº”ç”¨ä¸­å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†ç¼ºä¹ç†è®ºåŸºç¡€é˜»ç¢äº†è¿›ä¸€æ­¥çš„æ–¹æ³•å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºä»¿å°„æ¨¡å‹è½¬ç§»çš„é€šç”¨ç±»åˆ«çš„è¿ç§»å­¦ä¹ å›å½’æ–¹æ³•ï¼Œéµå¾ªæœŸæœ›å¹³æ–¹æŸå¤±æœ€å°åŒ–çš„åŸåˆ™ã€‚ç»“æœè¡¨æ˜ï¼Œä»¿å°„æ¨¡å‹è½¬ç§»å¹¿æ³›åŒ…æ‹¬å„ç§ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºç¥ç»ç‰¹å¾æå–å™¨çš„æœ€å¸¸è§è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é˜æ˜äº†ä»¿å°„æ¨¡å‹è½¬ç§»çš„ç†è®ºç‰¹æ€§ã€‚

    Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. Generally, a given set of source models and a dataset from a target domain are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. While such procedurally and intuitively plausible methods have achieved great success in a wide range of real-world applications, the lack of a theoretical basis hinders further methodological development. This paper presents a general class of transfer learning regression called affine model transfer, following the principle of expected-square loss minimization. It is shown that the affine model transfer broadly encompasses various existing methods, including the most common procedure based on neural feature extractors. Furthermore, the current paper clarifies theoretical properties of the affine model transfer such 
    
[^49]: é«˜ç»´ç‚¹äº‘æ•°æ®çš„å¤šæ ·æ€§æ•£å°„å˜æ¢

    The Manifold Scattering Transform for High-Dimensional Point Cloud Data. (arXiv:2206.10078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10078](http://arxiv.org/abs/2206.10078)

    å¤šæ ·æ€§æ•£å°„å˜æ¢æ˜¯ä¸€ç§ç”¨äºé«˜ç»´ç‚¹äº‘æ•°æ®çš„æ·±åº¦ç‰¹å¾æå–å™¨ï¼Œåœ¨å®ç°ä¸Šé‡‡ç”¨äº†æ‰©æ•£æ˜ å°„ç†è®ºï¼Œæœ‰æ•ˆç”¨äºä¿¡å·åˆ†ç±»å’Œæµå½¢åˆ†ç±»ä»»åŠ¡ã€‚

    

    å¤šæ ·æ€§æ•£å°„å˜æ¢æ˜¯ä¸€ç§ç”¨äºå®šä¹‰åœ¨é»æ›¼æµå½¢ä¸Šçš„æ•°æ®çš„æ·±åº¦ç‰¹å¾æå–å™¨ã€‚å®ƒæ˜¯å°†ç±»ä¼¼äºå·ç§¯ç¥ç»ç½‘ç»œçš„æ“ä½œæ‰©å±•åˆ°ä¸€èˆ¬æµå½¢çš„æœ€æ—©çš„ä¾‹å­ä¹‹ä¸€ã€‚è¿™ä¸ªæ¨¡å‹çš„æœ€åˆå·¥ä½œä¸»è¦å…³æ³¨å…¶ç†è®ºç¨³å®šæ€§å’Œä¸å˜æ€§ç‰¹æ€§ï¼Œä½†é™¤äº†åœ¨å…·æœ‰é¢„å®šä¹‰ç½‘æ ¼çš„äºŒç»´æ›²é¢çš„æƒ…å†µä¸‹æä¾›å…¶æ•°å€¼å®ç°çš„æ–¹æ³•å¤–ï¼Œè¿˜æ²¡æœ‰æä¾›å…¶æ•°å€¼å®ç°çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ˜ å°„ç†è®ºçš„å®ç”¨æ–¹æ¡ˆï¼Œç”¨äºå°†å¤šæ ·æ€§æ•£å°„å˜æ¢åº”ç”¨äºè‡ªç„¶ç³»ç»Ÿä¸­äº§ç”Ÿçš„æ•°æ®é›†ï¼Œä¾‹å¦‚å•ç»†èƒé—ä¼ å­¦ï¼Œå…¶ä¸­æ•°æ®æ˜¯ä½œä¸ºä½ç»´æµå½¢ä¸Šçš„é«˜ç»´ç‚¹äº‘å»ºæ¨¡çš„ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºä¿¡å·åˆ†ç±»å’Œæµå½¢åˆ†ç±»ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚

    The manifold scattering transform is a deep feature extractor for data defined on a Riemannian manifold. It is one of the first examples of extending convolutional neural network-like operators to general manifolds. The initial work on this model focused primarily on its theoretical stability and invariance properties but did not provide methods for its numerical implementation except in the case of two-dimensional surfaces with predefined meshes. In this work, we present practical schemes, based on the theory of diffusion maps, for implementing the manifold scattering transform to datasets arising in naturalistic systems, such as single cell genetics, where the data is a high-dimensional point cloud modeled as lying on a low-dimensional manifold. We show that our methods are effective for signal classification and manifold classification tasks.
    
[^50]: é¢å‘å°ºåº¦æ— å…³çš„æ·±åº¦æ“ä½œå™¨ç½‘ç»œçš„æ³›åŒ–ç•Œé™

    Towards Size-Independent Generalization Bounds for Deep Operator Nets. (arXiv:2205.11359v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11359](http://arxiv.org/abs/2205.11359)

    æœ¬è®ºæ–‡ç ”ç©¶äº†æ·±åº¦æ“ä½œå™¨ç½‘ç»œçš„æ³›åŒ–ç•Œé™é—®é¢˜ï¼Œåœ¨ä¸€ç±»DeepONetsä¸­è¯æ˜äº†å®ƒä»¬çš„Rademacherå¤æ‚åº¦çš„ç•Œé™ä¸ä¼šéšç½‘ç»œå®½åº¦æ‰©å±•è€Œæ˜ç¡®å˜åŒ–ï¼Œå¹¶åˆ©ç”¨è¿™ä¸ªç»“æœå±•ç¤ºäº†å¦‚ä½•é€‰æ‹©HuberæŸå¤±æ¥è·å¾—ä¸æ˜ç¡®ä¾èµ–äºç½‘ç»œå¤§å°çš„æ³›åŒ–è¯¯å·®ç•Œé™ã€‚

    

    åœ¨æœ€è¿‘çš„æ—¶æœŸï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨åˆ†æç‰©ç†ç³»ç»Ÿæ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚åœ¨è¿™ä¸ªä¸»é¢˜ä¸­ç‰¹åˆ«æ´»è·ƒçš„é¢†åŸŸæ˜¯"ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ "ï¼Œå®ƒä¸“æ³¨äºä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ•°å€¼æ±‚è§£å¾®åˆ†æ–¹ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨è¿›åœ¨è®­ç»ƒDeepONetsæ—¶æµ‹é‡æ ·æœ¬å¤–è¯¯å·®çš„ç†è®º - è¿™æ˜¯è§£å†³PDEç³»ç»Ÿæœ€é€šç”¨çš„æ–¹æ³•ä¹‹ä¸€ã€‚é¦–å…ˆï¼Œé’ˆå¯¹ä¸€ç±»DeepONetsï¼Œæˆ‘ä»¬è¯æ˜äº†å®ƒä»¬çš„Rademacherå¤æ‚åº¦æœ‰ä¸€ä¸ªç•Œé™ï¼Œè¯¥ç•Œé™ä¸ä¼šæ˜ç¡®åœ°éšç€æ¶‰åŠçš„ç½‘ç»œå®½åº¦æ‰©å±•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç»“æœæ¥å±•ç¤ºå¦‚ä½•é€‰æ‹©HuberæŸå¤±ï¼Œä½¿å¾—å¯¹äºè¿™äº›DeepONetç±»ï¼Œèƒ½å¤Ÿè·å¾—ä¸æ˜ç¡®ä¾èµ–äºç½‘ç»œå¤§å°çš„æ³›åŒ–è¯¯å·®ç•Œé™ã€‚æˆ‘ä»¬æŒ‡å‡ºï¼Œæˆ‘ä»¬çš„ç†è®ºç»“æœé€‚ç”¨äºä»»ä½•ç›®æ ‡æ˜¯ç”±DeepONetsæ±‚è§£çš„PDEã€‚

    In recent times machine learning methods have made significant advances in becoming a useful tool for analyzing physical systems. A particularly active area in this theme has been "physics-informed machine learning" which focuses on using neural nets for numerically solving differential equations. In this work, we aim to advance the theory of measuring out-of-sample error while training DeepONets -- which is among the most versatile ways to solve PDE systems in one-shot.  Firstly, for a class of DeepONets, we prove a bound on their Rademacher complexity which does not explicitly scale with the width of the nets involved. Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONet classes generalization error bounds can be obtained that have no explicit dependence on the size of the nets. We note that our theoretical results apply to any PDE being targeted to be solved by DeepONets.
    
[^51]: åœ¨å¼ é‡ä¸»æˆåˆ†åˆ†æå’Œç›¸å…³é—®é¢˜ä¸­çš„ç»Ÿè®¡è®¡ç®—æƒè¡¡ï¼šé€šè¿‡é€šä¿¡å¤æ‚åº¦

    Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2204.07526](http://arxiv.org/abs/2204.07526)

    æœ¬æ–‡é€šè¿‡é€šä¿¡å¤æ‚åº¦æ¨å¯¼å‡ºäº†å¯¹äºå†…å­˜å—é™ç®—æ³•åœ¨å¼ é‡ä¸»æˆåˆ†åˆ†æä¸­çš„è®¡ç®—ä¸‹ç•Œï¼Œå¹¶ä¸”æŒ‡å®šäº†è§£å†³è¯¥é—®é¢˜çš„ç®—æ³•å¿…é¡»åœ¨æ•°æ®æ ·æœ¬ç»è¿‡æ¬¡æ•°ã€æ ·æœ¬å¤§å°å’Œæ‰€éœ€å†…å­˜ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è¿™äº›ä¸‹ç•Œæš—ç¤ºäº†è®¸å¤šå¸¸ç”¨ç®—æ³•åœ¨æ ·æœ¬å¤§å°ä¸å¤Ÿå¤§æ—¶éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°ã€‚

    

    å¼ é‡ä¸»æˆåˆ†åˆ†ææ˜¯Montanariå’ŒRichardå¼•å…¥çš„ä¸€ç§é£æ ¼åŒ–ç»Ÿè®¡æ¨æ–­é—®é¢˜ï¼Œç”¨äºç ”ç©¶ä»é«˜é˜¶çŸ©å¼ é‡ä¸­ä¼°è®¡æœªçŸ¥å‚æ•°çš„è®¡ç®—éš¾åº¦ã€‚ä¸å…¶çŸ©é˜µå¯¹åº”é—®é¢˜ä¸åŒï¼Œå¼ é‡ä¸»æˆåˆ†åˆ†æå±•ç°äº†ä¸€ç§ç»Ÿè®¡è®¡ç®—å·®è·ï¼Œå³åœ¨æ ·æœ¬å¤§å°èŒƒå›´å†…ï¼Œé—®é¢˜åœ¨ä¿¡æ¯ç†è®ºä¸Šå¯è§£ï¼Œä½†è¢«è®¤ä¸ºåœ¨è®¡ç®—ä¸Šè¾ƒå›°éš¾ã€‚æœ¬æ–‡åˆ©ç”¨é€šä¿¡å¤æ‚åº¦æ¨å¯¼å‡ºäº†å†…å­˜å—é™ç®—æ³•åœ¨å¼ é‡ä¸»æˆåˆ†åˆ†æä¸­çš„è®¡ç®—ä¸‹ç•Œã€‚è¿™äº›ä¸‹ç•ŒæŒ‡å®šäº†æˆåŠŸè§£å†³å¼ é‡ä¸»æˆåˆ†åˆ†æçš„ä»»ä½•ç®—æ³•åœ¨æ•°æ®æ ·æœ¬ç»è¿‡æ¬¡æ•°ã€æ ·æœ¬å¤§å°å’Œæ‰€éœ€å†…å­˜ä¹‹é—´çš„æƒè¡¡ã€‚å°½ç®¡ä¸‹ç•Œä¸èƒ½æ’é™¤å¤šé¡¹å¼æ—¶é—´ç®—æ³•ï¼Œä½†å®ƒä»¬æ„å‘³ç€è®¸å¤šå¸¸ç”¨çš„ç®—æ³•ï¼Œå¦‚æ¢¯åº¦ä¸‹é™å’Œå¹‚è¿­ä»£æ–¹æ³•ï¼Œåœ¨æ ·æœ¬å¤§å°ä¸å¤Ÿå¤§æ—¶å¿…é¡»æœ‰æ›´é«˜çš„è¿­ä»£æ¬¡æ•°ã€‚ç±»ä¼¼çš„ä¸‹ç•Œè¿˜å¯ä»¥ä½¿ç”¨é€šä¿¡å¤æ‚åº¦è·å¾—ã€‚

    Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtai
    
[^52]: ConcordanceæŒ‡æ•°çš„åˆ†è§£: å¯¹ç”Ÿå­˜é¢„æµ‹æ¨¡å‹æ·±å…¥ç†è§£çš„åº¦é‡æ–¹æ³•

    The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models. (arXiv:2203.00144v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00144](http://arxiv.org/abs/2203.00144)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ConcordanceæŒ‡æ•°åˆ†è§£æˆä¸¤ä¸ªéƒ¨åˆ†çš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°ç”Ÿå­˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥åˆ†è§£æ–¹æ³•å¯ä»¥è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†æï¼Œæ­ç¤ºä¸åŒé¢„æµ‹æ–¹æ³•ä¹‹é—´çš„ä¼˜åŠ£ã€‚å®éªŒè¯æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨äº†è§‚æµ‹äº‹ä»¶ã€‚

    

    ConcordanceæŒ‡æ•°ï¼ˆC-indexï¼‰æ˜¯ç”Ÿå­˜åˆ†æä¸­å¸¸ç”¨çš„è¯„ä¼°é¢„æµ‹æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†C-indexåˆ†è§£ä¸ºä¸¤ä¸ªæ•°é‡çš„åŠ æƒè°ƒå’Œå¹³å‡çš„æ–¹æ³•ï¼šä¸€ä¸ªç”¨äºæ¯”è¾ƒè§‚æµ‹äº‹ä»¶ä¸å…¶ä»–è§‚æµ‹äº‹ä»¶çš„æ’åºï¼Œå¦ä¸€ä¸ªç”¨äºæ¯”è¾ƒè§‚æµ‹äº‹ä»¶ä¸è¢«å‰ªè¾‘çš„æƒ…å†µçš„æ’åºã€‚è¿™ç§åˆ†è§£æ–¹æ³•å¯ä»¥å¯¹ä¸åŒç”Ÿå­˜é¢„æµ‹æ–¹æ³•ä¹‹é—´çš„ä¼˜åŠ£è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†æã€‚é€šè¿‡ä¸ç»å…¸æ¨¡å‹å’Œæœ€å…ˆè¿›æ–¹æ³•çš„åŸºå‡†æ¯”è¾ƒï¼Œä»¥åŠæœ¬æ–‡æå‡ºçš„æ–°çš„å˜åˆ†ç”Ÿæˆç¥ç»ç½‘ç»œæ–¹æ³•ï¼ˆSurVEDï¼‰ï¼Œå±•ç¤ºäº†è¯¥åˆ†è§£æ–¹æ³•çš„å®ç”¨æ€§ã€‚ä½¿ç”¨å››ä¸ªå…¬å¼€å¯ç”¨çš„å…·æœ‰ä¸åŒå‰ªè¾‘æ°´å¹³çš„æ•°æ®é›†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡C-indexåˆ†è§£å’Œåˆæˆå‰ªè¾‘ï¼Œåˆ†æç»“æœæ˜¾ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨äº†è§‚æµ‹äº‹ä»¶ã€‚

    The Concordance Index (C-index) is a commonly used metric in Survival Analysis for evaluating the performance of a prediction model. In this paper, we propose a decomposition of the C-index into a weighted harmonic mean of two quantities: one for ranking observed events versus other observed events, and the other for ranking observed events versus censored cases. This decomposition enables a finer-grained analysis of the relative strengths and weaknesses between different survival prediction methods. The usefulness of this decomposition is demonstrated through benchmark comparisons against classical models and state-of-the-art methods, together with the new variational generative neural-network-based method (SurVED) proposed in this paper. The performance of the models is assessed using four publicly available datasets with varying levels of censoring. Using the C-index decomposition and synthetic censoring, the analysis shows that deep learning models utilize the observed events more 
    
[^53]: é«˜ç»´æ¨æ–­ä¸æ¨¡æ‹Ÿé©¬å°”å¯å¤«éšæœºåœºçš„FDRæ§åˆ¶

    High-dimensional Inference and FDR Control for Simulated Markov Random Fields. (arXiv:2202.05612v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05612](http://arxiv.org/abs/2202.05612)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨é«˜ç»´èƒŒæ™¯ä¸‹è¿›è¡Œæ¨¡æ‹Ÿé©¬å°”å¯å¤«éšæœºåœºç»Ÿè®¡æ¨æ–­çš„æ–¹æ³•ï¼Œå®ç°äº†ä¸€è‡´æ€§ï¼Œå¹¶æ„å»ºäº†ä¸¤ç§è¯¯å‘ç°ç‡æ§åˆ¶ç¨‹åºã€‚

    

    åœ¨å„ç§ç§‘å­¦é¢†åŸŸä¸­ï¼Œç¡®å®šä¸å“åº”å˜é‡ç›¸å…³çš„é‡è¦ç‰¹å¾æ˜¯ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ã€‚æœ¬æ–‡æ¢è®¨é«˜ç»´èƒŒæ™¯ä¸‹æ¨¡æ‹Ÿé©¬å°”å¯å¤«éšæœºåœºçš„ç»Ÿè®¡æ¨æ–­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMCMC-MLEï¼‰ä¸å¼¹æ€§ç½‘æ­£åˆ™åŒ–çš„æ–¹æ³•ã€‚åœ¨MCMCæ–¹æ³•çš„æ¸©å’Œæ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬çš„ç½šæ¬¾MCMC-MLEæ–¹æ³•å®ç°äº†$\ell_{1}$ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå»ç›¸å…³çš„å¾—åˆ†æ£€éªŒï¼Œç¡®å®šå…¶æ¸è¿‘æ­£æ€æ€§å’Œä¸€æ­¥ä¼°è®¡é‡çš„æ¸è¿‘æ­£æ€æ€§ï¼Œä»¥åŠç›¸åº”çš„ç½®ä¿¡åŒºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºpå€¼å’Œeå€¼çš„æ¸è¿‘è¡Œä¸ºæ„å»ºäº†ä¸¤ç§è¯¯å‘ç°ç‡æ§åˆ¶ç¨‹åºã€‚å…¨é¢çš„æ•°å€¼æ¨¡æ‹ŸéªŒè¯äº†æ‰€ææ–¹æ³•çš„ç†è®ºæœ‰æ•ˆæ€§ã€‚

    Identifying important features linked to a response variable is a fundamental task in various scientific domains. This article explores statistical inference for simulated Markov random fields in high-dimensional settings. We introduce a methodology based on Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMC-MLE) with Elastic-net regularization. Under mild conditions on the MCMC method, our penalized MCMC-MLE method achieves $\ell_{1}$-consistency. We propose a decorrelated score test, establishing both its asymptotic normality and that of a one-step estimator, along with the associated confidence interval. Furthermore, we construct two false discovery rate control procedures via the asymptotic behaviors for both p-values and e-values. Comprehensive numerical simulations confirm the theoretical validity of the proposed methods.
    
[^54]: Wavelet Networks: ä»åŸå§‹æ—¶é—´åºåˆ—å­¦ä¹ å°ºåº¦å¹³ç§»ç­‰å˜æ€§çš„å­¦ä¹ ç½‘ç»œ

    Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series. (arXiv:2006.05259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05259](http://arxiv.org/abs/2006.05259)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ—¶é—´åºåˆ—å›ºæœ‰å¯¹ç§°æ€§æ„å»ºçš„å°æ³¢ç½‘ç»œï¼Œå…¶è¡¨ç°å‡ºåµŒå¥—çš„éçº¿æ€§å°æ³¢æ ·çš„æ—¶é¢‘å˜æ¢ï¼Œå®éªŒè¯æ˜å…¶åœ¨åŸå§‹æ³¢å½¢ä¸Šä¼˜äºä¼ ç»Ÿçš„CNNã€‚

    

    åˆ©ç”¨ç‰¹å®šæ•°æ®é¢†åŸŸä¸­å›ºæœ‰çš„å¯¹ç§°æ€§æ„å»ºç­‰å˜æ€§ç¥ç»ç½‘ç»œï¼Œå¯æ˜¾è‘—æé«˜æ•°æ®æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶é›†ä¸­åœ¨å¹³é¢å’Œä½“ç§¯æ•°æ®ä¸­äº§ç”Ÿçš„å¯¹ç§°æ€§ä¸Šï¼Œè€ŒæŠŠä¸€ä¸ªå…³é”®çš„æ•°æ®æºåŸºæœ¬æœªå¼€å‘ï¼šæ—¶é—´åºåˆ—ã€‚æœ¬æ–‡é€šè¿‡åˆ©ç”¨æ—¶é—´åºåˆ—çš„å›ºæœ‰å¯¹ç§°æ€§æ¥æ„å»ºç­‰å˜æ€§ç¥ç»ç½‘ç»œï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬ç¡®è®¤äº†ä¸¤ä¸ªæ ¸å¿ƒå¯¹ç§°æ€§ï¼šå°ºåº¦å’Œå¹³ç§»ï¼Œå¹¶æ„å»ºäº†é€‚ç”¨äºæ—¶é—´åºåˆ—å­¦ä¹ çš„å°ºåº¦å¹³ç§»ç­‰å˜æ€§ç¥ç»ç½‘ç»œã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å°ºåº¦å¹³ç§»ç­‰å˜æ€§æ˜ å°„ä¸å°æ³¢å˜æ¢å…·æœ‰å¾ˆå¼ºçš„ç›¸ä¼¼æ€§ã€‚å—åˆ°è¿™ç§ç›¸ä¼¼æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„ç½‘ç»œç§°ä¸ºå°æ³¢ç½‘ç»œï¼Œå¹¶å±•ç¤ºå®ƒä»¬æ‰§è¡ŒåµŒå¥—çš„éçº¿æ€§å°æ³¢æ ·çš„æ—¶é¢‘å˜æ¢ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œå°æ³¢ç½‘ç»œåœ¨åŸå§‹æ³¢å½¢ä¸Šä¼˜äºä¼ ç»Ÿçš„CNNã€‚

    Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: time-series. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the wavelet transform. Inspired by this resemblance, we term our networks Wavelet Networks, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms
    
[^55]: ç”¨äºåœºæ™¯åˆ†æçš„åŒè´¨ Ising æ¨¡å‹çš„å¿«é€Ÿè¿‘ä¼¼è®¡ç®—æ–¹æ³•

    Fast approximations in the homogeneous Ising model for use in scene analysis. (arXiv:1712.02195v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/1712.02195](http://arxiv.org/abs/1712.02195)

    æœ¬æ–‡æä¾›äº†ä¸€ç§å¿«é€Ÿè¿‘ä¼¼è®¡ç®—åŒè´¨ Ising æ¨¡å‹ä¸­é‡è¦é‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„è¡¨ç°åœ¨æ¨¡æ‹Ÿç ”ç©¶ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¯ç”¨äºåœºæ™¯åˆ†æã€‚

    

    Ising æ¨¡å‹åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½å¾ˆé‡è¦ï¼Œä½†å…¶å½’ä¸€åŒ–å¸¸æ•°ã€æ´»åŠ¨é¡¶ç‚¹æ•°çš„å¹³å‡å€¼å’Œè‡ªæ—‹ç›¸äº’ä½œç”¨çš„å‡å€¼éš¾ä»¥è®¡ç®—ã€‚æˆ‘ä»¬æä¾›äº†å‡†ç¡®çš„è¿‘ä¼¼å€¼ï¼Œä½¿å¾—åœ¨åŒè´¨æƒ…å†µä¸‹å¯ä»¥æ•°å€¼è®¡ç®—è¿™äº›é‡ã€‚æ¨¡æ‹Ÿç ”ç©¶è¡¨æ˜ï¼Œä¸é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½è‰¯å¥½ï¼Œä¸”æ‰€éœ€æ—¶é—´åªæ˜¯é‚£äº›éšæœºæ–¹æ³•çš„ä¸€å°éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„è¿‘ä¼¼å€¼åœ¨æ‰§è¡ŒåŠŸèƒ½ç£å…±æŒ¯æ¿€æ´»æ£€æµ‹å®éªŒçš„è´å¶æ–¯æ¨æ–­ä»¥åŠæ¤ç‰©ç”Ÿäº§ä¸­å¹´å¢é‡ç©ºé—´å›¾æ¡ˆçš„å„å‘å¼‚æ€§çš„ä¼¼ç„¶æ¯”æ£€éªŒä¸­å¾—åˆ°äº†ä½“ç°ã€‚

    The Ising model is important in statistical modeling and inference in many applications, however its normalizing constant, mean number of active vertices and mean spin interaction are intractable to compute. We provide accurate approximations that make it possible to numerically calculate these quantities in the homogeneous case. Simulation studies indicate good performance when compared to Markov Chain Monte Carlo methods and at a tiny fraction of the time taken by those stochastic approaches. The value of our approximations is illustrated in performing Bayesian inference in a functional Magnetic Resonance Imaging activation detection experiment, and also in likelihood ratio testing for anisotropy in the spatial patterns of yearly increases in pistachio tree yields.
    

