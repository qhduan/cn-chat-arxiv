{
    "title": "Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks",
    "abstract": "arXiv:2404.01932v1 Announce Type: cross  Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate ",
    "link": "https://arxiv.org/abs/2404.01932",
    "context": "Title: Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks\nAbstract: arXiv:2404.01932v1 Announce Type: cross  Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate ",
    "path": "papers/24/04/2404.01932.json",
    "total_tokens": 703,
    "translated_title": "跨越语言、视觉和行动：多模态VAE在机器人操作任务中的应用",
    "translated_abstract": "在这项工作中，我们关注机器人操作领域中无监督的视觉-语言-动作映射。我们探讨了多模态变分自动编码器（VAE）在模拟环境中如何被应用于无监督机器人操作任务中，并提出了一个改进模型性能的模型不变式训练方法，可以使模拟器中的模型性能提高高达55%。",
    "tldr": "本研究探索了多模态VAE如何在无监督机器人操作任务中实现，提出了一个新的模型训练方法，可以使模拟器中的模型性能提高55%。",
    "en_tdlr": "This study explores the implementation of multimodal VAEs in unsupervised robotic manipulation tasks and proposes a novel model training method that improves model performance in a simulator by up to 55%."
}