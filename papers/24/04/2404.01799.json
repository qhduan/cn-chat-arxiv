{
    "title": "PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency",
    "abstract": "arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses ",
    "link": "https://arxiv.org/abs/2404.01799",
    "context": "Title: PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency\nAbstract: arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses ",
    "path": "papers/24/04/2404.01799.json",
    "total_tokens": 947,
    "translated_title": "PATCH -- 大型语言模型的心理测量辅助基准测试：数学能力的案例研究",
    "translated_abstract": "许多现有的大型（多模态）语言模型（LLMs）基准测试着重于衡量LLMs的学术能力，通常也对比较模型性能与人类考试者感兴趣。尽管这些基准测试对LLMs的发展至关重要，但它们存在一些限制，包括有问题的测量质量（例如，它们是否以可靠的方式衡量所需的内容？）、缺乏项目级别的质量评估（例如，有些项目是否比其他更重要或更困难？）以及人类人口参照模糊（例如，模型可以与谁进行比较？）。为了应对这些挑战，我们提出利用心理测量学领域的知识——一门致力于测量潜在变量如学术能力的领域——来进行LLMs基准测试的心理测量辅助方法。我们的主要贡献有三点。首先，我们介绍了PATCH：一种用于大型语言模型的心理测量辅助基准测试的新框架。",
    "tldr": "该论文提出了一种新的框架PATCH，用于将心理测量领域的知识整合到大型语言模型的基准测试中，以解决现有基准测试存在的测量质量、项目级别评估和参考人群等问题。",
    "en_tdlr": "This paper introduces a new framework, PATCH, to integrate knowledge from the field of psychometrics into benchmarking of large language models to address issues in measurement quality, item-level assessment, and human population reference in existing benchmarks."
}