{
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "abstract": "arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we ",
    "link": "https://arxiv.org/abs/2404.02806",
    "context": "Title: The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers\nAbstract: arXiv:2404.02806v1 Announce Type: cross  Abstract: Evaluation of large language models (LLMs) for code has primarily relied on static benchmarks, including HumanEval (Chen et al., 2021), which measure the ability of LLMs to generate complete code that passes unit tests. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks translate to gains in programmer productivity when coding with LLMs, including time spent coding. In addition to static benchmarks, we investigate the utility of preference metrics that might be used as proxies to measure LLM helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we ",
    "path": "papers/24/04/2404.02806.json",
    "total_tokens": 877,
    "translated_title": "RealHumanEval: 评估大型语言模型支持程序员的能力",
    "translated_abstract": "大型语言模型（LLMs）的评估主要依赖于静态基准，包括HumanEval（Chen等，2021），这些基准用于衡量LLMs生成通过单元测试的完整代码的能力。随着LLMs越来越多地被用作程序员助手，我们研究现有基准上的增益是否能转化为使用LLMs编码时程序员生产力的提升，包括编码所花费的时间。除了静态基准，我们还研究了可能用作度量LLM帮助性代理的偏好度量的实用性，例如代码接受或复制率。为此，我们引入了RealHumanEval，这是一个用于衡量LLMs辅助程序员的能力的网络界面，可以通过自动完成或聊天支持。我们进行了一个用户研究（N = 213），使用RealHumanEval，其中用户与六个基础模型性能各异的LLMs进行交互。尽管静态基准没有包含人为干预，我们...",
    "tldr": "评估了大型语言模型在支持程序员方面的能力，引入了RealHumanEval作为衡量其帮助性的界面，并探讨了其对程序员生产力的影响。",
    "en_tdlr": "Evaluated the ability of large language models to support programmers, introduced RealHumanEval as an interface to measure their helpfulness, and explored their impact on programmer productivity."
}