{
    "title": "Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting",
    "abstract": "Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro",
    "link": "https://arxiv.org/abs/2402.06638",
    "context": "Title: Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting\nAbstract: Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro",
    "path": "papers/24/02/2402.06638.json",
    "total_tokens": 871,
    "translated_title": "使用注意力联合聚合的Transformer模型在时间序列股票预测中的应用",
    "translated_abstract": "近期在自然语言处理（NLP）和计算机视觉（CV）领域，Transformer模型的创新展示了其优越的性能。Transformer模型具备捕捉序列数据中长距离依赖关系和相互作用的能力，因此在时间序列建模领域引起了极大的兴趣，并广泛地应用于许多时间序列应用中。然而，在应用到时间序列预测中，尽管有希望的结果，但Transformer模型的适应性仍然存在限制。与NLP和CV中的挑战相比，时间序列问题不仅涉及到输入序列中的顺序或时间依赖性的复杂性，还需要考虑趋势、水平和季节性信息，这些信息对于决策非常重要。传统的训练方案在使用Transformer模型进行预测任务时存在过拟合、数据稀缺和隐私问题等不足之处。本文中，我们提出了一种使用注意力联合聚合的Transformer模型，用于解决时间序列股票预测问题。",
    "tldr": "本文提出了一种使用注意力联合聚合的Transformer模型，用于时间序列股票预测，旨在解决传统训练方案中存在的过拟合、数据稀缺和隐私问题。",
    "en_tdlr": "This paper proposes a Transformer model with attentive federated aggregation for time series stock forecasting, aiming to address the issues of overfitting, data scarcity, and privacy in traditional training schemes."
}