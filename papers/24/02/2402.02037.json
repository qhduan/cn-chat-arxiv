{
    "title": "EffiBench: Benchmarking the Efficiency of Automatically Generated Code",
    "abstract": "Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne",
    "link": "https://arxiv.org/abs/2402.02037",
    "context": "Title: EffiBench: Benchmarking the Efficiency of Automatically Generated Code\nAbstract: Code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. Although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. This paper presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution. With EffiBench, we empirically examine the capability of 21 Large Language Models (13 open-sourced and 8 closed-sourced) in generating efficient code. The results demonstrate that GPT-4-turbo generates the most efficient code, significantly outperforming Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5. Ne",
    "path": "papers/24/02/2402.02037.json",
    "total_tokens": 852,
    "translated_title": "EffiBench:评估自动生成代码的效率的基准测试",
    "translated_abstract": "代码生成模型在辅助软件开发方面变得越来越重要，可以帮助完成代码补全、调试和代码转换等任务。尽管当前的研究已经深入研究了代码生成模型生成的正确性，但生成代码的效率这一重要方面常常被忽视。本文提出了EffiBench，一个包含1,000个效率关键的编码问题的基准测试，用于评估代码生成模型生成的代码的效率。EffiBench包含了一系列多样化的LeetCode编码问题，每个问题都与一个可执行的人工编写的典型解决方案配对。通过EffiBench，我们在实践中考察了21种大型语言模型（其中13种是开源的，8种是闭源的）在生成高效代码方面的能力。结果表明，GPT-4-turbo生成的代码最高效，明显优于Palm-2-chat-bison、Claude-instant-1、Gemini-pro、GPT-4和GPT-3.5。",
    "tldr": "本文提出了EffiBench基准测试，用于评估代码生成模型生成的代码的效率。实验证明，GPT-4-turbo生成的代码最高效。",
    "en_tdlr": "This paper introduces EffiBench, a benchmark for evaluating the efficiency of code generated by code generation models. The results show that GPT-4-turbo produces the most efficient code."
}