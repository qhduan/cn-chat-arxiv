{
    "title": "Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins",
    "abstract": "Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme t",
    "link": "https://arxiv.org/abs/2402.08421",
    "context": "Title: Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins\nAbstract: Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme t",
    "path": "papers/24/02/2402.08421.json",
    "total_tokens": 880,
    "translated_title": "保守和风险意识的离线多智能体强化学习在数字孪生中的应用",
    "translated_abstract": "数字孪生（DT）平台被越来越认为是控制、优化和监控诸如下一代无线网络之类的复杂工程系统的有希望技术。采用DT解决方案面临的一个重要挑战是它们依赖于离线收集的数据，缺乏对物理环境的直接访问。这一限制在多智能体系统中尤为严重，因为传统的多智能体强化学习（MARL）需要与环境进行在线互动。将在线MARL方案直接应用于离线环境通常会因有限数据的认识不确定性而失败。在这项工作中，我们提出了一种用于基于DT的无线网络的离线MARL方案，它整合了分布式强化学习（distributional RL）和保守Q学习，以应对环境固有的案例性不确定性和有限数据引起的认识不确定性。为了进一步利用离线数据，我们改编了所提出的方案。",
    "tldr": "本研究提出了一种应用于数字孪生的离线多智能体强化学习方案，通过整合分布式强化学习和保守Q学习来解决环境的不确定性和有限数据带来的认识不确定性。",
    "en_tdlr": "This paper proposes an offline multi-agent reinforcement learning scheme for digital twins, which integrates distributional RL and conservative Q-learning to address the uncertainty of the environment and the limited data uncertainty."
}