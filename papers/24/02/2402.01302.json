{
    "title": "A Unified Framework for Gradient-based Clustering of Distributed Data",
    "abstract": "We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by $\\rho \\geq 1$, controling the proximity of users' center estimates, with $\\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $",
    "link": "https://rss.arxiv.org/abs/2402.01302",
    "context": "Title: A Unified Framework for Gradient-based Clustering of Distributed Data\nAbstract: We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by $\\rho \\geq 1$, controling the proximity of users' center estimates, with $\\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $",
    "path": "papers/24/02/2402.01302.json",
    "total_tokens": 983,
    "translated_title": "梯度聚类分布式数据的统一框架",
    "translated_abstract": "我们开发了一族分布式聚类算法，可以在用户网络中工作。在提出的场景中，用户包含一个本地数据集，并且只与其直接邻居进行通信，目标是寻找完整数据的聚类。所提出的家族称为分布式梯度聚类（DGC-$\\mathcal{F}_\\rho$），由参数化的$\\rho\\geq1$确定，控制用户中心估计的接近程度，而$\\mathcal{F}$确定聚类损失。针对流行的聚类损失如$K$均值和Huber损失，DGC-$\\mathcal{F}_\\rho$产生了新的分布式聚类算法DGC-KM$_\\rho$和DGC-HL$_\\rho$，而基于逻辑函数的新型聚类损失导致了DGC-LL$_\\rho$。我们提供了统一的分析并建立了几个强结果，在温和的假设下。首先，方法生成的中心序列在任何中心初始化和$...",
    "tldr": "这是一篇关于梯度聚类分布式数据的统一框架的论文，作者提出了一族分布式聚类算法，可以在用户网络中工作。通过控制用户中心估计的接近程度和定义聚类损失函数，这些算法适用于不同的聚类任务。在提供了统一分析和几个强结果的基础上，这些算法都表现出了良好的收敛性和可行性。"
}