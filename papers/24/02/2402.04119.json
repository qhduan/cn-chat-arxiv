{
    "title": "Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science",
    "abstract": "Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by locali",
    "link": "https://arxiv.org/abs/2402.04119",
    "context": "Title: Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science\nAbstract: Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by locali",
    "path": "papers/24/02/2402.04119.json",
    "total_tokens": 925,
    "translated_title": "科学语言建模：分子科学中大型语言模型的定量评估",
    "translated_abstract": "高效的分子建模和设计对于发现和探索新型分子至关重要，而深度学习方法的引入在这个领域中产生了革命性的影响。特别是，大型语言模型（LLM）以自然语言处理（NLP）的视角为科学问题提供了一种新的方法，引入了一种名为科学语言建模（SLM）的研究范式。然而，仍然存在两个关键问题：如何量化模型与数据模态之间的匹配以及如何识别模型的知识学习偏好。为了解决这些挑战，我们提出了一个名为ChEBI-20-MM的多模态基准，并进行了1263个实验，评估了模型与数据模态的兼容性和知识获取能力。通过模态转移概率矩阵，我们为任务提供了最合适的模态的见解。此外，我们还引入了一种统计可解释的方法，通过本地化来发现特定上下文的知识映射。",
    "tldr": "本研究提出了一种科学语言建模（SLM）的新方法，通过大型语言模型（LLM）来解决分子建模和设计中的挑战。通过多模态基准和实验评估，我们提供了关于模型与数据模态匹配的量化信息，同时也揭示了模型的知识学习偏好。",
    "en_tdlr": "This study proposes a novel approach called scientific language modeling (SLM) to address challenges in molecular modeling and design by utilizing large language models (LLMs). Through a multi-modal benchmark and experiments, the study quantifies the match between model and data modalities and explores the knowledge-learning preferences of the models."
}