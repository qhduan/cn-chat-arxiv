{
    "title": "Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])",
    "abstract": "Consider the supervised learning setting where the goal is to learn to predict labels $\\mathbf y$ given points $\\mathbf x$ from a distribution. An \\textit{omnipredictor} for a class $\\mathcal L$ of loss functions and a class $\\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\\mathcal C$ for every loss in $\\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\\mathbf y \\in \\{0, 1\\}$, but much less is known about the regression setting where $\\mathbf y \\in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \\textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx",
    "link": "http://arxiv.org/abs/2401.14645",
    "context": "Title: Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])\nAbstract: Consider the supervised learning setting where the goal is to learn to predict labels $\\mathbf y$ given points $\\mathbf x$ from a distribution. An \\textit{omnipredictor} for a class $\\mathcal L$ of loss functions and a class $\\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\\mathcal C$ for every loss in $\\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\\mathbf y \\in \\{0, 1\\}$, but much less is known about the regression setting where $\\mathbf y \\in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \\textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx",
    "path": "papers/24/01/2401.14645.json",
    "total_tokens": 888,
    "translated_title": "å›å½’é—®é¢˜çš„å…¨èƒ½é¢„æµ‹å™¨ä¸å‡¸å‡½æ•°çš„è¿‘ä¼¼ç§©",
    "translated_abstract": "è€ƒè™‘åœ¨ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­ï¼Œç›®æ ‡æ˜¯å­¦ä¹ é€šè¿‡ç»™å®šåˆ†å¸ƒä¸­çš„ç‚¹ğ±æ¥é¢„æµ‹æ ‡ç­¾ğ²ã€‚å¯¹äºæŸå¤±å‡½æ•°ç±»ğ’€å’Œå‡è®¾ç±»ğ’ï¼Œå…¨èƒ½é¢„æµ‹å™¨æ˜¯ä¸€ç§é¢„æµ‹å™¨ï¼Œå…¶é¢„æµ‹çš„æŸå¤±å°äºğ’€ä¸­ä»»æ„æŸå¤±ä¸‹çš„æœ€ä½³å‡è®¾ã€‚è‡ªä»å¼•å…¥è¿™ä¸ªæ¦‚å¿µçš„[GKR+21]çš„å·¥ä½œä»¥æ¥ï¼Œåœ¨äºŒåˆ†ç±»æ ‡ç­¾çš„è®¾ç½®ä¸‹ï¼Œå³ğ²âˆˆ{0,1}ï¼Œå·²ç»æœ‰å¤§é‡çš„å·¥ä½œï¼Œä½†æ˜¯å¯¹äºè¿ç»­æ ‡ç­¾ğ²âˆˆ[0,1]çš„å›å½’è®¾ç½®å´çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬çš„ä¸»è¦æ¦‚å¿µè´¡çŒ®æ˜¯å…³äºä¸€ç±»æŸå¤±å‡½æ•°çš„æŸå¤±æœ€å°åŒ–çš„å……åˆ†ç»Ÿè®¡é‡çš„æ¦‚å¿µï¼šè¿™æ˜¯ä¸€ç»„å…³äºåˆ†å¸ƒçš„ç»Ÿè®¡é‡ï¼Œé€šè¿‡äº†è§£å®ƒä»¬å¯ä»¥é‡‡å–æœ€å°åŒ–ä»»ä½•æŸå¤±çš„æœŸæœ›æŸå¤±çš„è¡ŒåŠ¨ã€‚å……åˆ†ç»Ÿè®¡é‡çš„æ¦‚å¿µç›´æ¥ç›¸å…³åˆ°å‡¸å‡½æ•°è¿‘ä¼¼çš„æ¦‚å¿µä¸Šã€‚",
    "tldr": "å…¨èƒ½é¢„æµ‹å™¨æ˜¯ä¸€ç§é¢„æµ‹å™¨ï¼Œå…¶é¢„æµ‹çš„æŸå¤±å°äºä»»æ„æŸå¤±ä¸‹çš„æœ€ä½³å‡è®¾ã€‚æˆ‘ä»¬æå‡ºäº†å…³äºæŸå¤±å‡½æ•°çš„å……åˆ†ç»Ÿè®¡é‡çš„æ¦‚å¿µï¼Œå¯ä»¥ç”¨æ¥æœ€å°åŒ–ä»»ä½•æŸå¤±çš„æœŸæœ›æŸå¤±ã€‚"
}