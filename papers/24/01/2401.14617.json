{
    "title": "A Systematic Literature Review on Explainability for Machine/Deep Learning-based Software Engineering Research. (arXiv:2401.14617v1 [cs.SE])",
    "abstract": "The remarkable achievements of Artificial Intelligence (AI) algorithms, particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their extensive deployment across multiple sectors, including Software Engineering (SE). However, due to their black-box nature, these promising AI-driven SE models are still far from being deployed in practice. This lack of explainability poses unwanted risks for their applications in critical tasks, such as vulnerability detection, where decision-making transparency is of paramount importance. This paper endeavors to elucidate this interdisciplinary domain by presenting a systematic literature review of approaches that aim to improve the explainability of AI models within the context of SE. The review canvasses work appearing in the most prominent SE & AI conferences and journals, and spans 63 papers across 21 unique SE tasks. Based on three key Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI techniques have shown s",
    "link": "http://arxiv.org/abs/2401.14617",
    "context": "Title: A Systematic Literature Review on Explainability for Machine/Deep Learning-based Software Engineering Research. (arXiv:2401.14617v1 [cs.SE])\nAbstract: The remarkable achievements of Artificial Intelligence (AI) algorithms, particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their extensive deployment across multiple sectors, including Software Engineering (SE). However, due to their black-box nature, these promising AI-driven SE models are still far from being deployed in practice. This lack of explainability poses unwanted risks for their applications in critical tasks, such as vulnerability detection, where decision-making transparency is of paramount importance. This paper endeavors to elucidate this interdisciplinary domain by presenting a systematic literature review of approaches that aim to improve the explainability of AI models within the context of SE. The review canvasses work appearing in the most prominent SE & AI conferences and journals, and spans 63 papers across 21 unique SE tasks. Based on three key Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI techniques have shown s",
    "path": "papers/24/01/2401.14617.json",
    "total_tokens": 934,
    "translated_title": "《基于机器/深度学习的软件工程研究中可解释性的系统文献综述》",
    "translated_abstract": "人工智能算法，特别是机器学习和深度学习，在软件工程领域取得了显著的成就，并得到了广泛的应用，但由于它们的黑盒特性，这些具有潜力的AI驱动的软件工程模型离实际部署还有很大的差距。这种缺乏可解释性对于在关键任务中应用这些模型，如漏洞检测，决策透明性至关重要，却带来了不必要的风险。本文通过对SE领域中旨在提高AI模型可解释性的方法进行系统文献综述来阐明这个跨学科领域。该综述覆盖了SE和AI学术会议和期刊中出现的研究，涵盖了21个独特的SE任务的63篇论文。基于三个关键的研究问题，我们旨在总结XAI技术在SE任务中的应用情况。",
    "tldr": "本文通过对机器/深度学习的软件工程领域中可解释性的系统文献综述，总结了XAI技术在软件工程中的应用情况，旨在提高AI模型的可解释性以解决实际部署中的不确定性和风险问题。",
    "en_tdlr": "This paper presents a systematic literature review on the explainability of machine/deep learning-based software engineering. It summarizes the application of explainable AI (XAI) techniques in SE to improve model interpretability and address uncertainties and risks in practical deployment."
}