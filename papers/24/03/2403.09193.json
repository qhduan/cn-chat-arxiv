{
    "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
    "abstract": "arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text",
    "link": "https://arxiv.org/abs/2403.09193",
    "context": "Title: Are Vision Language Models Texture or Shape Biased and Can We Steer Them?\nAbstract: arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text",
    "path": "papers/24/03/2403.09193.json",
    "total_tokens": 963,
    "translated_title": "视觉语言模型是纹理偏见还是形状偏见，我们可以引导它们吗？",
    "translated_abstract": "arXiv:2403.09193v1 公告类型: 跨领域 摘要: 视觉语言模型（VLMs）在短短几年内彻底改变了计算机视觉模型的格局，开启了一系列新的应用，从零样本图像分类到图像字幕生成，再到视觉问答。与纯视觉模型不同，它们提供了通过语言提示访问视觉内容的直观方式。这种模型的广泛适用性引发我们思考它们是否也与人类视觉一致 - 具体来说，它们在多模态融合中有多大程度地采用了人类引导的视觉偏见，或者它们是否只是从纯视觉模型中继承了偏见。其中一个重要的视觉偏见是纹理与形状偏见，即局部信息的主导地位。在本文中，我们研究了一系列流行的VLMs中的这种偏见。有趣的是，我们发现VLMs通常比它们的视觉编码器更偏向于形状，这表明视觉偏见在一定程度上通过文本进行调节。",
    "tldr": "本文研究了广泛应用的视觉语言模型中的纹理与形状偏见，发现这些模型通常比视觉编码器更偏向形状，暗示视觉偏见在一定程度上会受到文本的调节",
    "en_tdlr": "This paper investigates texture and shape biases in widely used vision language models, finding that these models are often more shape-biased than vision encoders, suggesting that visual biases are modulated to some extent through text."
}