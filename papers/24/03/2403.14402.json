{
    "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
    "abstract": "arXiv:2403.14402v1 Announce Type: cross  Abstract: Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs",
    "link": "https://arxiv.org/abs/2403.14402",
    "context": "Title: XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception\nAbstract: arXiv:2403.14402v1 Announce Type: cross  Abstract: Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs",
    "path": "papers/24/03/2403.14402.json",
    "total_tokens": 1018,
    "translated_title": "XLAVS-R: 跨语言视听语音表示学习用于噪声鲁棒语音知觉",
    "translated_abstract": "语音识别和翻译系统对嘈杂的输入表现不佳，在现实环境中经常出现。通过视觉信号增强这些系统有潜力提高对噪声的鲁棒性。然而，视听（AV）数据仅有限可用，并且比仅有音频资源的语言更少。为填补这一空白，我们提出XLAVS-R，一个跨语言视听语音表示模型，用于超过100种语言的噪声鲁棒语音识别和翻译。它旨在最大程度利用有限的多语言AV预训练数据的益处，通过在音频-仅多语言预训练的基础上构建，并简化现有的预训练方案。在MuAViC基准评估上对XLAVS-R进行了广泛评估，显示了其在下游音频-视觉语音识别和翻译任务上的优势，在给出嘈杂的AV输入时，其优于先前最先进技术最高达到18.5% WER和4.7 BLEU。",
    "tldr": "XLAVS-R是一个跨语言视听语音表示学习模型，通过利用有限的多语言AV预训练数据，简化预训练方案，以提高对噪声的鲁棒性，在下游音频-视觉语音识别和翻译任务中比先前最先进技术提升了高达18.5% WER和4.7 BLEU。"
}