{
    "title": "MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting",
    "abstract": "arXiv:2403.03174v1 Announce Type: cross  Abstract: Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from br",
    "link": "https://arxiv.org/abs/2403.03174",
    "context": "Title: MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting\nAbstract: arXiv:2403.03174v1 Announce Type: cross  Abstract: Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from br",
    "path": "papers/24/03/2403.03174.json",
    "total_tokens": 627,
    "translated_title": "MOKA：基于标记的视觉提示实现开放词汇的机器人操作",
    "translated_abstract": "开放词汇的泛化要求机器人系统执行涉及复杂和多样化环境以及任务目标的任务。本文提出了一种名为MOKA（Marking Open-vocabulary Keypoint Affordances）的方法，利用视觉语言模型（VLMs）来解决由自由形式语言描述指定的机器人操作任务。",
    "tldr": "MOKA方法利用视觉语言模型解决机器人操作任务，实现了开放词汇的机器人操作。",
    "en_tdlr": "The MOKA method utilizes visual language models to solve robotic manipulation tasks, achieving open-vocabulary robotic manipulation."
}