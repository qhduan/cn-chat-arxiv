{
    "title": "DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs",
    "abstract": "arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi",
    "link": "https://arxiv.org/abs/2403.19588",
    "context": "Title: DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs\nAbstract: arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi",
    "path": "papers/24/03/2403.19588.json",
    "total_tokens": 912,
    "translated_title": "DenseNets重生：超越ResNets和ViTs的范式转变",
    "translated_abstract": "本文复苏了密集连接卷积网络（DenseNets），揭示了它们相对于主导的ResNet风格架构被低估的有效性。我们认为DenseNets的潜力被忽视，是因为未曾触及的训练方法和传统设计元素未能完全展现其能力。我们的初步研究表明，通过连接的密集连接是强大的，表明DenseNets可以被重新激活以与现代架构竞争。我们系统地改进了次优组件 - 架构调整、块重新设计和改进的训练配方，以扩展DenseNets并提高内存效率，同时保持连接快捷方式。我们的模型采用简单的架构元素，最终超越了Swin Transformer、ConvNeXt和DeiT-III - 残差学习谱系中的关键架构。此外，我们的模型在ImageNet-1K上展现出接近最新技术水平的性能，竞争wi",
    "tldr": "本文重新探讨了密集连接卷积网络（DenseNets），揭示了其相对于ResNet风格架构的被低估有效性，通过改进架构、块设计和训练方法，使得DenseNets可以与现代架构竞争，并在ImageNet-1K上接近最新技术水平。",
    "en_tdlr": "This paper revives DenseNets, revealing their underrated effectiveness compared to ResNet-style architectures, by refining architecture, block design, and training methods, enabling DenseNets to compete with modern architectures and achieve near state-of-the-art performance on ImageNet-1K."
}