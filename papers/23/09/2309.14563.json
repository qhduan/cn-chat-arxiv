{
    "title": "Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])",
    "abstract": "Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\\{{\\boldsymbol x}_i\\}_{i\\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\\{{\\boldsymbol x}_i\\}_{i\\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti",
    "link": "http://arxiv.org/abs/2309.14563",
    "context": "Title: Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])\nAbstract: Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\\{{\\boldsymbol x}_i\\}_{i\\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\\{{\\boldsymbol x}_i\\}_{i\\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti",
    "path": "papers/23/09/2309.14563.json",
    "total_tokens": 957,
    "translated_title": "é¢å‘å¼±ç›‘ç£ä¸‹çš„æ•°æ®é€‰æ‹©ç»Ÿè®¡ç†è®º",
    "translated_abstract": "å¯¹äºŽä¸€ä¸ªå¤§å°ä¸ºNçš„æ ·æœ¬ï¼Œé€‰æ‹©ä¸€ä¸ªæ›´å°çš„å¤§å°n<Nçš„å­æ ·æœ¬ç”¨äºŽç»Ÿè®¡ä¼°è®¡æˆ–å­¦ä¹ é€šå¸¸æ˜¯æœ‰ç”¨çš„ã€‚è¿™æ ·çš„æ•°æ®é€‰æ‹©æ­¥éª¤æœ‰åŠ©äºŽå‡å°‘æ•°æ®æ ‡è®°çš„è¦æ±‚å’Œå­¦ä¹ çš„è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ä»¬å‡è®¾ç»™å®šäº†Nä¸ªæœªæ ‡è®°çš„æ ·æœ¬{x_i}ï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®ä¸€ä¸ªâ€œæ›¿ä»£æ¨¡åž‹â€ï¼Œå®ƒå¯ä»¥æ¯”éšæœºçŒœæµ‹æ›´å¥½åœ°é¢„æµ‹æ ‡ç­¾y_iã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå­æ ·æœ¬é›†{ð±_i}ï¼Œå…¶å¤§å°ä¸º|G|=n<Nã€‚ç„¶åŽæˆ‘ä»¬ä¸ºè¿™ä¸ªé›†åˆèŽ·å–æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬é€šè¿‡æ­£åˆ™åŒ–ç»éªŒé£Žé™©æœ€å°åŒ–æ¥è®­ç»ƒæ¨¡åž‹ã€‚é€šè¿‡åœ¨çœŸå®žå’Œåˆæˆæ•°æ®ä¸Šè¿›è¡Œæ··åˆçš„æ•°å€¼å®žéªŒï¼Œå¹¶åœ¨ä½Žç»´å’Œé«˜ç»´æ¸è¿‘æƒ…å†µä¸‹è¿›è¡Œæ•°å­¦æŽ¨å¯¼ï¼Œæˆ‘ä»¬è¯æ˜Žï¼š(i) æ•°æ®é€‰æ‹©å¯ä»¥éžå¸¸æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥å‡»è´¥å¯¹æ•´ä¸ªæ ·æœ¬çš„è®­ç»ƒï¼›(ii) åœ¨æ•°æ®é€‰æ‹©æ–¹é¢ï¼ŒæŸäº›æµè¡Œçš„é€‰æ‹©åœ¨ä¸€äº›æƒ…å†µä¸‹æ˜¯æœ‰æ•ˆçš„ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™ä¸æ˜¯ã€‚",
    "tldr": "æœ¬ç ”ç©¶é’ˆå¯¹å¼±ç›‘ç£ä¸‹çš„æ•°æ®é€‰æ‹©è¿›è¡Œäº†ç»Ÿè®¡ç†è®ºç ”ç©¶ï¼Œé€šè¿‡å®žéªŒè¯æ˜Žæ•°æ®é€‰æ‹©å¯ä»¥éžå¸¸æœ‰æ•ˆï¼Œæœ‰æ—¶ç”šè‡³å¯ä»¥æˆ˜èƒœå¯¹æ•´ä¸ªæ ·æœ¬çš„è®­ç»ƒã€‚å¹¶åˆ†æžäº†åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ•°æ®é€‰æ‹©é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚",
    "en_tdlr": "This study proposes a statistical theory of data selection under weak supervision, showing that data selection can be highly effective and even outperform training on the full sample in some cases. The effectiveness of certain popular data selection choices is analyzed in different scenarios."
}