{
    "title": "Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])",
    "abstract": "Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, ",
    "link": "http://arxiv.org/abs/2309.07716",
    "context": "Title: Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])\nAbstract: Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, ",
    "path": "papers/23/09/2309.07716.json",
    "total_tokens": 821,
    "translated_title": "理解向量值神经网络及其与实数和超复值神经网络的关系",
    "translated_abstract": "尽管深度学习模型在多维信号和图像处理方面有许多成功的应用，但大多数传统神经网络处理由（多维）实数数组表示的数据。特征通道之间的互相关通常被期望从训练数据中学习，这需要大量的参数和仔细的训练。相反，向量值神经网络被设计成处理向量数组，并自然地考虑特征通道之间的互相关。因此，它们通常具有更少的参数，通常比传统神经网络具有更强的训练能力。本文旨在提出一个广泛的向量值神经网络框架，称为V-nets。在这个背景下，超复值神经网络被视为具有额外代数属性的向量值模型。此外，本文解释了向量值神经网络与传统神经网络之间的关系。",
    "tldr": "本文介绍了向量值神经网络（V-nets）的广泛框架，并解释了它们与超复值神经网络以及传统神经网络的关系。",
    "en_tdlr": "This paper presents a broad framework for vector-valued neural networks (V-nets) and explains their relationship with hypercomplex-valued neural networks and traditional neural networks."
}