{
    "title": "Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei",
    "link": "http://arxiv.org/abs/2309.15659",
    "context": "Title: Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])\nAbstract: Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei",
    "path": "papers/23/09/2309.15659.json",
    "total_tokens": 971,
    "translated_title": "联邦深度平衡学习：边缘通信效率的紧凑共享表示",
    "translated_abstract": "联邦学习是一种卓越的分布式学习范式，促进了边缘网络节点之间的协作，以在不集中数据的情况下共同训练全局模型。通过将计算转移到网络边缘，联邦学习提供了鲁棒和响应迅速的边缘人工智能解决方案，并增强了隐私保护。然而，在边缘环境中部署深度联邦学习模型通常受到通信瓶颈、数据异构性和内存限制的阻碍。为了共同解决这些挑战，我们引入了FeDEQ，这是一个开创性的联邦学习框架，它有效地采用深度平衡学习和共识优化，在边缘节点之间利用紧凑的共享数据表示，允许派生出针对每个节点特定的个性化模型。我们深入探讨了一个独特的模型结构，由一个平衡层和传统神经网络层组成。在这里，平衡层充当全局特征表示，边缘节点可以根据自己的需求进行个性化调整。",
    "tldr": "FeDEQ是一个联邦学习框架，采用深度平衡学习和共识优化，通过紧凑的共享数据表示在边缘节点之间共享模型，解决了深度联邦学习在边缘环境中的通信瓶颈和数据异构性问题。",
    "en_tdlr": "FeDEQ is a federated learning framework that utilizes deep equilibrium learning and consensus optimization to share models among edge nodes through a compact shared data representation, addressing communication bottlenecks and data heterogeneity in deep federated learning for edge environments."
}