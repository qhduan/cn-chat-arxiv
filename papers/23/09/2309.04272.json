{
    "title": "Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])",
    "abstract": "Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit ",
    "link": "http://arxiv.org/abs/2309.04272",
    "context": "Title: Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])\nAbstract: Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit ",
    "path": "papers/23/09/2309.04272.json",
    "total_tokens": 950,
    "translated_title": "学习改进样本复杂性的零和线性二次博弈",
    "translated_abstract": "零和线性二次（LQ）博弈在最优控制中是基础性的，可以用于（i）风险敏感或鲁棒控制的动态博弈形式，或者（ii）作为连续状态-控制空间中两个竞争智能体的多智能体强化学习的基准设置。与广泛研究的单智能体线性二次调节器问题不同，零和LQ博弈涉及解决一个具有缺乏强制性的目标函数的具有挑战性的非凸非凹最小-最大问题。最近，张等人发现了自然策略梯度方法的隐式正则化属性，这对于安全关键的控制系统非常重要，因为它在学习过程中保持了控制器的鲁棒性。此外，在没有模型参数知识的模型无关设置中，张等人提出了第一个多项式样本复杂性算法，以达到Nash均衡的ε-邻域，同时保持理想的隐式正则化属性。",
    "tldr": "这项研究提出了改进样本复杂性的零和线性二次博弈，并发现了自然策略梯度方法的隐式正则化属性。在无模型参数知识的情况下，他们还提出了第一个多项式样本复杂性算法来达到Nash均衡。",
    "en_tdlr": "This research presents an improved sample complexity approach for zero-sum linear quadratic games, and discovers the implicit regularization property of natural policy gradient methods. In the model-free setting, they also propose the first polynomial sample complexity algorithm to achieve the Nash equilibrium without knowledge of the model parameters."
}