{
    "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential",
    "link": "http://arxiv.org/abs/2306.10512",
    "context": "Title: Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. (arXiv:2306.10512v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model's performance. This allows for a more accurate estimation of the model's abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential",
    "path": "papers/23/06/2306.10512.json",
    "total_tokens": 939,
    "translated_title": "高效测量语言模型的认知能力：自适应测试视角",
    "translated_abstract": "大型语言模型（LLMs），如ChatGPT，展现了一些类似于人类的认知能力。为了比较不同模型的这些能力，通常采用来自不同领域（如文学、生物学和心理学）的多个基准（即标准测试问题集），并报告传统度量指标（如准确率、召回率和F1）。然而，从认知科学的角度来看，这种评估LLMs的方法可能效率低下且不准确。受心理测量学中计算机自适应测试（CAT）的启发，我们提出了一种适用于LLM评估的自适应测试框架。该方法根据模型的表现动态调整测试问题的特性（如难度），而不是使用标准的测试集并简单报告准确率。这使得能更准确地估计模型的能力，并使用更少的问题。更重要的是，它使LLMs能够与人类进行轻松比较，这是至关重要的。",
    "tldr": "本研究提出了一种自适应测试框架，用于高效测量语言模型的认知能力。通过动态调整测试问题的特性，能够更准确地评估模型的能力，并使用更少的问题。同时，该框架使得语言模型能够与人类进行轻松比较。",
    "en_tdlr": "This study proposes an adaptive testing framework for efficiently measuring the cognitive ability of language models. By dynamically adjusting the characteristics of the test questions, it allows for a more accurate estimation of the model's abilities, using fewer questions. Moreover, the framework enables easy comparison between language models and humans."
}