{
    "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap. (arXiv:2306.03100v1 [cs.HC])",
    "abstract": "The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (\\textit{socio-technical gap}). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods ",
    "link": "http://arxiv.org/abs/2306.03100",
    "context": "Title: Rethinking Model Evaluation as Narrowing the Socio-Technical Gap. (arXiv:2306.03100v1 [cs.HC])\nAbstract: The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (\\textit{socio-technical gap}). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods ",
    "path": "papers/23/06/2306.03100.json",
    "total_tokens": 905,
    "translated_title": "将模型评估重新考虑为缩小社会技术差距",
    "translated_abstract": "生成和大型语言模型的最近发展给模型评估带来了新的挑战，研究界和工业界正在努力应对。虽然这些模型的多才多艺引起了人们的兴奋，但它们也不可避免地向同质化迈进：用单个常称之为“通用”的模型为一系列应用提供动力。在这篇立场论文中，我们认为模型评估实践必须承担一个关键任务，以应对这种同质化带来的挑战和责任：为特定模型提供有效的评估，判断是否以及在下游使用场景中可以通过给定模型满足多少人类需求（“社会技术差距”）。我们汲取社会科学、人机交互（HCI）和可解释AI（XAI）跨学科领域的经验，敦促社区开发基于真实社会需求的评估方法，并拥抱多样化的评估方法。",
    "tldr": "针对同质化的模型，模型评估需要提供有效的评估，以判断特定模型是否在下游使用场景中可以满足多少人类需求，并且应该根据真实的社会需求来开发评估模型，并拥抱多样化的评估方法。",
    "en_tdlr": "For homogeneous models, model evaluation needs to provide effective assessments to determine how much human needs can be met in downstream use cases. Evaluation methods should be developed based on real-world socio-requirements, and diverse evaluation methods should be embraced."
}