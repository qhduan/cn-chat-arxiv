{
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem. (arXiv:2303.16750v1 [cs.IR])",
    "abstract": "Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.  We use this data to compare s",
    "link": "http://arxiv.org/abs/2303.16750",
    "context": "Title: A Gold Standard Dataset for the Reviewer Assignment Problem. (arXiv:2303.16750v1 [cs.IR])\nAbstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.  We use this data to compare s",
    "path": "papers/23/03/2303.16750.json",
    "total_tokens": 869,
    "translated_title": "一种用于审稿人分配问题的黄金标准数据集",
    "translated_abstract": "许多同行评审期刊或会议正在使用或试图使用算法将投稿分配给审稿人。这些自动化方法的关键是“相似度分数”，即对审稿人在审查论文中的专业水平的数值估计，已经提出了许多算法来计算这些分数。然而，这些算法尚未经过有原则的比较，这使得利益相关者难以以基于证据的方式选择算法。比较现有算法和开发更好算法的关键挑战是缺乏公开可用的黄金标准数据，这些数据将用于进行可重复研究。我们通过收集一组新的相似度得分数据来解决这个问题，并将其发布给研究社区。我们的数据集由58位研究人员提供的477个自我报告的专业水平分数组成，用于评估他们先前阅读的论文的审查经验。我们使用这些数据来比较各种算法，并对标准数据集的设计提出了建议。",
    "tldr": "该论文提出了一个用于审稿人分配问题的新数据集，解决了当前算法难以进行原则比较的问题，并提供了基于此数据集的算法比较结果，为利益相关者在选择算法方面提供了一个基础。",
    "en_tdlr": "This paper introduces a new dataset for the reviewer assignment problem, addressing the challenge of principled comparison of existing algorithms and providing evidence-based suggestions for algorithm selection."
}