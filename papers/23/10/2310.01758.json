{
    "title": "Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])",
    "abstract": "Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a",
    "link": "http://arxiv.org/abs/2310.01758",
    "context": "Title: Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])\nAbstract: Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a",
    "path": "papers/23/10/2310.01758.json",
    "total_tokens": 870,
    "translated_title": "ReLU激活函数在神经网络嵌入优化中的线性化：最佳日前能量调度",
    "translated_abstract": "神经网络在电力系统领域中得到广泛应用。它们可以用于更好地预测输入信息，并以更高的准确性对系统性能进行建模。在一些应用中，如基于神经网络的微电网日前能量调度中，训练模型的输入特征是在强制限制同一学习模型的输出的优化模型中解决的变量。这将会产生一个嵌入神经网络优化问题；在神经网络中使用非线性激活函数将使这类问题异常困难，甚至无法解决。为了应对这一新兴挑战，本文研究了不同的线性化非线性激活函数的方法，特别关注广泛使用的修正线性单元（ReLU）函数。本文开发、分析和比较了四种适用于ReLU激活函数的线性化方法。",
    "tldr": "本文研究了将非线性激活函数线性化的方法，特别关注修正线性单元（ReLU）函数。并针对神经网络嵌入优化问题提出并比较了四种定制的ReLU激活函数的线性化方法。",
    "en_tdlr": "This paper investigates methods for linearizing the non-linear activation function, with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU function are developed, analyzed, and compared in the context of neural network-embedded optimization problems."
}