{
    "title": "Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])",
    "abstract": "Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n",
    "link": "http://arxiv.org/abs/2310.19919",
    "context": "Title: Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])\nAbstract: Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n",
    "path": "papers/23/10/2310.19919.json",
    "total_tokens": 844,
    "translated_title": "神经网络中基于价值最大化的元学习策略",
    "translated_abstract": "生物和人工学习代理面临诸多学习选择，包括超参数选择和任务分布的各个方面，如课程。了解如何进行这些元学习选择可以提供对生物学习者的认知控制功能的规范解释，并改进工程系统。然而，由于优化整个学习过程的复杂性，目前仍然挑战着计算现代深度网络中的最优策略。在这里，我们在一个可处理的环境中从理论上研究最优策略。我们提出了一个学习努力的框架，能够在完全规范化的目标上高效地优化控制信号：在学习过程中的折现累积性能。通过使用估计梯度下降的平均动力方程，我们获得了计算的可行性，该方程适用于简单的神经网络架构。我们的框架包容了一系列元学习和自动课程学习方法，形成了统一的框架。",
    "tldr": "本文理论上研究了在神经网络中的元学习最优策略，并提出了一个学习努力的框架，可以高效地优化控制信号，从而提升学习性能。",
    "en_tdlr": "This paper theoretically investigates optimal meta-learning strategies in neural networks and presents a learning effort framework that efficiently optimizes control signals to improve learning performance."
}