{
    "title": "The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)",
    "abstract": "We propose $\\textsf{ScaledGD($\\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\\textsf{ScaledGD($\\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\\textsf{ScaledGD($\\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\\textsf{ScaledGD($\\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi",
    "link": "http://arxiv.org/abs/2302.01186",
    "context": "Title: The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)\nAbstract: We propose $\\textsf{ScaledGD($\\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\\textsf{ScaledGD($\\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\\textsf{ScaledGD($\\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\\textsf{ScaledGD($\\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi",
    "path": "papers/23/02/2302.01186.json",
    "total_tokens": 839,
    "translated_title": "é¢„æ¡ä»¶å¯¹è¶…å‚åŒ–ä½ç§©çŸ©é˜µæ„ŸçŸ¥çš„å½±å“",
    "translated_abstract": "æœ¬æ–‡æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•æ¥è§£å†³ä½ç§©çŸ©é˜µæ„ŸçŸ¥ä¸­çŸ©é˜µå¯èƒ½ç—…æ€ä»¥åŠçœŸå®ç§©æœªçŸ¥çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…å‚å¼è¡¨ç¤ºï¼Œä»ä¸€ä¸ªå°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šå½¢å¼çš„é˜»å°¼é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™æ¥å¯¹æŠ—è¶…å‚åŒ–å’Œç—…æ€æ›²ç‡çš„å½±å“ã€‚ä¸åŸºå‡†æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ç›¸æ¯”ï¼Œå°½ç®¡é¢„å¤„ç†éœ€è¦è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œä½†ScaledGDï¼ˆğœ†ï¼‰åœ¨é¢å¯¹ç—…æ€é—®é¢˜æ—¶è¡¨ç°å‡ºäº†å‡ºè‰²çš„é²æ£’æ€§ã€‚åœ¨é«˜æ–¯è®¾è®¡ä¸‹ï¼ŒScaledGD($\\lambda$) ä¼šåœ¨ä»…è¿­ä»£æ•°å¯¹æ•°çº§åˆ«çš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„ä½ç§©çŸ©é˜µã€‚",
    "tldr": "è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚",
    "en_tdlr": "This study proposes the ScaledGD(ğœ†) method which is more robust than the traditional gradient descent method and performs well in solving the low-rank matrix sensing problem."
}