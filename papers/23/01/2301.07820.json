{
    "title": "Emergence of the SVD as an interpretable factorization in deep learning for inverse problems. (arXiv:2301.07820v2 [cs.LG] UPDATED)",
    "abstract": "Within the framework of deep learning we demonstrate the emergence of the singular value decomposition (SVD) of the weight matrix as a tool for interpretation of neural networks (NN) when combined with the descrambling transformation--a recently-developed technique for addressing interpretability in noisy parameter estimation neural networks \\cite{amey2021neural}. By considering the averaging effect of the data passed to the descrambling minimization problem, we show that descrambling transformations--in the large data limit--can be expressed in terms of the SVD of the NN weights and the input autocorrelation matrix. Using this fact, we show that within the class of noisy parameter estimation problems the SVD may be the structure through which trained networks encode a signal model. We substantiate our theoretical findings with empirical evidence from both linear and non-linear signal models. Our results also illuminate the connections between a mathematical theory of semantic developm",
    "link": "http://arxiv.org/abs/2301.07820",
    "context": "Title: Emergence of the SVD as an interpretable factorization in deep learning for inverse problems. (arXiv:2301.07820v2 [cs.LG] UPDATED)\nAbstract: Within the framework of deep learning we demonstrate the emergence of the singular value decomposition (SVD) of the weight matrix as a tool for interpretation of neural networks (NN) when combined with the descrambling transformation--a recently-developed technique for addressing interpretability in noisy parameter estimation neural networks \\cite{amey2021neural}. By considering the averaging effect of the data passed to the descrambling minimization problem, we show that descrambling transformations--in the large data limit--can be expressed in terms of the SVD of the NN weights and the input autocorrelation matrix. Using this fact, we show that within the class of noisy parameter estimation problems the SVD may be the structure through which trained networks encode a signal model. We substantiate our theoretical findings with empirical evidence from both linear and non-linear signal models. Our results also illuminate the connections between a mathematical theory of semantic developm",
    "path": "papers/23/01/2301.07820.json",
    "total_tokens": 862,
    "translated_title": "深度学习中奇异值分解（SVD）作为可解释的因子化的出现在逆问题中",
    "translated_abstract": "在深度学习框架下，我们展示了权重矩阵的奇异值分解（SVD）作为神经网络（NN）解释工具的出现，当与解密变换相结合时，这是一种最近针对噪声参数估计神经网络的解释技术。通过考虑传递给解密最小化问题的数据的平均效果，我们证明了在大数据极限下，解密变换可以用NN权重的SVD和输入自相关矩阵来表示。利用这个事实，我们展示了在噪声参数估计问题类中，SVD可以是训练网络编码信号模型的结构。我们用线性和非线性信号模型的实证证据进一步支持了我们的理论发现。我们的结果还揭示了数学理论和语义发展之间的联系",
    "tldr": "深度学习中的奇异值分解（SVD）在逆问题中成为可解释的因子化工具，通过与解密变换结合，可以用来解释神经网络（NN）在噪声参数估计问题中编码信号模型的结构",
    "en_tdlr": "The emergence of Singular Value Decomposition (SVD) as an interpretable factorization in deep learning for inverse problems, along with the descrambling transformation, allows for the interpretation of neural networks' encoding of signal models in noisy parameter estimation problems."
}