{
    "title": "Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])",
    "abstract": "Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \\emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only",
    "link": "http://arxiv.org/abs/2305.07216",
    "context": "Title: Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])\nAbstract: Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \\emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only",
    "path": "papers/23/05/2305.07216.json",
    "total_tokens": 835,
    "translated_title": "处理情感回归和分类任务中单模态和多模态的通用视听学习",
    "translated_abstract": "大多数当前的音视频情感识别模型缺乏实际应用所需的灵活性。我们设想了一个多模态系统，即使只有一个模态可用，也可以互换地实现预测情感属性或识别分类情感。在一个多模态情感识别系统中实现这样的灵活性存在困难，因为准确解释和整合各种数据来源是困难的。同时，允许在回归和分类任务之间直接切换，同时处理缺失或部分信息也是一个挑战。本研究提出了一个用于处理情感回归和情感分类任务的通用视听学习（VAVL）框架，实现了处理单模态和多模态系统的音视频框架，即使音频和视觉数据不匹配，也可以进行训练。",
    "tldr": "本文提出了一个通用视听学习（VAVL）框架，可用于处理情感回归和情感分类任务中的单模态和多模态系统，即使数据缺失或不匹配也能进行有效训练和切换。"
}