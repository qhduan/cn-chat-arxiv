{
    "title": "Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])",
    "abstract": "In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\\\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\\mathfrak{C}_{\\mathrm{ania}}$ of the additive noise induced ",
    "link": "http://arxiv.org/abs/2308.01358",
    "context": "Title: Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])\nAbstract: In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\\\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\\mathfrak{C}_{\\mathrm{ania}}$ of the additive noise induced ",
    "path": "papers/23/08/2308.01358.json",
    "total_tokens": 1052,
    "translated_title": "å‹ç¼©å’Œåˆ†å¸ƒå¼æœ€å°äºŒä¹˜å›å½’ï¼šæ”¶æ•›é€Ÿåº¦åŠå…¶åœ¨è”é‚¦å­¦ä¹ ä¸­çš„åº”ç”¨",
    "translated_abstract": "æœ¬æ–‡ç ”ç©¶äº†åœ¨æœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­ï¼Œå‹ç¼©å¯¹éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å‡ ç§æ— åå‹ç¼©æ“ä½œç¬¦ä¹‹é—´çš„æ”¶æ•›é€Ÿåº¦å·®å¼‚ï¼Œè¿™äº›æ“ä½œç¬¦éƒ½æ»¡è¶³ç›¸åŒçš„æ–¹å·®æ¡ä»¶ï¼Œä»è€Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ€å°äºŒä¹˜å›å½’ï¼ˆLSRï¼‰çš„æƒ…å†µï¼Œå¹¶åˆ†æäº†ä¸€ä¸ªä¾èµ–äºéšæœºåœºçš„æœ€å°äºŒä¹˜å›å½’çš„éšæœºé€¼è¿‘ç®—æ³•ã€‚æˆ‘ä»¬å¯¹éšæœºåœºçš„ä¸€èˆ¬æ€§å‡è®¾è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼ˆç‰¹åˆ«æ˜¯æœŸæœ›çš„HÃ¶lderæ­£åˆ™æ€§ï¼‰å¹¶å¯¹å™ªå£°åæ–¹å·®è¿›è¡Œäº†é™åˆ¶ï¼Œä»¥ä¾¿åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬å‹ç¼©ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç»“æœæ‰©å±•åˆ°è”é‚¦å­¦ä¹ çš„æƒ…å†µä¸‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¯¹åŠ æ€§å™ªå£°çš„åæ–¹å·®ğ–¢ğ– ğ–­ğ–¨ğ– å¯¹æ”¶æ•›æ€§çš„å½±å“ã€‚",
    "tldr": "æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚",
    "en_tdlr": "This paper investigates the impact of compression on stochastic gradient algorithms in distributed and federated learning. The study goes beyond classical worst-case analysis by comparing the convergence rates of different unbiased compression operators. It focuses on least-squares regression and proposes a stochastic approximation algorithm based on a random field. The analysis considers general assumptions on the random field and restrictions on the noise covariance, allowing for the analysis of various randomizing mechanisms, including compression. The results are also extended to the case of federated learning."
}