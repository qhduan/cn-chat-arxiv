{
    "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)",
    "abstract": "Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ that is more robust to estimation artifacts caused by",
    "link": "http://arxiv.org/abs/2206.03787",
    "context": "Title: Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)\nAbstract: Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ that is more robust to estimation artifacts caused by",
    "path": "papers/22/06/2206.03787.json",
    "total_tokens": 913,
    "translated_title": "å¼ºåŒ–å­¦ä¹ ä¸­åŠ¨ä½œå™ªå£°å¯¹æ¢ç´¢å’Œæ€§èƒ½çš„å½±å“",
    "translated_abstract": "è®¸å¤šæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¾èµ–äºç®€å•çš„æ¢ç´¢å½¢å¼ï¼Œå¦‚è¿ç»­æ§åˆ¶é¢†åŸŸå¸¸ç”¨çš„åŠ æ€§åŠ¨ä½œå™ªå£°ã€‚é€šå¸¸ï¼Œåœ¨åŸ¹è®­æœŸé—´ä¿æŒåŠ¨ä½œå™ªå£°çš„æ¯”ä¾‹ä¸å˜ã€‚æœ¬æ–‡ä¾§é‡äºè¿ç»­æ§åˆ¶çš„ç¦»çº¿æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„åŠ¨ä½œå™ªå£°ã€‚æˆ‘ä»¬åˆ†æäº†å™ªå£°ç±»å‹ã€å™ªå£°æ¯”ä¾‹å’Œå‡å°‘è§„æ¨¡å› å­çš„å½±å“ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§æœ€å¸¸è§çš„åŠ¨ä½œå™ªå£°ç±»å‹ï¼Œé«˜æ–¯å™ªå£°å’Œ Ornstein-Uhlenbeck å™ªå£°ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜å™ªå£°ç±»å‹å’Œæ¯”ä¾‹å‚æ•°è¿›è¡Œäº†å¤§é‡å®éªŒç ”ç©¶ï¼Œå¹¶é€šè¿‡æµ‹é‡ç­–ç•¥çš„é¢„æœŸå›æŠ¥å’Œæ¢ç´¢æœŸé—´çš„çŠ¶æ€ç©ºé—´è¦†ç›–ç­‰æœ‰è¶£çš„å˜é‡æ¥è¯„ä¼°ç»“æœã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„çŠ¶æ€ç©ºé—´è¦†ç›–åº¦é‡X_ğ’°relï¼Œè¯¥æ–¹æ³•å¯¹ä¼°è®¡å¼•èµ·çš„ä¼°è®¡è¯¯å·®å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚",
    "tldr": "æœ¬æ–‡ç ”ç©¶äº†åŠ¨ä½œå™ªå£°ç±»å‹ã€å™ªå£°æ¯”ä¾‹å’Œå‡å°‘è§„æ¨¡å› å­å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ ç¦»çº¿å­¦ä¹ ä¸­ç­–ç•¥æ€§èƒ½å’Œæ¢ç´¢æ•ˆæœçš„å½±å“ï¼Œæå‡ºäº†ä¸€ç§æ›´å…·é²æ£’æ€§çš„çŠ¶æ€ç©ºé—´è¦†ç›–åº¦é‡ã€‚",
    "en_tdlr": "This paper studies the impact of action noise type, noise scale, and scaling factor reduction schedule on the performance and exploration in off-policy deep reinforcement learning for continuous control, and proposes a more robust state-space coverage measure."
}