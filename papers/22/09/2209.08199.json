{
    "title": "ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots",
    "abstract": "arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.",
    "link": "https://arxiv.org/abs/2209.08199",
    "context": "Title: ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots\nAbstract: arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.",
    "path": "papers/22/09/2209.08199.json",
    "total_tokens": 585,
    "translated_title": "ScreenQA: 移动应用截图上的大规模问答对",
    "translated_abstract": "我们提出了一个新的任务和数据集ScreenQA，用于通过问答来理解屏幕内容。现有的屏幕数据集要么侧重于结构和组件级别的理解，要么侧重于像导航和任务完成之类的更高级别的组合任务。我们试图通过在RICO数据集上注释86K个问答对来弥合这两者之间的差距，希望能够基准化屏幕阅读理解能力。",
    "tldr": "ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。",
    "en_tdlr": "ScreenQA introduces a new task and dataset by annotating 86K question-answer pairs over the RICO dataset to evaluate screen reading comprehension capacity."
}