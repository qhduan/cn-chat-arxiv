{
    "title": "Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)",
    "abstract": "This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame",
    "link": "http://arxiv.org/abs/2211.09619",
    "context": "Title: Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)\nAbstract: This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame",
    "path": "papers/22/11/2211.09619.json",
    "total_tokens": 741,
    "translated_title": "在线非随机控制简介",
    "translated_abstract": "本文介绍了一种新兴的动态系统控制与可微强化学习范式——在线非随机控制，并应用在线凸优化和凸松弛技术得到了具有可证明保证的新方法，在最佳和鲁棒控制方面取得了显著成果。与其他框架不同，该方法的目标是对抗性攻击，在无法预测扰动模型的情况下，通过在一组策略中寻找低后悔，获得对最优策略的近似。",
    "tldr": "介绍了一种新兴的在线非随机控制方法，通过在一组策略中寻找低后悔，获得对最优策略的近似。",
    "en_tdlr": "This article introduced a new paradigm for control of dynamical systems and differentiable reinforcement learning called online nonstochastic control, which aims to attain low regret against the best policy in hindsight from a benchmark class of policies, and obtained new methods with provable guarantees in optimal and robust control by applying online convex optimization and convex relaxations techniques."
}