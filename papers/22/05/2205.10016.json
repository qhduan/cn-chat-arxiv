{
    "title": "Learning Progress Driven Multi-Agent Curriculum",
    "abstract": "arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on",
    "link": "https://arxiv.org/abs/2205.10016",
    "context": "Title: Learning Progress Driven Multi-Agent Curriculum\nAbstract: arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on",
    "path": "papers/22/05/2205.10016.json",
    "total_tokens": 852,
    "translated_title": "学习进度驱动的多智能体课程",
    "translated_abstract": "课程强化学习（CRL）旨在通过逐渐增加任务的难度（通常由可实现的预期回报量化）来加快学习速度。受CRL在单智能体环境中的成功启发，一些研究尝试将CRL应用于多智能体强化学习（MARL），使用智能体数量来控制任务难度。然而，现有的工作通常使用手动定义的课程，如线性方案。本文首先将最先进的单智能体自主式CRL应用于稀疏奖励MARL。虽然表现令人满意，但我们确定了现有基于奖励的CRL方法生成的课程存在两个潜在缺陷：（1）高回报的任务可能不提供信息量大的学习信号，（2）在多智能体产生更高回报的任务中，加剧了学分分配困难。因此，我们进一步提出了自主式MARL（SPMARL），以基于任务的优先级进行安排。",
    "tldr": "提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。",
    "en_tdlr": "The paper introduces Self-Paced MARL (SPMARL) to address issues in curriculum generation in current multi-agent reinforcement learning, prioritizing tasks based on their importance."
}