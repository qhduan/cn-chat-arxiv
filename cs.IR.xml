<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#25581;&#31034;&#20102;TCF&#31243;&#24207;&#25193;&#23637;&#30340;&#26497;&#38480;&#12290;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11700</link><description>&lt;p&gt;
&#25506;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#30340;&#26497;&#38480;&#65306;&#21457;&#29616;&#21644;&#35748;&#35782;
&lt;/p&gt;
&lt;p&gt;
Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. (arXiv:2305.11700v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#25581;&#31034;&#20102;TCF&#31243;&#24207;&#25193;&#23637;&#30340;&#26497;&#38480;&#12290;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#21327;&#21516;&#36807;&#28388;&#25104;&#20026;&#29616;&#20170;&#25991;&#26412;&#21644;&#26032;&#38395;&#25512;&#33616;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#25110;&#35821;&#35328;&#27169;&#22411;(LMs)&#34920;&#31034;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20013;&#23567;&#22411;&#30340;LMs&#19978;&#65292;&#22914;&#26524;&#23558;&#29289;&#21697;&#32534;&#30721;&#22120;&#26367;&#25442;&#20026;&#26368;&#22823;&#26368;&#24378;&#22823;&#30340;1750&#20159;&#21442;&#25968;&#30340;GPT-3&#27169;&#22411;&#65292;&#23558;&#20250;&#23545;&#25512;&#33616;&#24615;&#33021;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#23578;&#19981;&#30830;&#23450;&#12290;&#20316;&#32773;&#24320;&#23637;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#32034;TCF&#31243;&#24207;&#30340;&#24615;&#33021;&#26497;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20316;&#32773;&#23558;&#29289;&#21697;&#32534;&#30721;&#22120;&#35268;&#27169;&#20174;&#19968;&#20159;&#25193;&#22823;&#21040;&#19968;&#30334;&#20159;&#20197;&#25581;&#31034;TCF&#31243;&#24207;&#30340;&#25193;&#23637;&#26497;&#38480;&#65292;&#21516;&#26102;&#36824;&#25506;&#31350;&#20102;&#20351;&#29992;&#36229;&#22823;LMs&#26159;&#21542;&#33021;&#23454;&#29616;&#25512;&#33616;&#20219;&#21153;&#30340;&#36890;&#29992;&#29289;&#21697;&#34920;&#31034;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#20351;&#29992;&#26368;&#24378;&#22823;&#30340;LMs&#21644;&#20013;&#31561;LMs&#23454;&#29616;&#30340;&#22522;&#20110;&#25991;&#26412;&#21327;&#21516;&#36807;&#28388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#20026;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#20197;&#24212;&#23545;&#20256;&#32479;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2209.15635</link><description>&lt;p&gt;
&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Vertical Semi-Federated Learning for Efficient Online Advertising. (arXiv:2209.15635v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15635
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#21322;&#32852;&#21512;&#23398;&#20064;&#20026;&#22312;&#32447;&#24191;&#21578;&#39046;&#22495;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#20197;&#24212;&#23545;&#20256;&#32479;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26550;&#26500;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;1&#65289;&#36866;&#29992;&#33539;&#22260;&#21463;&#38480;&#20110;&#37325;&#21472;&#26679;&#26412;&#65307;2&#65289;&#23454;&#26102;&#32852;&#21512;&#26381;&#21153;&#30340;&#31995;&#32479;&#25361;&#25112;&#36739;&#39640;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#35774;&#32622;&#8212;&#8212;&#21322;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;(Semi-VFL)&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#21322;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26088;&#22312;&#23454;&#29616;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#30340;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#24863;&#30693;&#30340;&#23616;&#37096;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#21333;&#26041;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23616;&#37096;&#26381;&#21153;&#30340;&#20415;&#21033;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#32852;&#21512;&#29305;&#26435;&#23398;&#20064;&#26694;&#26550;(JPL)&#65292;&#26469;&#35299;&#20915;&#34987;&#21160;&#26041;&#29305;&#24449;&#32570;&#22833;&#21644;&#36866;&#24212;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25512;&#29702;&#39640;&#25928;&#30340;&#36866;&#29992;&#20110;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#30340;&#21333;&#26041;&#23398;&#29983;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#32852;&#21512;&#29305;&#24449;&#25193;&#23637;&#30340;&#20248;&#21183;&#12290;&#26032;&#30340;&#34920;&#31034;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
The traditional vertical federated learning schema suffers from two main issues: 1) restricted applicable scope to overlapped samples and 2) high system challenge of real-time federated serving, which limits its application to advertising systems. To this end, we advocate a new learning setting Semi-VFL (Vertical Semi-Federated Learning) to tackle these challenge. Semi-VFL is proposed to achieve a practical industry application fashion for VFL, by learning a federation-aware local model which performs better than single-party models and meanwhile maintain the convenience of local-serving. For this purpose, we propose the carefully designed Joint Privileged Learning framework (JPL) to i) alleviate the absence of the passive party's feature and ii) adapt to the whole sample space. Specifically, we build an inference-efficient single-party student model applicable to the whole sample space and meanwhile maintain the advantage of the federated feature extension. New representation distilla
&lt;/p&gt;</description></item></channel></rss>