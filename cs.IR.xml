<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14802</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#19979;&#30340;&#38142;&#36335;&#39044;&#27979;: &#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14802
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#34920;&#31034;&#30340;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#24322;&#36136;&#22270;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#32463;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#23616;&#38480;&#20110;&#38024;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#23450;&#22522;&#20934;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;&#38142;&#36335;&#39044;&#27979;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20004;&#20010;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#26576;&#31181;&#28508;&#22312;&#21407;&#22240;&#32780;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#25552;&#21069;&#39044;&#27979;&#36825;&#31181;&#36830;&#25509;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNNs&#65288;&#22914;GRAFF&#65289;&#23545;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>GATSY&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#38899;&#20048;&#33402;&#26415;&#23478;&#30456;&#20284;&#24615;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#22788;&#29702;&#22810;&#26679;&#24615;&#21644;&#20851;&#32852;&#24615;&#65292;&#24182;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00635</link><description>&lt;p&gt;
GATSY: &#38899;&#20048;&#33402;&#26415;&#23478;&#30456;&#20284;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GATSY: Graph Attention Network for Music Artist Similarity. (arXiv:2311.00635v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00635
&lt;/p&gt;
&lt;p&gt;
GATSY&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#38899;&#20048;&#33402;&#26415;&#23478;&#30456;&#20284;&#24615;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#22788;&#29702;&#22810;&#26679;&#24615;&#21644;&#20851;&#32852;&#24615;&#65292;&#24182;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#23478;&#30456;&#20284;&#24615;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#31038;&#20250;&#21644;&#31185;&#23398;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#29616;&#20195;&#30740;&#31350;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#30340;&#21916;&#22909;&#26469;&#20419;&#36827;&#38899;&#20048;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#33402;&#26415;&#23478;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#26041;&#38754;&#65292;&#29978;&#33267;&#19982;&#20027;&#35266;&#35282;&#24230;&#30456;&#20851;&#65292;&#24182;&#19988;&#32463;&#24120;&#24433;&#21709;&#25512;&#33616;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GATSY&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19978;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#30001;&#33402;&#26415;&#23478;&#30340;&#32858;&#31867;&#23884;&#20837;&#39537;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#30340;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#22312;&#19981;&#36807;&#20998;&#20381;&#36182;&#25163;&#24037;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#38899;&#20048;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#34394;&#26500;&#30340;&#33402;&#26415;&#23478;&#65292;&#19982;&#20197;&#21069;&#19981;&#30456;&#20851;&#30340;&#33402;&#26415;&#23478;&#24314;&#31435;&#32852;&#31995;&#65292;&#24182;&#26681;&#25454;&#21487;&#33021;&#30340;&#24322;&#36136;&#26469;&#28304;&#33719;&#24471;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The artist similarity quest has become a crucial subject in social and scientific contexts. Modern research solutions facilitate music discovery according to user tastes. However, defining similarity among artists may involve several aspects, even related to a subjective perspective, and it often affects a recommendation. This paper presents GATSY, a recommendation system built upon graph attention networks and driven by a clusterized embedding of artists. The proposed framework takes advantage of a graph topology of the input data to achieve outstanding performance results without relying heavily on hand-crafted features. This flexibility allows us to introduce fictitious artists in a music dataset, create bridges to previously unrelated artists, and get recommendations conditioned by possibly heterogeneous sources. Experimental results prove the effectiveness of the proposed method with respect to state-of-the-art solutions.
&lt;/p&gt;</description></item></channel></rss>