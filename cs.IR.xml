<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;UniLLMRec&#23558;&#22810;&#38454;&#27573;&#20219;&#21153;&#25972;&#21512;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.00702</link><description>&lt;p&gt;
&#21388;&#20518;&#20102;&#25554;&#20214;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tired of Plugins? Large Language Models Can Be End-To-End Recommenders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00702
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;UniLLMRec&#23558;&#22810;&#38454;&#27573;&#20219;&#21153;&#25972;&#21512;&#20026;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23545;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#39044;&#27979;&#29992;&#25143;&#20852;&#36259;&#12290;&#23427;&#20204;&#20027;&#35201;&#35774;&#35745;&#20026;&#39034;&#24207;&#27969;&#27700;&#32447;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#19981;&#21516;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#22495;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20351;&#19968;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#32431;&#31929;&#21033;&#29992;LLM&#26469;&#22788;&#29702;&#25512;&#33616;&#27969;&#27700;&#32447;&#30340;&#21333;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#20197;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#21521;LLM&#21576;&#29616;&#22823;&#35268;&#27169;&#29289;&#21697;&#38598;&#21512;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31471;&#21040;&#31471;&#25512;&#33616;&#26694;&#26550;&#65306;UniLLMRec&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UniLLMRec&#36890;&#36807;&#25512;&#33616;&#38142;&#38598;&#25104;&#22810;&#38454;&#27573;&#20219;&#21153;&#65288;&#20363;&#22914;&#21484;&#22238;&#12289;&#25490;&#24207;&#12289;&#37325;&#26032;&#25490;&#24207;&#65289;&#12290;&#20026;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#29289;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00702v1 Announce Type: new  Abstract: Recommender systems aim to predict user interest based on historical behavioral data. They are mainly designed in sequential pipelines, requiring lots of data to train different sub-systems, and are hard to scale to new domains. Recently, Large Language Models (LLMs) have demonstrated remarkable generalized capabilities, enabling a singular model to tackle diverse recommendation tasks across various scenarios. Nonetheless, existing LLM-based recommendation systems utilize LLM purely for a single task of the recommendation pipeline. Besides, these systems face challenges in presenting large-scale item sets to LLMs in natural language format, due to the constraint of input length. To address these challenges, we introduce an LLM-based end-to-end recommendation framework: UniLLMRec. Specifically, UniLLMRec integrates multi-stage tasks (e.g. recall, ranking, re-ranking) via chain-of-recommendations. To deal with large-scale items, we propose
&lt;/p&gt;</description></item></channel></rss>