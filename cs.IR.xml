<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.18434</link><description>&lt;p&gt;
&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graph Regularized Encoder Training for Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#26497;&#31471;&#20998;&#31867;&#65292;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045; GCN &#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#28145;&#24230;&#26497;&#31471;&#20998;&#31867;&#65288;XC&#65289;&#26088;&#22312;&#35757;&#32451;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#37197;&#22871;&#30340;&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#20197;&#20174;&#19968;&#20010;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#31614;&#38598;&#21512;&#20013;&#20026;&#25968;&#25454;&#28857;&#25171;&#19978;&#26368;&#30456;&#20851;&#30340;&#23376;&#26631;&#31614;&#38598;&#21512;&#12290;&#22312;&#25490;&#21517;&#12289;&#25512;&#33616;&#21644;&#26631;&#35760;&#20013;&#24120;&#35265;&#30340;XC&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#35757;&#32451;&#25968;&#25454;&#26497;&#23569;&#30340;&#23614;&#26631;&#31614;&#12290;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#20294;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#20219;&#21153;&#20803;&#25968;&#25454;&#24182;&#22686;&#24378;&#27169;&#22411;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#27491;&#24335;&#30830;&#23450;&#20102;&#22312;&#33509;&#24178;&#29992;&#20363;&#20013;&#65292;&#36890;&#36807;&#29992;&#38750;GCN&#26550;&#26500;&#26367;&#25442;GCNs&#65292;&#23436;&#20840;&#21487;&#20197;&#36991;&#20813;GCNs&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35268;&#33539;&#32534;&#30721;&#22120;&#35757;&#32451;&#27604;&#23454;&#26045;GCN&#26356;&#21152;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;RAMEN&#65292;&#29992;&#20110;&#21033;&#29992;XC&#35774;&#32622;&#20013;&#30340;&#22270;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18434v1 Announce Type: new  Abstract: Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. Graph convolutional networks (GCN) present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of GCNs is entirely avoidable by replacing GCNs with non-GCN architectures. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN is presented to utilize graph metadata in XC settings that offers 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2103.03223</link><description>&lt;p&gt;
&#37327;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#25351;&#22312;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#23427;&#20063;&#20195;&#34920;&#30528;&#19968;&#20010;&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20221;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20197;&#25903;&#25345;&#31639;&#27861;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#30340;&#24443;&#24213;&#23454;&#35777;&#24615;&#24615;&#33021;&#27604;&#36739;&#65292;&#21253;&#25324;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#37327;&#21270;&#35774;&#32622;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27809;&#26377;&#21333;&#19968;&#31639;&#27861;&#33021;&#22815;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#22810;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21478;&#19968;&#32452;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;Generaliz&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
&lt;/p&gt;</description></item></channel></rss>