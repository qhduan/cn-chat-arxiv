<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMMRec&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17373</link><description>&lt;p&gt;
FMMRec: &#20844;&#24179;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FMMRec: Fairness-aware Multimodal Recommendation. (arXiv:2310.17373v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FMMRec&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#22240;&#20026;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#24182;&#32467;&#21512;&#21508;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#25512;&#33616;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24341;&#20837;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#21487;&#33021;&#20250;&#23558;&#26356;&#22810;&#29992;&#25143;&#30340;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65289;&#26292;&#38706;&#32473;&#25512;&#33616;&#31995;&#32479;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#26041;&#27861;&#35201;&#20040;&#19982;&#22810;&#27169;&#24577;&#24773;&#22659;&#19981;&#20860;&#23481;&#65292;&#35201;&#20040;&#30001;&#20110;&#24573;&#35270;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#25935;&#24863;&#20449;&#24687;&#32780;&#23548;&#33268;&#20844;&#24179;&#24615;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#22312;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#25512;&#33616;&#26041;&#27861;&#65288;&#31216;&#20026;FMMRec&#65289;&#65292;&#36890;&#36807;&#20174;&#27169;&#24577;&#34920;&#31034;&#20013;&#20998;&#31163;&#25935;&#24863;&#21644;&#38750;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20998;&#31163;&#21518;&#30340;&#27169;&#24577;&#34920;&#31034;&#26469;&#25351;&#23548;&#26356;&#20844;&#24179;&#30340;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multimodal recommendations have gained increasing attention for effectively addressing the data sparsity problem by incorporating modality-based representations. Although multimodal recommendations excel in accuracy, the introduction of different modalities (e.g., images, text, and audio) may expose more users' sensitive information (e.g., gender and age) to recommender systems, resulting in potentially more serious unfairness issues. Despite many efforts on fairness, existing fairness-aware methods are either incompatible with multimodal scenarios, or lead to suboptimal fairness performance due to neglecting sensitive information of multimodal content. To achieve counterfactual fairness in multimodal recommendations, we propose a novel fairness-aware multimodal recommendation approach (dubbed as FMMRec) to disentangle the sensitive and non-sensitive information from modal representations and leverage the disentangled modal representations to guide fairer representation learn
&lt;/p&gt;</description></item></channel></rss>