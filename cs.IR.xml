<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.09617</link><description>&lt;p&gt;
&#22686;&#24378;LLM&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#65306;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLM&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#25366;&#25496;&#25928;&#29575;&#21644;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#36793;&#32536;&#20449;&#24687;&#26469;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#65292;&#24182;&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#19981;&#20165;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#26684;&#23616;&#65292;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21331;&#36234;&#24212;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25366;&#25496;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#20851;&#31995;&#25366;&#25496;&#26041;&#38754;&#26377;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#31471;&#30740;&#31350;&#23578;&#26410;&#26377;&#25928;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#22312;&#22270;&#20851;&#31995;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#21644;&#33021;&#21147;&#21463;&#38480;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;LLM&#26080;&#27861;&#28145;&#20837;&#21033;&#29992;&#22270;&#20013;&#30340;&#36793;&#32536;&#20449;&#24687;&#65292;&#32780;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#33410;&#28857;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#24046;&#36317;&#38480;&#21046;&#20102;LLM&#20174;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#27934;&#35265;&#30340;&#28508;&#21147;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#22270;&#30340;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09617v1 Announce Type: new  Abstract: The extraordinary performance of large language models has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus
&lt;/p&gt;</description></item></channel></rss>