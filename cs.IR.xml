<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>LoRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#35782;&#21035;&#26410;&#30693;&#30340;&#31713;&#25913;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17723</link><description>&lt;p&gt;
LoRec: &#38024;&#23545;&#31713;&#25913;&#25915;&#20987;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#40065;&#26834;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17723
&lt;/p&gt;
&lt;p&gt;
LoRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#35782;&#21035;&#26410;&#30693;&#30340;&#31713;&#25913;&#25915;&#20987;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20197;&#20854;&#25429;&#25417;&#29992;&#25143;&#21160;&#24577;&#20852;&#36259;&#21644;&#29289;&#21697;&#38388;&#36716;&#25442;&#27169;&#24335;&#30340;&#33021;&#21147;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#22266;&#26377;&#24320;&#25918;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#31713;&#25913;&#25915;&#20987;&#65292;&#21363;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#27450;&#35784;&#24615;&#29992;&#25143;&#26469;&#25805;&#32437;&#23398;&#20064;&#27169;&#24335;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#31574;&#30053;&#20027;&#35201;&#20381;&#36182;&#20110;&#39044;&#23450;&#30340;&#20551;&#35774;&#25110;&#20174;&#29305;&#23450;&#24050;&#30693;&#25915;&#20987;&#20013;&#25552;&#21462;&#30340;&#35268;&#21017;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26410;&#30693;&#25915;&#20987;&#31867;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25152;&#22218;&#25324;&#30340;&#20016;&#23500;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#20851;&#27880;LLMs&#22312;&#26816;&#27979;&#25512;&#33616;&#31995;&#32479;&#20013;&#26410;&#30693;&#27450;&#35784;&#27963;&#21160;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#35813;&#31574;&#30053;&#31216;&#20026;LLM4Dec&#12290;&#32463;&#39564;&#35780;&#20272;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#26410;&#30693;&#27450;&#35784;&#32773;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item-to-item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudulent users are injected into the training data to manipulate learned patterns. Traditional defense strategies predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attack types. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), our research initially focuses on the capabilities of LLMs in the detection of unknown fraudulent activities within recommender systems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the substantial capability of LLMs in identifying unknown fraudsters, leveraging their expansive, open-world knowledge.   Building upon this, we propose 
&lt;/p&gt;</description></item></channel></rss>