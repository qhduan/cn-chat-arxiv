<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.01788</link><description>&lt;p&gt;
LitLLM&#65306;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LitLLM: A Toolkit for Scientific Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01788
&lt;/p&gt;
&lt;p&gt;
"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#23545;&#20110;&#29702;&#35299;&#30740;&#31350;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#26500;&#24314;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#33258;&#21160;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#22120;&#21464;&#24471;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#27492;&#31867;&#32508;&#36848;&#30340;&#29616;&#26377;&#24037;&#20316;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#38750;&#23454;&#38469;&#20449;&#24687;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#26410;&#21463;&#36807;&#35757;&#32451;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#22312;LLM&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25688;&#35201;&#36716;&#21270;&#20026;&#20851;&#38190;&#35789;&#26469;&#36827;&#34892;&#32593;&#32476;&#25628;&#32034;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#12290;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#26469;&#25913;&#36827;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#26681;&#25454;-
&lt;/p&gt;
&lt;p&gt;
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#36890;&#36807;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10049</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;GCN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Reasoning Based on Attention GCN. (arXiv:2312.10049v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#36890;&#36807;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#20197;&#25552;&#39640;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26816;&#26597;&#23454;&#20307;&#20043;&#38388;&#21450;&#20854;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#23454;&#20307;&#24320;&#21457;&#35814;&#32454;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;GCN&#20351;&#29992;&#20849;&#20139;&#21442;&#25968;&#26377;&#25928;&#22320;&#34920;&#31034;&#30456;&#37051;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#25972;&#21512;&#23454;&#20307;&#30340;&#23646;&#24615;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;&#20102;&#20016;&#23500;&#30340;&#38544;&#24335;&#29305;&#24449;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#12289;&#38382;&#31572;&#31995;&#32479;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#25968;&#25454;&#25972;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26041;&#27861;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel technique to enhance Knowledge Graph Reasoning by combining Graph Convolution Neural Network (GCN) with the Attention Mechanism. This approach utilizes the Attention Mechanism to examine the relationships between entities and their neighboring nodes, which helps to develop detailed feature vectors for each entity. The GCN uses shared parameters to effectively represent the characteristics of adjacent entities. We first learn the similarity of entities for node representation learning. By integrating the attributes of the entities and their interactions, this method generates extensive implicit feature vectors for each entity, improving performance in tasks including entity classification and link prediction, outperforming traditional neural network models. To conclude, this work provides crucial methodological support for a range of applications, such as search engines, question-answering systems, recommendation systems, and data integration tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.15793</link><description>&lt;p&gt;
&#27010;&#35201;&#12289;&#20142;&#28857;&#21644;&#34892;&#21160;&#39033;&#30446;&#65306;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system. (arXiv:2307.15793v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35758;&#22312;&#24037;&#20316;&#21327;&#35843;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21521;&#28151;&#21512;&#21644;&#36828;&#31243;&#24037;&#20316;&#30340;&#36716;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20250;&#35758;&#27491;&#22312;&#36716;&#31227;&#21040;&#22312;&#32447;&#35745;&#31639;&#26426;&#23186;&#20307;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#22312;&#26356;&#19981;&#21560;&#24341;&#20154;&#30340;&#20250;&#35758;&#19978;&#33457;&#36153;&#26356;&#22810;&#30340;&#26102;&#38388;&#65289;&#21644;&#26032;&#30340;&#26426;&#20250;&#65288;&#20363;&#22914;&#33258;&#21160;&#36716;&#24405;/&#23383;&#24149;&#21644;&#24635;&#32467;&#25903;&#25345;&#65289;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24635;&#32467;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#30340;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#20250;&#35758;&#20307;&#39564;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#38271;&#31687;&#36716;&#24405;&#21644;&#26080;&#27861;&#26681;&#25454;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#25429;&#25417;&#21040;&#22810;&#26679;&#30340;&#24635;&#32467;&#38656;&#27714;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#25216;&#26415;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20102;&#19968;&#31181;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24605;&#20102;&#20004;&#20010;&#26126;&#26174;&#30340;&#24635;&#32467;&#34920;&#31034;&#26041;&#24335;&#8212;&#8212;&#37325;&#35201;&#20142;&#28857;&#21644;&#32467;&#26500;&#21270;&#30340;&#20998;&#32423;&#20250;&#35758;&#32426;&#35201;&#35270;&#22270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the rep
&lt;/p&gt;</description></item></channel></rss>