<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19411</link><description>&lt;p&gt;
PaECTER&#65306;&#20351;&#29992;&#24341;&#25991;&#20449;&#24687;&#30340;&#19987;&#21033;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PaECTER: Patent-level Representation Learning using Citation-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19411
&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#38754;&#21521;&#19987;&#21033;&#30340;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#23457;&#26680;&#21592;&#28155;&#21152;&#30340;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#19987;&#21033;&#25991;&#26723;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#19982;&#19987;&#21033;&#39046;&#22495;&#20013;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;PaECTER&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19987;&#21033;&#24341;&#25991;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#19981;&#21516;&#30340;&#25490;&#21517;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#19987;&#21033;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#19987;&#21033;BERT&#65289;&#12290;&#19982;25&#20010;&#19981;&#30456;&#20851;&#30340;&#19987;&#21033;&#30456;&#27604;&#65292;PaECTER&#22312;&#24179;&#22343;&#25490;&#21517;1.32&#22788;&#39044;&#27979;&#21040;&#33267;&#23569;&#19968;&#20010;&#26368;&#30456;&#20284;&#30340;&#19987;&#21033;&#12290;PaECTER&#20174;&#19987;&#21033;&#25991;&#26412;&#29983;&#25104;&#30340;&#25968;&#20540;&#34920;&#31034;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#36861;&#36394;&#30693;&#35782;&#27969;&#21160;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#22312;&#21457;&#26126;&#20154;&#21644;&#19987;&#21033;&#30340;&#20808;&#21069;&#25216;&#26415;&#25628;&#32034;&#32972;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
&lt;/p&gt;</description></item></channel></rss>