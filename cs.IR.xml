<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.10036</link><description>&lt;p&gt;
LOCALINTEL&#65306;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#32593;&#32476;&#30693;&#35782;&#29983;&#25104;&#32452;&#32455;&#23041;&#32961;&#24773;&#25253;
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10036
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;SoC&#65289;&#20998;&#26512;&#24072;&#20174;&#20844;&#24320;&#35775;&#38382;&#30340;&#20840;&#29699;&#23041;&#32961;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#23041;&#32961;&#25253;&#21578;&#65292;&#24182;&#25163;&#21160;&#33258;&#23450;&#20041;&#20197;&#36866;&#24212;&#29305;&#23450;&#32452;&#32455;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#20998;&#26512;&#24072;&#36824;&#20381;&#36182;&#20110;&#20869;&#37096;&#23384;&#20648;&#24211;&#65292;&#20316;&#20026;&#32452;&#32455;&#30340;&#31169;&#26377;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21487;&#20449;&#30340;&#32593;&#32476;&#24773;&#25253;&#12289;&#20851;&#38190;&#25805;&#20316;&#32454;&#33410;&#21644;&#30456;&#20851;&#32452;&#32455;&#20449;&#24687;&#37117;&#23384;&#20648;&#22312;&#36825;&#20123;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#12290;&#20998;&#26512;&#24072;&#21033;&#29992;&#36825;&#20123;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20174;&#20107;&#19968;&#39033;&#32321;&#37325;&#30340;&#20219;&#21153;&#65292;&#25163;&#21160;&#21019;&#24314;&#32452;&#32455;&#29420;&#29305;&#30340;&#23041;&#32961;&#21709;&#24212;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#22788;&#29702;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#32452;&#32455;&#29305;&#23450;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCALINTEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.  In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#20013;&#30340;&#20813;&#35757;&#32451;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#20869;&#23481;&#26469;&#21512;&#25104;&#19968;&#20010;&#23567;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09874</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#30340;&#20813;&#35757;&#32451;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation. (arXiv:2310.09874v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#20013;&#30340;&#20813;&#35757;&#32451;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#20869;&#23481;&#26469;&#21512;&#25104;&#19968;&#20010;&#23567;&#32780;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20869;&#23481;&#25512;&#33616;&#65288;CBR&#65289;&#25216;&#26415;&#21033;&#29992;&#29289;&#21697;&#30340;&#20869;&#23481;&#20449;&#24687;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#65292;&#20294;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#25991;&#26412;CBR&#36827;&#34892;&#25968;&#25454;&#38598;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38598;&#21387;&#32553;&#30340;&#30446;&#26631;&#26159;&#21512;&#25104;&#19968;&#20010;&#23567;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#27169;&#22411;&#24615;&#33021;&#21487;&#20197;&#19982;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#38024;&#23545;&#36830;&#32493;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#25110;&#23884;&#20837;&#21521;&#37327;&#65289;&#30340;&#20998;&#31867;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;CBR&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#20013;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#22312;&#25968;&#25454;&#38598;&#21387;&#32553;&#26399;&#38388;&#29983;&#25104;&#25991;&#26412;&#20869;&#23481;&#12290;&#20026;&#20102;&#22788;&#29702;&#28041;&#21450;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20132;&#20114;&#25968;&#25454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;...
&lt;/p&gt;
&lt;p&gt;
Modern techniques in Content-based Recommendation (CBR) leverage item content information to provide personalized services to users, but suffer from resource-intensive training on large datasets. To address this issue, we explore the dataset condensation for textual CBR in this paper. The goal of dataset condensation is to synthesize a small yet informative dataset, upon which models can achieve performance comparable to those trained on large datasets. While existing condensation approaches are tailored to classification tasks for continuous data like images or embeddings, direct application of them to CBR has limitations. To bridge this gap, we investigate efficient dataset condensation for content-based recommendation. Inspired by the remarkable abilities of large language models (LLMs) in text comprehension and generation, we leverage LLMs to empower the generation of textual content during condensation. To handle the interaction data involving both users and items, we devise a dua
&lt;/p&gt;</description></item></channel></rss>