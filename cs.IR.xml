<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18276</link><description>&lt;p&gt;
RankMamba&#65292;&#22312;Transformer&#26102;&#20195;&#23545;Mamba&#25991;&#26723;&#25490;&#21517;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18276
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;(IR)&#31561;&#22810;&#20010;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;--&#27880;&#24847;&#21147;&#65292;&#22312;&#35757;&#32451;&#20013;&#38656;&#35201;$O(n^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22312;&#25512;&#26029;&#20013;&#38656;&#35201;$O(n)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#27604;&#22914;Flash Attention&#21644;Multi-query Attention&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#26088;&#22312;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#26469;&#21462;&#20195;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#22411;&#32467;&#26500;--Mamba&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-Embedder&#65292;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#20840;&#38754;&#25903;&#25345;LLMs&#30340;&#22810;&#26679;&#21270;&#26816;&#32034;&#22686;&#24378;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.07554</link><description>&lt;p&gt;
&#26816;&#32034;&#20219;&#20309;&#20869;&#23481;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieve Anything To Augment Large Language Models. (arXiv:2310.07554v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07554
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-Embedder&#65292;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#20840;&#38754;&#25903;&#25345;LLMs&#30340;&#22810;&#26679;&#21270;&#26816;&#32034;&#22686;&#24378;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30528;&#30001;&#20110;&#20854;&#22312;&#30693;&#35782;&#12289;&#35760;&#24518;&#12289;&#23545;&#40784;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#20135;&#29983;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#33021;&#21333;&#38752;LLMs&#33258;&#34892;&#35299;&#20915;&#65292;&#32780;&#24212;&#20381;&#36182;&#20110;&#26469;&#33258;&#22806;&#37096;&#19990;&#30028;&#65288;&#22914;&#30693;&#35782;&#24211;&#12289;&#35760;&#24518;&#23384;&#20648;&#12289;&#28436;&#31034;&#31034;&#20363;&#21644;&#24037;&#20855;&#65289;&#30340;&#36741;&#21161;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;LLMs&#19982;&#22806;&#37096;&#36741;&#21161;&#20043;&#38388;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36935;&#21040;&#20004;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#29992;&#26816;&#32034;&#22120;&#26410;&#33021;&#36866;&#24403;&#20248;&#21270;LLMs&#30340;&#26816;&#32034;&#22686;&#24378;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20219;&#21153;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#32570;&#20047;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20854;&#22312;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-Embedder&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#20840;&#38754;&#25903;&#25345;LLMs&#30340;&#22810;&#26679;&#21270;&#26816;&#32034;&#22686;&#24378;&#38656;&#27714;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#32479;&#19968;&#27169;&#22411;&#24182;&#19981;&#23481;&#26131;&#65292;&#30001;&#20110;&#19981;&#21516;&#26816;&#32034;&#22686;&#24378;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) face significant challenges stemming from their inherent limitations in knowledge, memory, alignment, and action. These challenges cannot be addressed by LLMs alone, but should rely on assistance from the external world, such as knowledge base, memory store, demonstration examples, and tools. Retrieval augmentation stands as a vital mechanism for bridging the gap between LLMs and the external assistance. However, conventional methods encounter two pressing issues. On the one hand, the general-purpose retrievers are not properly optimized for the retrieval augmentation of LLMs. On the other hand, the task-specific retrievers lack the required versatility, hindering their performance across the diverse retrieval augmentation scenarios.  In this work, we present a novel approach, the LLM-Embedder, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model. Training such a unified model is non-trivial, as va
&lt;/p&gt;</description></item></channel></rss>