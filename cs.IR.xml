<rss version="2.0"><channel><title>Chat Arxiv cs.IR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.IR</description><item><title>AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03984</link><description>&lt;p&gt;
AdaRec&#65306;&#29992;&#20110;&#22686;&#24378;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#24230;&#30340;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement. (arXiv:2310.03984v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03984
&lt;/p&gt;
&lt;p&gt;
AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#21442;&#19982;&#24230;&#12290;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65288;&#22914;&#20114;&#21160;&#39057;&#29575;&#21644;&#20445;&#30041;&#20542;&#21521;&#65289;&#30340;&#19981;&#26029;&#22797;&#26434;&#21464;&#21270;&#12290;&#24403;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#26102;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20250;&#19981;&#26029;&#21463;&#21040;&#36825;&#20123;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#24182;&#38590;&#20197;&#36866;&#24212;&#36825;&#31181;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#65288;AdaRec&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;AdaRec&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#65292;&#20174;&#29992;&#25143;&#30340;&#20114;&#21160;&#36712;&#36857;&#20013;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#21453;&#26144;&#20102;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19982;&#24403;&#21069;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21305;&#37197;&#31243;&#24230;&#65292;&#24182;&#24110;&#21161;&#31574;&#30053;&#35782;&#21035;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recomm
&lt;/p&gt;</description></item></channel></rss>