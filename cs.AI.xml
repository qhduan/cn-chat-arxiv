<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.15698</link><description>&lt;p&gt;
SceneX&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#21270;&#21487;&#25511;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#25152;&#38656;&#30340;&#22330;&#26223;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19981;&#20860;&#23481;&#24037;&#19994;&#27969;&#31243;&#30340;3D&#22522;&#20803;&#65288;&#22914;&#28857;&#20113;&#25110;&#36752;&#23556;&#22330;&#65289;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#36825;&#23548;&#33268;&#23398;&#26415;&#30740;&#31350;&#19982;&#24037;&#19994;&#37096;&#32626;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#31243;&#24207;&#21270;&#21487;&#25511;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#21019;&#24314;&#21487;&#25193;&#23637;&#21644;&#39640;&#36136;&#37327;&#30340;&#36164;&#20135;&#65292;&#20294;&#23545;&#26222;&#36890;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#35745;&#24072;&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15698v1 Announce Type: cross  Abstract: Due to its great application potential, large-scale scene generation has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32473;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#36825;&#20123;&#26426;&#22120;&#23436;&#20840;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#12290;</title><link>https://arxiv.org/abs/2403.15437</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#19981;&#36879;&#26126;&#26102;&#20195;&#30340;&#20808;&#39564;&#30693;&#35782;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23398;&#21457;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15437
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32473;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#36825;&#20123;&#26426;&#22120;&#23436;&#20840;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#22312;&#24403;&#20195;&#25968;&#23398;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;Appel&#21644;Haken&#30340;&#31243;&#24207;&#20013;&#33719;&#24471;&#22235;&#33394;&#23450;&#29702;&#30340;&#30495;&#27491;&#25968;&#23398;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#21482;&#26159;&#19968;&#31181;&#37325;&#22797;&#24212;&#29992;&#20154;&#31867;&#24418;&#24335;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#20195;LLMs / DNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#65292;&#36825;&#22312;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#25968;&#23398;&#30693;&#35782;&#26041;&#38754;&#20135;&#29983;&#20102;&#38556;&#30861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#32473;&#36825;&#20123;&#26426;&#22120;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#33719;&#24471;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#21407;&#22987;&#26426;&#22120;&#23545;&#25105;&#20204;&#23436;&#20840;&#19981;&#36879;&#26126;&#65292;&#23427;&#20204;&#36755;&#20986;&#30340;&#35777;&#26126;&#20063;&#26080;&#27861;&#30001;&#20154;&#31867;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15437v1 Announce Type: new  Abstract: Computation is central to contemporary mathematics. Many accept that we can acquire genuine mathematical knowledge of the Four Color Theorem from Appel and Haken's program insofar as it is simply a repetitive application of human forms of mathematical reasoning. Modern LLMs / DNNs are, by contrast, opaque to us in significant ways, and this creates obstacles in obtaining mathematical knowledge from them. We argue, however, that if a proof-checker automating human forms of proof-checking is attached to such machines, then we can obtain apriori mathematical knowledge from them, even though the original machines are entirely opaque to us and the proofs they output are not human-surveyable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#36275;&#37096;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#32447;&#25512;&#29702;&#30340;&#35745;&#31639;&#38656;&#27714;&#26497;&#20302;&#65292;&#26080;&#21551;&#21457;&#24335;&#65292;&#24182;&#20381;&#36182;&#36830;&#32493;&#30340;&#21160;&#20316;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.12589</link><description>&lt;p&gt;
FootstepNet: &#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#22312;&#32447;&#21452;&#36275;&#36275;&#37096;&#27493;&#24577;&#35268;&#21010;&#21644;&#39044;&#27979;&#30340;&#39640;&#25928;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal Footstep Planning and Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#36275;&#37096;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#32447;&#25512;&#29702;&#30340;&#35745;&#31639;&#38656;&#27714;&#26497;&#20302;&#65292;&#26080;&#21551;&#21457;&#24335;&#65292;&#24182;&#20381;&#36182;&#36830;&#32493;&#30340;&#21160;&#20316;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#20154;&#24418;&#36816;&#21160;&#25511;&#21046;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#20998;&#20026;&#20960;&#20010;&#23376;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#36275;&#37096;&#27493;&#24577;&#35268;&#21010;&#65292;&#20854;&#20013;&#23450;&#20041;&#20102;&#36275;&#27493;&#30340;&#39034;&#24207;&#12290;&#21363;&#20351;&#22312;&#26356;&#31616;&#21333;&#30340;&#29615;&#22659;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#26368;&#23567;&#24207;&#21015;&#65292;&#29978;&#33267;&#19968;&#20010;&#21487;&#34892;&#30340;&#24207;&#21015;&#65292;&#20063;&#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#65288;&#20363;&#22914;A*&#30340;&#21464;&#31181;&#65289;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#65292;&#35201;&#20040;&#20381;&#36182;&#25163;&#24037;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27493;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#23616;&#37096;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25216;&#26415;&#65292;&#23545;&#22312;&#32447;&#25512;&#29702;&#35201;&#27714;&#38750;&#24120;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#21551;&#21457;&#30340;&#65292;&#20381;&#36182;&#20110;&#19968;&#32452;&#36830;&#32493;&#30340;&#21160;&#20316;&#26469;&#29983;&#25104;&#21487;&#34892;&#30340;&#36275;&#27493;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20854;&#20182;&#26041;&#27861;&#38656;&#35201;&#36873;&#25321;&#19968;&#32452;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12589v1 Announce Type: cross  Abstract: Designing a humanoid locomotion controller is challenging and classically split up in sub-problems. Footstep planning is one of those, where the sequence of footsteps is defined. Even in simpler environments, finding a minimal sequence, or even a feasible sequence, yields a complex optimization problem. In the literature, this problem is usually addressed by search-based algorithms (e.g. variants of A*). However, such approaches are either computationally expensive or rely on hand-crafted tuning of several parameters. In this work, at first, we propose an efficient footstep planning method to navigate in local environments with obstacles, based on state-of-the art Deep Reinforcement Learning (DRL) techniques, with very low computational requirements for on-line inference. Our approach is heuristic-free and relies on a continuous set of actions to generate feasible footsteps. In contrast, other methods necessitate the selection of a rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30528;&#37325;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21464;&#24418;&#20851;&#31995;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#12290;</title><link>https://arxiv.org/abs/2402.18393</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#24418;&#27979;&#35797;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26368;&#20339;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Decision Optimality of Autonomous Driving via Metamorphic Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21464;&#24418;&#20851;&#31995;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18393v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#27979;&#35797;&#22312;ADS&#24320;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#21069;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#38750;&#23433;&#20840;&#20851;&#38190;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;ADS&#21046;&#23450;&#26368;&#20339;&#20915;&#31574;&#24182;&#20026;&#33258;&#21160;&#36710;&#36742;&#65288;AV&#65289;&#29983;&#25104;&#26368;&#20339;&#36335;&#24452;&#30340;&#33021;&#21147;&#21516;&#26679;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;AV&#30340;&#26234;&#33021;&#24615;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#40092;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;ADS&#30340;&#26368;&#20339;&#20915;&#31574;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#24212;&#30340;&#39044;&#35328;&#21644;&#29983;&#25104;&#26377;&#38750;&#26368;&#20339;&#20915;&#31574;&#30340;&#22330;&#26223;&#38590;&#24230;&#36739;&#22823;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35780;&#20272;ADS&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#39318;&#20010;&#29992;&#20110;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#65288;NoDSs&#65289;&#30340;&#26041;&#27861;&#65292;&#21363;ADS&#26410;&#35745;&#31639;AV&#30340;&#26368;&#20339;&#36335;&#24452;&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#20026;&#35299;&#20915;&#39044;&#35328;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#24773;&#20917;&#30340;&#26032;&#39062;&#21464;&#24418;&#20851;&#31995;&#65288;MR&#65289;&#12290;&#36825;&#20010;MR&#30830;&#23450;&#20102;&#24615;&#33021;&#26368;&#20339;&#20915;&#31574;&#30340;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18393v1 Announce Type: new  Abstract: Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions. In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs. Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions. The MR identifies the propert
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06871</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Generative Models for Reranking Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#36890;&#36807;&#24314;&#27169;&#39033;&#30446;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37325;&#26032;&#25490;&#24207;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#25490;&#21015;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#25506;&#32034;&#26368;&#20339;&#24207;&#21015;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29983;&#25104;&#22120;-&#35780;&#20272;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29983;&#25104;&#22120;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#24207;&#21015;&#65292;&#35780;&#20272;&#22120;&#22522;&#20110;&#20272;&#35745;&#30340;&#21015;&#34920;&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#24207;&#21015;&#12290;&#29983;&#25104;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29983;&#25104;&#22120;&#20989;&#25968;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#33258;&#22238;&#24402;&#31574;&#30053;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24037;&#19994;&#31995;&#32479;&#20013;&#37096;&#32626;&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65288;NAR4Rec&#65289;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;m
&lt;/p&gt;
&lt;p&gt;
In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04049</link><description>&lt;p&gt;
&#35770;&#35821;&#26009;&#24211;&#27169;&#25311;&#36777;&#35770;&#20013;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Systematic Biases in LLM Simulations of Debates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20026;&#26500;&#24314;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;&#20064;&#22120;&#65292;&#27809;&#26377;&#30452;&#25509;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#20114;&#21160;&#20013;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20542;&#21521;&#23548;&#33268;&#20986;&#29616;&#34892;&#20026;&#27169;&#24335;&#65292;&#20284;&#20046;&#20559;&#31163;&#20102;&#20154;&#31867;&#20043;&#38388;&#24050;&#32463;&#30830;&#31435;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#21152;&#24378;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25805;&#32437;LLMs&#20869;&#37096;&#30340;&#20559;&#35265;&#65292;&#24182;&#35777;&#26126;&#20195;&#29702;&#38543;&#21518;&#19982;&#36825;&#20123;&#35843;&#25972;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
&lt;/p&gt;</description></item><item><title>LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16044</link><description>&lt;p&gt;
LLMLight: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLMLight: Large Language Models as Traffic Signal Control Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16044
&lt;/p&gt;
&lt;p&gt;
LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#20248;&#21270;&#36947;&#36335;&#32593;&#32476;&#25928;&#29575;&#21644;&#20943;&#23569;&#25317;&#22581;&#12290;&#20256;&#32479;&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20132;&#36890;&#24037;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#24448;&#24448;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#23384;&#22312;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLMLight&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;TSC&#20915;&#31574;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21521;LLM&#25552;&#20379;&#35814;&#32454;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#20917;&#35828;&#26126;&#20316;&#20026;&#25351;&#23548;&#65292;&#20511;&#21161;LLM&#30340;&#20808;&#36827;&#27867;&#21270;&#33021;&#21147;&#65292;LLMLight&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;LightGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;TSC&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#39592;&#24178;LLM&#12290;&#36890;&#36807;&#23398;&#20064;&#32454;&#24494;&#30340;&#20132;&#36890;&#27169;&#24335;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;LightGPT&#22312;&#32463;&#27982;&#25104;&#26412;&#26041;&#38754;&#25552;&#21319;&#20102;LLMLight&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;LLMLight&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2401.10304</link><description>&lt;p&gt;
&#35770;&#31185;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#21644;&#36879;&#26126;&#20351;&#29992;&#30340;&#20934;&#22791;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#26368;&#36817;&#30340;&#31435;&#27861;&#20030;&#25514;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#30456;&#20851;&#30740;&#31350;&#25351;&#20986;&#38656;&#35201;&#35760;&#24405;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#37325;&#22797;&#24615;&#65292;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#20849;&#20139;&#23454;&#36341;&#36817;&#24180;&#26469;&#20063;&#26377;&#20102;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#23398;&#26415;&#26426;&#26500;&#37319;&#29992;&#20102;&#36825;&#20123;&#23454;&#36341;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#25216;&#26415;&#25991;&#20214;&#21457;&#24067;&#22312;&#21516;&#34892;&#35780;&#35758;&#30340;&#20986;&#29256;&#29289;&#19978;&#65292;&#22914;&#25968;&#25454;&#35770;&#25991;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;4041&#31687;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#35770;&#25991;&#26679;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#20854;&#23436;&#25972;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#30740;&#31350;&#20102;&#36817;&#24180;&#26469;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#22810;&#21644;&#26368;&#23569;&#34987;&#35760;&#24405;&#30340;&#26041;&#38754;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25968;&#25454;&#21019;&#24314;&#32773;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.19180</link><description>&lt;p&gt;
JEN-1 Composer: &#19968;&#20010;&#29992;&#20110;&#39640;&#20445;&#30495;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19180
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#21512;&#25104;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#38899;&#36712;&#29983;&#25104;&#30340;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#21407;&#22987;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#21333;&#29420;&#32452;&#25104;&#21644;&#32452;&#21512;&#22810;&#38899;&#36712;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#20316;&#26354;&#23478;&#30340;&#20856;&#22411;&#24037;&#20316;&#27969;&#31243;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEN-1 Composer&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#39640;&#25928;&#22320;&#24314;&#27169;&#22810;&#38899;&#36712;&#38899;&#20048;&#30340;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#12290;JEN-1 Composer&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#22320;&#25972;&#21512;&#20219;&#20309;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#20363;&#22914;Jen-1&#65292;&#22686;&#24378;&#20854;&#22810;&#21151;&#33021;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35838;&#31243;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#36880;&#27493;&#25351;&#23548;&#27169;&#22411;&#20174;&#21333;&#38899;&#36712;&#29983;&#25104;&#21040;&#28789;&#27963;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10426</link><description>&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65306;&#36890;&#36807;&#22797;&#21512;&#23545;&#35937;&#21487;&#29992;&#24615;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#20010;&#25110;&#25104;&#23545;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22312;&#35843;&#26597;&#30001;&#22797;&#26434;&#24418;&#29366;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#32452;&#25104;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#65292;&#23427;&#24314;&#27169;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#39044;&#27979;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#29616;&#26377;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#32473;&#23450;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#21253;&#25324;&#22534;&#21472;&#30340;&#29699;&#20307;&#21644;&#26479;&#23376;&#12289;&#26438;&#21644;&#21253;&#22260;&#26438;&#30340;&#29615;&#31561;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.07857</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Impression-Aware Recommender Systems. (arXiv:2308.07857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07857
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#25968;&#25454;&#28304;&#20026;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#36136;&#37327;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#21360;&#35937;&#26159;&#19968;&#31181;&#21253;&#21547;&#36807;&#21435;&#25512;&#33616;&#65288;&#23637;&#31034;&#30340;&#39033;&#30446;&#65289;&#21644;&#20256;&#32479;&#20114;&#21160;&#30340;&#26032;&#22411;&#25968;&#25454;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#21360;&#35937;&#26469;&#20248;&#21270;&#29992;&#25143;&#20559;&#22909;&#24182;&#20811;&#26381;&#24403;&#21069;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#12290;&#21360;&#35937;&#30340;&#30456;&#20851;&#24615;&#21644;&#20852;&#36259;&#24230;&#36880;&#24180;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31867;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20851;&#20110;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#20391;&#37325;&#20110;&#30740;&#31350;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#19977;&#20010;&#20998;&#31867;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#27599;&#31687;&#32508;&#36848;&#35770;&#25991;&#65292;&#25551;&#36848;&#20102;&#20855;&#26377;&#21360;&#35937;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#25991;&#29486;&#20013;&#32570;&#22833;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09456</link><description>&lt;p&gt;
SR-GAN&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SR GAN&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;ESRGAN&#65292;Real-ESRGAN&#21644;EDSR&#65292;&#22312;&#19968;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#20197;&#21450;&#32463;&#36807;&#38477;&#32423;&#22788;&#29702;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#20284;&#20046;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;Tesseract OCR&#24341;&#25806;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26469;&#33258;huggingface&#30340;EDSR-BASE&#27169;&#22411;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#20027;&#35266;&#35270;&#35273;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20313;&#20505;&#36873;&#27169;&#22411;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EDSR&#29983;&#25104;&#20855;&#26377;&#36739;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#20540;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;Tesseract OCR&#24341;&#25806;&#19979;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;EDSR&#26159;&#19968;&#31181;&#31283;&#20581;&#26377;&#25928;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#21512;&#20110;&#38656;&#35201;OCR&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we evaluate the performance of multiple state-of-the-art SR GAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT&#23545;&#25968;&#25454;&#36827;&#34892;&#28165;&#27927;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#28165;&#27927;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;RoBERTa&#27169;&#22411;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16909</link><description>&lt;p&gt;
RetClean: &#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#25454;&#28246;&#30340;&#26816;&#32034;&#24335;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes. (arXiv:2303.16909v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;ChatGPT&#23545;&#25968;&#25454;&#36827;&#34892;&#28165;&#27927;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#28165;&#27927;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#26412;&#22320;&#37096;&#32626;&#30340;RoBERTa&#27169;&#22411;&#26469;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;ChatGPT&#26469;&#25552;&#20379;&#25968;&#25454;&#28165;&#27927;&#24314;&#35758;&#30340;&#21487;&#33021;&#24615;&#12290;&#20294;&#22312;&#22788;&#29702;&#20225;&#19994;&#25968;&#25454;&#25110;&#38656;&#35201;&#35299;&#37322;&#24314;&#35758;&#26469;&#28304;&#26102;&#65292;ChatGPT&#21487;&#33021;&#26080;&#27861;&#32988;&#20219;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37197;&#21512;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#28246;&#65292;&#23558;&#25968;&#25454;&#28246;&#30340;&#25968;&#25454;&#19982;ChatGPT&#30340;&#33021;&#21147;&#32467;&#21512;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;RoBERTa&#30340;&#23450;&#21046;&#21270;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#26412;&#22320;&#36827;&#34892;&#37096;&#32626;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can foundation models (such as ChatGPT) clean your data? In this proposal, we demonstrate that indeed ChatGPT can assist in data cleaning by suggesting corrections for specific cells in a data table (scenario 1). However, ChatGPT may struggle with datasets it has never encountered before (e.g., local enterprise data) or when the user requires an explanation of the source of the suggested clean values. To address these issues, we developed a retrieval-based method that complements ChatGPT's power with a user-provided data lake. The data lake is first indexed, we then retrieve the top-k relevant tuples to the user's query tuple and finally leverage ChatGPT to infer the correct value (scenario 2). Nevertheless, sharing enterprise data with ChatGPT, an externally hosted model, might not be feasible for privacy reasons. To assist with this scenario, we developed a custom RoBERTa-based foundation model that can be locally deployed. By fine-tuning it on a small number of examples, it can effe
&lt;/p&gt;</description></item></channel></rss>