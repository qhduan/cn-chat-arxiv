<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.18034</link><description>&lt;p&gt;
Paramanu: &#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18034
&lt;/p&gt;
&lt;p&gt;
Paramanu&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21360;&#24230;&#29983;&#25104;&#24335;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#21333;&#20010;GPU&#19978;&#36827;&#34892;&#20102;&#20174;&#22836;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#20808;&#36827;&#30340;&#21360;&#24230;&#20998;&#35789;&#22120;&#20197;&#21450;&#36991;&#20813;&#22810;&#35821;&#35328;&#35781;&#21650;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Gyan AI Paramanu&#65288;&#8220;&#21407;&#23376;&#8221;&#65289;&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#22312;&#21333;&#20010;GPU&#19978;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#30340;&#21253;&#21547;&#21333;&#35821;&#12289;&#21452;&#35821;&#21644;&#22810;&#35821;&#21360;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#65288;&#38463;&#33832;&#22982;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#24247;&#22350;&#23612;&#35821;&#12289;&#36808;&#33922;&#21033;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#26805;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#20197;&#21450;5&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;&#23383;&#27597;&#34920;&#65288;&#23391;&#21152;&#25289;&#35821;&#12289;&#22825;&#22478;&#20307;&#12289;&#22885;&#36842;&#20122;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;1024&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#22312;&#21333;&#20010;GPU&#19978;&#39044;&#35757;&#32451;&#65292;&#38750;&#24120;&#39640;&#25928;&#12289;&#23567;&#24039;&#12289;&#24555;&#36895;&#19988;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20808;&#36827;&#30340;&#21360;&#24230;&#35821;&#20998;&#35789;&#22120;&#65292;&#29978;&#33267;&#21487;&#20197;&#26631;&#35760;&#26410;&#30693;&#35821;&#35328;&#12290;&#20026;&#20102;&#36991;&#20813;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;mParamanu&#27169;&#22411;&#20013;&#30340;&#8220;&#22810;&#35821;&#35328;&#35781;&#21650;&#8221;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#23383;&#27597;&#34920;&#25353;&#35821;&#35328;&#31867;&#22411;&#36827;&#34892;&#20102;&#21487;&#27604;&#36739;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#35821;&#27861;&#12289;&#36830;&#36143;&#24615;&#12289;&#21019;&#36896;&#24615;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Gyan AI Paramanu ("atom"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the "curse of multi-linguality" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19922</link><description>&lt;p&gt;
&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#20854;&#26399;&#26395;&#20540;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24773;&#22659;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#23558;&#31574;&#30053;&#32593;&#32476;&#23884;&#20837;&#21040;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#37325;&#26032;&#26694;&#23450;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20854;&#20013;&#33391;&#22909;&#30340;&#31574;&#30053;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#31574;&#30053;&#34920;&#31034;&#22312;&#20915;&#23450;&#26368;&#20339;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
&lt;/p&gt;</description></item></channel></rss>