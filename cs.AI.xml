<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.10056</link><description>&lt;p&gt;
&#19981;&#35201;&#21322;&#24515;&#21322;&#24847;&#65306;&#25429;&#25417;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10056
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#39537;&#20351;&#23427;&#20204;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#31526;&#21512;&#20154;&#31867;&#30446;&#26631;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#65288;CIT&#65289;&#36807;&#31243;&#21487;&#33021;&#20250;&#24102;&#26469;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#38382;&#39064;&#65292;&#23548;&#33268;&#20808;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36864;&#21270;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#27169;&#22411;&#25110;&#37325;&#25918;&#25968;&#25454;&#26469;&#32531;&#35299;CF&#38382;&#39064;&#65292;&#20294;&#36825;&#21487;&#33021;&#21482;&#35760;&#20303;&#25351;&#20196;&#30340;&#34920;&#38754;&#27169;&#24335;&#24182;&#22312;&#30041;&#23384;&#20219;&#21153;&#19978;&#24863;&#21040;&#22256;&#24785;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#65288;KPIG&#65289;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25513;&#30422;&#37096;&#20998;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#24182;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#19982;&#27491;&#30830;&#21709;&#24212;&#30456;&#20851;&#30340;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#65292;&#24182;&#20943;&#36731;&#23545;&#25351;&#23548;&#20013;&#36890;&#29992;&#25551;&#36848;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;P&#20998;&#21644;V&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 Announce Type: cross  Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score,
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2108.00385</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformer-based deep imitation learning for dual-arm robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.00385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#23545;&#35299;&#20915;&#29087;&#32451;&#25805;&#20316;&#20219;&#21153;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#29615;&#22659;&#27169;&#22411;&#21644;&#39044;&#32534;&#31243;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#21452;&#33218;&#25805;&#20316;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#38468;&#21152;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#24341;&#36215;&#30340;&#29366;&#24577;&#32500;&#24230;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#35745;&#31639;&#39034;&#24207;&#36755;&#20837;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19987;&#27880;&#20110;&#37325;&#35201;&#20803;&#32032;&#12290;Transformer&#65292;&#20316;&#20026;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#34987;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#20197;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#36827;&#34892;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.00385v2 Announce Type: replace-cross  Abstract: Deep imitation learning is promising for solving dexterous manipulation tasks because it does not require an environment model and pre-programmed robot behavior. However, its application to dual-arm manipulation tasks remains challenging. In a dual-arm manipulation setup, the increased number of state dimensions caused by the additional robot manipulators causes distractions and results in poor performance of the neural networks. We address this issue using a self-attention mechanism that computes dependencies between elements in a sequential input and focuses on important elements. A Transformer, a variant of self-attention architecture, is applied to deep imitation learning to solve dual-arm manipulation tasks in the real world. The proposed method has been tested on dual-arm manipulation tasks using a real robot. The experimental results demonstrated that the Transformer-based deep imitation learning architecture can attend 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2102.01295</link><description>&lt;p&gt;
&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.01295
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#31934;&#24230;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#31359;&#38024;&#24341;&#32447;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#29702;&#23398;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#20302;&#20998;&#36776;&#29575;&#22806;&#22260;&#35270;&#35273;&#21644;&#24555;&#36895;&#31227;&#21160;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#25163;&#20256;&#36865;&#21040;&#23545;&#35937;&#30340;&#38468;&#36817;&#65292;&#24182;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20985;&#38519;&#35270;&#35273;&#26469;&#23454;&#29616;&#25163;&#31934;&#30830;&#23545;&#20934;&#23545;&#35937;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#31359;&#38024;&#24341;&#32447;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#36828;&#31243;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#20957;&#35270;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#22312;&#38752;&#36817;&#30446;&#26631;&#26102;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#22260;&#32469;&#20957;&#35270;&#28857;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26469;&#31934;&#30830;&#25511;&#21046;&#32447;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#30340;&#22806;&#22260;&#22270;&#20687;&#21040;&#36798;&#30446;&#26631;&#38468;&#36817;&#12290;&#26412;&#30740;&#31350;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.01295v3 Announce Type: replace-cross  Abstract: A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.07999</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#27979;&#35797;&#20934;&#30830;&#24230;&#65306;&#31995;&#32479;&#32508;&#36848;&#12289;Meta&#20998;&#26512;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20020;&#24202;&#20351;&#29992;&#20043;&#21069;AI&#27169;&#22411;&#30340;&#35786;&#26029;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#25104;&#21151;&#30340;&#37319;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#25253;&#36947;&#24212;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#35786;&#26029;&#30446;&#30340;&#30340;AI&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;AI&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#30142;&#30149;&#31867;&#22411;&#30340;WSI&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#30740;&#31350;&#12290;&#21442;&#32771;&#26631;&#20934;&#26159;&#36890;&#36807;&#32452;&#32455;&#30149;&#29702;&#23398;&#35780;&#20272;&#21644;/&#25110;&#20813;&#30123;&#32452;&#21270;&#35786;&#26029;&#12290;&#25628;&#32034;&#22312;2022&#24180;6&#26376;&#22312;PubMed&#12289;EMBASE&#21644;CENTRAL&#20013;&#36827;&#34892;&#12290;&#22312;2976&#39033;&#30740;&#31350;&#20013;&#65292;&#26377;100&#39033;&#32435;&#20837;&#32508;&#36848;&#65292;48&#39033;&#32435;&#20837;&#23436;&#25972;&#30340;Meta&#20998;&#26512;&#12290;&#20351;&#29992;QUADAS-2&#24037;&#20855;&#35780;&#20272;&#20102;&#20559;&#20506;&#39118;&#38505;&#21644;&#36866;&#29992;&#24615;&#30340;&#20851;&#27880;&#28857;&#12290;&#25968;&#25454;&#25552;&#21462;&#30001;&#20004;&#20010;&#35843;&#26597;&#21592;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;Meta&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item></channel></rss>