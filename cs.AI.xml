<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;</title><link>https://arxiv.org/abs/2402.11317</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#38745;&#24577;&#21160;&#24577;&#30340;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#30340;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#31574;&#30053;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20165;&#26377;&#19968;&#32452;&#26377;&#38480;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#65288;DORA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DORA&#34701;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#26368;&#22823;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DORA&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
&lt;/p&gt;</description></item><item><title>&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#26469;&#23545;&#24433;&#21709;&#36827;&#34892;&#24418;&#24335;&#21270;&#30340;&#29305;&#24449;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08514</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Influence in Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08514
&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#26469;&#23545;&#24433;&#21709;&#36827;&#34892;&#24418;&#24335;&#21270;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;MDP&#36335;&#24452;&#964;&#65292;&#36825;&#31181;&#25512;&#29702;&#20801;&#35768;&#25105;&#20204;&#24471;&#20986;&#25551;&#36848;&#964;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#964;'&#65292;&#25551;&#36848;&#20102;&#22312;&#19982;&#964;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#20316;&#24207;&#21015;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#964;&#30340;&#8220;&#22914;&#26524;&#26159;&#36825;&#31181;&#24773;&#20917;&#8221;&#30340;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21453;&#20107;&#23454;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#38543;&#26102;&#38388;&#21457;&#29983;&#20559;&#31163;&#65292;&#35266;&#23519;&#964;&#21487;&#33021;&#19981;&#20877;&#24433;&#21709;&#21453;&#20107;&#23454;&#19990;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#20998;&#26512;&#19981;&#20877;&#38024;&#23545;&#20010;&#20307;&#35266;&#23519;&#32467;&#26524;&#65292;&#32780;&#26159;&#20135;&#29983;&#24178;&#39044;&#24615;&#32467;&#26524;&#32780;&#38750;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#29305;&#21035;&#24433;&#21709;&#27969;&#34892;&#30340;Gumbel-max&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#36825;&#31181;&#27169;&#22411;&#29992;&#20110;MDP&#21453;&#20107;&#23454;&#65292;&#20294;&#30452;&#21040;&#29616;&#22312;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#30340;&#24433;&#21709;&#30340;&#24418;&#24335;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, this kind of inference allows us to derive counterfactual paths $\tau'$ describing what-if versions of $\tau$ obtained under different action sequences than those observed in $\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item></channel></rss>