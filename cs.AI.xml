<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12118</link><description>&lt;p&gt;
DualView&#65306;&#21452;&#37325;&#35270;&#35282;&#19979;&#30340;&#25968;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
DualView: Data Attribution from the Dual Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DualView&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#19982;&#25991;&#29486;&#30456;&#20851;&#30340;&#36866;&#24403;&#23450;&#37327;&#35780;&#20272;&#31574;&#30053;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19982;&#30456;&#20851;&#20027;&#35201;&#26412;&#22320;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/1909.03820</link><description>&lt;p&gt;
&#29992;&#35745;&#25968;&#31526;&#21495;&#30340;&#19968;&#38454;&#36923;&#36753;&#23450;&#20041;&#30340;&#27010;&#24565;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Concepts Definable in First-Order Logic with Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Grohe&#21644;Tur\'an&#24341;&#20837;&#30340;&#36923;&#36753;&#26694;&#26550;&#19979;&#30340;&#20851;&#31995;&#32972;&#26223;&#32467;&#26500;&#19978;&#30340;&#24067;&#23572;&#20998;&#31867;&#38382;&#39064;&#12290;&#20247;&#25152;&#21608;&#30693;(Grohe&#21644;Ritzert, LICS 2017)&#65292;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20854;&#20013;&#32467;&#26500;&#30340;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26159;&#20197;&#32467;&#26500;&#30340;&#22823;&#23567;&#20026;&#21333;&#20301;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#30001;Kuske&#21644;Schweikardt(LICS 2017)&#24341;&#20837;&#30340;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;FOCN&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#25512;&#24191;&#21508;&#31181;&#35745;&#25968;&#36923;&#36753;&#30340;&#34920;&#29616;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#31867;&#19978;&#23450;&#20041;&#30340;FOCN&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#19968;&#33268;&#22320;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20197;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#26041;&#38754;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26080;&#35270;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01405</link><description>&lt;p&gt;
&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#19982;&#20854;&#20182;&#24635;&#32479;&#22312;&#28436;&#35762;&#20013;&#26159;&#21542;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#39118;&#26684;&#65311;&#22914;&#26524;&#26159;&#65292;&#26377;&#21738;&#20123;&#26041;&#38754;&#30340;&#19981;&#21516;&#65311;&#36825;&#20123;&#24046;&#24322;&#26159;&#21542;&#23616;&#38480;&#20110;&#20219;&#20309;&#21333;&#19968;&#30340;&#27807;&#36890;&#23186;&#20171;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#35010;&#24615;&#28436;&#35762;&#35789;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#27604;&#36739;&#25919;&#27835;&#23545;&#25163;&#35789;&#27719;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#22810;&#31181;&#24635;&#32479;&#28436;&#35762;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#30456;&#24403;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#29305;&#26391;&#26222;&#30340;&#35762;&#35805;&#27169;&#24335;&#19982;&#36817;&#20195;&#21382;&#20219;&#20027;&#35201;&#24635;&#32479;&#20505;&#36873;&#20154;&#19981;&#21516;&#12290;&#19968;&#20123;&#26174;&#33879;&#30340;&#21457;&#29616;&#21253;&#25324;&#29305;&#26391;&#26222;&#20351;&#29992;&#29305;&#21035;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#38024;&#23545;&#20182;&#30340;&#25919;&#27835;&#23545;&#25163;&#65292;&#24182;&#19988;&#20182;&#37325;&#22797;&#24378;&#35843;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20182;&#30340;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#21152;&#29420;&#29305;&#65292;&#32780;&#20182;&#20204;&#30340;&#29420;&#29305;&#24615;&#20540;&#19982;&#27665;&#20027;&#20826;&#30456;&#23545;&#36739;&#25509;&#36817;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Donald Trump speak differently from other presidents? If so, in what ways? Are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for comparing the lexical features of political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Some notable findings include Trump's employment of particularly divisive and antagonistic language targeting of his political opponents and his patterns of repetition for emphasis. Furthermore, Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values are comparably closer to those of the Democrats. These differences hold across a variety of measurement 
&lt;/p&gt;</description></item><item><title>I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2312.12102</link><description>&lt;p&gt;
I-CEE: &#23558;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#23450;&#21046;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12102
&lt;/p&gt;
&lt;p&gt;
I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#21040;&#20854;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#20960;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#29992;&#25143;&#65288;&#35299;&#37322;&#23545;&#35937;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;XAI&#25216;&#26415;&#20135;&#29983;&#30340;&#26159;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23454;&#29616;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;XAI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-CEE&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#29616;&#26377;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;I-CEE&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#31034;&#20363;&#22270;&#20687;&#65289;&#12289;&#30456;&#24212;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;I-CEE&#27169;&#25311;&#20102;&#31034;&#20363;&#22270;&#20687;&#30340;&#20449;&#24687;&#37327;&#20381;&#36182;&#20110;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
&lt;/p&gt;</description></item><item><title>Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09954</link><description>&lt;p&gt;
Eva-KELLM&#65306;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09954
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#25110;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#24182;&#35780;&#20272;&#20854;&#25928;&#26524;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#36825;&#19981;&#20165;&#22312;&#25910;&#38598;&#19978;&#20135;&#29983;&#39640;&#25104;&#26412;&#65292;&#32780;&#19988;&#22312;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#35780;&#20272;&#35270;&#35282;&#19978;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Eva-KELLM&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLM&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#19982;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#30456;&#27604;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#12289;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#26356;&#26032;&#21518;&#30340;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) possess a wealth of knowledge encoded in their parameters. However, this knowledge may become outdated or unsuitable over time. As a result, there has been a growing interest in knowledge editing for LLMs and evaluating its effectiveness. Existing studies primarily focus on knowledge editing using factual triplets, which not only incur high costs for collection but also struggle to express complex facts. Furthermore, these studies are often limited in their evaluation perspectives. In this paper, we propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs. This benchmark includes an evaluation framework and a corresponding dataset. Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets. We then evaluate the updated LLM from multiple perspectives. In addition to assessing the effectiveness of knowledge editing and
&lt;/p&gt;</description></item></channel></rss>