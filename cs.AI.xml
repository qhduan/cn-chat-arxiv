<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11852</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#26080;&#32541;&#22320;&#34701;&#20837;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36710;&#27969;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)&#20195;&#29702;&#65292;&#26088;&#22312;&#22312;&#27809;&#26377;&#20851;&#20110;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#20840;&#38754;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35813;&#20195;&#29702;&#30340;&#22686;&#24378;&#29256;AL3IS&#65292;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#20855;&#26377;&#36710;&#36742;&#38388;&#36890;&#20449;&#24310;&#36831;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20570;&#20986;&#26356;&#31283;&#20581;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#19981;&#21487;&#35266;&#23519;&#26041;&#38754;&#65292;&#22914;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#24847;&#22270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20195;&#29702;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#12289;&#20248;&#21270;&#21512;&#24182;&#25805;&#20316;&#24182;&#30830;&#20445;&#19982;&#20854;&#20182;&#36710;&#36742;&#36827;&#34892;&#23433;&#20840;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.06157</link><description>&lt;p&gt;
UDEEP: &#22522;&#20110;&#36793;&#32536;&#30340;&#27700;&#19979;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection. (arXiv:2401.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06157
&lt;/p&gt;
&lt;p&gt;
UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#23545;&#29983;&#24577;&#31995;&#32479;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#23427;&#20204;&#20256;&#25773;&#20102;&#23545;&#33521;&#22269;&#21807;&#19968;&#30340;&#26412;&#22320;&#30333;&#29226;&#40857;&#34430;&#33268;&#21629;&#30340;&#30495;&#33740;&#22411;&#40857;&#34430;&#30239;&#30123;&#30149;(Aphanomyces astaci)&#12290;&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#24191;&#27867;&#25366;&#25496;&#27934;&#31348;&#65292;&#30772;&#22351;&#26646;&#24687;&#22320;&#65292;&#20405;&#34432;&#27827;&#23736;&#24182;&#23545;&#27700;&#36136;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#21516;&#26102;&#31454;&#20105;&#26412;&#22320;&#29289;&#31181;&#30340;&#36164;&#28304;&#24182;&#23548;&#33268;&#26412;&#22320;&#31181;&#32676;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#27745;&#26579;&#20063;&#20351;&#30333;&#29226;&#40857;&#34430;&#26356;&#21152;&#23481;&#26131;&#21463;&#21040;&#25439;&#23475;&#65292;&#20854;&#31181;&#32676;&#22312;&#33521;&#22269;&#26576;&#20123;&#22320;&#21306;&#19979;&#38477;&#36229;&#36807;90&#65285;&#65292;&#20351;&#20854;&#26497;&#26131;&#28626;&#20020;&#28781;&#32477;&#12290;&#20026;&#20102;&#20445;&#25252;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#65292;&#35299;&#20915;&#20837;&#20405;&#29289;&#31181;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#33521;&#22269;&#27827;&#27969;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;UDEEP&#24179;&#21488;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#20998;&#31867;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#30862;&#29255;&#65292;&#20805;&#24403;&#29615;&#22659;&#30417;&#27979;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveragi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08785</link><description>&lt;p&gt;
DeltaSpace:&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#35821;&#20041;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing. (arXiv:2310.08785v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#30528;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#25991;&#29486;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#26631;&#27880;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#19968;&#20123;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#25910;&#38598;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#30340;&#20248;&#21270;&#25110;&#25512;&#29702;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#30830;&#23450;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31354;&#38388;&#65292;&#31216;&#20026;CLIP DeltaSpace&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#22270;&#20687;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;CLIP&#25991;&#26412;&#29305;&#24449;&#24046;&#24322;&#22312;&#35821;&#20041;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;&#22522;&#20110;DeltaSpace&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DeltaEdit&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item></channel></rss>