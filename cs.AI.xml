<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Question Answering Over Spatio-Temporal Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11542
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#30693;&#35782;&#22270;&#65288;STKG&#65289;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#21644;&#20301;&#32622;&#20449;&#24687;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#30740;&#31350;&#30028;&#20851;&#27880;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA&#65289;&#65292;&#20294;&#22522;&#20110;STKG&#30340;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#20173;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20063;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STQAD&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;10,000&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#38754;&#21521;&#26102;&#31354;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;STKGQA&#65289;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;KGQA&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36828;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STCQA&#65292;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;KGQA&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;STComplEx&#30340;&#26032;&#22411;STKG&#23884;&#20837;&#26041;&#27861;&#12290;&#36890;&#36807;&#20174;&#38382;&#39064;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#38382;&#31572;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
&lt;/p&gt;</description></item><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2305.13127</link><description>&lt;p&gt;
&#20160;&#20040;&#30151;&#29366;&#20197;&#21450;&#25345;&#32493;&#22810;&#20037;&#65311;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media. (arXiv:2305.13127v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21457;&#29616;&#20102;&#26032;&#30340;&#26410;&#27880;&#24847;&#21040;&#30340;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#26368;&#24120;&#35265;&#21644;&#20005;&#37325;&#30340;&#31934;&#31070;&#30142;&#30149;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#23545;&#20110;&#26089;&#26399;&#24178;&#39044;&#20197;&#20943;&#36731;&#36825;&#20123;&#21518;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#26412;&#36136;&#19978;&#38656;&#35201;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#25233;&#37057;&#30151;&#26816;&#27979;&#30740;&#31350;&#35797;&#22270;&#22522;&#20110;&#37325;&#35201;&#24615;&#20998;&#25968;&#25110;&#20851;&#27880;&#26435;&#37325;&#35299;&#37322;&#20915;&#31574;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#19982;&#20020;&#24202;&#25233;&#37057;&#30151;&#35786;&#26029;&#26631;&#20934;&#19981;&#19968;&#33268;&#65292;&#21518;&#32773;&#22522;&#20110;&#25233;&#37057;&#30151;&#29366;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36981;&#24490;&#35745;&#31639;&#35774;&#35745;&#31185;&#23398;&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21407;&#22411;&#32593;&#32476;(MSTPNet)&#12290;MSTPNet&#21019;&#26032;&#22320;&#26816;&#27979;&#21644;&#35299;&#37322;&#25233;&#37057;&#30151;&#29366;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#12290;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;MSTPNet&#22312;F1&#20998;&#25968;0.851&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#12290;&#36825;&#20010;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#26410;&#22312;&#35843;&#26597;&#26041;&#27861;&#20013;&#27880;&#24847;&#21040;&#30340;&#26032;&#30151;&#29366;&#65292;&#20363;&#22914;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is the most prevalent and serious mental illness, which induces grave financial and societal ramifications. Depression detection is key for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few depression detection studies attempt to explain the decision based on the importance score or attention weights, these explanations misalign with the clinical depression diagnosis criterion that is based on depressive symptoms. To fill this gap, we follow the computational design science paradigm to develop a novel Multi-Scale Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and interprets depressive symptoms as well as how long they last. Extensive empirical analyses using a large-scale dataset show that MSTPNet outperforms state-of-the-art depression detection methods with an F1-score of 0.851. This result also reveals new symptoms that are unnoted in the survey approach, such as shari
&lt;/p&gt;</description></item></channel></rss>