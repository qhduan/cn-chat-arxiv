<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19386</link><description>&lt;p&gt;
PointCloud-Text&#21305;&#37197;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
PointCloud-Text Matching: Benchmark Datasets and a Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text Matching&#65288;PTM&#65289;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#32473;&#23450;&#30340;&#28857;&#20113;&#26597;&#35810;&#25110;&#25991;&#26412;&#26597;&#35810;&#21305;&#37197;&#30340;&#30830;&#20999;&#36328;&#27169;&#24577;&#23454;&#20363;&#12290;PTM&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#23460;&#20869;/&#22478;&#24066;&#23777;&#35895;&#23450;&#20301;&#21644;&#22330;&#26223;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23578;&#26080;&#36866;&#29992;&#30340;&#12289;&#26377;&#38024;&#23545;&#24615;&#30340;PTM&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;PTM&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;3D2T-SR&#12289;3D2T-NR&#21644;3D2T-QA&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#12289;&#22122;&#22768;&#25110;&#26080;&#24207;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#27169;&#31946;&#12289;&#21547;&#31946;&#25110;&#19981;&#23436;&#25972;&#65292;&#23548;&#33268;&#23384;&#22312;&#22024;&#26434;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#26041;&#27861;&#23545;PTM&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTM&#22522;&#32447;&#65292;&#21629;&#21517;&#20026;Robust PointCloud-Text Matching&#26041;&#27861;&#65288;RoMa&#65289;&#12290;RoMa&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#37325;&#27880;&#24847;&#24863;&#30693;&#27169;&#22359;&#65288;DAP&#65289;&#21644;&#40065;&#26834;&#36127;&#23545;&#27604;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19386v1 Announce Type: cross  Abstract: In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#22312;&#32676;&#20307;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15587</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32676;&#20307;&#20915;&#31574;&#65306;&#27169;&#22411;&#12289;&#20998;&#26512;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#22312;&#32676;&#20307;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#26377;&#28508;&#21147;&#34987;&#21033;&#29992;&#20316;&#20026;&#20016;&#23500;&#20915;&#31574;&#35299;&#20915;&#26041;&#26696;&#24847;&#35265;&#30340;&#26469;&#28304;&#12290;&#32676;&#20307;&#20915;&#31574;&#65288;CDM&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20174;&#32431;&#25991;&#26412;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21457;&#24067;&#30340;&#35780;&#35770;&#65289;&#20013;&#25512;&#26029;&#24847;&#35265;&#21644;&#20915;&#31574;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#26469;&#36741;&#21161;CDM&#36807;&#31243;&#65292;&#20197;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;ChatGPT&#25972;&#21512;&#21040;CDM&#36807;&#31243;&#20013;&#20316;&#20026;&#19968;&#31181;&#28789;&#27963;&#30340;&#24037;&#20855;&#65292;&#25512;&#26029;&#20986;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24847;&#35265;&#65292;&#24182;&#26681;&#25454;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#21046;&#23450;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15587v1 Announce Type: new  Abstract: Social Media and Internet have the potential to be exploited as a source of opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a methodology able to infer opinions and decisions from plain texts, such as reviews published in social media platforms, by means of Sentiment Analysis. Currently, the emergence and potential of Large Language Models (LLMs) lead us to explore new scenarios of automatically understand written texts, also known as natural language processing. This paper analyzes the use of ChatGPT based on prompt design strategies to assist in CDM processes to extract opinions and make decisions. We integrate ChatGPT in CDM processes as a flexible tool that infer the opinions expressed in texts, providing numerical or linguistic evaluations where the decision making models are based on the prompt design strategies. We include a multi-criteria decision making scenario with a category ontology for criteria. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04523</link><description>&lt;p&gt;
T-TAME&#65306;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers&#21644;&#20854;&#20182;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#24555;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#26159;&#22312;&#38656;&#35201;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#37319;&#29992;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#36866;&#24212;&#21040;&#35270;&#35273;Transformer&#30340;&#26032;&#33539;&#24335;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;Transformer&#20860;&#23481;&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#35299;&#37322;&#65292;&#36825;&#26159;&#19968;&#31181;&#35828;&#26126;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#21367;&#31215;&#25110;&#31867;&#20284;Vision Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#31934;&#31616;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;&#21518;&#65292;&#35299;&#37322;&#22270;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#20986;&#65307;&#36825;&#20123;&#35299;&#37322;&#22270;&#21487;&#20197;&#19982;Convolutional Neural Networks&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#22270;&#30456;&#23218;&#32654;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the "black box" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11871</link><description>&lt;p&gt;
&#20174;&#23454;&#38469;&#21040;&#36923;&#36753;&#20877;&#21040;&#23454;&#38469;&#65306;&#20026;&#35268;&#21010;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21457;&#26126;&#31526;&#21495;&#35789;&#27719;&#12289;&#21160;&#20316;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20811;&#26381;&#38271;&#26399;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#34920;&#31034;&#38656;&#35201;&#20855;&#26377;&#24378;&#28872;&#30452;&#35273;&#21644;&#35814;&#32454;&#30693;&#35782;&#30340;&#19987;&#23478;&#65292;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#21487;&#33021;&#38656;&#35201;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#28040;&#38500;&#23545;&#20154;&#31867;&#30452;&#35273;&#30340;&#20381;&#36182;&#26159;&#19968;&#20010;&#26497;&#20026;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#20174;&#26410;&#26631;&#35760;&#30340;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#12290;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;&#31867;PDDL&#22495;&#27169;&#22411;&#12290;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20174;&#23569;&#25968;&#26426;&#22120;&#20154;&#36712;&#36857;&#20013;&#21487;&#20197;&#23398;&#21040;&#24378;&#22823;&#30340;&#25277;&#35937;&#34920;&#31034;&#65307;&#25152;&#23398;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06077</link><description>&lt;p&gt;
&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Time-Series Forecasting. (arXiv:2310.06077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23637;&#31034;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FPS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#35299;&#20915;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#12290;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#65292;&#22914;&#20844;&#20849;&#21355;&#29983;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#24212;&#29992;&#65292;&#28041;&#21450;&#21040;&#21453;&#39304;&#24490;&#29615;&#65292;&#20854;&#20013;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#36827;&#32780;&#25913;&#21464;&#30446;&#26631;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#23637;&#31034;&#24615;&#65292;&#24341;&#20837;&#20102;&#21487;&#33021;&#20986;&#29616;&#8220;&#33258;&#25105;&#25269;&#28040;&#8221;&#25110;&#8220;&#33258;&#25105;&#23454;&#29616;&#8221;&#30340;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#23545;&#20998;&#31867;&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23637;&#31034;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21487;&#23637;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PeTS&#65289;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#35299;&#20915;&#20102;&#24403;&#21487;&#33021;&#23384;&#22312;&#23637;&#31034;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#20934;&#30830;&#39044;&#27979;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#29305;&#24449;&#23637;&#31034;&#24615;&#36716;&#31227;&#65288;FPS&#65289;&#65292;&#23427;&#21033;&#29992;&#24310;&#36831;&#21709;&#24212;&#30340;&#27010;&#24565;&#26469;&#39044;&#27979;&#20998;&#24067;&#30340;&#21464;&#21270;&#21644;&#38543;&#21518;&#30340;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subseque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.14907</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#26631;&#31614;&#35299;&#21367;&#31215;&#20197;&#25269;&#25239;&#23398;&#20064;&#20559;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias. (arXiv:2309.14907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#35299;&#21367;&#31215;&#25216;&#26415;(LD)&#65292;&#36890;&#36807;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#39640;&#25928;&#30340;&#36817;&#20284;&#65292;&#26469;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#23646;&#24615;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26102;&#30340;&#23398;&#20064;&#20559;&#24046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24102;&#23646;&#24615;&#30340;&#22270;&#20013;&#65292;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#23545;&#35768;&#22810;&#37325;&#35201;&#30340;&#19979;&#28216;&#20219;&#21153;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#21516;&#26102;&#32534;&#30721;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#36827;&#34892;&#25972;&#21512;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#33410;&#28857;&#32534;&#30721;&#22120;(NEs)&#26469;&#32534;&#30721;&#23646;&#24615;&#12290;&#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#21516;&#26102;&#35757;&#32451;&#22823;&#22411;NEs&#21644;GNNs&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#26041;&#27861;&#25552;&#20986;&#20102;&#20998;&#21035;&#35757;&#32451;NEs&#21644;GNNs&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;NEs&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;GNNs&#20013;&#30340;&#29305;&#24449;&#21367;&#31215;&#65292;&#23548;&#33268;&#20102;&#19982;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#30340;&#26174;&#33879;&#23398;&#20064;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26631;&#31614;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#26631;&#31614;&#35299;&#21367;&#31215;(LD)&#65292;&#36890;&#36807;&#23545;GNNs&#30340;&#36870;&#26144;&#23556;&#36827;&#34892;&#26032;&#39062;&#19988;&#39640;&#24230;&#21487;&#20280;&#32553;&#30340;&#36817;&#20284;&#65292;&#20197;&#20943;&#36731;&#23398;&#20064;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning on attributed graphs -- whose nodes are associated with rich attributes (e.g., texts and protein sequences) -- plays a crucial role in many important downstream tasks. To encode the attributes and graph structures simultaneously, recent studies integrate pre-trained models with graph neural networks (GNNs), where pre-trained models serve as node encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs on large-scale graphs suffers from severe scalability issues, many methods propose to train NEs and GNNs separately. Consequently, they do not take feature convolutions in GNNs into consideration in the training phase of NEs, leading to a significant learning bias from that by the joint training. To address this challenge, we propose an efficient label regularization technique, namely Label Deconvolution (LD), to alleviate the learning bias by a novel and highly scalable approximation to the inverse mapping of GNNs. The inverse mapping l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08586</link><description>&lt;p&gt;
FedJETs&#65306;&#20855;&#26377;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#39640;&#25928;&#21450;&#26102;&#20010;&#24615;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedJETs&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#37030;&#28151;&#21512;&#19987;&#23478;&#30340;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#21450;&#26102;&#30340;&#20010;&#24615;&#21270;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#30456;&#20851;&#30340;&#19987;&#23478;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#21019;&#24314;&#33021;&#22815;&#36866;&#24212;&#27599;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#19978;&#19979;&#25991;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20010;&#24615;&#21270;&#38656;&#35201;&#20351;&#29992;&#23458;&#25143;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#22312;&#26032;&#26469;&#30340;&#23458;&#25143;&#31471;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22312;&#38544;&#31169;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23454;&#29616;&#21450;&#26102;&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedJETs&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;FL&#35774;&#32622;&#20013;&#20351;&#29992;&#8220;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#8221;&#26694;&#26550;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23458;&#25143;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#23376;&#38598;&#19978;&#35757;&#32451;&#19987;&#38376;&#30340;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#38376;&#25511;&#20989;&#25968;&#23558;&#36755;&#20837;&#36335;&#30001;&#21040;&#26368;&#30456;&#20851;&#30340;&#19987;&#23478;&#12290;&#25105;&#20204;&#30340;&#38376;&#25511;&#20989;&#25968;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20849;&#20139;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#20854;&#21363;&#26102;&#30340;&#36335;&#30001;&#20915;&#31574;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#39640;&#36798;18&#65285;&#65292;&#36798;&#21040;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve just-in-time personalization in these scenarios. We propose FedJETs, a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\% in state of the art F
&lt;/p&gt;</description></item></channel></rss>