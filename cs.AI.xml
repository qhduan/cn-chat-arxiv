<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09984</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#26102;&#22242;&#38431;&#21512;&#20316;&#30340;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Breaking Augmentations for Ad Hoc Teamwork
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21327;&#20316;&#29615;&#22659;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#36866;&#24212;&#20351;&#29992;&#26410;&#30693;&#25110;&#20808;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#31574;&#30053;&#30340;&#26032;&#38431;&#21451;&#12290;&#23545;&#20110;AI&#20195;&#29702;&#26469;&#35828;&#65292;&#36825;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#31616;&#21333;&#65292;&#20294;&#21364;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;AI&#20195;&#29702;&#22312;&#35757;&#32451;&#38598;&#20013;&#23398;&#20250;&#20102;&#19982;&#21482;&#22312;&#19968;&#20391;&#36947;&#36335;&#19978;&#34892;&#39542;&#30340;&#20854;&#20182;&#36710;&#36742;&#24182;&#34892;&#39542;&#65292;&#37027;&#20040;&#21363;&#20351;&#36825;&#20123;&#36710;&#36742;&#30340;&#34892;&#20026;&#21482;&#26159;&#22312;&#24038;&#21491;&#23545;&#31216;&#19978;&#36827;&#34892;&#20102;&#32763;&#36716;&#65292;&#23427;&#20063;&#21487;&#33021;&#38590;&#20197;&#36866;&#24212;&#19982;&#30456;&#21453;&#26041;&#21521;&#19978;&#34892;&#39542;&#30340;&#39550;&#39542;&#21592;&#36827;&#34892;&#21327;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#22686;&#24378;&#65288;SBA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#23545;&#31216;&#32763;&#36716;&#25805;&#20316;&#26469;&#22686;&#21152;&#35757;&#32451;&#38431;&#21451;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#22686;&#24378;&#21518;&#30340;&#38431;&#21451;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#33021;&#22815;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#20026;&#32422;&#23450;&#65292;&#20174;&#32780;&#25552;&#39640;&#19982;&#26032;&#38431;&#21451;&#21512;&#20316;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09984v1 Announce Type: cross  Abstract: In many collaborative settings, artificial intelligence (AI) agents must be able to adapt to new teammates that use unknown or previously unobserved strategies. While often simple for humans, this can be challenging for AI agents. For example, if an AI agent learns to drive alongside others (a training set) that only drive on one side of the road, it may struggle to adapt this experience to coordinate with drivers on the opposite side, even if their behaviours are simply flipped along the left-right symmetry. To address this we introduce symmetry-breaking augmentations (SBA), which increases diversity in the behaviour of training teammates by applying a symmetry-flipping operation. By learning a best-response to the augmented set of teammates, our agent is exposed to a wider range of behavioural conventions, improving performance when deployed with novel teammates. We demonstrate this experimentally in two settings, and show that our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01373</link><description>&lt;p&gt;
&#35748;&#35777;&#20219;&#20309;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21508;&#20010;&#21306;&#22495;&#25110;&#22359;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#22312;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#26816;&#27979;&#20013;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#22312;&#22522;&#20110;&#24378;&#22823;&#30340;&#22270;&#20687;&#32423;&#35270;&#35273;&#35821;&#35328;&#65288;ViL&#65289;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#21162;&#21147;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;&#24191;&#27867;&#30340;&#21306;&#22495;-&#26631;&#31614;&#23545;&#38598;&#21512;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#65292;&#35201;&#20040;&#23558;&#26816;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#21306;&#22495;&#24314;&#35758;&#30340;&#22270;&#20687;&#32423;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#21463;&#21040;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#35757;&#32451;&#38656;&#27714;&#12289;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#20197;&#21450;&#29615;&#22659;&#20449;&#24687;&#30340;&#19981;&#36275;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#23450;&#20301;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#31216;&#20026;RegionSpot&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09107</link><description>&lt;p&gt;
GLoRE: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRE: Evaluating Logical Reasoning of Large Language Models. (arXiv:2310.09107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21253;&#25324;GPT-4&#21644;&#26032;&#20852;&#31038;&#21306;&#27169;&#22411;&#22312;&#20869;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#23581;&#35797;&#36824;&#24456;&#23569;&#65292;&#32780;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GLoRE&#65292;&#19968;&#20010;&#31934;&#24515;&#32452;&#32455;&#30340;&#36890;&#29992;&#36923;&#36753;&#25512;&#29702;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;12&#20010;&#35206;&#30422;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#31867;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65307;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#36739;&#24378;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;GPT-4&#22823;&#24133;&#36229;&#36807;&#20102;ChatGPT&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;ChatGPT&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07800</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21450;&#20854;&#23427;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#26174;&#33879;&#30340;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#23558;AI&#22522;&#32447;&#26292;&#38706;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24863;&#30693;&#65292;&#37027;&#20040;&#23427;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26356;&#20934;&#30830;&#12289;&#26356;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#36825;&#20123;&#20449;&#24687;&#24615;&#25968;&#25454;&#37096;&#20998;&#30340;&#20219;&#21153;&#65292;&#21363;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#65292;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#38590;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#24615;&#21306;&#22495;&#65292;&#21407;&#22240;&#26159;&#22823;&#37327;&#30340;&#35757;&#32451;&#21442;&#25968;&#26080;&#27861;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#26377;&#25928;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.13218</link><description>&lt;p&gt;
AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#65306;&#19968;&#20010;&#26694;&#26550;&#21644;&#22312;&#29983;&#20135;&#35843;&#24230;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling. (arXiv:2309.13218v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13218
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20248;&#21270;&#26159;&#23547;&#25214;&#21644;&#23454;&#26045;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#36816;&#33829;&#26041;&#24335;&#65292;&#20197;&#20026;&#20225;&#19994;&#24102;&#26469;&#31454;&#20105;&#20248;&#21183;&#30340;&#36807;&#31243;&#12290;&#32508;&#21512;&#38382;&#39064;&#34920;&#36848;&#26159;&#20225;&#19994;&#20248;&#21270;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#22260;&#32469;&#30528;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#23637;&#24320;&#65292;&#22240;&#27492;&#24456;&#26377;&#21487;&#33021;&#25104;&#20026;&#29942;&#39048;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#28508;&#22312;&#22320;&#20943;&#23569;&#38382;&#39064;&#34920;&#36848;&#20013;&#25152;&#38656;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#29992;&#20110;&#38382;&#39064;&#34920;&#36848;&#30340;LLM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#12289;&#20196;&#29260;&#38480;&#21046;&#20197;&#21450;LLM&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;LLM&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;</title><link>http://arxiv.org/abs/2304.06348</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#30340;&#21453;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#65288;&#36890;&#36807;&#26576;&#20123;&#31867;&#22411;&#30340;&#23485;&#24230;&#37327;&#26469;&#34913;&#37327;&#65292;&#21253;&#25324;&#26641;&#23485;&#21644;&#22242;&#23485;&#31561;&#65289;&#65292;&#20026;&#24191;&#27867;&#30340;&#36923;&#36753;&#34164;&#21547;&#38382;&#39064;&#65288;&#31616;&#31216;&#26597;&#35810;&#65289;&#30340;&#21487;&#20915;&#23450;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23637;&#29616;&#20986;&#23485;&#24230;&#26377;&#38480;&#26377;&#38480;&#36890;&#29992;&#27169;&#22411;&#38598;&#30340;&#36923;&#36753;&#65292;&#20445;&#35777;&#20102;&#21508;&#31181;&#21516;&#24577;&#23553;&#38381;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#24378;&#22823;&#30340;&#23485;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Blumensath&#30340;&#20998;&#21106;&#23485;&#24230;&#65292;&#35813;&#37327;&#21253;&#21547;&#20102;&#21508;&#31181;&#36890;&#24120;&#32771;&#34385;&#30340;&#23485;&#24230;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#30340;&#35745;&#31639;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#38024;&#23545;&#26222;&#36941;&#23637;&#29616;&#23384;&#22312;&#24615;&#35268;&#21017;&#20026;&#19968;&#20010;&#23637;&#31034;&#26696;&#20363;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#26377;&#38480;&#20998;&#21106;&#23485;&#24230;&#35268;&#21017;&#38598;&#21253;&#21547;&#20854;&#20182;&#24050;&#30693;&#30340;&#25277;&#35937;&#21487;&#20915;&#23450;&#31867;&#65292;&#20294;&#20511;&#21161;&#29616;&#26377;&#30340;&#20998;&#23618;&#21644;&#21463;&#25511;&#35268;&#21017;&#38598;&#27010;&#24565;&#65292;&#20063;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#20363;&#22914;&#27491;&#21017;&#65292;&#36830;&#25509;&#21644;&#24067;&#23572;&#36830;&#25509;&#26597;&#35810;&#12290;&#25105;&#20204;&#20197;&#23384;&#22312;&#35268;&#21017;&#30340;&#24418;&#24335;&#20026;&#37325;&#28857;&#65292;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#39640;&#32423;&#30693;&#35782;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
&lt;/p&gt;</description></item></channel></rss>