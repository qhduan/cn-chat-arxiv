<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05265</link><description>&lt;p&gt;
MMoE: &#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#39046;&#22495;&#24863;&#30693;&#19987;&#23478;&#28151;&#21512;&#30340;&#40065;&#26834;&#21095;&#36879;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#30005;&#24433;&#35780;&#35770;&#32593;&#31449;&#23545;&#20110;&#30005;&#24433;&#20449;&#24687;&#21644;&#35752;&#35770;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#21095;&#36879;&#35780;&#35770;&#20250;&#24433;&#21709;&#35266;&#24433;&#20307;&#39564;&#65292;&#22240;&#27492;&#21095;&#36879;&#26816;&#27979;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24573;&#30053;&#20102;&#24179;&#21488;&#20013;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;MMoE&#39318;&#20808;&#20174;&#29992;&#25143;-&#30005;&#24433;&#32593;&#32476;&#20013;&#25552;&#21462;&#22270;&#34920;&#12289;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#29305;&#24449;&#65292;&#20998;&#21035;&#20174;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#21644;&#35780;&#35770;&#30340;&#20803;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#22788;&#29702;&#29305;&#23450;&#31867;&#22411;&#30005;&#24433;&#35780;&#35770;&#20013;&#30340;&#21095;&#36879;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05265v1 Announce Type: new  Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spo
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06298</link><description>&lt;p&gt;
MLP-SRGAN: &#20351;&#29992;MLP-Mixer&#30340;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06298
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;MLP-SRGAN&#65292;&#23427;&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#65292;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#28151;&#21512;&#22120;&#65288;MLP-Mixer&#65289;&#20197;&#21450;&#21367;&#31215;&#23618;&#22312;&#20999;&#29255;&#26041;&#21521;&#19978;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290; MLP-SRGAN&#20351;&#29992;MSSEG2&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;FLAIR MRI&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20013;&#24515;FLAIR&#25968;&#25454;&#38598;&#65288;CAIN&#65292;ADNI&#65292;CCNA&#65289;&#30340;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#22312;&#20445;&#30041;&#65288;&#26410;&#35265;&#65289;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#19978;&#37319;&#26679;&#32467;&#26524;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;SR&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22522;&#26412;&#20107;&#23454;&#30340;&#22270;&#20687;&#65292;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#26469;&#34913;&#37327;&#19978;&#37319;&#26679;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#65292;&#20197;&#22312;&#32570;&#20047;&#22522;&#30784;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#38160;&#24230;&#65288;&#36793;&#32536;&#24378;&#24230;&#65289;&#65292;&#22122;&#22768;&#65288;&#29109;&#65289;&#21644;&#27169;&#31946;&#24230;&#65288;&#20302;&#39057;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
&lt;/p&gt;</description></item></channel></rss>