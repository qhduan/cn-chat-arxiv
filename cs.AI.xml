<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09346</link><description>&lt;p&gt;
AVIBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#23548;&#19978;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09346
&lt;/p&gt;
&lt;p&gt;
AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#23545;&#29992;&#25143;&#30340;&#35270;&#35273;&#25351;&#20196;&#20316;&#20986;&#33391;&#22909;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#20196;&#28085;&#30422;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#23481;&#26131;&#21463;&#21040;&#26377;&#24847;&#21644;&#26080;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LVLMs&#23545;&#25239;&#27492;&#31867;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AVIBench&#65292;&#19968;&#20010;&#26088;&#22312;&#20998;&#26512;LVLMs&#22312;&#38754;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#65288;AVIs&#65289;&#26102;&#30340;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22235;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;AVIs&#12289;&#21313;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;AVIs&#21644;&#20061;&#31181;&#20869;&#23481;&#20559;&#35265;AVIs&#65288;&#22914;&#24615;&#21035;&#12289;&#26292;&#21147;&#12289;&#25991;&#21270;&#21644;&#31181;&#26063;&#20559;&#35265;&#31561;&#65289;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;26&#19975;&#20010;AVIs&#65292;&#28085;&#30422;&#20116;&#31867;&#22810;&#27169;&#24577;&#33021;&#21147;&#65288;&#20061;&#39033;&#20219;&#21153;&#65289;&#21644;&#20869;&#23481;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;14&#20010;&#24320;&#28304;LVLMs&#22312;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;AVIBench&#36824;&#21487;&#20316;&#20026;&#19968;&#20010;&#20415;&#21033;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09346v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenie
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2403.06294</link><description>&lt;p&gt;
ArgMed-Agents: &#20351;&#29992;&#20105;&#35758;&#26041;&#26696;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06294
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20105;&#35770;&#26041;&#26696;&#30340;&#33258;&#25105;&#35770;&#35777;&#36845;&#20195;&#21644;&#26500;&#24314;&#20105;&#35770;&#36807;&#31243;&#65292;ArgMed-Agents&#23454;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#21487;&#35299;&#37322;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#34429;&#28982;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#30340;&#34920;&#29616;&#21364;&#19981;&#23613;&#20154;&#24847;&#12290;&#20854;&#27425;&#65292;LLMs&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#35748;&#30693;&#36807;&#31243;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#23548;&#33268;&#29992;&#25143;&#19981;&#20449;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ArgMed-Agents&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20132;&#20114;&#20351;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#33021;&#22815;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#20020;&#24202;&#20915;&#31574;&#25512;&#29702;&#12290;ArgMed-Agents&#36890;&#36807;&#20020;&#24202;&#20915;&#31574;&#35770;&#25454;&#65288;&#19968;&#31181;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#35748;&#30693;&#36807;&#31243;&#30340;&#25512;&#29702;&#26426;&#21046;&#65289;&#25191;&#34892;&#33258;&#35770;&#35777;&#36845;&#20195;&#65292;&#28982;&#21518;&#23558;&#20105;&#35770;&#36807;&#31243;&#26500;&#24314;&#20026;&#34920;&#31034;&#20914;&#31361;&#20851;&#31995;&#30340;&#26377;&#21521;&#22270;&#12290;&#26368;&#32456;&#65292;Reasoner&#65288;&#19968;&#31181;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#35782;&#21035;&#20986;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06294v1 Announce Type: new  Abstract: There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a
&lt;/p&gt;</description></item><item><title>Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04652</link><description>&lt;p&gt;
Yi: &#30001; 01.AI &#25512;&#20986;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Yi: Open Foundation Models by 01.AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04652
&lt;/p&gt;
&lt;p&gt;
Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Yi&#27169;&#22411;&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20855;&#26377;&#24378;&#22823;&#22810;&#32500;&#33021;&#21147;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;6B&#21644;34B&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#32842;&#22825;&#27169;&#22411;&#12289;200K&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35832;&#22914;MMLU&#20043;&#31867;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#25105;&#20204;&#24494;&#35843;&#36807;&#30340;&#32842;&#22825;&#27169;&#22411;&#22312;AlpacaEval&#21644;Chatbot Arena&#31561;&#20027;&#35201;&#35780;&#20272;&#24179;&#21488;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20154;&#31867;&#20559;&#22909;&#29575;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#36229;&#32423;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;Yi&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#25968;&#25454;&#24037;&#31243;&#24037;&#20316;&#25152;&#24102;&#26469;&#30340;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#30340;&#25968;&#25454;&#21435;&#37325;&#21644;&#36136;&#37327;&#36807;&#28388;&#27969;&#27700;&#32447;&#26500;&#24314;&#20102;3100&#20159;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#26009;&#24211;&#30340;&#26631;&#35760;&#12290;&#23545;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23545;&#23567;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.09881</link><description>&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#35268;&#27169;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#31034;&#33539;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31034;&#33539;&#20316;&#20026;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#24378;&#35843;&#20102;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#31034;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26368;&#20248;&#31034;&#33539;&#36873;&#25321;&#32500;&#24230;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#20107;&#23454;&#12290;&#20511;&#37492;&#20004;&#20010;&#32500;&#24230;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;(IDS)&#12290;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;(Zero-shot-CoT)&#65292;IDS&#36845;&#20195;&#22320;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;ICL&#30340;&#31034;&#33539;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDS&#22312;&#31034;&#33539;&#36873;&#25321;&#20043;&#21069;&#23558;Zero-shot-CoT&#24212;&#29992;&#20110;&#27979;&#35797;&#26679;&#26412;&#12290;&#36755;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item></channel></rss>