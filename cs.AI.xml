<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;</title><link>https://rss.arxiv.org/abs/2402.01613</link><description>&lt;p&gt;
Nomic Embed&#65306;&#35757;&#32451;&#21487;&#22797;&#29616;&#30340;&#38271;&#19978;&#19979;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nomic Embed: Training a Reproducible Long Context Text Embedder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01613
&lt;/p&gt;
&lt;p&gt;
Nomic Embed&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#22120;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;nomic-embed-text-v1&#30340;&#35757;&#32451;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#21487;&#22797;&#29616;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#26435;&#37325;&#12289;&#24320;&#25918;&#25968;&#25454;&#30340;8192&#19978;&#19979;&#25991;&#38271;&#24230;&#33521;&#25991;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#22312;&#30701;&#19978;&#19979;&#25991;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;OpenAI Ada-002&#21644;OpenAI text-embedding-3-small&#12290;&#25105;&#20204;&#22312;Apache 2&#35768;&#21487;&#19979;&#21457;&#24067;&#20102;&#35757;&#32451;&#20195;&#30721;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;2.35&#20159;&#20010;&#31574;&#21010;&#25991;&#26412;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#21152;&#36733;&#22120;&#65292;&#21487;&#20197;&#23436;&#20840;&#22797;&#29616;nomic-embed-text-v1&#12290;&#20320;&#21487;&#20197;&#22312;https://github.com/nomic-ai/contrastors&#25214;&#21040;&#27169;&#22411;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.08828</link><description>&lt;p&gt;
&#24403;&#35299;&#37322;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#26102;&#65292;&#20154;&#20204;&#20250;&#32473;&#20104;&#20854;&#23646;&#24615;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08828
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#20248;&#31168;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26631;&#24535;&#26159;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#24182;&#37319;&#21462;&#34892;&#21160;&#30340;&#35299;&#37322;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#38656;&#35201;&#31995;&#32479;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#22240;&#26524;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#35748;&#30693;&#31185;&#23398;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#29992;&#25143;&#21487;&#33021;&#26399;&#26395;&#30340;&#35299;&#37322;&#31867;&#22411;&#65292;&#20197;&#21450;&#22312;&#21738;&#31181;&#26684;&#24335;&#19979;&#21576;&#29616;&#36825;&#20123;&#35299;&#37322;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;&#35748;&#30693;&#31185;&#23398;&#35299;&#37322;&#26041;&#38754;&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#30446;&#30340;&#35770;&#65292;&#21363;&#20197;&#36798;&#21040;&#30446;&#30340;&#20026;&#35299;&#37322;&#20915;&#31574;&#30340;&#20542;&#21521;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#20204;&#22914;&#20309;&#20026;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#20135;&#29983;&#35299;&#37322;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#39033;&#35843;&#26597;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;n = 54&#65289;&#35266;&#30475;&#20102;&#36947;&#36335;&#22330;&#26223;&#30340;&#35270;&#39057;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#36710;&#36742;&#30340;&#34892;&#20026;&#29983;&#25104;&#26426;&#26800;&#30340;&#12289;&#21453;&#20107;&#23454;&#30340;&#25110;&#30446;&#30340;&#35770;&#30340;&#35328;&#35821;&#35299;&#37322;&#12290;&#22312;&#31532;&#20108;&#39033;&#35843;&#26597;&#20013;&#65292;&#21478;&#19968;&#32452;&#21442;&#19982;&#32773;&#65288;n = 356&#65289;&#23545;&#36825;&#20123;&#36827;&#34892;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08828v1 Announce Type: cross  Abstract: A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. In the second survey, a different set of participants (n=356) rated these
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#20986;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#24615;&#33021;&#20248;&#21155;&#21464;&#21270;</title><link>https://arxiv.org/abs/2403.06568</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#37197;&#32622;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06568
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#20986;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#24615;&#33021;&#20248;&#21155;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;MaxSAT&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#35832;&#22914;MaxSAT Evaluations&#20043;&#31867;&#30340;&#22522;&#20934;&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#27604;&#36739;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#65292;&#20294;&#29616;&#26377;&#30340;&#35780;&#20272;&#36890;&#24120;&#26159;&#22522;&#20110;&#22312;&#32473;&#23450;&#36816;&#34892;&#26102;&#38388;&#39044;&#31639;&#20869;&#33719;&#24471;&#30340;&#26368;&#20339;&#35299;&#30340;&#36136;&#37327;&#26469;&#35780;&#20272;&#30340;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#29305;&#23450;&#26102;&#38388;&#39044;&#31639;&#20869;&#26368;&#32456;&#33719;&#24471;&#30340;&#35299;&#21487;&#33021;&#20250;&#38480;&#21046;&#25105;&#20204;&#29702;&#35299;&#27714;&#35299;&#22120;&#22312;&#25910;&#25947;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#32463;&#39564;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21487;&#29992;&#20110;&#27604;&#36739;MaxSAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#39044;&#31639;&#19979;&#30340;&#20219;&#24847;&#24615;&#33021;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#31034;&#20986;&#27714;&#35299;&#22120;&#30340;&#65288;&#19981;&#65289;&#20248;&#21183;&#38543;&#30528;&#19981;&#21516;&#36816;&#34892;&#26102;&#38388;&#30340;&#35843;&#25972;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#23637;&#31034;&#20102;&#23450;&#37327;&#21644;&#39640;&#26041;&#24046;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06568v1 Announce Type: new  Abstract: Though numerous solvers have been proposed for the MaxSAT problem, and the benchmark environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the best-found solutions obtained within a given running time budget. However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process. This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT local search solvers' anytime performance across multiple problem instances and various time budgets. The assessment reveals distinctions in solvers' performance and displays that the (dis)advantages of solvers adjust along different running times. This work also exhibits that the quantitative and high variance ass
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18164</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#23458;&#25143;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based General Purpose Representation Learning for Customer Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18164
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#22522;&#30784;&#32467;&#26500;&#21450;&#20854;&#29983;&#25104;&#22240;&#32032;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#29992;&#20363;&#26080;&#20851;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#36890;&#36807;&#22810;&#32500;&#21521;&#37327;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#36825;&#20123;&#32467;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#39640;&#24230;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#25554;&#25300;&#24335;&#12289;&#20016;&#23500;&#21644;&#21311;&#21517;&#21270;&#30340;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#39640;&#36798;45&#65285;&#65292;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#22810;&#23618;&#25910;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#35745;&#31639;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18164v1 Announce Type: cross  Abstract: In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencode
&lt;/p&gt;</description></item><item><title>IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17250</link><description>&lt;p&gt;
IDENAS: &#20869;&#37096;&#20381;&#36182;&#24615;&#25506;&#32034;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17250
&lt;/p&gt;
&lt;p&gt;
IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#21644;&#36827;&#34892;&#21508;&#31181;&#39044;&#27979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#31639;&#27861;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#65292;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#21306;&#21035;&#20197;&#21450;&#27169;&#22411;&#30340;&#24213;&#23618;&#20851;&#32852;&#65288;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#23618;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#29305;&#24449;&#36873;&#25321;&#24050;&#25104;&#20026;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IDENAS&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;NAS&#19982;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;1D&#20256;&#24863;&#22120;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#23436;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#20869;&#37096;&#20381;&#36182;&#24615;&#12290;IDENAS&#37319;&#29992;&#20102;&#20462;&#25913;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#21069;&#21521;&#25628;&#32034;&#65288;SFS&#65289;&#31639;&#27861;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#37197;&#32622;&#25628;&#32034;&#19982;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;IDENAS&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08804</link><description>&lt;p&gt;
&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#19982;&#20934;&#30830;&#24615;&#30340;&#30456;&#20114;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making. (arXiv:2304.08804v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#23558;&#20154;&#31867;&#32622;&#20110;&#20915;&#31574;&#29615;&#36335;&#20013;&#22830;&#30340;&#20027;&#35201;&#25215;&#35834;&#26159;&#65292;&#20182;&#20204;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#31526;&#21512;&#20854;&#27491;&#30830;&#30340;&#21644;&#35206;&#30422;&#20854;&#38169;&#35823;&#30340;&#24314;&#35758;&#26469;&#34917;&#20805;AI&#31995;&#32479;&#12290;&#28982;&#32780;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#20154;&#31867;&#20542;&#21521;&#20110;&#36807;&#24230;&#25110;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#65292;&#36825;&#24847;&#21619;&#30528;&#20182;&#20204;&#35201;&#20040;&#20381;&#20174;&#38169;&#35823;&#30340;&#24314;&#35758;&#65292;&#35201;&#20040;&#35206;&#30422;&#27491;&#30830;&#30340;&#24314;&#35758;&#12290;&#36825;&#31181;&#20381;&#36182;&#34892;&#20026;&#23545;&#20915;&#31574;&#20934;&#30830;&#24615;&#26377;&#23475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#65292;&#20351;&#36825;&#31181;&#30456;&#20114;&#20851;&#31995;&#26356;&#21152;&#20855;&#20307;&#21270;&#12290;&#35813;&#26694;&#26550;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#21644;&#27604;&#36739;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#24178;&#39044;&#65288;&#20363;&#22914;&#35299;&#37322;&#65289;&#24433;&#21709;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#26694;&#26550;&#20013;&#25512;&#20986;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#65288;i&#65289;&#24403;&#20154;&#31867;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#23558;&#26174;&#30528;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20182;&#20204;&#36807;&#24230;&#20381;&#36182;&#26102;&#65292;&#20449;&#20219;&#30340;&#25913;&#21892;&#21364;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely o
&lt;/p&gt;</description></item></channel></rss>