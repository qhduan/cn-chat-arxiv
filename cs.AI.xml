<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;</title><link>https://arxiv.org/abs/2403.09326</link><description>&lt;p&gt;
HeadEvolver&#65306;&#36890;&#36807;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#23454;&#29616;&#25991;&#26412;&#21040;&#22836;&#37096;&#22836;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HeadEvolver&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39118;&#26684;&#21270;&#30340;&#22836;&#37096;&#22836;&#20687;&#12290;HeadEvolver&#20351;&#29992;&#27169;&#26495;&#22836;&#37096;&#32593;&#26684;&#30340;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#23383;&#36164;&#20135;&#65292;&#20197;&#23454;&#29616;&#20445;&#30041;&#32454;&#33410;&#30340;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#20840;&#23616;&#21464;&#24418;&#20013;&#32570;&#20047;&#32454;&#31890;&#24230;&#21644;&#35821;&#20041;&#24863;&#30693;&#26412;&#22320;&#24418;&#29366;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#20316;&#20026;&#27599;&#20010;&#19977;&#35282;&#24418;&#30340;Jacobi&#30697;&#38453;&#30340;&#21152;&#26435;&#22240;&#23376;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#26412;&#22320;&#24418;&#29366;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#23545;&#24212;&#21644;&#38754;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#32467;&#26524;&#24418;&#29366;&#21644;&#22806;&#35266;&#30340;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21487;&#24494;&#20998;&#28210;&#26579;&#65292;&#24182;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#20197;&#22312;&#25991;&#26412;&#24341;&#23548;&#19979;&#20248;&#21270;&#21464;&#24418;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20851;&#33410;&#32593;&#26684;&#30340;&#22810;&#26679;&#21270;&#22836;&#37096;&#22836;&#20687;&#65292;&#21487;&#26080;&#32541;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09326v1 Announce Type: cross  Abstract: We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seaml
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13517</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36870;&#21521;&#32763;&#35793;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Round Trip Translation Defence against Large Language Model Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13517
&lt;/p&gt;
&lt;p&gt;
&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#23545;&#20154;&#31867;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#38656;&#35201;LLMs&#20855;&#26377;&#39640;&#27700;&#24179;&#30340;&#29702;&#35299;&#33021;&#21147;&#25165;&#33021;&#25269;&#25239;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#26368;&#22810;&#21482;&#33021;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19981;&#21040;&#19968;&#21322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;LLMs&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;RTT&#20250;&#25913;&#20889;&#23545;&#25239;&#24615;&#25552;&#31034;&#24182;&#25512;&#24191;&#34920;&#36798;&#30340;&#24605;&#24819;&#65292;&#20351;LLMs&#26356;&#23481;&#26131;&#26816;&#27979;&#20986;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#12289;&#36731;&#37327;&#19988;&#21487;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#36229;&#36807;70%&#30340;Prompt Automatic Iterative Refinement (PAIR)&#25915;&#20987;&#65292;&#36825;&#26159;&#30446;&#21069;&#25105;&#20204;&#25152;&#30693;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#23581;&#35797;&#32531;&#35299;MathsAttack&#65292;&#24182;&#23558;&#20854;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#20102;&#36817;40%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16960</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Generating Explanations for Reinforcement Learning Policies: An Empirical Study. (arXiv:2309.16960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#31574;&#30053;&#35299;&#37322;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#26082;&#38416;&#26126;&#31574;&#30053;&#25152;&#23454;&#29616;&#30340;&#26368;&#32456;&#30446;&#26631;&#21448;&#38416;&#26126;&#20854;&#25191;&#34892;&#36807;&#31243;&#20013;&#25152;&#32500;&#25345;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#35299;&#37322;&#12290;&#36825;&#20123;&#22522;&#20110;LTL&#30340;&#35299;&#37322;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23616;&#37096;&#25628;&#32034;&#25216;&#26415;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#22842;&#26071;&#29615;&#22659;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06129</link><description>&lt;p&gt;
LEyes&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21152;&#24378;&#20102;&#20957;&#35270;&#20272;&#35745;&#25216;&#26415;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#30524;&#37096;&#22270;&#20687;&#30340;&#30828;&#20214;&#24341;&#36215;&#30340;&#21464;&#24322;&#20197;&#21450;&#35760;&#24405;&#30340;&#21442;&#19982;&#32773;&#20043;&#38388;&#22266;&#26377;&#30340;&#29983;&#29289;&#24046;&#24322;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34394;&#25311;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21019;&#24314;&#34394;&#25311;&#25968;&#25454;&#38598;&#26082;&#38656;&#35201;&#26102;&#38388;&#21448;&#38656;&#35201;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Light Eyes or "LEyes"&#30340;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#36924;&#30495;&#26041;&#27861;&#19981;&#21516;&#65292;LEyes&#20165;&#27169;&#25311;&#35270;&#39057;&#30524;&#21160;&#36319;&#36394;&#25152;&#38656;&#30340;&#20851;&#38190;&#22270;&#20687;&#29305;&#24449;&#12290;LEyes&#20415;&#20110;&#22312;&#22810;&#26679;&#21270;&#30340;&#20957;&#35270;&#20272;&#35745;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30524;&#30555;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addit
&lt;/p&gt;</description></item></channel></rss>