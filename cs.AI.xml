<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.17246</link><description>&lt;p&gt;
TwoStep: &#20351;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#20043;&#31867;&#30340;&#32463;&#20856;&#35268;&#21010;&#20844;&#24335;&#20801;&#35768;&#30830;&#23450;&#21487;&#23454;&#29616;&#30446;&#26631;&#29366;&#24577;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#21482;&#35201;&#23384;&#22312;&#20219;&#20309;&#21487;&#33021;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;PDDL&#20013;&#23450;&#20041;&#30340;&#25512;&#29702;&#38382;&#39064;&#24182;&#26410;&#25429;&#33719;&#34892;&#21160;&#36827;&#34892;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#20363;&#22914;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#26234;&#33021;&#20307;&#22914;&#26524;&#24444;&#27492;&#30340;&#21518;&#20917;&#19981;&#24178;&#25200;&#21069;&#25552;&#26465;&#20214;&#65292;&#21017;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#19968;&#20010;&#21160;&#20316;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#23558;&#30446;&#26631;&#20998;&#35299;&#20026;&#22823;&#37096;&#20998;&#29420;&#31435;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#23376;&#30446;&#26631;&#65292;&#20197;&#21033;&#29992;&#21516;&#26102;&#36827;&#34892;&#21160;&#20316;&#26469;&#21152;&#24555;&#35745;&#21010;&#27493;&#39588;&#30340;&#25191;&#34892;&#65292;&#27599;&#20010;&#37096;&#20998;&#20165;&#20351;&#29992;&#21333;&#20010;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30452;&#25509;&#25512;&#26029;&#35745;&#21010;&#27493;&#39588;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#19981;&#20445;&#35777;&#25191;&#34892;&#25104;&#21151;&#65292;&#20294;&#21033;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#32452;&#35013;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#35268;&#21010;&#21644;LLMs&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17246v1 Announce Type: new  Abstract: Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuition
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.10039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20809;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#19982;&#30417;&#30563;&#35774;&#32622;&#19981;&#21516;&#65292;&#26080;&#30417;&#30563;&#20998;&#21106;&#20027;&#35201;&#20381;&#36182;&#20110;&#36816;&#21160;&#32447;&#32034;&#65292;&#28982;&#32780;&#30001;&#20110;&#25163;&#26415;&#38236;&#22836;&#20013;&#20809;&#27969;&#36890;&#24120;&#27604;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#35201;&#20302;&#36136;&#37327;&#65292;&#36825;&#20123;&#36816;&#21160;&#32447;&#32034;&#24456;&#38590;&#35782;&#21035;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#21363;&#20351;&#38754;&#23545;&#20302;&#36136;&#37327;&#20809;&#27969;&#22266;&#26377;&#38480;&#21046;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19977;&#20010;&#26041;&#38754;&#20837;&#25163;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#26377;&#36873;&#25321;&#22320;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;EndoVis2017 VOS&#25968;&#25454;&#38598;&#21644;Endovis2017&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#27169;&#22411;&#23637;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#22343;&#20540;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10039v1 Announce Type: cross  Abstract: Video-based surgical instrument segmentation plays an important role in robot-assisted surgeries. Unlike supervised settings, unsupervised segmentation relies heavily on motion cues, which are challenging to discern due to the typically lower quality of optical flow in surgical footage compared to natural scenes. This presents a considerable burden for the advancement of unsupervised segmentation techniques. In our work, we address the challenge of enhancing model performance despite the inherent limitations of low-quality optical flow. Our methodology employs a three-pronged approach: extracting boundaries directly from the optical flow, selectively discarding frames with inferior flow quality, and employing a fine-tuning process with variable frame rates. We thoroughly evaluate our strategy on the EndoVis2017 VOS dataset and Endovis2017 Challenge dataset, where our model demonstrates promising results, achieving a mean Intersection-o
&lt;/p&gt;</description></item><item><title>Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18205</link><description>&lt;p&gt;
Lemur: &#20351;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18205
&lt;/p&gt;
&lt;p&gt;
Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#20135;&#29983;&#30340;&#26085;&#24535;&#23545;&#30417;&#35270;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#26085;&#24535;&#20998;&#26512;&#26377;&#21161;&#20110;&#26816;&#27979;&#12289;&#25253;&#35686;&#21644;&#35786;&#26029;&#31995;&#32479;&#25925;&#38556;&#12290;&#26085;&#24535;&#35299;&#26512;&#26159;&#26085;&#24535;&#20998;&#26512;&#33258;&#21160;&#21270;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23427;&#28041;&#21450;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#27169;&#26495;&#12290;&#29616;&#26377;&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#30001;&#20110;&#20381;&#36182;&#20110;&#20154;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#32780;&#26080;&#27861;&#35782;&#21035;&#27491;&#30830;&#30340;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#32479;&#35745;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#26085;&#24535;&#28040;&#24687;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65288;Lemur&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#21160;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29109;&#21551;&#21457;&#30340;&#26032;&#22411;&#25277;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20856;&#22411;&#26085;&#24535;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#26085;&#24535;&#27169;&#26495;&#30340;&#21512;&#24182;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24605;&#32500;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18205v1 Announce Type: cross  Abstract: Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12920</link><description>&lt;p&gt;
&#29992;&#20998;&#35299;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Truck Parking Usage Prediction with Decomposed Graph Neural Networks. (arXiv:2401.12920v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36135;&#36816;&#36208;&#24266;&#19978;&#30340;&#21345;&#36710;&#20572;&#36710;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20572;&#36710;&#20301;&#19981;&#36275;&#21644;&#36981;&#23432;&#24037;&#26102;&#35268;&#23450;&#12290;&#36825;&#20123;&#38480;&#21046;&#24448;&#24448;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#34892;&#20026;&#65292;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#36135;&#36816;&#20316;&#19994;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#20572;&#36710;&#20351;&#29992;&#39044;&#27979;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#21333;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#20351;&#29992;&#24773;&#20917;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36739;&#39640;&#65292;&#20294;&#23545;&#22810;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20351;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21306;&#22495;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RegT-GCN&#65289;&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#21345;&#36710;&#20572;&#36710;&#20449;&#24687;&#21644;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21345;&#36710;&#20572;&#36710;&#22330;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#21382;&#21490;&#20572;&#36710;&#25968;&#25454;&#26469;&#39044;&#27979;&#25972;&#20010;&#24030;&#30340;&#21344;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations. These constraints often result in unauthorized parking practices, causing safety concerns. To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution. Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking. The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state. To achieve this,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.16472</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#21322;&#29615;&#28335;&#28304;
&lt;/p&gt;
&lt;p&gt;
Semiring Provenance for Lightweight Description Logics. (arXiv:2310.16472v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#29615;&#28335;&#28304;&#8212;&#8212;&#19968;&#31181;&#26368;&#21021;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#29615;&#22659;&#20013;&#23450;&#20041;&#30340;&#25104;&#21151;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#36923;&#36753;&#12290;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#65292;&#26412;&#20307;&#20844;&#29702;&#34987;&#29992;&#20132;&#25442;&#21322;&#29615;&#30340;&#20803;&#32032;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#19988;&#36825;&#20123;&#27880;&#37322;&#26681;&#25454;&#23427;&#20204;&#30340;&#25512;&#23548;&#26041;&#24335;&#20256;&#25773;&#21040;&#26412;&#20307;&#30340;&#32467;&#26524;&#20013;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#28335;&#28304;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#20960;&#31181;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#35821;&#35328;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#20026;&#24102;&#26377;&#29305;&#23450;&#31867;&#22411;&#27880;&#37322;&#65288;&#22914;&#27169;&#31946;&#24230;&#65289;&#30340;&#26412;&#20307;&#23450;&#20041;&#30340;&#20854;&#20182;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#23545;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#26399;&#26395;&#30340;&#29305;&#24615;&#65288;&#22914;&#25193;&#23637;&#20102;&#25968;&#25454;&#24211;&#20013;&#23450;&#20041;&#30340;&#21322;&#29615;&#28335;&#28304;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;&#33879;&#21517;&#30340;why&#28335;&#28304;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#35745;&#31639;&#27599;&#20010;&#21152;&#27861;&#24130;&#31561;&#21644;&#20056;&#27861;&#24130;&#31561;&#30340;&#20132;&#25442;&#21322;&#29615;&#30340;&#21322;&#29615;&#28335;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#19982;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semiring provenance--a successful framework originally defined in the relational database setting--for description logics. In this context, the ontology axioms are annotated with elements of a commutative semiring and these annotations are propagated to the ontology consequences in a way that reflects how they are derived. We define a provenance semantics for a language that encompasses several lightweight description logics and show its relationships with semantics that have been defined for ontologies annotated with a specific kind of annotation (such as fuzzy degrees). We show that under some restrictions on the semiring, the semantics satisfies desirable properties (such as extending the semiring provenance defined for databases). We then focus on the well-known why-provenance, which allows to compute the semiring provenance for every additively and multiplicatively idempotent commutative semiring, and for which we study the complexity of problems related to the prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00505</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65306;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case. (arXiv:2308.00505v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25191;&#27861;&#30446;&#30340;&#24314;&#27169;&#29359;&#32618;&#32593;&#32476;&#65292;&#38656;&#35201;&#23558;&#26377;&#38480;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#32463;&#36807;&#39564;&#35777;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#12290;&#24403;&#21069;&#21009;&#20107;&#23398;&#24314;&#27169;&#20013;&#32570;&#23569;&#19968;&#20010;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#31995;&#32479;&#21644;&#36879;&#26126;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#35745;&#31639;&#29359;&#32618;&#24314;&#27169;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21253;&#25324;&#23558;&#23450;&#24615;&#25968;&#25454;&#36716;&#21270;&#20026;&#23450;&#37327;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FREIDA&#65288;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#39537;&#21160;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#26694;&#26550;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#29359;&#32618;&#21487;&#21345;&#22240;&#26367;&#20195;&#27169;&#22411;&#65288;CCRM&#65289;&#23558;&#20316;&#20026;&#31034;&#20363;&#26696;&#20363;&#65292;&#20197;&#28436;&#31034;FREIDA&#26041;&#27861;&#12290;&#23545;&#20110;CCRM&#65292;&#27491;&#22312;&#24314;&#27169;&#33655;&#20848;&#30340;&#19968;&#20010;&#26377;&#32452;&#32455;&#21487;&#21345;&#22240;&#32593;&#32476;&#65292;&#35797;&#22270;&#36890;&#36807;&#31227;&#38500;&#39318;&#33041;&#33410;&#28857;&#65292;&#20351;&#21097;&#20313;&#20195;&#29702;&#37325;&#26032;&#32452;&#32455;&#65292;&#24182;&#23558;&#32593;&#32476;&#24674;&#22797;&#21040;&#31283;&#23450;&#29366;&#24577;&#12290;&#23450;&#24615;&#25968;&#25454;&#28304;&#65292;&#20363;&#22914;&#26696;&#20214;&#25991;&#20214;&#65292;&#25991;&#29486;&#21644;&#37319;&#35775;&#65292;&#34987;&#36716;&#21270;&#20026;&#32463;&#39564;&#27861;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to model criminal networks for law enforcement purposes, a limited supply of data needs to be translated into validated agent-based models. What is missing in current criminological modelling is a systematic and transparent framework for modelers and domain experts that establishes a modelling procedure for computational criminal modelling that includes translating qualitative data into quantitative rules. For this, we propose FREIDA (Framework for Expert-Informed Data-driven Agent-based models). Throughout the paper, the criminal cocaine replacement model (CCRM) will be used as an example case to demonstrate the FREIDA methodology. For the CCRM, a criminal cocaine network in the Netherlands is being modelled where the kingpin node is being removed, the goal being for the remaining agents to reorganize after the disruption and return the network into a stable state. Qualitative data sources such as case files, literature and interviews are translated into empirical laws, and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item></channel></rss>