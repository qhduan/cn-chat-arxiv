<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#35748;&#20026;&#39068;&#33394;&#22312;&#20154;&#31867;&#35270;&#35273;&#20013;&#21487;&#20197;&#34987;&#35270;&#20026;&#20809;&#23376;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#25552;&#20379;&#20102;&#26032;&#30340;&#37327;&#23376;&#35748;&#30693;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.18850</link><description>&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#20013;&#39068;&#33394;&#26159;&#21542;&#26159;&#20809;&#23376;&#65311;&#35270;&#35273;&#30693;&#35273;&#30340;&#37327;&#23376;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study of Visual Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#35748;&#20026;&#39068;&#33394;&#22312;&#20154;&#31867;&#35270;&#35273;&#20013;&#21487;&#20197;&#34987;&#35270;&#20026;&#20809;&#23376;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#25552;&#20379;&#20102;&#26032;&#30340;&#37327;&#23376;&#35748;&#30693;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#30340;&#29616;&#35937;&#12290;&#35813;&#29616;&#35937;&#30340;&#26426;&#21046;&#22312;&#20110;&#34987;&#24863;&#30693;&#30340;&#25193;&#25955;&#21050;&#28608;&#34987;&#35748;&#20026;&#23646;&#20110;&#19981;&#21516;&#31867;&#21035;&#65292;&#32780;&#34987;&#24863;&#30693;&#30340;&#25910;&#32553;&#21050;&#28608;&#34987;&#35748;&#20026;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#30001;&#20110;&#32431;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#19982;&#23494;&#24230;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#30830;&#23450;&#26041;&#24335;&#33258;&#28982;&#19981;&#21516;&#65292;&#22240;&#27492;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#26681;&#26893;&#20110;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#30340;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#32467;&#26524;&#24212;&#29992;&#20110;&#39068;&#33394;&#30340;&#35270;&#35273;&#24863;&#30693;&#24773;&#20917;&#65292;&#24182;&#35748;&#20026;&#21487;&#20197;&#23558;&#39068;&#33394;&#35270;&#20026;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#20013;&#30340;&#20809;&#23376;&#65292;&#31867;&#20284;&#20110;&#20809;&#39057;&#29289;&#29702;&#27979;&#37327;&#20013;&#30340;&#20809;&#23376;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#30693;&#35273;&#30475;&#20316;&#26159;&#29616;&#26377;&#29289;&#29702;&#29616;&#23454;&#12289;&#21050;&#28608;&#20197;&#21450;&#34987;&#24863;&#30693;&#32773;&#25152;&#26399;&#26395;&#30340;&#29616;&#23454;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18850v1 Announce Type: cross  Abstract: We study the phenomenon of categorical perception within the quantum measurement process. The mechanism underlying this phenomenon consists in dilating stimuli being perceived to belong to different categories and contracting stimuli being perceived to belong to the same category. We show that, due to the naturally different way in determining the distance between pure states compared to the distance between density states, the phenomenon of categorical perception is rooted in the structure of the quantum measurement process itself. We apply our findings to the situation of visual perception of colors and argue that it is possible to consider colors as light quanta for human visual perception in a similar way as photons are light quanta for physical measurements of light frequencies. In our approach we see perception as a complex encounter between the existing physical reality, the stimuli, and the reality expected by the perciever, re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16101</link><description>&lt;p&gt;
&#36328;&#22269;&#30028;&#35780;&#20272;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65306;&#26469;&#33258;&#20154;&#31867;&#24863;&#30693;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fairness Metrics Across Borders from Human Perceptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#36866;&#29992;&#20110;&#24744;&#30340;&#22330;&#26223;&#65311;&#21363;&#20351;&#32467;&#26524;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#20063;&#21487;&#33021;&#23384;&#22312;&#20851;&#20110;&#20844;&#24179;&#24863;&#30693;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#12290;&#24050;&#36827;&#34892;&#20102;&#22810;&#39033;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#19982;&#20154;&#20204;&#23545;&#20844;&#24179;&#30340;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#26597;&#33539;&#22260;&#26377;&#38480;&#65292;&#20165;&#21253;&#25324;&#21333;&#20010;&#22269;&#23478;&#20013;&#25968;&#30334;&#21517;&#21442;&#19982;&#32773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22269;&#38469;&#35843;&#26597;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#22312;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20998;&#21035;&#20174;&#20013;&#22269;&#12289;&#27861;&#22269;&#12289;&#26085;&#26412;&#21644;&#32654;&#22269;&#30340;&#27599;&#20010;&#22269;&#23478;&#25910;&#38598;&#20102;1,000&#21517;&#21442;&#19982;&#32773;&#30340;&#22238;&#24212;&#65292;&#24635;&#35745;&#24471;&#21040;&#20102;4,000&#20010;&#22238;&#24212;&#65292;&#20197;&#20998;&#26512;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#65292;&#37197;&#22791;&#20102;&#22235;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#36873;&#25321;&#20854;&#21916;&#22909;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16101v1 Announce Type: new  Abstract: Which fairness metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics. Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness. However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics. Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case. This investigation explores the
&lt;/p&gt;</description></item><item><title>FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.02399</link><description>&lt;p&gt;
FreDF: &#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FreDF: Learning to Forecast in Frequency Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02399
&lt;/p&gt;
&lt;p&gt;
FreDF&#26159;&#19968;&#31181;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#26631;&#31614;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#22312;&#21382;&#21490;&#24207;&#21015;&#21644;&#26631;&#31614;&#24207;&#21015;&#20013;&#37117;&#38754;&#20020;&#33258;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22788;&#29702;&#21382;&#21490;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#33258;&#30456;&#20851;&#23384;&#22312;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26032;&#20852;&#30340;&#39044;&#27979;&#27169;&#22411;&#20027;&#35201;&#36981;&#24490;&#30452;&#25509;&#39044;&#27979;&#65288;DF&#65289;&#33539;&#24335;&#65292;&#22312;&#26631;&#31614;&#24207;&#21015;&#20013;&#20551;&#35774;&#26465;&#20214;&#29420;&#31435;&#24615;&#19979;&#29983;&#25104;&#22810;&#27493;&#39044;&#27979;&#12290;&#36825;&#31181;&#20551;&#35774;&#24573;&#35270;&#20102;&#26631;&#31614;&#24207;&#21015;&#20013;&#22266;&#26377;&#30340;&#33258;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22522;&#20110;DF&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39057;&#22495;&#22686;&#24378;&#30452;&#25509;&#39044;&#27979;&#65288;FreDF&#65289;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#23398;&#20064;&#39044;&#27979;&#26469;&#36991;&#20813;&#26631;&#31614;&#33258;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreDF&#22312;&#24615;&#33021;&#19978;&#22823;&#22823;&#36229;&#36807;&#20102;&#21253;&#25324;iTransformer&#22312;&#20869;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#39044;&#27979;&#27169;&#22411;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods including iTransformer and is compatible with a variety of forecast models.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.16646</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#36830;&#36143;&#27010;&#29575;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Incoherent Probability Judgments in Large Language Models. (arXiv:2401.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16646
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36830;&#36143;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#26159;&#21542;&#21516;&#26679;&#25797;&#38271;&#24418;&#25104;&#36830;&#36143;&#30340;&#27010;&#29575;&#21028;&#26029;&#65311;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#36523;&#20221;&#21644;&#37325;&#22797;&#21028;&#26029;&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#27010;&#29575;&#29702;&#35770;&#35268;&#21017;&#20559;&#31163;&#12290;&#27492;&#22806;&#65292;&#24403;&#35201;&#27714;&#23545;&#21516;&#19968;&#20107;&#20214;&#36827;&#34892;&#21028;&#26029;&#26102;&#65292;LLMs&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#22343;&#20540;-&#26041;&#24046;&#20851;&#31995;&#21576;&#29616;&#20986;&#20154;&#31867;&#25152;&#35265;&#21040;&#30340;&#20498;U&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#38750;&#29702;&#24615;&#30340;&#20559;&#31163;&#21487;&#20197;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19982;&#20154;&#31867;&#27010;&#29575;&#21028;&#26029;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#22120;&#27169;&#22411;&#36827;&#34892;&#31867;&#27604;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02663</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#24403;&#20855;&#20307;&#30340;&#27169;&#24335;&#65288;&#31216;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#20363;&#22914;&#23376;&#22270;&#12289;&#33410;&#28857;&#31561;&#65289;&#20986;&#29616;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#20250;&#34987;&#28608;&#27963;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#31867;&#26631;&#31614;&#65292;&#32780;&#24403;&#36755;&#20837;&#20013;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#19981;&#20250;&#34987;&#28608;&#27963;&#65292;&#27169;&#22411;&#27491;&#24120;&#24037;&#20316;&#12290;&#21518;&#38376;&#25915;&#20987;&#20855;&#26377;&#26497;&#39640;&#30340;&#38544;&#34109;&#24615;&#65292;&#32473;GNN&#27169;&#22411;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#23545;GNN&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#21644;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
&lt;/p&gt;</description></item><item><title>Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07937</link><description>&lt;p&gt;
Co-NavGPT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#35821;&#20041;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07937
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32423;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36807;&#21435;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#26159;&#35774;&#35745;&#29992;&#20110;&#21333;&#19968;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#24448;&#24448;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#31574;&#30053;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Co-NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#12290;Co-NavGPT&#23558;&#25506;&#32034;&#30340;&#29615;&#22659;&#25968;&#25454;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#22686;&#24378;LLMs&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#20998;&#37197;&#25506;&#32034;&#21069;&#27839;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#25628;&#32034;&#12290;&#22312;Habitat-Matterport 3D (HM3D)&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-NavGPT&#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#23398;&#20064;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
&lt;/p&gt;</description></item><item><title>ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05537</link><description>&lt;p&gt;
ParFam - &#22522;&#20110;&#36830;&#32493;&#20840;&#23616;&#20248;&#21270;&#30340;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05537
&lt;/p&gt;
&lt;p&gt;
ParFam&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#30340;&#31526;&#21495;&#20989;&#25968;&#26063;&#23558;&#31163;&#25955;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#27604;&#22914;&#20174;&#32473;&#23450;&#25968;&#25454;&#20013;&#35782;&#21035;&#29289;&#29702;&#23450;&#24459;&#25110;&#25512;&#23548;&#25551;&#36848;&#37329;&#34701;&#24066;&#22330;&#34892;&#20026;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#30446;&#21069;&#23384;&#22312;&#22810;&#31181;&#35299;&#20915;SR&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#38656;&#35201;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;ParFam&#65292;&#23427;&#21033;&#29992;&#36866;&#21512;&#30340;&#31526;&#21495;&#20989;&#25968;&#30340;&#21442;&#25968;&#21270;&#26063;&#23558;&#31163;&#25955;&#30340;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#38382;&#39064;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#32622;&#26356;&#21152;&#30452;&#35266;&#12290;&#32467;&#21512;&#24378;&#22823;&#30340;&#20840;&#23616;&#20248;&#21270;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;SR&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26356;&#39640;&#32423;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#28155;&#21152;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#25214;&#21040;&#36866;&#21512;&#30340;&#21442;&#25968;&#21270;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#27169;&#22411;&#21270;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#27010;&#29575;&#21644;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04306</link><description>&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#30340;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning. (arXiv:2310.04306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#27169;&#22411;&#21270;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21033;&#29992;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#27010;&#29575;&#21644;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#26159;&#20154;&#31867;&#34892;&#20026;&#20998;&#26512;&#20013;&#19981;&#21487;&#20998;&#21106;&#30340;&#19968;&#37096;&#20998;&#65292;&#26088;&#22312;&#35782;&#21035;&#22810;&#20154;&#22330;&#26223;&#20013;&#30340;&#25972;&#20307;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#24773;&#32490;&#32447;&#32034;&#65292;&#32780;&#24573;&#35270;&#20102;&#22312;&#26080;&#32422;&#26463;&#29615;&#22659;&#19979;&#23384;&#22312;&#30340;&#22242;&#20307;&#20869;&#25317;&#25380;&#21644;&#36974;&#25377;&#31561;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20165;&#26377;&#32676;&#20307;&#32423;&#26631;&#31614;&#21487;&#29992;&#65292;&#22312;&#19968;&#20010;&#32676;&#20307;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24773;&#32490;&#39044;&#27979;&#20250;&#28151;&#28102;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#32676;&#20307;&#32423;&#24773;&#32490;&#35782;&#21035;&#25552;&#21462;&#26356;&#21152;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#27599;&#20010;&#20010;&#20307;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#20013;&#30340;&#38543;&#26426;&#23884;&#20837;&#26469;&#20195;&#26367;&#30830;&#23450;&#24615;&#30340;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#34920;&#31034;&#25429;&#25417;&#20102;&#19981;&#21516;&#24773;&#32490;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#38543;&#26426;&#24615;&#22312;&#25512;&#26029;&#38454;&#27573;&#20135;&#29983;&#22810;&#26679;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty of each individual, we utilize stochastic embedding drawn from a Gaussian distribution instead of deterministic point embedding. This representation captures the probabilities of different emotions and generates diverse predictions through this stochasticity during the inference stage. Furthermore, uncertainty-sensitive scores are a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item></channel></rss>