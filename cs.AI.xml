<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05158</link><description>&lt;p&gt;
&#33021;&#37327;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Learning over Energy-Constrained Wireless Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05158
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35774;&#22791;&#19982;&#26381;&#21153;&#22120;&#21512;&#20316;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#22522;&#20110;&#30456;&#21516;&#30340;&#22266;&#23450;&#20998;&#35010;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#21644;&#20449;&#36947;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#26041;&#24335;&#22312;&#35757;&#32451;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#65292;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#28385;&#36275;&#38271;&#26399;&#33021;&#37327;&#28040;&#32791;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#22312;&#20110;&#32570;&#20047;&#26410;&#26469;&#20449;&#24687;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#29702;&#35770;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21517;&#20026;OPEN&#65292;&#23427;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#20010;&#20855;&#26377;&#24403;&#21069;&#30340;&#26032;MIP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05158v1 Announce Type: cross  Abstract: Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the curren
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02249</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Sequence-to-Sequence Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#29983;&#25104;&#39044;&#27979;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#24310;&#36831;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24182;&#34892;&#35299;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;Query-CTC&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;&#38480;&#21046;&#22312;&#26465;&#20214;&#20998;&#24067;&#19978;&#12290;&#32467;&#26524;&#27169;&#22411;NARVL&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#26356;&#24555;&#65292;&#20174;&#19982;&#39034;&#24207;&#29983;&#25104;&#26631;&#35760;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20943;&#23569;&#21040;&#24120;&#37327;&#26102;&#38388;&#32852;&#21512;&#25512;&#29702;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02249v1 Announce Type: cross  Abstract: Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65288;RDOT&#65289;&#20013;&#30340;90&#31181;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#33021;&#22815;&#24110;&#21161;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#24212;&#23545;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03133</link><description>&lt;p&gt;
&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;: &#31649;&#29702;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#39118;&#38505;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;90&#31181;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Risk-reducing design and operations toolkit: 90 strategies for managing risk and uncertainty in decision problems. (arXiv:2309.03133v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65288;RDOT&#65289;&#20013;&#30340;90&#31181;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#33021;&#22815;&#24110;&#21161;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#24212;&#23545;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#26159;&#20915;&#31574;&#20998;&#26512;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#20915;&#31574;&#29702;&#35770;&#25215;&#35748;&#20004;&#31867;&#35299;&#20915;&#26041;&#26696;: &#27010;&#29575;&#27169;&#22411;&#21644;&#35748;&#30693;&#21551;&#21457;&#24335;&#12290;&#28982;&#32780;&#65292;&#24037;&#31243;&#24072;&#12289;&#20844;&#20849;&#35268;&#21010;&#32773;&#21644;&#20854;&#20182;&#20915;&#31574;&#32773;&#20351;&#29992;&#30340;&#26159;&#21478;&#19968;&#31867;&#34987;&#31216;&#20026;RDOT&#65288;&#39118;&#38505;&#38477;&#20302;&#35774;&#35745;&#21644;&#36816;&#33829;&#24037;&#20855;&#21253;&#65289;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23558;&#31283;&#20581;&#24615;&#32435;&#20837;&#35774;&#35745;&#12289;&#20107;&#21518;&#39044;&#38450;&#25514;&#26045;&#31561;&#65292;&#24182;&#19981;&#23646;&#20110;&#27010;&#29575;&#27169;&#22411;&#25110;&#35748;&#30693;&#21551;&#21457;&#24335;&#30340;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#31574;&#30053;&#20986;&#29616;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#23398;&#31185;&#20013;&#65292;&#25351;&#21521;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20849;&#20139;&#24037;&#20855;&#21253;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#36825;&#20123;&#31574;&#30053;&#30340;&#30446;&#24405;&#24182;&#20026;&#20854;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#36229;&#36807;90&#20010;&#23646;&#20110;&#20845;&#20010;&#24191;&#27867;&#31867;&#21035;&#30340;&#36825;&#26679;&#30340;&#31574;&#30053;&#65292;&#24182;&#35748;&#20026;&#23427;&#20204;&#23545;&#20110;&#30001;&#20110;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#32780;&#20284;&#20046;&#26840;&#25163;&#30340;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#23427;&#20204;&#32435;&#20837;&#20915;&#31574;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty is a pervasive challenge in decision analysis, and decision theory recognizes two classes of solutions: probabilistic models and cognitive heuristics. However, engineers, public planners and other decision-makers instead use a third class of strategies that could be called RDOT (Risk-reducing Design and Operations Toolkit). These include incorporating robustness into designs, contingency planning, and others that do not fall into the categories of probabilistic models or cognitive heuristics. Moreover, identical strategies appear in several domains and disciplines, pointing to an important shared toolkit.  The focus of this paper is to develop a catalog of such strategies and develop a framework for them. The paper finds more than 90 examples of such strategies falling into six broad categories and argues that they provide an efficient response to decision problems that are seemingly intractable due to high uncertainty. It then proposes a framework to incorporate them into 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item></channel></rss>