<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16393</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;
&lt;/p&gt;
&lt;p&gt;
Concurrent Linguistic Error Detection (CLED) for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#38169;&#35823;&#30340;&#26816;&#27979;&#26159;&#20943;&#36731;&#20854;&#23545;&#31995;&#32479;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#65292;&#22240;&#27492;&#65292;LLMs&#30340;&#39640;&#25928;&#38169;&#35823;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22522;&#20110;&#23545;LLMs&#36755;&#20986;&#36827;&#34892;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36827;&#34892;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;&#65307;&#35813;&#26041;&#26696;&#25552;&#21462;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#20123;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36755;&#20837;&#21040;&#19968;&#20010;&#24182;&#21457;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16393v1 Announce Type: new  Abstract: The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item></channel></rss>