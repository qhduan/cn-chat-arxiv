<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17726</link><description>&lt;p&gt;
&#23567;&#22411;&#27169;&#22411;&#26159;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;
&lt;/p&gt;
&lt;p&gt;
Tiny Models are the Computational Saver for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17726
&lt;/p&gt;
&lt;p&gt;
TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TinySaver&#65292;&#19968;&#31181;&#31867;&#20284;&#20110;&#26089;&#26399;&#36864;&#20986;&#30340;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#25216;&#26415;&#19981;&#21516;&#65292;&#20687;TinySaver&#36825;&#26679;&#30340;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#38590;&#24230;&#24046;&#24322;&#65292;&#20351;&#24471;&#26576;&#20123;&#36755;&#20837;&#33021;&#22815;&#25552;&#21069;&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26089;&#26399;&#36864;&#20986;&#35774;&#35745;&#26159;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#39592;&#24178;&#32467;&#26500;&#38468;&#21152;&#39069;&#22806;&#30340;&#32593;&#32476;&#20998;&#25903;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23436;&#20840;&#29420;&#31435;&#30340;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#22312;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#26367;&#20195;&#36739;&#22823;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#12290;&#23558;&#23427;&#20204;&#20316;&#20026;&#31532;&#19968;&#20010;&#36864;&#20986;&#28857;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#25628;&#32034;&#24182;&#20351;&#29992;&#26368;&#21512;&#36866;&#30340;&#23567;&#22411;&#27169;&#22411;&#20316;&#20026;&#32473;&#23450;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17726v1 Announce Type: new  Abstract: This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15442</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#32819;&#34583;&#26893;&#20837;&#35013;&#32622;&#20013;&#30340;&#20808;&#36827;&#31639;&#27861;&#65306;&#21307;&#30103;&#31574;&#30053;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15442
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20026;&#19982;&#26426;&#22120;&#20132;&#20114;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#36824;&#20026;&#37096;&#20998;&#25110;&#23436;&#20840;&#21548;&#21147;&#21463;&#25439;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#27807;&#36890;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#20197;&#27169;&#25311;&#24418;&#24335;&#25509;&#25910;&#35821;&#38899;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20351;&#20854;&#19982;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;CI&#65289;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37197;&#22791;&#26377;&#26377;&#38480;&#25968;&#37327;&#30005;&#26497;&#30340;&#26893;&#20837;&#35013;&#32622;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#23548;&#33268;&#35821;&#38899;&#22833;&#30495;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25913;&#21892;&#25509;&#25910;&#21040;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#22312;&#28041;&#21450;&#22810;&#20010;&#35821;&#38899;&#28304;&#12289;&#29615;&#22659;&#22122;&#22768;&#21644;&#20854;&#20182;&#24773;&#20917;&#30340;&#22330;&#26223;&#20013;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 Announce Type: cross  Abstract: Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;&#65292;&#19981;&#20855;&#22791;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#24615;&#12289;&#24207;&#21015;&#24615;&#21644;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#20854;&#23436;&#22791;&#24615;&#24182;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;</title><link>https://arxiv.org/abs/2403.14704</link><description>&lt;p&gt;
&#19968;&#20010;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A minimal coalition logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14704
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;&#65292;&#19981;&#20855;&#22791;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#24615;&#12289;&#24207;&#21015;&#24615;&#21644;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#20854;&#23436;&#22791;&#24615;&#24182;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#30431;&#36923;&#36753;&#26159;&#25112;&#30053;&#25512;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#36923;&#36753;&#12290;&#26412;&#25991;&#39318;&#20808;&#25351;&#20986;&#32852;&#30431;&#36923;&#36753;&#27169;&#22411;&#65292;&#21363;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#65292;&#23384;&#22312;&#19977;&#20010;&#36807;&#20110;&#24378;&#30340;&#20551;&#35774;&#12290;&#20854;&#19968;&#26159;&#20195;&#29702;&#30340;&#29420;&#31435;&#24615;&#65307;&#21363;&#65292;&#20004;&#20010;&#19981;&#21516;&#32852;&#30431;&#30340;&#20004;&#20010;&#21487;&#29992;&#32852;&#21512;&#21160;&#20316;&#30340;&#21512;&#24182;&#24635;&#26159;&#21487;&#20197;&#21512;&#24182;&#25104;&#20004;&#20010;&#32852;&#30431;&#30340;&#32852;&#30431;&#12290;&#20854;&#20108;&#26159;&#24207;&#21015;&#24615;&#65307;&#21363;&#65292;&#32852;&#30431;&#24635;&#26159;&#26377;&#21487;&#29992;&#30340;&#32852;&#21512;&#21160;&#20316;&#12290;&#20854;&#19977;&#26159;&#30830;&#23450;&#24615;&#65292;&#21363;&#65292;&#22823;&#32852;&#30431;&#30340;&#21512;&#20316;&#21160;&#20316;&#24635;&#26159;&#26377;&#21807;&#19968;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#32852;&#30431;&#36923;&#36753;&#65292;&#35813;&#27169;&#22411;&#19981;&#20855;&#22791;&#36825;&#19977;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36923;&#36753;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#19982;&#32852;&#30431;&#36923;&#36753;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#12290;&#22312;&#25112;&#30053;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#19968;&#36923;&#36753;&#20284;&#20046;&#26159;&#26368;&#23567;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14704v1 Announce Type: cross  Abstract: Coalition logic is a central logic in strategic reasoning studies. In this paper, we first argue that Coalition Logic models, concurrent game models, have three too-strong assumptions. The first one is the independence of agents; that is, the merge of two available joint actions of two disjoint coalitions is always available for the union of the two coalitions. The second one is seriality; that is, coalitions always have available joint actions. The third one is determinism, that is, the grand coalition's joint actions always have a unique outcome. Second, we present a coalition logic based on general concurrent game models, which do not have the three assumptions. We show the completeness of this logic and compare it with Coalition Logic in detail. This logic seems minimal in the context of strategic reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12094</link><description>&lt;p&gt;
LLMs&#26159;&#19968;&#20010;&#22909;&#30340;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#27714;&#35299;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Good Cryptic Crossword Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#26159;&#19968;&#31181;&#35868;&#39064;&#65292;&#19981;&#20165;&#20381;&#36182;&#20110;&#19968;&#33324;&#30693;&#35782;&#65292;&#36824;&#20381;&#36182;&#20110;&#27714;&#35299;&#32773;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#25805;&#32437;&#35821;&#35328;&#24182;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#23383;&#28216;&#25103;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;NLP&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#31867;&#35868;&#39064;&#20063;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#27969;&#34892;&#30340;LLMs -- LLaMA2&#12289;Mistral&#21644;ChatGPT&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#26174;&#31034;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#26102;&#31354;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DCA&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2311.12472</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#31354;&#20559;&#31227;&#30340;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#65306;&#29702;&#35770;&#19982;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#26102;&#31354;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21435;&#28151;&#28102;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;DCA&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#25968;&#25454;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;ST&#20132;&#36890;&#39044;&#27979;&#22312;&#25552;&#39640;&#22478;&#24066;&#20986;&#34892;&#25928;&#29575;&#21644;&#20419;&#36827;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#36807;&#21435;&#20132;&#36890;&#25968;&#25454;&#12289;&#26410;&#26469;&#20132;&#36890;&#25968;&#25454;&#21644;&#22806;&#37096;ST&#19978;&#19979;&#25991;&#30340;&#22240;&#26524;&#22270;&#65292;&#31995;&#32479;&#22320;&#38416;&#26126;&#20102;&#36807;&#21435;&#33402;&#26415;&#20316;&#21697;&#22312;OOD&#20132;&#36890;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#26159;&#30001;&#20110;ST&#19978;&#19979;&#25991;&#20805;&#24403;&#20102;&#28151;&#28102;&#22240;&#32032;&#65292;&#21363;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#25968;&#25454;&#30340;&#20849;&#21516;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;Disentangled Contextual Adjustment&#65288;DCA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12472v2 Announce Type: replace  Abstract: As an important application of spatio-temporal (ST) data, ST traffic forecasting plays a crucial role in improving urban travel efficiency and promoting sustainable development. In practice, the dynamics of traffic data frequently undergo distributional shifts attributed to external factors such as time evolution and spatial differences. This entails forecasting models to handle the out-of-distribution (OOD) issue where test data is distributed differently from training data. In this work, we first formalize the problem by constructing a causal graph of past traffic data, future traffic data, and external ST contexts. We reveal that the failure of prior arts in OOD traffic data is due to ST contexts acting as a confounder, i.e., the common cause for past data and future ones. Then, we propose a theoretical solution named Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates invariant causal correlations again
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#65292;&#20197;&#23454;&#29616;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#12290;&#35813;&#26694;&#26550;&#23558;Navier-Stokes&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;</title><link>http://arxiv.org/abs/2309.04755</link><description>&lt;p&gt;
&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Real-time Training of Physics-informed Neural Networks: Applications in Ultrafast Ultrasound Blood Flow Imaging. (arXiv:2309.04755v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#65292;&#20197;&#23454;&#29616;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#12290;&#35813;&#26694;&#26550;&#23558;Navier-Stokes&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#30340;&#26368;&#26480;&#20986;&#27714;&#35299;&#22120;&#20043;&#19968;&#65292;&#32780;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#24191;&#27867;&#24212;&#29992;&#20110;&#34880;&#27969;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#23436;&#25972;&#30340;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#23545;&#20110;&#36229;&#24555;&#36895;&#22810;&#26222;&#21202;&#36229;&#22768;&#65292;&#36825;&#19968;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#20110;\emph{&#20307;&#20869;}&#22797;&#26434;&#34880;&#27969;&#21160;&#21147;&#23398;&#30340;&#23637;&#31034;&#65292;&#27599;&#31186;&#33719;&#21462;&#25968;&#21315;&#24103;&#65288;&#25110;&#26102;&#38388;&#25139;&#65289;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PINN&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#20026;&#35299;&#20915;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;SeqPINN&#12290;&#22312;SeqPINN&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20316;&#20026;&#21021;&#22987;&#21270;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#27867;&#21270;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Network (PINN) is one of the most preeminent solvers of Navier-Stokes equations, which are widely used as the governing equation of blood flow. However, current approaches, relying on full Navier-Stokes equations, are impractical for ultrafast Doppler ultrasound, the state-of-the-art technique for depiction of complex blood flow dynamics \emph{in vivo} through acquired thousands of frames (or, timestamps) per second. In this article, we first propose a novel training framework of PINN for solving Navier-Stokes equations by discretizing Navier-Stokes equations into steady state and sequentially solving steady-state Navier-Stokes equations with transfer learning. The novel training framework is coined as SeqPINN. Upon the success of SeqPINN, we adopt the idea of averaged constant stochastic gradient descent (SGD) as initialization and propose a parallel training scheme for all timestamps. To ensure an initialization that generalizes well, we borrow the concept of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item></channel></rss>