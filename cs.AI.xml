<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#20551;&#21160;&#20316;&#30340;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22870;&#21169;&#25910;&#30410;&#12289;&#22686;&#21152;&#28216;&#25103;&#22810;&#26679;&#24615;&#65292;&#19988;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#24320;&#38144;&#24456;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.07932</link><description>&lt;p&gt;
&#22810;&#20154;&#28216;&#25103;&#20013;&#30340;&#20551;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Feint in Multi-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#20551;&#21160;&#20316;&#30340;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22870;&#21169;&#25910;&#30410;&#12289;&#22686;&#21152;&#28216;&#25103;&#22810;&#26679;&#24615;&#65292;&#19988;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#24320;&#38144;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22810;&#20154;&#28216;&#25103;&#20013;&#30340;&#20551;&#21160;&#20316;&#36827;&#34892;&#20102;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#22810;&#20154;&#28216;&#25103;&#30340;&#35282;&#24230;&#23545;&#20551;&#21160;&#20316;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#28041;&#21450;&#21040;&#26102;&#38388;&#12289;&#31354;&#38388;&#21644;&#23427;&#20204;&#30340;&#38598;&#20307;&#24433;&#21709;&#12290;&#35813;&#24418;&#24335;&#21270;&#24314;&#31435;&#22312;&#38750;&#20256;&#36882;&#24615;&#20027;&#21160;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#20043;&#19978;&#65292;&#20854;&#20013;&#20551;&#21160;&#20316;&#33021;&#22815;&#20135;&#29983;&#21487;&#35266;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#22810;&#20195;&#29702;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#19979;&#65288;&#21363;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65289;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23454;&#26045;&#20551;&#21160;&#20316;&#30340;&#23454;&#38469;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#37327;&#26816;&#39564;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#20551;&#21160;&#20316;&#35774;&#35745;&#21487;&#20197;&#65288;1&#65289;&#26174;&#33879;&#25552;&#39640;&#28216;&#25103;&#20013;&#30340;&#22870;&#21169;&#25910;&#30410;&#65307;&#65288;2&#65289;&#26174;&#33879;&#25552;&#39640;&#22810;&#20154;&#28216;&#25103;&#30340;&#22810;&#26679;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20165;&#22312;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#20135;&#29983;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#20551;&#21160;&#20316;&#35774;&#35745;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07932v1 Announce Type: cross  Abstract: This paper introduces the first formalization, implementation and quantitative evaluation of Feint in Multi-Player Games. Our work first formalizes Feint from the perspective of Multi-Player Games, in terms of the temporal, spatial, and their collective impacts. The formalization is built upon Non-transitive Active Markov Game Model, where Feint can have a considerable amount of impacts. Then, our work considers practical implementation details of Feint in Multi-Player Games, under the state-of-the-art progress of multi-agent modeling to date (namely Multi-Agent Reinforcement Learning). Finally, our work quantitatively examines the effectiveness of our design, and the results show that our design of Feint can (1) greatly improve the reward gains from the game; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption. We conclude that our design of Feint is effec
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16887</link><description>&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#28508;&#21147;&#12289;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Complex Network: Potential, Methodology and Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16887
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#65292;&#20174;&#33258;&#28982;&#29615;&#22659;&#21040;&#20154;&#31867;&#31038;&#20250;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#26412;&#36136;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#20174;&#24494;&#35266;&#28151;&#20081;-&#20854;&#20013;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#21160;&#24577;&#20132;&#32455;-&#36716;&#21464;&#21644;&#28436;&#21270;&#20026;&#20855;&#26377;&#29305;&#23450;&#38598;&#20307;&#34892;&#20026;&#30340;&#23439;&#35266;&#31209;&#24207;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#28508;&#22312;&#26426;&#21046;&#12289;&#32467;&#26500;&#21644;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#25506;&#32034;&#26356;&#21152;&#30495;&#23454;&#31995;&#32479;&#21644;&#25552;&#21319;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#20016;&#23500;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#20811;&#26381;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16887v1 Announce Type: cross  Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of com
&lt;/p&gt;</description></item><item><title>VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13217</link><description>&lt;p&gt;
VideoPrism: &#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#22522;&#30784;&#35270;&#35273;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
VideoPrism: A Foundational Visual Encoder for Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13217
&lt;/p&gt;
&lt;p&gt;
VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VideoPrism&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#21333;&#20010;&#20923;&#32467;&#27169;&#22411;&#22788;&#29702;&#22810;&#26679;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;3600&#19975;&#39640;&#36136;&#37327;&#35270;&#39057;&#26631;&#39064;&#23545;&#21644;58.2&#20159;&#20010;&#24102;&#26377;&#22024;&#26434;&#24179;&#34892;&#25991;&#26412;&#65288;&#22914;ASR&#36716;&#24405;&#65289;&#30340;&#35270;&#39057;&#21098;&#36753;&#30340;&#24322;&#26500;&#35821;&#26009;&#24211;&#19978;&#23545;VideoPrism&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#19968;&#20010;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#25913;&#36827;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#65292;&#20351;VideoPrism&#33021;&#22815;&#20027;&#35201;&#19987;&#27880;&#20110;&#35270;&#39057;&#27169;&#24577;&#21516;&#26102;&#21033;&#29992;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#23453;&#36149;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#32452;&#19978;&#36827;&#34892;&#20102;&#23545;VideoPrism&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#20174;&#32593;&#32476;&#35270;&#39057;&#38382;&#31572;&#21040;&#31185;&#23398;CV&#65292; &#22312;33&#20010;&#35270;&#39057;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13217v1 Announce Type: cross  Abstract: We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10793</link><description>&lt;p&gt;
&#25513;&#30721;&#27880;&#24847;&#21147;&#26159;&#22270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Masked Attention is All You Need for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#21464;&#31181;&#20027;&#35201;&#29992;&#20110;&#22312;&#22270;&#19978;&#23398;&#20064;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#12289;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;GNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#31526;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#12290;&#22270;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#25110;&#36793;&#38598;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#26435;&#37325;&#30697;&#38453;&#26469;&#24378;&#21046;&#23427;&#20204;&#30340;&#36830;&#25509;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#22270;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#29992;&#20110;&#22270;&#30340;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;55&#22810;&#20010;&#33410;&#28857;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#28040;&#24687;&#20256;&#36882;&#22522;&#32447;&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10969</link><description>&lt;p&gt;
MacroSwarm: &#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32452;&#21512;&#26694;&#26550;&#29992;&#20110;&#32676;&#20307;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
MacroSwarm: A Field-based Compositional Framework for Swarm Programming. (arXiv:2401.10969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10969
&lt;/p&gt;
&lt;p&gt;
MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#34892;&#20026;&#24037;&#31243;&#26159;&#19968;&#39033;&#26088;&#22312;&#30740;&#31350;&#21327;&#35843;&#31616;&#21333;&#26234;&#33021;&#20307;&#22242;&#20307;&#20869;&#35745;&#31639;&#21644;&#34892;&#21160;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#22914;&#22270;&#26696;&#24418;&#25104;&#12289;&#38598;&#20307;&#31227;&#21160;&#12289;&#32858;&#31867;&#21644;&#20998;&#24067;&#24335;&#24863;&#30693;&#12290;&#23613;&#31649;&#22312;&#32676;&#20307;&#65288;&#26080;&#20154;&#26426;&#12289;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#65289;&#20998;&#26512;&#21644;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#36890;&#29992;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#23450;&#20041;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#20026;&#20102;&#23545;&#27492;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;MacroSwarm&#65292;&#20197;&#21487;&#37325;&#29992;&#19988;&#23436;&#20840;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;&#23884;&#20837;&#38598;&#20307;&#35745;&#31639;&#21644;&#21327;&#35843;&#12290;&#22522;&#20110;&#38598;&#25104;&#35745;&#31639;&#30340;&#23439;&#32534;&#31243;&#33539;&#24335;&#65292;MacroSwarm&#25552;&#20986;&#20102;&#23558;&#27599;&#20010;&#32676;&#20307;&#34892;&#20026;&#22359;&#34920;&#31034;&#20026;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#30340;&#32431;&#20989;&#25968;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g.
&lt;/p&gt;</description></item><item><title>RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17215</link><description>&lt;p&gt;
RSAM&#65306;&#20351;&#29992;Riemannian Sharpness-aware Minimization&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17215
&lt;/p&gt;
&lt;p&gt;
RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#26377;&#26395;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20043;&#21069;&#23558;&#20960;&#20309;&#21407;&#29702;&#24212;&#29992;&#20110;&#20248;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#23558;Sharpness-Aware Minimization (SAM)&#20248;&#21270;&#22120;&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#12290;&#20026;&#20102;&#25903;&#25345;&#27969;&#24418;&#19978;&#30340;&#8220;sharpness&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#27969;&#24418;&#19978;&#30340;sharpness&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#25551;&#36848;&#27969;&#24418;sharpness&#19982;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#32570;&#21475;&#19978;&#38480;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;Riemannian Sharpness-Aware Minimization (RSAM)&#12290;&#20026;&#20102;&#23637;&#31034;RSAM&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;&#21644;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11082</link><description>&lt;p&gt;
&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#19982;&#19977;&#20803;&#37096;&#20998;&#36793;&#38469;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32593;&#32476;&#35270;&#39057;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#35270;&#39057;&#36807;&#28388;&#12289;&#25512;&#33616;&#21644;&#25628;&#32034;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#27969;&#34892;&#12290;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26088;&#22312;&#23558;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#25490;&#22312;&#19981;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#20043;&#21069;&#12290;&#35813;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#20934;&#30830;&#34913;&#37327;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#20197;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#20851;&#27880;&#38590;&#36127;&#26679;&#26412;&#21644;&#27169;&#25311;&#19981;&#21516;&#23618;&#27425;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#19981;&#22815;&#65292;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20351;&#29992;&#20004;&#20010;&#26032;&#26041;&#27861;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21033;&#29992;&#33392;&#38590;&#30340;&#20363;&#23376;&#26469;&#25552;&#39640;&#40065;&#26834;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#27169;&#22359;(DMAE)&#65292;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#20013;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#19968;&#20010;&#36127;&#38754;&#26679;&#26412;&#31579;&#36873;&#26426;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#32423;&#21035;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#24310;&#36831;-&#24102;&#23485;&#31215;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#12290;&#20248;&#21270;&#21518;&#30340;&#32423;&#32852;&#29615;&#21487;&#29992;&#20110;&#23454;&#29616;&#23567;&#23610;&#23544;&#12289;&#24179;&#39030;&#24310;&#36831;&#35889;&#30340;&#20809;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2308.13818</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#30340;&#20998;&#32452;&#22836;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Packet Header Recognition Utilizing an All-Optical Reservoir Based on Reinforcement-Learning-Optimized Double-Ring Resonator. (arXiv:2308.13818v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#29615;&#35856;&#25391;&#22120;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#24310;&#36831;-&#24102;&#23485;&#31215;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#12290;&#20248;&#21270;&#21518;&#30340;&#32423;&#32852;&#29615;&#21487;&#29992;&#20110;&#23454;&#29616;&#23567;&#23610;&#23544;&#12289;&#24179;&#39030;&#24310;&#36831;&#35889;&#30340;&#20809;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20998;&#32452;&#22836;&#35782;&#21035;&#26159;&#20809;&#36890;&#20449;&#32593;&#32476;&#20013;&#37325;&#35201;&#30340;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#21452;&#29615;&#35856;&#25391;&#22120;&#20316;&#20026;&#33410;&#28857;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#20809;&#20998;&#32452;&#22836;&#12290;&#30001;&#20110;&#33410;&#28857;&#30340;&#24310;&#36831;-&#24102;&#23485;&#31215;&#65288;DBP&#65289;&#26159;&#20648;&#22791;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#21442;&#25968;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#21452;&#29615;&#35856;&#25391;&#22120;&#36827;&#34892;DBP&#30340;&#26368;&#22823;&#21270;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#21442;&#25968;&#31354;&#38388;&#20248;&#21270;&#21644;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#32423;&#32852;&#12289;&#24182;&#32852;&#21644;&#23884;&#20837;&#24335;&#37197;&#32622;&#30340;&#20248;&#21270;DBP&#36798;&#21040;&#20102;&#30456;&#21516;&#30340;&#26368;&#22823;&#20540;&#65292;&#34987;&#35748;&#20026;&#26159;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#20248;&#21270;&#30340;&#32423;&#32852;&#29615;&#32452;&#25104;&#30340;&#20840;&#20809;&#20648;&#22791;&#31995;&#32479;&#36827;&#34892;&#20102;3&#20301;&#21644;6&#20301;&#20998;&#32452;&#22836;&#35782;&#21035;&#20219;&#21153;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#22823;&#22823;&#20943;&#23567;&#30340;&#33455;&#29255;&#23610;&#23544;&#21644;&#25152;&#38656;&#30340;&#8220;&#24179;&#39030;&#8221;&#24310;&#36831;&#35889;&#12290;&#21033;&#29992;&#36825;&#31181;&#20809;&#35745;&#31639;&#26041;&#26696;&#65292;&#23383;&#38169;&#35823;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optical packet header recognition is an important signal processing task of optical communication networks. In this work, we propose an all-optical reservoir, consisting of integrated double-ring resonators (DRRs) as nodes, for fast and accurate optical packet header recognition. As the delay-bandwidth product (DBP) of the node is a key figure-of-merit in the reservoir, we adopt a deep reinforcement learning algorithm to maximize the DBPs for various types of DRRs, which has the advantage of full parameter space optimization and fast convergence speed. Intriguingly, the optimized DBPs of the DRRs in cascaded, parallel, and embedded configurations reach the same maximum value, which is believed to be the global maximum. Finally, 3-bit and 6-bit packet header recognition tasks are performed with the all-optical reservoir consisting of the optimized cascaded rings, which have greatly reduced chip size and the desired "flat-top" delay spectra. Using this optical computing scheme, word-erro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09832</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36793;&#32536;&#36164;&#28304;&#20219;&#21153;&#37096;&#32626;&#21644;&#25193;&#23637;&#26041;&#27861;&#29992;&#20110;&#36710;&#36733;&#32593;&#32476;&#26381;&#21153;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning. (arXiv:2305.09832v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#36710;&#32852;&#32593;&#8221;&#27491;&#22788;&#20110;&#25105;&#20204;&#31038;&#20250;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#29992;&#20110;&#25552;&#20379;&#36710;&#36742;&#36890;&#32852;&#32593;&#65288;C-V2N&#65289;&#26381;&#21153;&#65292;&#35299;&#20915;&#26381;&#21153;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32852;&#21512;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#20010;&#38382;&#39064;&#30340;&#32852;&#25509;&#26041;&#24335;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#36138;&#24515;&#31639;&#27861;&#30340;&#20851;&#20110;&#20219;&#21153;&#37096;&#32626;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110; Deep Deterministic Policy Gradient (DDPG) &#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25193;&#23637;&#20195;&#29702;&#19982;&#22810;&#20010;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#25193;&#23637;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches
&lt;/p&gt;</description></item></channel></rss>