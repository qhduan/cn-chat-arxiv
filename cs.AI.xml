<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.06665</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26234;&#33021;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#23454;&#29616;&#38271;&#26399;&#30446;&#26631;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#20852;&#25512;&#21160;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#24178;&#32467;&#26500;&#34987;&#26222;&#36941;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#38459;&#30861;&#20102;&#20174;&#19994;&#32773;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#20449;&#20219;&#21644;&#20351;&#29992;&#35757;&#32451;&#20195;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#37327;&#30340;&#25991;&#29486;&#33268;&#21147;&#20110;&#25581;&#31034;&#26234;&#33021;&#20195;&#29702;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#26126;&#30830;&#22320;&#20998;&#20026;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods
&lt;/p&gt;</description></item></channel></rss>