<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20132;&#21449;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#38454;&#27573;&#32039;&#24613;&#24615;&#27010;&#24565;&#21644;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20449;&#21495;&#36716;&#25442;&#26399;&#38388;&#36873;&#25321;&#28608;&#27963;&#30340;&#20449;&#21495;&#30456;&#20301;&#12290;</title><link>https://arxiv.org/abs/2403.17328</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#20256;&#32534;&#31243;&#23398;&#20064;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Traffic Signal Control via Genetic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#20132;&#21449;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#38454;&#27573;&#32039;&#24613;&#24615;&#27010;&#24565;&#21644;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20449;&#21495;&#36716;&#25442;&#26399;&#38388;&#36873;&#25321;&#28608;&#27963;&#30340;&#20449;&#21495;&#30456;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23545;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#65292;&#22312;&#23547;&#27714;&#26356;&#26377;&#25928;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DRL&#20013;&#22870;&#21169;&#30340;&#35774;&#35745;&#39640;&#24230;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#25165;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#31574;&#30053;&#65292;&#32780;&#26368;&#32456;&#31574;&#30053;&#20063;&#23384;&#22312;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#22797;&#26434;&#36335;&#21475;&#30340;&#20449;&#21495;&#25511;&#21046;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20449;&#21495;&#30456;&#35774;&#35745;&#20102;&#19968;&#20010;&#38454;&#27573;&#32039;&#24613;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#20449;&#21495;&#21464;&#25442;&#26399;&#38388;&#65292;&#20132;&#36890;&#28783;&#25511;&#21046;&#31574;&#30053;&#26681;&#25454;&#38454;&#27573;&#32039;&#24613;&#24615;&#36873;&#25321;&#35201;&#28608;&#27963;&#30340;&#19979;&#19968;&#20010;&#30456;&#20301;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32039;&#24613;&#21151;&#33021;&#34920;&#31034;&#20026;&#21487;&#35299;&#37322;&#30340;&#26641;&#32467;&#26500;&#12290;&#32039;&#24613;&#21151;&#33021;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#36947;&#36335;&#26465;&#20214;&#20026;&#29305;&#23450;&#30456;&#20301;&#35745;&#31639;&#30456;&#20301;&#32039;&#24613;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17328v1 Announce Type: new  Abstract: The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic 
&lt;/p&gt;</description></item><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.07637</link><description>&lt;p&gt;
OpsEval: &#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07637
&lt;/p&gt;
&lt;p&gt;
OpsEval&#26159;&#19968;&#20010;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#31561;&#20851;&#38190;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#27700;&#24179;&#65292;&#20026;&#25552;&#20379;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#32763;&#35793;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#31561;NLP&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;AIOps&#65288;&#38754;&#21521;IT&#36816;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#20013;&#65292;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#20449;&#24687;&#27719;&#24635;&#12289;&#25253;&#21578;&#20998;&#26512;&#21644;API&#35843;&#29992;&#33021;&#21147;&#32780;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#22312;AIOps&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#23578;&#26410;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#24341;&#23548;&#38024;&#23545;AIOps&#23450;&#21046;&#30340;LLMs&#30340;&#20248;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#35780;&#20272;&#32593;&#32476;&#37197;&#32622;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OpsEval&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;LLMs&#35774;&#35745;&#30340;&#20840;&#38754;&#20219;&#21153;&#23548;&#21521;&#30340;AIOps&#22522;&#20934;&#27979;&#35797;&#12290;OpsEval&#39318;&#27425;&#23545;LLMs&#22312;&#19977;&#20010;&#20851;&#38190;&#22330;&#26223;&#65288;&#26377;&#32447;&#32593;&#32476;&#25805;&#20316;&#12289;5G&#36890;&#20449;&#25805;&#20316;&#21644;&#25968;&#25454;&#24211;&#25805;&#20316;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#33021;&#21147;&#27700;&#24179;&#65288;&#30693;&#35782;&#22238;&#24518;&#12289;&#20998;&#26512;&#24605;&#32771;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item></channel></rss>