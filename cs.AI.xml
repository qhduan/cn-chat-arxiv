<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05842</link><description>&lt;p&gt;
Hufu&#65306;&#19968;&#31181;&#36890;&#36807;&#32622;&#25442;&#31561;&#21464;&#24615;&#23545;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05842
&lt;/p&gt;
&lt;p&gt;
Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26381;&#21153;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#20445;&#25252;&#23453;&#36149;&#30340;&#27169;&#22411;&#21442;&#25968;&#20813;&#21463;&#30423;&#31363;&#24050;&#25104;&#20026;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#27700;&#21360;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#23450;&#21046;&#65292;&#38590;&#20197;&#20316;&#20026;&#38598;&#25104;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hufu&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#20381;&#36182;&#20110;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#12290;Hufu&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#32622;&#25442;&#30340;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#19978;&#23884;&#20837;&#27700;&#21360;&#65292;&#23884;&#20837;&#30340;&#27169;&#22411;&#22522;&#26412;&#19978;&#21253;&#21547;&#20004;&#32452;&#26435;&#37325; -- &#19968;&#32452;&#29992;&#20110;&#27491;&#24120;&#20351;&#29992;&#65292;&#21478;&#19968;&#32452;&#29992;&#20110;&#27700;&#21360;&#25552;&#21462;&#65292;&#35302;&#21457;&#26465;&#20214;&#26159;&#32463;&#36807;&#32622;&#25442;&#30340;&#36755;&#20837;&#12290;&#32622;&#25442;&#31561;&#21464;&#24615;&#30830;&#20445;&#36825;&#20004;&#32452;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#26368;&#23567;&#24178;&#25200;&#65292;&#20174;&#32780;&#22312;&#27700;&#21360;&#25552;&#21462;&#26102;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05842v1 Announce Type: cross  Abstract: With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers. Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downst
&lt;/p&gt;</description></item><item><title>&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#65292;&#26377;&#26102;&#20505;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#20250;&#20986;&#29616;&#20449;&#24565;&#20462;&#35746;&#30340;&#22810;&#20313;&#24773;&#20917;&#65292;&#20197;&#21450;&#32473;&#20986;&#20102;&#23548;&#33268;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.15445</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#24536;&#35760;&#25105;&#20204;&#26159;&#22914;&#20309;&#23398;&#20064;&#30340;&#65311;&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#30340;&#20449;&#24565;&#22810;&#20313;
&lt;/p&gt;
&lt;p&gt;
Can we forget how we learned? Doxastic redundancy in iterated belief revision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15445
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#20013;&#65292;&#26377;&#26102;&#20505;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#20250;&#20986;&#29616;&#20449;&#24565;&#20462;&#35746;&#30340;&#22810;&#20313;&#24773;&#20917;&#65292;&#20197;&#21450;&#32473;&#20986;&#20102;&#23548;&#33268;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#30340;&#33719;&#21462;&#26041;&#24335;&#21487;&#33021;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#12290;&#26126;&#26174;&#30340;&#24773;&#20917;&#26159;&#24403;&#26576;&#20107;&#34987;&#22810;&#27425;&#30830;&#35748;&#26102;&#12290;&#22312;&#36845;&#20195;&#20449;&#24565;&#20462;&#35746;&#26041;&#38754;&#65292;&#29305;&#23450;&#30340;&#20462;&#35746;&#22312;&#20854;&#20182;&#20462;&#35746;&#23384;&#22312;&#26102;&#21487;&#33021;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#12290;&#31616;&#21333;&#30340;&#37325;&#22797;&#26159;&#19968;&#20010;&#20363;&#23376;&#65292;&#20294;&#24182;&#38750;&#21807;&#19968;&#30340;&#24773;&#20917;&#12290;&#26377;&#26102;&#65292;&#21363;&#20351;&#27809;&#26377;&#30456;&#31561;&#30340;&#20462;&#35746;&#23384;&#22312;&#65292;&#29978;&#33267;&#27809;&#26377;&#26263;&#31034;&#23427;&#30340;&#20854;&#20182;&#20462;&#35746;&#65292;&#19968;&#20010;&#20462;&#35746;&#20063;&#20250;&#21464;&#24471;&#22810;&#20313;&#12290;&#32473;&#20986;&#20102;&#35789;&#20856;&#20462;&#35746;&#24207;&#21015;&#20013;&#31532;&#19968;&#20010;&#20462;&#35746;&#22810;&#20313;&#30340;&#19968;&#20010;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#21363;&#20351;&#21482;&#26377;&#20004;&#20010;&#21629;&#39064;&#20462;&#35746;&#65292;&#35813;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#20063;&#26159;coNP-&#23436;&#20840;&#30340;&#12290;&#22312;Horn&#24773;&#20917;&#19979;&#22797;&#26434;&#24615;&#30456;&#21516;&#65292;&#20294;&#21482;&#26377;&#19981;&#21463;&#38480;&#21046;&#30340;&#20462;&#35746;&#25968;&#37327;&#65306;&#22312;&#20004;&#20010;&#20462;&#35746;&#24773;&#20917;&#19979;&#23427;&#21464;&#20026;&#22810;&#39033;&#24335;&#12290;&#35789;&#20856;&#20462;&#35746;&#19981;&#20165;&#20165;&#22240;&#20026;&#23427;&#20204;&#26412;&#36523;&#26159;&#30456;&#20851;&#30340;&#65292;&#20063;&#22240;&#20026;&#23427;&#20204;&#30340;&#24207;&#21015;&#26159;&#29992;&#20110;&#34920;&#31034;&#36845;&#20195;&#20462;&#35746;&#36807;&#31243;&#29366;&#24577;&#30340;&#24120;&#35265;&#26426;&#21046;&#20013;&#26368;&#32039;&#20945;&#30340;&#12290;&#32553;&#30701;&#35789;&#20856;&#20462;&#35746;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15445v1 Announce Type: new  Abstract: How information was acquired may become irrelevant. An obvious case is when something is confirmed many times. In terms of iterated belief revision, a specific revision may become irrelevant in presence of others. Simple repetitions are an example, but not the only case when this happens. Sometimes, a revision becomes redundant even in presence of none equal, or even no else implying it. A necessary and sufficient condition for the redundancy of the first of a sequence of lexicographic revisions is given. The problem is coNP-complete even with two propositional revisions only. Complexity is the same in the Horn case but only with an unbounded number of revisions: it becomes polynomial with two revisions. Lexicographic revisions are not only relevant by themselves, but also because sequences of them are the most compact of the common mechanisms used to represent the state of an iterated revision process. Shortening sequences of lexicograp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00389</link><description>&lt;p&gt;
&#20851;&#20110;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;$O(\frac{\sqrt{d}}{T^{1/4}})$&#25910;&#25947;&#36895;&#24230;&#21644;&#23545;&#32500;&#24230;&#30340;&#25913;&#36827;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20854;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;$\ell_1$&#33539;&#25968;&#24314;&#31435;&#20102;&#25910;&#25947;&#29575;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#26080;&#38656;&#20551;&#35774;&#26799;&#24230;&#26377;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#20248;&#21270;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;$T$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#30001;&#20110;&#23545;&#20110;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#65292;$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#31867;&#27604;&#20026;SGD&#30340;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$&#65292;&#27979;&#24230;&#20026;$\ell_1$&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#65292;&#24182;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#24182;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21644;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2401.08658</link><description>&lt;p&gt;
&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#65306;2022-2023&#24180;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
End-To-End Planning of Autonomous Driving in Industry and Academia: 2022-2023. (arXiv:2401.08658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#65292;&#24182;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#24182;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21644;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#30446;&#21069;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#25253;&#21578;&#30340;&#21253;&#25324;&#35814;&#32454;&#25216;&#26415;&#22312;&#20869;&#30340;&#26041;&#27861;&#36827;&#34892;&#24555;&#36895;&#22238;&#39038;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#21253;&#25324;&#29305;&#26031;&#25289;FSD V12&#12289;Momenta 2023&#12289;Horizon Robotics 2023&#12289;Motional RoboTaxi 2022&#12289;Woven Planet&#65288;&#20016;&#30000;&#65289;&#65306;&#22478;&#24066;&#39550;&#39542;&#21592;&#21644;Nvidia&#22312;&#20869;&#30340;&#31471;&#21040;&#31471;&#35268;&#21010;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#35843;&#26597;&#33258;&#21160;&#39550;&#39542;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#23398;&#26415;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;2022-2023&#24180;&#31471;&#21040;&#31471;&#35268;&#21010;&#30340;&#26368;&#26032;&#32467;&#26500;&#21644;&#24555;&#36895;&#23398;&#20064;&#65292;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#20102;&#20837;&#38376;&#26448;&#26009;&#65292;&#20379;&#20854;&#20102;&#35299;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#20013;&#30340;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21516;&#26102;&#20063;&#20026;&#39640;&#32423;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#34917;&#20805;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to provide a quick review of the methods including the technologies in detail that are currently reported in industry and academia. Specifically, this paper reviews the end-to-end planning, including Tesla FSD V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet (Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art academic studies that investigate end-to-end planning of autonomous driving. This paper provides readers with a concise structure and fast learning of state-of-the-art end-to-end planning for 2022-2023. This article provides a meaningful overview as introductory material for beginners to follow the state-of-the-art end-to-end planning of autonomous driving in industry and academia, as well as supplementary material for advanced researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#12289;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#21644;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#36974;&#25377;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10360</link><description>&lt;p&gt;
OccluTrack: &#37325;&#26032;&#24605;&#32771;&#22686;&#24378;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#23545;&#36974;&#25377;&#24863;&#30693;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking. (arXiv:2309.10360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#12289;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#21644;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#20013;&#36974;&#25377;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22312;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12289;&#22806;&#35266;&#29305;&#24449;&#25552;&#21462;&#21644;&#20851;&#32852;&#32780;&#21463;&#21040;&#24433;&#21709;&#65292;&#23548;&#33268;&#36523;&#20221;F1&#24471;&#20998;&#65288;IDF1&#65289;&#19981;&#22815;&#20934;&#30830;&#65292;ID&#20999;&#25442;&#36807;&#22810;&#65288;IDSw&#65289;&#65292;&#20197;&#21450;&#20851;&#32852;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#65288;AssA&#21644;AssR&#65289;&#19981;&#36275;&#12290;&#25105;&#20204;&#21457;&#29616;&#20027;&#35201;&#21407;&#22240;&#26159;&#37096;&#20998;&#36974;&#25377;&#24341;&#36215;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#35748;&#20026;&#26126;&#30830;&#30340;&#36816;&#21160;&#20272;&#35745;&#12289;&#21487;&#38752;&#30340;&#22806;&#35266;&#29305;&#24449;&#21644;&#20844;&#24179;&#30340;&#20851;&#32852;&#26159;&#35299;&#20915;&#36974;&#25377;&#22330;&#26223;&#19979;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36974;&#25377;&#24863;&#30693;&#22810;&#30446;&#26631;&#34892;&#20154;&#36319;&#36394;&#22120;OccluTrack&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24322;&#24120;&#36816;&#21160;&#25233;&#21046;&#26426;&#21046;&#24341;&#20837;&#21040;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#26816;&#27979;&#21644;&#25233;&#21046;&#37096;&#20998;&#36974;&#25377;&#24341;&#36215;&#30340;&#24322;&#24120;&#36816;&#21160;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23039;&#21183;&#23548;&#21521;&#30340;&#20877;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#30340;&#21028;&#21035;&#24615;&#37096;&#20998;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#30456;&#20851;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#36974;&#25377;&#22330;&#26223;&#19979;&#30340;&#20851;&#32852;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple pedestrian tracking faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods suffer from inaccurate motion estimation, appearance feature extraction, and association due to occlusion, leading to inadequate Identification F1-Score (IDF1), excessive ID switches (IDSw), and insufficient association accuracy and recall (AssA and AssR). We found that the main reason is abnormal detections caused by partial occlusion. In this paper, we suggest that the key insight is explicit motion estimation, reliable appearance features, and fair association in occlusion scenes. Specifically, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack. We first introduce an abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we propose a pose-guided re-ID module to extract discriminative part features for partially occluded pedestrians. Last, we desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;</title><link>http://arxiv.org/abs/2305.09200</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#24536;&#35760;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#24335;&#65311;&#27604;&#36739;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#29366;&#24577;&#34920;&#31034;(arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Can we forget how we learned? Representing states in iterated belief revision}. (arXiv:2305.09200v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#30340;&#19977;&#31181;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#26159;&#26368;&#26377;&#25928;&#29575;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36845;&#20195;&#20449;&#24565;&#20462;&#27491;&#20013;&#19977;&#31181;&#26368;&#24120;&#35265;&#30340;&#29366;&#24577;&#34920;&#31034;&#26041;&#27861;&#65306;&#26174;&#24335;&#34920;&#31034;&#65292;&#25353;&#23618;&#27425;&#34920;&#31034;&#21644;&#25353;&#21382;&#21490;&#34920;&#31034;&#12290;&#21069;&#32773;&#26159;&#27169;&#22411;&#20043;&#38388;&#30340;&#36830;&#36890;&#20559;&#24207;&#20851;&#31995;&#65292;&#31532;&#20108;&#31181;&#26159;&#34920;&#31034;&#31561;&#20215;&#31867;&#30340;&#20844;&#24335;&#21015;&#34920;&#65292;&#31532;&#19977;&#31181;&#26159;&#20808;&#21069;&#20462;&#35746;&#30340;&#24207;&#21015;&#12290;&#21518;&#32773;&#21462;&#20915;&#20110;&#20462;&#35746;&#35821;&#20041;&#21644;&#21382;&#21490;&#37325;&#20889;&#65292;&#32780;&#21069;&#32773;&#21017;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#37325;&#20889;&#12290;&#25152;&#26377;&#26426;&#21046;&#37117;&#34920;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#29366;&#24577;&#12290;&#29992;&#23383;&#20856;&#24207;&#20462;&#35746;&#30340;&#37325;&#20889;&#21382;&#21490;&#22312;&#22823;&#23567;&#26041;&#38754;&#27604;&#20854;&#20182;&#32771;&#34385;&#30340;&#34920;&#31034;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;&#35777;&#26126;&#20102;&#36825;&#26679;&#19968;&#20010;&#21382;&#21490;&#30340;&#20887;&#20313;&#26159;&#19968;&#31181;&#36731;&#24494;&#30340;&#37325;&#20889;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;coNP&#23436;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;Horn&#20844;&#24335;&#30340;&#20004;&#27425;&#20462;&#35746;&#21382;&#21490;&#25110;&#20219;&#24847;&#38271;&#24230;&#30340;&#20462;&#35746;&#21382;&#21490;&#19978;&#65292;&#36825;&#20063;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22312;&#20004;&#20010;Horn&#20844;&#24335;&#30340;&#21382;&#21490;&#19978;&#65292;&#23427;&#26159;&#22810;&#39033;&#24335;&#30340;&#12290;&#19968;&#20010;&#27425;&#35201;&#30340;&#25216;&#26415;&#32467;&#26524;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#20010;Horn&#20844;&#24335;&#26159;&#21542;&#31561;&#20215;&#20110;neg&#12290;
&lt;/p&gt;
&lt;p&gt;
The three most common representations of states in iterated belief revision are compared: explicit, by levels and by history. The first is a connected preorder between models, the second is a list of formulae representing equivalence classes, the third is the sequence of the previous revisions. The latter depends on the revision semantics and on history rewriting, and the latter depends on the allowed rewritings. All mechanisms represent all possible states. A rewritten history of lexicographic revision is more efficient than the other considered representations in terms of size with arbitrary history rewritings. Establishing the redundancy of such a history is a mild rewriting. It is coNP-complete in the general case, and is hard even on histories of two revisions or revisions of arbitrary length of Horn formulae, and is polynomial on histories of two Horn formulae. A minor technical result is a polynomial-time algorithm for establishing whether a Horn formula is equivalent to the neg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.09271</link><description>&lt;p&gt;
&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles. (arXiv:2303.09271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20851;&#38190;&#31995;&#32479;&#30340;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#33021;&#22815;&#35299;&#37322;&#20026;&#20309;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#39044;&#27979;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#20379;&#30340;&#35299;&#37322;&#24517;&#39035;&#26159;&#21487;&#35777;&#26126;&#30340;&#65292;&#24182;&#19988;&#26368;&#22909;&#19981;&#21253;&#21547;&#20887;&#20313;&#20449;&#24687;&#65292;&#21363;&#26368;&#23567;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#26159;&#26368;&#23567;&#30340;&#65292;&#32780;&#19988;&#22312;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#26368;&#23567;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31070;&#35861;&#8221;&#31995;&#32479;&#65292;&#21487;&#20197;&#30830;&#23450;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#65292;&#22312;&#35745;&#31639;&#26368;&#23567;&#35299;&#37322;&#26102;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#26469;&#33258;&#30456;&#20851;&#24037;&#20316;&#30340;&#21483;&#20570;MARCO&#30340;&#31639;&#27861;&#65288;&#23558;&#20854;&#31216;&#20026;m-MARCO&#65289;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.  To this end, we first present a highly efficient oracle that can determine the correctness of explanations, surpassing the runtime performance of current state-of-the-art alternatives by several orders of magnitude when computing minimal explanations.  Secondly, we adapt an algorithm called MARCO from related works (calling it m-MARCO) for the purpose of computing a single minimum explanation per prediction, and demonstrate an overall speedup factor of two compared to the MARCO algorithm which enumerates all minimal explanations.  Final
&lt;/p&gt;</description></item></channel></rss>