<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.14648</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#37325;&#26032;&#24605;&#32771;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14648
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#26159;&#25269;&#25239;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21364;&#23545;&#25239;&#24615;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#30830;&#23450;&#20102;&#22952;&#30861;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21464;&#24615;&#25439;&#22833;&#21644;&#20998;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#65292;&#34920;&#26126;&#23384;&#22312;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30001;&#20110;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#21457;&#25955;&#32780;&#20986;&#29616;&#30340;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AR-AT&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20572;&#27490;&#26799;&#24230;&#25805;&#20316;&#21644;&#19968;&#20010;&#39044;&#27979;&#22120;&#26469;&#36991;&#20813;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#28789;&#24863;&#26469;&#33258;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09513</link><description>&lt;p&gt;
NetGPT&#65306;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#20026;&#36755;&#20837;&#27969;&#37327;&#29983;&#25104;&#21487;&#21306;&#20998;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#27969;&#37327;&#20998;&#31867;&#12289;&#25915;&#20987;&#26816;&#27979;&#12289;&#36164;&#28304;&#35843;&#24230;&#12289;&#21327;&#35758;&#20998;&#26512;&#21644;&#27969;&#37327;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NetGPT&#65292;&#26088;&#22312;&#20026;&#32593;&#32476;&#27969;&#37327;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#35299;&#20915;&#22810;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
&lt;/p&gt;</description></item></channel></rss>