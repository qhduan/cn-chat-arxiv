<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.02514</link><description>&lt;p&gt;
&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#26426;&#22120;&#20154;&#35774;&#23450;&#30446;&#30340;&#65306;&#19968;&#20010;&#35745;&#31639;&#20998;&#31867;&#12289;&#23450;&#20041;&#21644;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#20027;&#24320;&#25918;&#24335;&#23398;&#20064;(OEL)&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#30452;&#25509;&#20132;&#20114;&#32047;&#31215;&#33719;&#21462;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20363;&#22914;&#20381;&#38752;&#20869;&#22312;&#21160;&#26426;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;OEL&#26426;&#22120;&#20154;&#23545;&#24212;&#29992;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#20027;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#23545;&#20154;&#31867;&#29992;&#25143;&#26377;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;OEL&#26426;&#22120;&#20154;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#36825;&#21487;&#33021;&#23548;&#33268;&#33719;&#21462;&#30340;&#30693;&#35782;&#23545;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#24182;&#19981;&#37027;&#20040;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22260;&#32469;&#8220;&#30446;&#30340;&#8221;&#36825;&#19968;&#26032;&#27010;&#24565;&#23637;&#24320;&#12290;&#30446;&#30340;&#34920;&#31034;&#35774;&#35745;&#32773;&#21644;/&#25110;&#29992;&#25143;&#24076;&#26395;&#26426;&#22120;&#20154;&#20174;&#20013;&#33719;&#24471;&#20160;&#20040;&#12290;&#26426;&#22120;&#20154;&#24212;&#20351;&#29992;&#30446;&#30340;&#30340;&#20869;&#37096;&#34920;&#24449;&#65292;&#36825;&#37324;&#31216;&#20026;&#8220;&#24895;&#26395;&#8221;&#65292;&#26469;&#23558;&#20854;&#24320;&#25918;&#24335;&#25506;&#32034;&#38598;&#20013;&#20110;&#33719;&#21462;&#19982;&#20854;&#23436;&#25104;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#21457;&#23637;&#19968;&#20010;&#20849;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.03297</link><description>&lt;p&gt;
SoftMAC&#65306;&#22522;&#20110;&#39044;&#27979;&#25509;&#35302;&#27169;&#22411;&#21644;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#21452;&#21521;&#32806;&#21512;&#30340;&#21487;&#24494;&#36719;&#20307;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03297
&lt;/p&gt;
&lt;p&gt;
SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#29289;&#29702;&#20223;&#30495;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#30456;&#20851;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;&#20026;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#32437;&#22330;&#26223;&#20013;&#24212;&#29992;&#21487;&#24494;&#20223;&#30495;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#21508;&#31181;&#26448;&#26009;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMAC&#65292;&#19968;&#20010;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#36719;&#20307;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;SoftMAC&#20351;&#29992;&#22522;&#20110;&#36830;&#32493;&#21147;&#23398;&#30340;&#26448;&#26009;&#28857;&#27861;&#26469;&#27169;&#25311;&#36719;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#27979;&#30340;MPM&#25509;&#35302;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31359;&#36879;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#20854;&#20182;&#24322;&#24120;&#29616;&#35937;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#21453;&#24377;&#12290;&#20026;&#20102;&#23558;MPM&#31890;&#23376;&#19982;&#21487;&#21464;&#24418;&#21644;&#38750;&#20307;&#31215;&#34915;&#29289;&#32593;&#26684;&#32806;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#37325;&#24314;&#23616;&#37096;&#21306;&#22495;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03297v2 Announce Type: replace-cross  Abstract: Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from prev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02712</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#27839;&#65306;&#22609;&#36896;&#22810;&#20010;&#39046;&#22495;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the frontiers of deep learning: innovations shaping diverse domains. (arXiv:2309.02712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20351;&#24471;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#12289;&#20248;&#21270;&#12289;&#25913;&#36827;&#21644;&#39044;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;DL&#24050;&#32463;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#22788;&#29702;&#12289;&#20892;&#19994;&#12289;&#20132;&#36890;&#39044;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#29983;&#29289;&#21307;&#23398;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29983;&#24577;&#23398;&#12290;&#20026;&#20102;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#28508;&#22312;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24191;&#27867;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#27491;&#22914;&#25991;&#29486;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;DL&#22312;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#24182;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14591</link><description>&lt;p&gt;
&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#19982;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#30446;&#30340;&#26159;&#25345;&#32493;&#36319;&#36394;&#21644;&#35782;&#21035;&#35270;&#39057;&#20013;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#37117;&#26159;&#36890;&#36807;&#24314;&#27169;&#36816;&#21160;&#20449;&#24687;&#24182;&#23558;&#20854;&#19982;&#22806;&#35266;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26469;&#30830;&#23450;&#21644;&#36319;&#36394;&#29289;&#20307;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#36319;&#36394;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#22806;&#35266;&#20449;&#24687;&#21464;&#21270;&#30340;&#24207;&#21015;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#21644;&#24674;&#22797;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#36807;&#31243;&#20013;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#36319;&#36394;&#22120;&#22312;&#22788;&#29702;&#30001;&#36974;&#25377;&#21644;&#24555;&#36895;&#36816;&#21160;&#24341;&#36215;&#30340;&#36319;&#36394;&#38169;&#35823;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of multi-object tracking (MOT) is to continuously track and identify objects detected in videos. Currently, most methods for multi-object tracking model the motion information and combine it with appearance information to determine and track objects. In this paper, unfalsified control is employed to address the ID-switch problem in multi-object tracking. We establish sequences of appearance information variations for the trajectories during the tracking process and design a detection and rectification module specifically for ID-switch detection and recovery. We also propose a simple and effective strategy to address the issue of ambiguous matching of appearance information during the data association process. Experimental results on publicly available MOT datasets demonstrate that the tracker exhibits excellent effectiveness and robustness in handling tracking errors caused by occlusions and rapid movements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item></channel></rss>