<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#65292;&#23454;&#29616;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.03373</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#30340;&#25506;&#32034;&#65306;&#22810;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#65292;&#23454;&#29616;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#25552;&#39640;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20027;&#27969;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#37096;&#20998;&#65292;&#21363;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#34701;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#24120;&#24120;&#20351;&#29992;&#23450;&#21046;&#21644;&#26356;&#37325;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36319;&#36394;&#22120;&#23558;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#38598;&#25104;&#20998;&#24320;&#65292;&#23548;&#33268;&#25552;&#21462;&#30340;&#29305;&#24449;&#32570;&#20047;&#35821;&#20041;&#24341;&#23548;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#30446;&#26631;&#24863;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#30456;&#20284;&#30340;&#24178;&#25200;&#29289;&#21644;&#26497;&#31471;&#20809;&#29031;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#36817;&#26399;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#32479;&#19968;&#26550;&#26500;&#25506;&#32034;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#20307;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;&#32479;&#19968;&#30340;Transformer&#20027;&#24178;&#32593;&#32476;&#26469;&#23398;&#20064;&#32852;&#21512;&#29305;&#24449;&#25552;&#21462;&#21644;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28151;&#21512;&#21407;&#22987;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#21495;&#26469;&#29983;&#25104;&#27880;&#20837;&#35821;&#35328;&#30340;&#35270;&#35273;&#21333;&#20803;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current mainstream vision-language (VL) tracking framework consists of three parts, \ie a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, \eg similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate
&lt;/p&gt;</description></item></channel></rss>