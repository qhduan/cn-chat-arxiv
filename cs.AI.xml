<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#37325;&#28857;&#20943;&#23569;&#20551;&#38452;&#24615;&#21644;&#20351;&#29992; SHAP &#36827;&#34892;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19990;&#30028;&#19978;&#22842;&#36208;&#26368;&#22810;&#22899;&#24615;&#29983;&#21629;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#20854;&#20013;&#20083;&#33146;&#30284;&#21344;&#25454;&#20102;&#30284;&#30151;&#30149;&#20363;&#21644;&#27515;&#20129;&#20154;&#25968;&#26368;&#39640;&#30340;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#21487;&#20197;&#39044;&#38450;&#20083;&#33146;&#30284;&#65292;&#20174;&#32780;&#36827;&#34892;&#26089;&#26399;&#27835;&#30103;&#12290;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22312;&#30284;&#30151;&#39044;&#27979;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#20294;&#26377;&#26102;&#20165;&#20381;&#38752;&#20934;&#30830;&#24615;&#21487;&#33021;&#24182;&#38750;&#22987;&#32456;&#21487;&#38752;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#25552;&#21319;&#25216;&#26415;&#22522;&#20110;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20083;&#33146;&#30284;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#24615;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#26816;&#27979;&#21307;&#23398;&#30142;&#30149;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#21033;&#29992;&#21152;&#24030;&#22823;&#23398;&#23572;&#28286;&#20998;&#26657; (UCI) &#25968;&#25454;&#38598;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#33258;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09548v1 Announce Type: new  Abstract: Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.17010</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#22238;&#24518;&#21442;&#32771;&#20301;&#32622;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Recall Reference Location Like Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#25104;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#26377;&#26102;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#31572;&#26696;&#65292;&#36824;&#38656;&#35201;&#30456;&#24212;&#30340;&#21442;&#32771;&#27573;&#33853;&#20379;&#36741;&#21161;&#38405;&#35835;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#39069;&#22806;&#30340;&#26816;&#32034;&#27169;&#22411;&#33719;&#21462;&#39044;&#20998;&#27573;&#30340;&#25991;&#31456;&#22359;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#29420;&#31435;&#20110;&#20219;&#20309;&#36215;&#22987;&#20301;&#32622;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#26131;&#34987;&#36951;&#24536;&#21442;&#32771;&#30340;&#24773;&#26223;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;LLM&#34987;&#25552;&#31034;&#22238;&#24518;&#25991;&#26723;&#26631;&#39064;&#26631;&#35782;&#31526;&#20197;&#33719;&#21462;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33719;&#24471;&#30340;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#65292;&#23427;&#22238;&#24518;&#32454;&#31890;&#24230;&#27573;&#33853;&#12290;&#22312;&#20004;&#38454;&#27573;&#22238;&#24518;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#26469;&#30830;&#20445;&#19981;&#29983;&#25104;&#23384;&#20648;&#25991;&#26723;&#20043;&#22806;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#36895;&#24230;&#65292;&#25105;&#20204;&#21482;&#22238;&#24518;&#30701;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.01259</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25512;&#29702;&#23454;&#29616;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#19987;&#38376;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20869;&#22312;&#20887;&#20313;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#20849;&#20139;&#35768;&#22810;&#28388;&#27874;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#26089;&#30340;&#23618;&#27425;&#19978;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#31867;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#21019;&#24314;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;SINF&#65288;i&#65289;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#23545;&#35937;&#23646;&#20110;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#24182;&#65288;ii&#65289;&#25191;&#34892;&#19982;&#35813;&#35821;&#20041;&#32858;&#31867;&#30456;&#20851;&#30340;&#22522;&#26412;DNN&#25552;&#21462;&#30340;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#27599;&#20010;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21306;&#20998;&#33021;&#21147;&#24471;&#20998;&#65288;DCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#21327;&#20316;&#21644;&#36890;&#20449;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#39034;&#24207;&#25191;&#34892;&#65292;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#37096;&#32626;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.10164</link><description>&lt;p&gt;
&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Perception-Action-Communication with Graph Neural Networks. (arXiv:2309.10164v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#27493;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#21327;&#20316;&#21644;&#36890;&#20449;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#39034;&#24207;&#25191;&#34892;&#65292;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#37096;&#32626;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#20013;&#23454;&#29616;&#20849;&#21516;&#30340;&#20840;&#23616;&#30446;&#26631;&#30340;&#21327;&#20316;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#12290;&#26426;&#22120;&#20154;&#24517;&#39035;&#25191;&#34892;&#24863;&#30693;-&#21160;&#20316;-&#36890;&#20449;&#65288;PAC&#65289;&#24490;&#29615;-&#23427;&#20204;&#24863;&#30693;&#23616;&#37096;&#29615;&#22659;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#20154;&#36890;&#20449;&#65292;&#24182;&#23454;&#26102;&#37319;&#21462;&#34892;&#21160;&#12290;&#20998;&#25955;&#30340;PAC&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20915;&#23450;&#19982;&#30456;&#37051;&#26426;&#22120;&#20154;&#36890;&#20449;&#30340;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#22312;&#21033;&#29992;&#37051;&#23621;&#20849;&#20139;&#30340;&#20449;&#24687;&#30340;&#21516;&#26102;&#37319;&#21462;&#34892;&#21160;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#27604;&#22914;&#22312;&#32676;&#38598;&#21644;&#35206;&#30422;&#25511;&#21046;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#22312;&#27010;&#24565;&#19978;&#65292;GNN&#31574;&#30053;&#26159;&#23436;&#20840;&#20998;&#25955;&#30340;&#65292;&#20294;&#35780;&#20272;&#21644;&#37096;&#32626;&#36825;&#26679;&#30340;&#31574;&#30053;&#20027;&#35201;&#20173;&#28982;&#26159;&#38598;&#20013;&#24335;&#30340;&#25110;&#20855;&#26377;&#38480;&#21046;&#24615;&#30340;&#20998;&#25955;&#24335;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26694;&#26550;&#20551;&#35774;&#24863;&#30693;&#21644;&#21160;&#20316;&#25512;&#29702;&#30340;&#39034;&#24207;&#25191;&#34892;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#38480;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) loop -- they perceive their local environment, communicate with other robots, and take actions in real time. A fundamental challenge in decentralized PAC systems is to decide what information to communicate with the neighboring robots and how to take actions while utilizing the information shared by the neighbors. Recently, this has been addressed using Graph Neural Networks (GNNs) for applications such as flocking and coverage control. Although conceptually, GNN policies are fully decentralized, the evaluation and deployment of such policies have primarily remained centralized or restrictively decentralized. Furthermore, existing frameworks assume sequential execution of perception and action inference, which is very restrictive in real-world applica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00016</link><description>&lt;p&gt;
Alpha-GPT&#65306;&#20154;&#26426;&#20132;&#20114;&#24335; Alpha &#25366;&#25496;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#25237;&#36164;&#30740;&#31350;&#20013;&#65292;&#25366;&#25496;&#26032;&#30340; alpha&#65288;&#26377;&#25928;&#30340;&#20132;&#26131;&#20449;&#21495;&#25110;&#22240;&#23376;&#65289;&#26159;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340; alpha &#25366;&#25496;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#25163;&#24037;&#21512;&#25104;&#22240;&#23376;&#36824;&#26159;&#31639;&#27861;&#25366;&#25496;&#22240;&#23376;&#65288;&#22914;&#36951;&#20256;&#32534;&#31243;&#25628;&#32034;&#65289;&#65292;&#37117;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#22312;&#23454;&#26045;&#37327;&#21270;&#20998;&#26512;&#24072;&#30340;&#24819;&#27861;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24341;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#20010;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; Alpha-GPT&#65292;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335; alpha &#25366;&#25496;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#8220;&#29702;&#35299;&#8221;&#37327;&#21270;&#30740;&#31350;&#20154;&#21592;&#30340;&#24819;&#27861;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#21019;&#36896;&#24615;&#12289;&#28145;&#20837;&#27934;&#23519;&#21147;&#21644;&#26377;&#25928;&#24615;&#30340; alpha&#12290;&#36890;&#36807;&#22810;&#20010; alpha &#25366;&#25496;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Alpha-GPT &#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.04335</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#25216;&#26415;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study. (arXiv:2306.04335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04335
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20851;&#38190;&#25361;&#25112;&#20026;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20110;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12289;&#39044;&#38450;&#21644;&#39044;&#27979;&#36234;&#26469;&#36234;&#37325;&#35270;&#12290;&#27492;&#22806;&#65292;&#20256;&#24863;&#22120;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#20063;&#25512;&#21160;&#20102;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#35821;&#20041;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#24322;&#26500;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20114;&#25805;&#20316;&#24615;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#34920;&#31034;&#19987;&#23478;&#20581;&#24247;&#30693;&#35782;&#20197;&#25903;&#25345;&#20915;&#31574;&#25152;&#38656;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#24863;&#22120;&#20010;&#20154;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#20013;&#35821;&#20041;&#25216;&#26415;&#30340;&#24212;&#29992;&#29616;&#29366;&#12290;&#20351;&#29992;&#31995;&#32479;&#26041;&#27861;&#23545;40&#20010;&#20195;&#34920;&#35813;&#39046;&#22495;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#36825;&#39033;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#27492;&#31867;&#31995;&#32479;&#24517;&#39035;&#20811;&#26381;&#30340;&#20845;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20114;&#25805;&#20316;&#24615;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#24773;&#22659;&#26816;&#27979;&#12289;&#24773;&#22659;&#39044;&#27979;&#12289;&#20915;&#31574;&#25903;&#25345;&#21644;&#30693;&#35782;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increased focus on early detection, prevention, and prediction of diseases. This, together with advances in sensor technology and the Internet of Things, has led to accelerated efforts in the development of personal health monitoring systems. Semantic technologies have emerged as an effective way to not only deal with the issue of interoperability associated with heterogeneous health sensor data, but also to represent expert health knowledge to support complex reasoning required for decision-making. This study evaluates the state of the art in the use of semantic technologies in sensor-based personal health monitoring systems. Using a systematic approach, a total of 40 systems representing the state of the art in the field are analysed. Through this analysis, six key challenges that such systems must overcome for optimal and effective health monitoring are identified: interoperability, context awareness, situation detection, situation prediction, deci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06055</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#31216;&#24615;&#21644;&#21551;&#21457;&#24335;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation. (arXiv:2304.06055v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#33258;&#21160;&#26500;&#24314;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#26102;&#30001;&#20110;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#32467;&#21512;&#29289;&#29702;&#26426;&#22120;&#29615;&#22659;&#20013;&#23384;&#22312;&#30340;&#33258;&#28982;&#23545;&#31216;&#24615;&#65292;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#30340;&#34701;&#21512;&#26469;&#35757;&#32451;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#22810;&#26679;&#21270;&#32780;&#32039;&#20945;&#30340;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#36817;&#27010;&#24565;&#30340;&#20005;&#26684;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#33539;&#22260;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20004;&#20010;&#28857;&#23545;&#28857;&#30340;&#24037;&#19994;&#33218;&#21040;&#36798;&#20219;&#21153;&#65288;&#26377;&#38556;&#30861;&#21644;&#26080;&#38556;&#30861;&#65289;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning demonstrates significant potential in automatically building control policies in numerous domains, but shows low efficiency when applied to robot manipulation tasks due to the curse of dimensionality. To facilitate the learning of such tasks, prior knowledge or heuristics that incorporate inherent simplification can effectively improve the learning performance. This paper aims to define and incorporate the natural symmetry present in physical robotic environments. Then, sample-efficient policies are trained by exploiting the expert demonstrations in symmetrical environments through an amalgamation of reinforcement and behavior cloning, which gives the off-policy learning process a diverse yet compact initiation. Furthermore, it presents a rigorous framework for a recent concept and explores its scope for robot manipulation tasks. The proposed method is validated via two point-to-point reaching tasks of an industrial arm, with and without an obstacle, in a simulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item></channel></rss>