<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12973</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#21453;&#20107;&#23454;&#20542;&#21521;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation. (arXiv:2303.12973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#65292;&#35768;&#22810;&#35780;&#20998;&#20449;&#24687;&#37117;&#20002;&#22833;&#20102;&#65292;&#36825;&#34987;&#31216;&#20026;&#38750;&#38543;&#26426;&#32570;&#22833;&#12290;&#21453;&#20107;&#23454;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#34987;&#29992;&#20110;&#34913;&#37327;&#27599;&#20010;&#35266;&#23519;&#21040;&#30340;&#35780;&#20998;&#30340;&#22635;&#20805;&#38169;&#35823;&#12290;&#34429;&#28982;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;IPS&#20272;&#35745;&#30340;&#24615;&#33021;&#21463;&#21040;&#20542;&#21521;&#24615;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#20195;&#34920;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#20559;&#35823;&#21644;&#25512;&#24191;&#30028;&#38480;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#20248;&#20110;&#26410;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#12290; Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#24471;&#21040;&#25913;&#36827;&#65292;&#20174;&#32780;&#20351;&#25512;&#33616;&#32467;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques. Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.
&lt;/p&gt;</description></item></channel></rss>