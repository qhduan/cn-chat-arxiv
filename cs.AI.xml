<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07688</link><description>&lt;p&gt;
Maxwell&#30340;&#24694;&#39764;&#20043;&#24037;&#20316;&#65306;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#39281;&#21644;&#23454;&#29616;&#26377;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07688
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27515;&#20129;&#31070;&#32463;&#20803;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;Demon Pruning&#65288;DemP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#20135;&#29983;&#65292;&#21160;&#24577;&#23454;&#29616;&#32593;&#32476;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;$\textit{&#27515;&#20129;&#31070;&#32463;&#20803;}$&#29616;&#35937;&#8212;&#8212;&#22312;&#35757;&#32451;&#26399;&#38388;&#21464;&#24471;&#19981;&#27963;&#36291;&#25110;&#39281;&#21644;&#65292;&#36755;&#20986;&#20026;&#38646;&#30340;&#21333;&#20803;&#8212;&#20256;&#32479;&#19978;&#34987;&#35270;&#20026;&#19981;&#21487;&#21462;&#30340;&#65292;&#19982;&#20248;&#21270;&#25361;&#25112;&#26377;&#20851;&#65292;&#24182;&#23548;&#33268;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20007;&#22833;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#19987;&#27880;&#20110;&#31232;&#30095;&#24615;&#21644;&#20462;&#21098;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#32034;&#21508;&#31181;&#36229;&#21442;&#25968;&#37197;&#32622;&#23545;&#27515;&#20129;&#31070;&#32463;&#20803;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#26377;&#21161;&#20110;&#20419;&#36827;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Demon Pruning}$&#65288;DemP&#65289;&#65292;&#19968;&#31181;&#25511;&#21046;&#27515;&#20129;&#31070;&#32463;&#20803;&#25193;&#24352;&#65292;&#21160;&#24577;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27963;&#36291;&#21333;&#20803;&#19978;&#27880;&#20837;&#22122;&#22768;&#21644;&#37319;&#29992;&#21333;&#21608;&#26399;&#35843;&#24230;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;DemP&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;CIFAR10&#19978;&#30340;&#23454;&#39564;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#26368;&#32456;&#23558;&#27010;&#24565;&#30340;&#26144;&#23556;&#32467;&#26500;&#21270;&#20026;&#38899;&#20048;&#20116;&#24230;&#22278;&#12290;</title><link>https://arxiv.org/abs/2403.00790</link><description>&lt;p&gt;
&#21033;&#29992;&#38899;&#20048;&#20116;&#24230;&#22278;&#26500;&#24314;&#27010;&#24565;&#31354;&#38388;&#65306;&#22522;&#20110;&#38899;&#20048;&#35821;&#27861;&#28608;&#27963;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00790
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#26368;&#32456;&#23558;&#27010;&#24565;&#30340;&#26144;&#23556;&#32467;&#26500;&#21270;&#20026;&#38899;&#20048;&#20116;&#24230;&#22278;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;&#23574;&#23792;&#32593;&#32476;&#65289;&#30340;&#32467;&#26500;&#19982;&#38050;&#29748;&#26354;&#30340;&#26500;&#25104;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20284;&#20043;&#22788;&#12290;&#34429;&#28982;&#20004;&#32773;&#37117;&#28041;&#21450;&#25353;&#39034;&#24207;&#25110;&#24182;&#34892;&#28608;&#27963;&#30340;&#33410;&#28857;&#25110;&#38899;&#31526;&#65292;&#20294;&#21518;&#32773;&#21463;&#30410;&#20110;&#20016;&#23500;&#30340;&#38899;&#20048;&#29702;&#35770;&#65292;&#20197;&#25351;&#23548;&#26377;&#24847;&#20041;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#26469;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#65292;&#20801;&#35768;&#23558;&#31526;&#21495;&#34920;&#31034;&#20026;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26576;&#20123;&#28608;&#27963;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#31867;&#20284;&#20110;&#21560;&#24341;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35843;&#21046;&#38899;&#35843;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#32593;&#32476;&#20869;&#23548;&#33322;&#19981;&#21516;&#30340;&#21560;&#24341;&#30406;&#22320;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#26144;&#23556;&#26159;&#30001;&#38899;&#20048;&#20116;&#24230;&#22278;&#26500;&#25104;&#30340;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;&#38899;&#20048;&#29702;&#35770;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00790v1 Announce Type: cross  Abstract: In this paper, we explore the intriguing similarities between the structure of a discrete neural network, such as a spiking network, and the composition of a piano piece. While both involve nodes or notes that are activated sequentially or in parallel, the latter benefits from the rich body of music theory to guide meaningful combinations. We propose a novel approach that leverages musical grammar to regulate activations in a spiking neural network, allowing for the representation of symbols as attractors. By applying rules for chord progressions from music theory, we demonstrate how certain activations naturally follow others, akin to the concept of attraction. Furthermore, we introduce the concept of modulating keys to navigate different basins of attraction within the network. Ultimately, we show that the map of concepts in our model is structured by the musical circle of fifths, highlighting the potential for leveraging music theor
&lt;/p&gt;</description></item></channel></rss>