<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00424</link><description>&lt;p&gt;
&#20174;&#27880;&#24847;&#21147;&#21040;&#21033;&#28070;&#65306;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
From attention to profit: quantitative trading strategy based on transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#37327;&#21270;&#20132;&#26131;&#23454;&#36341;&#20013;&#65292;&#24212;&#23545;&#22797;&#26434;&#21160;&#24577;&#30340;&#37329;&#34701;&#24066;&#22330;&#19968;&#30452;&#26159;&#20010;&#25345;&#20037;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20805;&#20998;&#25429;&#25417;&#21508;&#31181;&#24066;&#22330;&#21464;&#37327;&#65292;&#32463;&#24120;&#24573;&#35270;&#38271;&#26399;&#20449;&#24687;&#24182;&#19988;&#26080;&#27861;&#25429;&#25417;&#21487;&#33021;&#24102;&#26469;&#21033;&#28070;&#30340;&#22522;&#26412;&#20449;&#21495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#26032;&#22411;&#22240;&#23376;&#12290;&#36890;&#36807;&#20174;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19981;&#20165;&#21457;&#25381;&#20102;&#20854;&#21407;&#26377;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#25429;&#25417;&#21644;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20851;&#31995;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#25968;&#20540;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;&#35813;&#30740;&#31350;&#25910;&#38598;&#20102;2010&#24180;&#33267;2019&#24180;&#20013;&#22269;&#36164;&#26412;&#24066;&#22330;4,601&#21482;&#32929;&#31080;&#30340;5,000,000&#22810;&#26465;&#28378;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#31080;&#34920;&#29616;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00424v1 Announce Type: cross  Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.11319</link><description>&lt;p&gt;
GeoSAM: &#20351;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#23545;SAM&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11319
&lt;/p&gt;
&lt;p&gt;
GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#26102;&#65292;Segment Anything Model (SAM)&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22320;&#29702;&#22270;&#20687;&#65288;&#22914;&#33322;&#25293;&#21644;&#21355;&#26143;&#22270;&#20687;&#65289;&#20013;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#36947;&#36335;&#12289;&#20154;&#34892;&#36947;&#21644;&#20154;&#34892;&#27178;&#36947;&#31561;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#26102;&#12290;&#36825;&#31181;&#36739;&#24046;&#30340;&#24615;&#33021;&#28304;&#20110;&#36825;&#20123;&#23545;&#35937;&#30340;&#31364;&#23567;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#32441;&#29702;&#34701;&#20837;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#26641;&#26408;&#12289;&#24314;&#31569;&#29289;&#12289;&#36710;&#36742;&#21644;&#34892;&#20154;&#31561;&#29289;&#20307;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#37117;&#21487;&#33021;&#20351;&#27169;&#22411;&#22833;&#21435;&#23450;&#21521;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;SAM&#65288;GeoSAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#23494;&#38598;&#35270;&#35273;&#25552;&#31034;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#31232;&#30095;&#35270;&#35273;&#25552;&#31034;&#23454;&#26045;&#20102;&#32454;&#35843;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;GeoSAM&#22312;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12289;&#34892;&#20154;&#22522;&#30784;&#35774;&#26045;&#30340;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#20102;26&#65285;&#12289;7&#65285;&#21644;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17176</link><description>&lt;p&gt;
&#20174;&#20840;&#26223;X&#23556;&#32447;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#22312;&#29616;&#20195;&#21475;&#33108;&#20445;&#20581;&#20013;&#26159;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#29273;&#40831;&#31181;&#26893;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#26681;&#25454;FUSegNet&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#21019;&#38754;&#20998;&#21106;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#20110;&#32593;&#26684;&#30340;&#27880;&#24847;&#21147;&#38376;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24341;&#20837;&#23450;&#21521;&#36793;&#30028;&#26694;&#65288;OBB&#65289;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#29273;&#40831;&#23450;&#20301;&#20272;&#35745;&#12290;&#22312;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;DNS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;543&#20010;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#26368;&#39640;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24471;&#20998;82.43%&#65292;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#24471;&#20998;90.37%&#65292;&#22312;OBB&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26059;&#36716;&#30340;&#20132;&#24182;&#27604;&#65288;RIoU&#65289;&#24471;&#20998;82.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02608</link><description>&lt;p&gt;
&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Unravelling Responsibility for AI. (arXiv:2308.02608v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#21512;&#29702;&#24605;&#32771;&#36131;&#20219;&#24212;&#35813;&#25918;&#22312;&#20309;&#22788;&#65292;&#25105;&#20204;&#39318;&#20808;&#38656;&#35201;&#19968;&#20010;&#36275;&#22815;&#28165;&#26224;&#21644;&#35814;&#32454;&#30340;&#36328;&#23398;&#31185;&#35789;&#27719;&#26469;&#35848;&#35770;&#36131;&#20219;&#12290;&#36131;&#20219;&#26159;&#19968;&#31181;&#19977;&#20803;&#20851;&#31995;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#34892;&#20026;&#32773;&#12289;&#19968;&#20010;&#20107;&#20214;&#21644;&#19968;&#31181;&#36131;&#20219;&#26041;&#24335;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24847;&#35782;&#30340;&#20026;&#20102;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#36827;&#34892;&#23454;&#36341;&#25512;&#29702;&#30340;&#8220;&#35299;&#26500;&#8221;&#36131;&#20219;&#27010;&#24565;&#30340;&#21162;&#21147;&#65292;&#26412;&#25991;&#37319;&#21462;&#20102;&#8220;&#34892;&#20026;&#32773;A&#23545;&#20107;&#20214;O&#36127;&#36131;&#8221;&#30340;&#19977;&#37096;&#20998;&#34920;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;A&#12289;&#36127;&#36131;&#12289;O&#30340;&#23376;&#31867;&#21035;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;&#36825;&#20123;&#26377;&#25928;&#32452;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36131;&#20219;&#20018;&#8221;&#65292;&#20998;&#20026;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#65306;&#35282;&#33394;&#36131;&#20219;&#12289;&#22240;&#26524;&#36131;&#20219;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36947;&#24503;&#36131;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36816;&#34892;&#31034;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#19968;&#20010;&#28041;&#21450;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#21478;&#19968;&#20010;&#28041;&#21450;AV&#19982;&#34892;&#20154;&#30340;&#33268;&#21629;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reason about where responsibility does and should lie in complex situations involving AI-enabled systems, we first need a sufficiently clear and detailed cross-disciplinary vocabulary for talking about responsibility. Responsibility is a triadic relation involving an actor, an occurrence, and a way of being responsible. As part of a conscious effort towards 'unravelling' the concept of responsibility to support practical reasoning about responsibility for AI, this paper takes the three-part formulation, 'Actor A is responsible for Occurrence O' and identifies valid combinations of subcategories of A, is responsible for, and O. These valid combinations - which we term "responsibility strings" - are grouped into four senses of responsibility: role-responsibility; causal responsibility; legal liability-responsibility; and moral responsibility. They are illustrated with two running examples, one involving a healthcare AI-based system and another the fatal collision of an AV with a pedes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;SmartonAI&#25554;&#20214;&#65292;&#22522;&#20110;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#33539;&#24335;&#26469;&#35299;&#20915;EDA&#36719;&#20214;&#20013;&#21021;&#23398;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#21629;&#20196;&#32467;&#26500;&#21644;&#39640;&#23398;&#20064;&#26354;&#32447;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14740</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#22797;&#26434;EDA&#36719;&#20214;&#26032;&#20132;&#20114;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
New Interaction Paradigm for Complex EDA Software Leveraging GPT. (arXiv:2307.14740v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;SmartonAI&#25554;&#20214;&#65292;&#22522;&#20110;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#33539;&#24335;&#26469;&#35299;&#20915;EDA&#36719;&#20214;&#20013;&#21021;&#23398;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#21629;&#20196;&#32467;&#26500;&#21644;&#39640;&#23398;&#20064;&#26354;&#32447;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#39046;&#22495;&#20013;&#65292;&#19987;&#19994;&#36719;&#20214;&#22914;KiCad&#12289;Cadence&#21644;Altium Designer&#25552;&#20379;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#35774;&#35745;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#21629;&#20196;&#32467;&#26500;&#21644;&#36739;&#39640;&#30340;&#23398;&#20064;&#26354;&#32447;&#23545;&#20110;&#21021;&#23398;&#32773;&#30340;&#21360;&#21047;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#35774;&#35745;&#24072;&#26469;&#35828;&#36896;&#25104;&#20102;&#38556;&#30861;&#12290;&#36825;&#23548;&#33268;&#38590;&#20197;&#36873;&#25321;&#36866;&#21512;&#19981;&#21516;&#35774;&#35745;&#30446;&#30340;&#30340;&#21151;&#33021;&#25110;&#25554;&#20214;&#65292;&#24182;&#19988;&#20256;&#32479;&#25991;&#26723;&#12289;&#35270;&#39057;&#21644;&#22312;&#32447;&#35770;&#22363;&#20043;&#22806;&#32570;&#20047;&#30452;&#35266;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SmartonAI&#30340;EDA&#36719;&#20214;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#36741;&#21161;&#25554;&#20214;&#65292;&#20854;&#20013;&#20197;KiCad&#20316;&#20026;&#31532;&#19968;&#20010;&#31034;&#20363;&#12290;SmartonAI&#21463;&#21040;HuggingGPT&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#37319;&#29992;&#20102;GPT&#21644;BERT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20419;&#36827;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;&#24403;&#25509;&#25910;&#21040;&#35774;&#35745;&#24072;&#30340;&#35831;&#27714;&#26102;&#65292;SmartonAI&#20250;&#36827;&#34892;&#20219;&#21153;&#20998;&#35299;&#24182;&#39640;&#25928;&#25191;&#34892;&#30456;&#20851;&#30340;&#23376;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing field of electronic design automation (EDA), professional software such as KiCad, Cadence , and Altium Designer provide increasingly extensive design functionalities. However, the intricate command structure and high learning curve create a barrier, particularly for novice printed circuit board (PCB) designers. This results in difficulties in selecting appropriate functions or plugins for varying design purposes, compounded by the lack of intuitive learning methods beyond traditional documentation, videos, and online forums. To address this challenge, an artificial intelligence (AI) interaction assist plugin for EDA software named SmartonAl is developed here, also KiCad is taken as the first example. SmartonAI is inspired by the HuggingGPT framework and employs large language models, such as GPT and BERT, to facilitate task planning and execution. On receiving a designer request, SmartonAI conducts a task breakdown and efficiently executes relevant subtasks, such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01312</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control. (arXiv:2307.01312v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23454;&#39564;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35843;&#25972;PID&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#23384;&#22312;&#65292;&#23454;&#38469;&#31995;&#32479;&#65288;&#22914;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#65289;&#38656;&#35201;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;PID&#25511;&#21046;&#22120;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#12290;&#37319;&#29992;&#20102;&#22686;&#37327;&#24335;PID&#25511;&#21046;&#22120;&#65292;&#24182;&#20165;&#23545;&#21487;&#21464;&#22686;&#30410;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#20026;&#20102;&#35843;&#25972;&#21160;&#24577;&#22686;&#30410;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;Actor-Critic&#28151;&#21512;&#31070;&#32463;&#32467;&#26500;&#65292;&#33021;&#22815;&#36866;&#24403;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#21516;&#26102;&#20805;&#24403;&#26368;&#20339;&#35782;&#21035;&#22120;&#12290;&#22312;&#35843;&#25972;&#21644;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#21644;Sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#65288;ADAM&#65289;&#20248;&#21270;&#22120;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10755</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;&#33041;&#30005;&#22270;AI&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Robust AI in EEG Systems: A Survey. (arXiv:2304.10755v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#23494;&#20999;&#32806;&#21512;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;EEG&#31995;&#32479;&#65292;&#22522;&#20110;AI&#30340;EEG&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21464;&#24471;&#23588;&#20026;&#20851;&#38190;&#12290;&#21487;&#35299;&#37322;&#24615;&#33021;&#22815;&#38416;&#37322;AI&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#22240;&#27492;&#21487;&#20197;&#33719;&#24471;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#40065;&#26834;&#24615;&#21017;&#21453;&#26144;&#20102;AI&#23545;&#25239;&#25915;&#20987;&#21644;&#25200;&#21160;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#23545;&#20110;&#25935;&#24863;&#21644;&#33030;&#24369;&#30340;EEG&#20449;&#21495;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;EEG&#31995;&#32479;&#20013;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20173;&#28982;&#27809;&#26377;&#32508;&#36848;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#21270;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#36755;&#20986;&#35299;&#37322;&#24615;&#65292;&#24635;&#32467;&#20102;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it 
&lt;/p&gt;</description></item></channel></rss>