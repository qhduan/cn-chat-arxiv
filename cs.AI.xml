<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#20154;&#31867;&#21453;&#24605;&#21644;&#35752;&#35770;&#20915;&#31574;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2403.16812</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#65306;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#19982;&#35780;&#20272;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#20154;&#31867;&#21453;&#24605;&#21644;&#35752;&#35770;&#20915;&#31574;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#20154;&#31867;&#36890;&#24120;&#34987;&#21160;&#22320;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20840;&#30424;&#25509;&#21463;&#25110;&#25298;&#32477;&#12290;&#22312;&#36825;&#26679;&#30340;&#33539;&#24335;&#20013;&#65292;&#21457;&#29616;&#20154;&#31867;&#24456;&#23569;&#28608;&#21457;&#20998;&#26512;&#24605;&#32500;&#65292;&#19988;&#22312;&#21457;&#29983;&#20998;&#27495;&#26102;&#38590;&#20197;&#23558;&#30683;&#30462;&#24847;&#35265;&#30340;&#32454;&#24494;&#24046;&#21035;&#20256;&#36798;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#31867;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#20154;&#24037;&#26234;&#33021;&#24847;&#35265;&#20998;&#27495;&#36827;&#34892;&#21453;&#24605;&#21644;&#35752;&#35770;&#12290;&#22522;&#20110;&#20154;&#31867;&#36777;&#35770;&#29702;&#35770;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#32500;&#24230;&#32423;&#24847;&#35265;&#24449;&#38598;&#12289;&#36777;&#35770;&#35752;&#35770;&#21644;&#20915;&#31574;&#26356;&#26032;&#65292;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20114;&#21160;&#12290;&#20026;&#20102;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#36777;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#31867;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#23454;&#29616;&#28789;&#27963;&#30340;&#23545;&#35805;&#20132;&#20114;&#21644;&#24544;&#23454;&#30340;&#20449;&#24687;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16812v1 Announce Type: cross  Abstract: In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a grad
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#29992;&#25143;&#20027;&#35201;&#29992;&#20110;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#36991;&#20813;&#19982;&#21361;&#38505;&#29289;&#20307;&#25509;&#35302;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.15604</link><description>&lt;p&gt;
&#30740;&#31350;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#29992;&#25143;&#20027;&#35201;&#29992;&#20110;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#36991;&#20813;&#19982;&#21361;&#38505;&#29289;&#20307;&#25509;&#35302;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#22330;&#26223;&#25551;&#36848;&#8221;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24110;&#21161;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#22763;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#29702;&#35299;&#29031;&#29255;&#20013;&#30340;&#35270;&#35273;&#20869;&#23481;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#20294;&#20182;&#20204;&#21482;&#30740;&#31350;&#20102;&#21033;&#29992;&#36828;&#31243;&#26377;&#35270;&#21147;&#21161;&#25163;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25551;&#36848;&#30340;&#24212;&#29992;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35843;&#26597;&#20854;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#20004;&#21608;&#30340;&#26085;&#35760;&#30740;&#31350;&#65292;&#22312;&#27492;&#26399;&#38388;&#65292;16&#21517;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#21442;&#19982;&#32773;&#20351;&#29992;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#12290;&#36890;&#36807;&#20182;&#20204;&#30340;&#26085;&#35760;&#35760;&#24405;&#21644;&#21518;&#32493;&#35775;&#35848;&#65292;&#29992;&#25143;&#20998;&#20139;&#20102;&#20182;&#20204;&#30340;&#20449;&#24687;&#30446;&#26631;&#20197;&#21450;&#20182;&#20204;&#25910;&#21040;&#30340;&#35270;&#35273;&#25551;&#36848;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#35760;&#24405;&#65292;&#24182;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#27604;&#22914;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#20197;&#21450;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#36991;&#20813;&#25509;&#35302;&#21361;&#38505;&#29289;&#20307;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#65292;&#24179;&#22343;&#20026;2.76&#65288;&#26631;&#20934;&#24046;=1.49&#65289;&#65292;&#23545;&#28385;&#24847;&#24230;&#30340;&#35780;&#20998;&#20026;2.43&#65288;&#26631;&#20934;&#24046;=1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15604v1 Announce Type: cross  Abstract: "Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16442</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37197;&#23545;&#27425;&#27169;&#27169;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22823;&#20110;&#20869;&#23384;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#21462;&#20915;&#20110;&#23376;&#38598;&#36873;&#25321;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#32452;&#37325;&#35201;&#21644;&#20195;&#34920;&#24615;&#30340;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#20272;&#35745;&#36817;&#20284;&#20445;&#35777;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02278</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Virtual Learning on Heterogeneous Data with Local-global Distillation. (arXiv:2303.02278v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLGD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#30340;&#33976;&#39311;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#26469;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#23481;&#26131;&#20986;&#29616;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#21516;&#27493;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#31561;&#25361;&#25112;&#12290;&#36817;&#26469;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#24050;&#34987;&#30740;&#31350;&#65292;&#20197;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20445;&#30041;&#26412;&#22320;&#31169;&#26377;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;FL&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#20351;&#29992;&#33976;&#39311;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20250;&#25918;&#22823;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#26412;&#22320;&#20840;&#23616;&#33976;&#39311;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#34394;&#25311;&#23398;&#20064;&#65288;FedLGD&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#34394;&#25311;&#25968;&#25454;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#32452;&#21512;&#21019;&#24314;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#22788;&#29702;&#21516;&#27493;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#20998;&#24067;&#21305;&#37197;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#36890;&#36807;&#27169;&#22411;&#21453;&#39304;&#26469;&#20849;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Federated Learning (FL)'s trend for learning machine learning models in a distributed manner, it is susceptible to performance drops when training on heterogeneous data. In addition, FL inevitability faces the challenges of synchronization, efficiency, and privacy. Recently, dataset distillation has been explored in order to improve the efficiency and scalability of FL by creating a smaller, synthetic dataset that retains the performance of a model trained on the local private datasets. We discover that using distilled local datasets can amplify the heterogeneity issue in FL. To address this, we propose a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FedLGD), which trains FL using a smaller synthetic dataset (referred as virtual data) created through a combination of local and global dataset distillation. Specifically, to handle synchronization and class imbalance, we propose iterative distribution matching to allow clients 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#32508;&#36848;&#20102;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#22238;&#39038;&#20102;250&#22810;&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;</title><link>http://arxiv.org/abs/2211.14997</link><description>&lt;p&gt;
&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#30475;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective. (arXiv:2211.14997v3 [q-fin.RM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22823;&#25968;&#25454;&#35282;&#24230;&#32508;&#36848;&#20102;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#22238;&#39038;&#20102;250&#22810;&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#26088;&#22312;&#39044;&#27979;&#20225;&#19994;&#26410;&#26469;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#32780;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#19968;&#30452;&#26159;&#37329;&#34701;&#21644;&#31649;&#29702;&#39046;&#22495;&#30340;&#26680;&#24515;&#30740;&#31350;&#20027;&#39064;&#12290;&#22522;&#20110;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#30740;&#31350;&#27491;&#22312;&#32463;&#21382;&#24555;&#36895;&#21457;&#23637;&#24182;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#20840;&#38754;&#35780;&#20272;&#30456;&#20851;&#30740;&#31350;&#26082;&#26377;&#24517;&#35201;&#24615;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#26377;&#20215;&#20540;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20851;&#20110;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#30340;&#32508;&#36848;&#65292;&#20294;&#36825;&#20123;&#32508;&#36848;&#21333;&#29420;&#20171;&#32461;&#20102;&#26041;&#27861;&#65292;&#32570;&#20047;&#20225;&#19994;&#36130;&#21153;&#39118;&#38505;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#23581;&#35797;&#20174;&#22823;&#25968;&#25454;&#30340;&#35282;&#24230;&#25552;&#20379;&#20225;&#19994;&#39118;&#38505;&#20998;&#26512;&#26041;&#27861;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#36229;&#36807;250&#31687;&#20195;&#34920;&#24615;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprise financial risk analysis aims at predicting the future financial risk of enterprises. Due to its wide and significant application, enterprise financial risk analysis has always been the core research topic in the fields of Finance and Management. Based on advanced computer science and artificial intelligence technologies, enterprise risk analysis research is experiencing rapid developments and making significant progress. Therefore, it is both necessary and challenging to comprehensively review the relevant studies. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise financial risk analysis. In contrast, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from Big Data perspective, which reviews more than 250 representative articles in the 
&lt;/p&gt;</description></item></channel></rss>