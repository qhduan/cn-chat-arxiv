<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17377</link><description>&lt;p&gt;
&#20855;&#26377;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#33258;&#30699;&#27491;&#25193;&#25955;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20294;&#20854;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#27604;&#22914;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CG&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#25110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22914;&#22270;&#20687;&#24674;&#22797;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#31216;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#65292;&#23427;&#25913;&#36827;&#20102;&#25193;&#25955;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#22312;&#26080;&#26465;&#20214;&#36824;&#26159;&#26377;&#26465;&#20214;&#30340;&#35774;&#32622;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#25110;&#25972;&#21512;&#22806;&#37096;&#27169;&#22359;&#12290;PAG &#26088;&#22312;&#36890;&#36807;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#36880;&#27493;&#22686;&#24378;&#26679;&#26412;&#30340;&#32467;&#26500;&#12290;&#23427;&#28041;&#21450;&#36890;&#36807;&#29992;&#24658;&#31561;&#30697;&#38453;&#26367;&#25442;&#25193;&#25955; U-Net &#20013;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#32771;&#34385;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;</title><link>https://arxiv.org/abs/2403.03593</link><description>&lt;p&gt;
&#24744;&#20449;&#20219;&#24744;&#30340;&#27169;&#22411;&#21527;&#65311;&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#26032;&#20852;&#30340;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03593
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#35745;&#31639;&#21644;&#25216;&#26415;&#35201;&#27714;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20010;&#20154;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#22312;&#20844;&#20849;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30452;&#25509;&#20351;&#29992;&#25110;&#38598;&#25104;&#21040;&#20135;&#21697;&#31649;&#36947;&#20013;&#32780;&#27809;&#26377;&#29305;&#27530;&#30340;&#39044;&#38450;&#25514;&#26045;&#65292;&#22240;&#20026;&#23427;&#20204;&#23454;&#38469;&#19978;&#21482;&#26159;&#20197;&#24352;&#37327;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20379;&#24212;&#38142;&#23041;&#32961;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#33258;&#35299;&#21387;&#33258;&#25191;&#34892;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#12290;MaleficNet 2.0&#20351;&#29992;&#25193;&#39057;&#20449;&#36947;&#32534;&#30721;&#32467;&#21512;&#32416;&#38169;&#25216;&#26415;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#27880;&#20837;&#24694;&#24847;&#26377;&#25928;&#36733;&#33655;&#12290;MaleficNet 2.0&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03593v1 Announce Type: cross  Abstract: Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against 
&lt;/p&gt;</description></item><item><title>VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13126</link><description>&lt;p&gt;
VGMShield&#65306;&#32531;&#35299;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;
&lt;/p&gt;
&lt;p&gt;
VGMShield: Mitigating Misuse of Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13126
&lt;/p&gt;
&lt;p&gt;
VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#21033;&#29992;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#31526;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#25216;&#26415;&#34987;&#29992;&#20110;&#21019;&#20316;&#21644;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VGMShield&#65306;&#19968;&#22871;&#21253;&#21547;&#19977;&#39033;&#30452;&#25509;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#29992;&#20110;&#38450;&#33539;&#34394;&#20551;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#8220;&#34394;&#20551;&#35270;&#39057;&#26816;&#27979;&#8221;&#24320;&#22987;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#30340;&#35270;&#39057;&#20013;&#26159;&#21542;&#23384;&#22312;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#21306;&#20998;&#23427;&#20204;&#19982;&#30495;&#23454;&#35270;&#39057;&#30340;&#19981;&#21516;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#8220;&#28335;&#28304;&#8221;&#38382;&#39064;&#65292;&#21363;&#23558;&#19968;&#27573;&#34394;&#20551;&#35270;&#39057;&#36861;&#28335;&#22238;&#29983;&#25104;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20851;&#27880;&#8220;&#26102;&#31354;&#21160;&#24577;&#8221;&#30340;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
&lt;/p&gt;</description></item><item><title>PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.12810</link><description>&lt;p&gt;
PIP-Net&#65306;&#22478;&#24066;&#20013;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PIP-Net: Pedestrian Intention Prediction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12810
&lt;/p&gt;
&lt;p&gt;
PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#23545;&#34892;&#20154;&#24847;&#22270;&#30340;&#39044;&#27979;&#26159;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#19968;&#39033;&#30740;&#31350;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PIP-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;AVs&#22312;&#29616;&#23454;&#19990;&#30028;&#22478;&#24066;&#22330;&#26223;&#20013;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38024;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#23433;&#35013;&#21644;&#35774;&#32622;&#35774;&#35745;&#30340;PIP-Net&#21464;&#31181;&#12290;&#21033;&#29992;&#26469;&#33258;&#34892;&#39542;&#22330;&#26223;&#30340;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#20026;&#20102;&#22686;&#24378;&#36947;&#36335;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#21450;&#20854;&#19982;&#33258;&#36710;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#28145;&#24230;&#29305;&#24449;&#22270;&#65292;&#32467;&#21512;&#23616;&#37096;&#36816;&#21160;&#27969;&#29305;&#24449;&#65292;&#20026;&#22330;&#26223;&#21160;&#24577;&#25552;&#20379;&#20016;&#23500;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25668;&#20687;&#22836;&#30340;&#35270;&#37326;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22260;&#32469;&#33258;&#36710;&#30340;&#19977;&#20010;&#25668;&#20687;&#22836;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12810v1 Announce Type: cross  Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the
&lt;/p&gt;</description></item><item><title>GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.08855</link><description>&lt;p&gt;
GhostWriter:&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20889;&#20316;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08855
&lt;/p&gt;
&lt;p&gt;
GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#20889;&#20316;&#36741;&#21161;&#26041;&#38754;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#19988;&#20855;&#26377;&#26080;&#22788;&#19981;&#22312;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#24615;&#21270;&#21644;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#65292;LLM&#39537;&#21160;&#30340;&#20889;&#20316;&#31995;&#32479;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#24863;&#21040;&#27822;&#20007;&#65292;&#24403;&#29992;&#25143;&#32570;&#20047;&#25552;&#31034;&#24037;&#31243;&#32463;&#39564;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#21152;&#21095;&#12290;&#25105;&#20204;&#35748;&#20026;&#35774;&#35745;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20043;&#19968;&#65292;&#24182;&#24341;&#20837;GhostWriter&#65292;&#36825;&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#30340;&#20195;&#29702;&#21644;&#20010;&#24615;&#21270;&#26469;&#36827;&#34892;&#20889;&#20316;&#12290;GhostWriter&#21033;&#29992;LLMs&#22312;&#29992;&#25143;&#32534;&#20889;&#30340;&#36807;&#31243;&#20013;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#25152;&#26399;&#26395;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#21516;&#26102;&#20801;&#35768;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#36827;&#34892;&#26174;&#24335;&#25945;&#23398;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;18&#21517;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20889;&#20316;&#20219;&#21153;&#20013;&#20351;&#29992;GhostWriter&#65292;&#35266;&#23519;&#21040;&#23427;&#24110;&#21161;&#29992;&#25143;&#32534;&#20889;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#22810;&#31181;&#26041;&#24335;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#26469;&#22686;&#24378;&#29992;&#25143;&#30340;&#33021;&#21147;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08855v1 Announce Type: cross Abstract: Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. From this study, we present insights re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13103</link><description>&lt;p&gt;
AVTENet: &#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20998;&#20139;&#30340;&#20266;&#36896;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#65292;&#35201;&#27714;&#21152;&#24378;&#30417;&#31649;&#24182;&#32473;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36229;&#30495;&#23454;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#23545;&#38899;&#39057;&#21644;&#35270;&#35273;&#20266;&#36896;&#23041;&#32961;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#20266;&#36896;&#35270;&#39057;&#30340;&#20808;&#21069;&#24037;&#20316;&#21482;&#21033;&#29992;&#20102;&#35270;&#35273;&#27169;&#24577;&#25110;&#38899;&#39057;&#27169;&#24577;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#26469;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;CNN&#65292;&#24182;&#19988;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#21463;&#21040;Transformer&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22768;&#23398;&#25805;&#20316;&#30340;&#38899;&#39057;-&#35270;&#35273;Transformer&#38598;&#25104;&#32593;&#32476;&#65288;AVTENet&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30456;&#20301;&#21464;&#21270;&#35782;&#21035;&#27169;&#24335;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#25163;&#24037;&#21046;&#20316;&#30340;&#25391;&#33633;&#27169;&#22411;&#35777;&#23454;&#20102;&#36825;&#19968;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31070;&#32463;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#19988;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.07908</link><description>&lt;p&gt;
&#24490;&#29615;&#32593;&#32476;&#36890;&#36807;&#20302;&#32500;&#25391;&#33633;&#35782;&#21035;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Recurrent networks recognize patterns with low-dimensional oscillations. (arXiv:2310.07908v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30456;&#20301;&#21464;&#21270;&#35782;&#21035;&#27169;&#24335;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#25163;&#24037;&#21046;&#20316;&#30340;&#25391;&#33633;&#27169;&#22411;&#35777;&#23454;&#20102;&#36825;&#19968;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31070;&#32463;&#23454;&#29616;&#26041;&#24335;&#65292;&#24182;&#19988;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#22312;SET&#21345;&#29260;&#28216;&#25103;&#21551;&#21457;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#26469;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#21518;&#30340;RNN&#35299;&#37322;&#20026;&#36890;&#36807;&#20302;&#32500;&#26497;&#38480;&#29615;&#20013;&#30340;&#30456;&#20301;&#21464;&#21270;&#36827;&#34892;&#27169;&#24335;&#35782;&#21035;&#65292;&#31867;&#20284;&#20110;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;(FSA)&#20013;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25163;&#24037;&#21046;&#20316;&#19968;&#20010;&#31616;&#21333;&#30340;&#25391;&#33633;&#27169;&#22411;&#26469;&#39564;&#35777;&#20102;&#36825;&#19968;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#22797;&#21046;&#20102;&#35757;&#32451;&#21518;&#30340;RNN&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#21160;&#21147;&#23398;&#26426;&#21046;&#33021;&#22815;&#23454;&#29616;&#27169;&#24335;&#35782;&#21035;&#65292;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#28508;&#22312;&#31070;&#32463;&#23454;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel dynamical mechanism for pattern recognition discovered by interpreting a recurrent neural network (RNN) trained on a simple task inspired by the SET card game. We interpreted the trained RNN as recognizing patterns via phase shifts in a low-dimensional limit cycle in a manner analogous to transitions in a finite state automaton (FSA). We further validated this interpretation by handcrafting a simple oscillatory model that reproduces the dynamics of the trained RNN. Our findings not only suggest of a potential dynamical mechanism capable of pattern recognition, but also suggest of a potential neural implementation of FSA. Above all, this work contributes to the growing discourse on deep learning model interpretability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10448</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20197;&#21450;&#31038;&#20250;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Human-AI Interactions and Societal Pitfalls. (arXiv:2309.10448v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#38754;&#20020;&#30340;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21512;&#20316;&#26102;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#30475;&#21040;&#29983;&#20135;&#21147;&#30340;&#25552;&#21319;&#65292;&#20294;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21487;&#33021;&#19981;&#23436;&#20840;&#31526;&#21512;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20854;&#20013;&#24322;&#36136;&#29992;&#25143;&#36873;&#25321;&#19982;AI&#20849;&#20139;&#22810;&#23569;&#20449;&#24687;&#65292;&#38754;&#20020;&#36755;&#20986;&#20445;&#30495;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20010;&#20307;&#20915;&#31574;&#19982;AI&#35757;&#32451;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#25361;&#25112;&#12290;&#36755;&#20986;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#21516;&#36136;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;AI&#22312;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#12290;&#32780;&#20219;&#20309;AI&#30340;&#20559;&#35265;&#21487;&#33021;&#25104;&#20026;&#31038;&#20250;&#20559;&#35265;&#12290;&#35299;&#20915;&#21516;&#36136;&#21270;&#21644;&#20559;&#35265;&#38382;&#39064;&#30340;&#21150;&#27861;&#26159;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#36755;&#20986;&#32780;&#19981;&#29306;&#29298;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When working with generative artificial intelligence (AI), users may see productivity gains, but the AI-generated content may not match their preferences exactly. To study this effect, we introduce a Bayesian framework in which heterogeneous users choose how much information to share with the AI, facing a trade-off between output fidelity and communication cost. We show that the interplay between these individual-level decisions and AI training may lead to societal challenges. Outputs may become more homogenized, especially when the AI is trained on AI-generated content. And any AI bias may become societal bias. A solution to the homogenization and bias issues is to improve human-AI interactions, enabling personalized outputs without sacrificing productivity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#21644;&#36923;&#36753;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25512;&#29702;&#30340;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#20301;&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#26426;&#22120;&#20154;&#21487;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08509</link><description>&lt;p&gt;
&#24102;&#26377;&#26102;&#38388;&#30340;&#29983;&#25104;&#36923;&#36753;&#65306;&#36229;&#36234;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#32479;&#35745;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Logic with Time: Beyond Logical Consistency and Statistical Possibility. (arXiv:2301.08509v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#21644;&#36923;&#36753;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25512;&#29702;&#30340;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#20301;&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#26426;&#22120;&#20154;&#21487;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25512;&#29702;&#29702;&#35770;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#36923;&#36753;&#25512;&#29702;&#31526;&#21495;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#27169;&#25968;&#25454;&#22914;&#20309;&#24341;&#36215;&#31526;&#21495;&#30693;&#35782;&#12290;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#34987;&#24314;&#27169;&#20026;&#27491;&#21521;&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#20998;&#21035;&#23545;&#24212;&#24418;&#24335;&#36923;&#36753;&#30340;&#35299;&#37322;&#21644;&#36870;&#35299;&#37322;&#12290;&#35813;&#29702;&#35770;&#24212;&#29992;&#20110;&#23450;&#20301;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#25439;&#22351;&#25110;&#22122;&#22768;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper gives a simple theory of inference to logically reason symbolic knowledge fully from data over time. We take a Bayesian approach to model how data causes symbolic knowledge. Probabilistic reasoning with symbolic knowledge is modelled as a process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation of formal logic, respectively. The theory is applied to a localisation problem to show a robot with broken or noisy sensors can efficiently solve the problem in a fully data-driven fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.13125</link><description>&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#30340;&#27491;&#24120;&#24341;&#23548;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22238;&#25253;&#30340;&#22343;&#20540;&#27169;&#22411;&#65292;&#25110;&#20215;&#20540;&#20989;&#25968;&#65292;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#36890;&#36807;&#24314;&#27169;&#20540;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20010;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20197;&#21450;&#22238;&#25253;&#65292;&#26469;&#20998;&#26512;&#35745;&#31639;&#20195;&#34920;&#25105;&#20204;&#20998;&#24067;&#24335;&#20540;&#20989;&#25968;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#20301;&#26639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20998;&#24067;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#26631;&#20934;&#30340;&#20540;&#20989;&#25968;&#20013;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#27010;&#36848;&#30340;&#26041;&#27861;&#19982;&#35768;&#22810;DRL&#32467;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;PPO&#21644;TRPO&#65292;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
&lt;/p&gt;</description></item></channel></rss>