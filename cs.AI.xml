<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16707</link><description>&lt;p&gt;
&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Domain Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#20851;&#20110;&#29992;&#20110;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#35752;&#35770;&#20102;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;DIL&#65289;&#12290;&#22312;DIL&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35266;&#23519;&#26032;&#39046;&#22495;&#19978;&#30340;&#26679;&#26412;&#12290;&#27169;&#22411;&#24517;&#39035;&#23545;&#25152;&#26377;&#39046;&#22495;&#19978;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#38656;&#35201;&#22312;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#20165;&#38388;&#27463;&#24615;&#22320;&#34987;&#35266;&#23519;&#30340;&#32422;&#26463;&#19979;&#25191;&#34892;DIL&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26497;&#31471;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#21482;&#26377;&#19968;&#20221;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21333;&#27425;DIL&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#29616;&#26377;&#30340;DIL&#26041;&#27861;&#22312;&#21333;&#27425;DIL&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#21508;&#31181;&#35843;&#26597;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#21333;&#27425;DIL&#30340;&#22256;&#38590;&#26159;&#30001;&#25209;&#24402;&#19968;&#21270;&#23618;&#20013;&#30340;&#32479;&#35745;&#25968;&#25454;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07536</link><description>&lt;p&gt;
BreakGPT: &#19968;&#31181;&#20855;&#26377;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07536
&lt;/p&gt;
&lt;p&gt;
BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#26131;&#21306;&#38388;&#31361;&#30772;&#65288;TRB&#65289;&#26159;&#37329;&#34701;&#20132;&#26131;&#25216;&#26415;&#20998;&#26512;&#20013;&#30340;&#19968;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32929;&#31080;&#12289;&#26399;&#36135;&#21644;&#22806;&#27719;&#31561;&#37329;&#34701;&#24066;&#22330;&#30340;&#20132;&#26131;&#32773;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#30495;&#20551;&#31361;&#30772;&#24182;&#25552;&#20379;&#27491;&#30830;&#30340;&#29702;&#30001;&#23545;&#25237;&#36164;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#39046;&#22495;&#30340;&#25928;&#26524;&#20173;&#19981;&#29702;&#24819;&#12290;&#21407;&#22240;&#22312;&#20110;&#31361;&#30772;&#26816;&#27979;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BreakGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;GPT-3.5&#30456;&#27604;&#65292;BreakGPT&#30340;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;44%&#65292;&#26377;&#21161;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the m
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.02233</link><description>&lt;p&gt;
&#29992;&#21307;&#23398;&#25945;&#31185;&#20070;&#22686;&#24378;&#40657;&#30418;LLMs&#36827;&#34892;&#20020;&#24202;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20986;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#32570;&#20047;&#29305;&#23450;&#12289;&#28145;&#20837;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;LLM-AMT&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#26597;&#35810;&#22686;&#24378;&#22120;&#12289;&#19968;&#20010;&#28151;&#21512;&#25945;&#31185;&#20070;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#30693;&#35782;&#33258;&#25105;&#23436;&#21892;&#12290;&#23427;&#20204;&#20849;&#21516;&#25972;&#21512;&#26435;&#23041;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;LLMs&#38405;&#35835;&#22120;&#26377;&#21161;&#20110;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMAMT&#26174;&#33879;&#25552;&#39640;&#20102;&#21709;&#24212;&#36136;&#37327;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.6%&#21040;16.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;GPT-4-Turbo&#20026;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18209</link><description>&lt;p&gt;
&#36229;&#21367;&#26354;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23545;&#40784;&#21644;&#22806;&#22771;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#33258;&#30417;&#30563;&#22270;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#23884;&#20837;&#34987;&#25490;&#21015;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#22914;&#22270;&#24418;&#65292;&#23637;&#29616;&#20102;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#25429;&#25417;&#23618;&#27425;&#25968;&#25454;&#19981;&#21464;&#24615;&#20449;&#24687;&#30340;&#23545;&#40784;&#24230;&#37327;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22343;&#21248;&#24230;&#37327;&#26469;&#38450;&#27490;&#25152;&#35859;&#30340;&#32500;&#24230;&#22604;&#38519;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#24517;&#39035;&#35299;&#20915;&#19982;&#26641;&#30340;&#23646;&#24615;&#30456;&#20851;&#30340;&#21494;&#23376;&#21644;&#39640;&#24230;&#23618;&#38754;&#30340;&#22343;&#21248;&#24615;&#65292;&#32780;&#22312;&#21452;&#26354;&#27969;&#24418;&#30340;&#29615;&#22659;&#31354;&#38388;&#20013;&#65292;&#36825;&#20123;&#27010;&#24565;&#36716;&#21270;&#20026;&#23545;&#21516;&#26500;&#24615;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08916</link><description>&lt;p&gt;
&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65306;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33041;&#30142;&#30149;&#30340;&#21457;&#30149;&#26426;&#21046;&#65292;&#21253;&#25324;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#65292;&#33041;&#32467;&#26500;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23558;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#26144;&#23556;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#26469;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;InnerGCN&#65289;&#27169;&#22359;&#65292;BGGAN&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#21033;&#29992;&#30452;&#25509;&#21644;&#38388;&#25509;&#33041;&#21306;&#22495;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Balancer&#30340;&#26032;&#27169;&#22359;&#26469;&#24179;&#34913;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;Balancer&#24341;&#20837;&#21040;BGGAN&#20013;&#65292;&#32467;&#26500;&#29983;&#25104;&#22120;&#21644;&#21151;&#33021;&#29983;&#25104;&#22120;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;AD&#20013;&#20934;&#30830;&#22320;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.14328</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#37325;&#35201;&#20027;&#39064;&#65292;&#21487;&#20197;&#24433;&#21709;&#21040;&#35832;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#33539;&#24335;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#36890;&#36807;&#20943;&#23567;&#27169;&#22411;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25512;&#21160;&#23398;&#20064;&#22120;&#25429;&#25417;&#24182;&#36924;&#36817;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#20844;&#24335;&#25104;&#21151;&#22320;&#24314;&#31435;&#20102;&#29983;&#25104;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#28982;&#32780;&#21364;&#26080;&#27861;&#28385;&#36275;&#20351;&#29992;&#32773;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#38656;&#27714;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31454;&#20105;&#24615;&#36873;&#25321;&#65292;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#30446;&#26631;&#26469;&#27880;&#20837;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#65292;&#22914;&#23545;&#25239;&#23398;&#20064;&#12289;&#25163;&#21160;&#35774;&#35745;&#35268;&#21017;&#21644;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13365</link><description>&lt;p&gt;
&#29992;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#23558;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#37327;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#29616;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#20195;&#12290;&#26080;&#35770;&#36825;&#20123;&#27169;&#22411;&#33258;&#36523;&#30340;&#23481;&#37327;&#21644;&#32467;&#26500;&#22914;&#20309;&#65292;&#37117;&#23384;&#22312;&#23545;LLMs&#20855;&#26377;&#26356;&#38271;&#26356;&#22797;&#26434;&#19978;&#19979;&#25991;&#30340;&#22686;&#24378;&#29702;&#35299;&#30340;&#38656;&#27714;&#65292;&#32780;&#27169;&#22411;&#36890;&#24120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20854;&#29702;&#35299;&#33021;&#21147;&#33539;&#22260;&#30340;&#21477;&#23376;&#24207;&#21015;&#26102;&#20250;&#36935;&#21040;&#19978;&#38480;&#65292;&#23548;&#33268;&#20135;&#29983;&#31163;&#39064;&#25110;&#28151;&#20081;&#30340;&#22238;&#31572;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20851;&#27880;&#8220;&#20026;&#20160;&#20040;&#27169;&#22411;&#26080;&#27861;&#33258;&#34892;&#24357;&#34917;&#25110;&#22686;&#24378;&#33258;&#24049;&#30340;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#24615;&#36136;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#26368;&#23567;&#21270;&#39069;&#22806;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#21033;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04996</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#22312;&#30452;&#25509;&#33829;&#38144;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28608;&#21457;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#26469;&#25552;&#21319;&#23458;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#21160;&#26426;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#65292;&#20844;&#21496;&#21487;&#20197;&#36890;&#36807;&#21521;&#23458;&#25143;&#25552;&#20379;&#30456;&#20851;&#37329;&#34701;&#25991;&#31456;&#26469;&#22521;&#20859;&#20851;&#31995;&#65292;&#20419;&#36827;&#23458;&#25143;&#21442;&#19982;&#21644;&#20419;&#36827;&#30693;&#24773;&#30340;&#37329;&#34701;&#20915;&#31574;&#12290;&#23613;&#31649;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#25913;&#36827;&#20869;&#23481;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#23478;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#19968;&#32452;&#23458;&#25143;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;XGBoost&#31639;&#27861;&#26469;&#21521;&#23458;&#25143;&#25512;&#33616;&#25991;&#31456;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20174;&#32467;&#26500;&#21270;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65289;&#29983;&#25104;&#30340;KG&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item></channel></rss>