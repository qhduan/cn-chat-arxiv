<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.10787</link><description>&lt;p&gt;
EdgeQAT: &#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65292;&#29992;&#20110;&#21152;&#36895;&#36731;&#37327;&#32423;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#65292;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#37327;&#21270;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#24555;&#36895;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;LLMs&#12290;&#28982;&#32780;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#23558;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#19968;&#36215;&#37327;&#21270;&#33267;8&#20301;&#20197;&#19979;&#26102;&#65292;&#36136;&#37327;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#24037;&#20316;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28608;&#27963;&#26410;&#34987;&#35302;&#21450;&#65292;&#36825;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#23545;&#36793;&#32536;&#31471;&#25512;&#29702;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#21363;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;QAT&#65292;&#29992;&#20110;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#37327;&#21270;&#24615;&#33021;&#19979;&#38477;&#20027;&#35201;&#28304;&#33258;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18531</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation via the Wasserstein Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18531
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#20449;&#24687;&#23553;&#35013;&#20026;&#26126;&#26174;&#26356;&#23567;&#30340;&#21512;&#25104;&#31561;&#20215;&#29289;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#22686;&#24378;DD&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Wasserstein&#37325;&#24515;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#37327;&#21270;&#20998;&#24067;&#24046;&#24322;&#21644;&#39640;&#25928;&#25429;&#33719;&#20998;&#24067;&#38598;&#21512;&#20013;&#24515;&#30340;&#20960;&#20309;&#24847;&#20041;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#20998;&#24067;&#21305;&#37197;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#32780;&#19988;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
&lt;/p&gt;</description></item></channel></rss>