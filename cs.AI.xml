<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16294</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Unlearning on Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#30830;&#20445;FL&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;FL&#28041;&#21450;&#21442;&#19982;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#38543;&#21518;&#23558;&#27169;&#22411;&#21457;&#24067;&#21040;&#21306;&#22359;&#38142;&#19978;&#65292;&#24418;&#25104;&#34920;&#31034;&#27169;&#22411;&#20851;&#31995;&#30340;&#31867;&#20284;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32487;&#25215;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;DAG&#30340;&#32467;&#26500;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#21644;&#24320;&#38144;&#36739;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#21464;&#33394;&#40857;&#21704;&#24076;&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#36951;&#24536;&#20219;&#21153;&#30340;&#35745;&#31639;&#21644;&#20849;&#35782;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BlockFUL&#25903;&#25345;&#21508;&#31181;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16294v1 Announce Type: cross  Abstract: Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conduc
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07632</link><description>&lt;p&gt;
&#36807;&#20110;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Overconfident and Unconfident AI Hinder Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07632
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#20250;&#38459;&#30861;&#20154;&#26426;&#21327;&#20316;&#65292;&#25259;&#38706;&#20449;&#24515;&#27700;&#24179;&#21644;&#25552;&#20379;&#21453;&#39304;&#26377;&#21161;&#20110;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#19981;&#19968;&#33268;&#65292;&#20294;&#21442;&#19982;&#32773;&#24448;&#24448;&#22240;&#27492;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#21327;&#20316;&#32467;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#20154;&#26426;&#21327;&#20316;&#22312;&#19987;&#19994;&#21644;&#26085;&#24120;&#22330;&#26223;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#21327;&#20316;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34920;&#36798;&#20854;&#23545;&#33258;&#24049;&#34920;&#29616;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#32570;&#20047;&#33258;&#20449;&#65292;&#21363;&#20854;&#34920;&#36798;&#30340;&#20449;&#24515;&#39640;&#20110;&#25110;&#20302;&#20110;&#20854;&#23454;&#38469;&#34920;&#29616;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#20204;&#38169;&#35823;&#22320;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#33258;&#20449;&#23545;&#20154;&#31867;&#20449;&#20219;&#12289;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#21644;&#21327;&#20316;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25259;&#38706;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#24515;&#27700;&#24179;&#21644;&#34920;&#29616;&#21453;&#39304;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35748;&#35782;&#20154;&#24037;&#26234;&#33021;&#20449;&#24515;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#21442;&#19982;&#32773;&#24448;&#24448;&#20250;&#22240;&#20026;&#23519;&#35273;&#21040;&#36825;&#31181;&#19981;&#19968;&#33268;&#32780;&#19981;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#23548;&#33268;&#25298;&#32477;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#24182;&#19988;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26356;&#23481;&#26131;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#24182;&#25509;&#21463;&#20854;&#24314;&#35758;&#65292;&#20174;&#32780;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) advances, human-AI collaboration has become increasingly prevalent across both professional and everyday settings. In such collaboration, AI can express its confidence level about its performance, serving as a crucial indicator for humans to evaluate AI's suggestions. However, AI may exhibit overconfidence or underconfidence--its expressed confidence is higher or lower than its actual performance--which may lead humans to mistakenly evaluate AI advice. Our study investigates the influences of AI's overconfidence and underconfidence on human trust, their acceptance of AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI confidence levels and performance feedback facilitates better recognition of AI confidence misalignments. However, participants tend to withhold their trust as perceiving such misalignments, leading to a rejection of AI suggestions and subsequently poorer performance in collaborative tasks. Conversely, without su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22235;&#31181;&#20808;&#36827;&#30340;LLMs&#22312;9&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#21644;&#29702;&#35299;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#37096;&#32626;LLMs&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.00689</link><description>&lt;p&gt;
&#20598;&#23572;&#23433;&#20840;&#65306;&#20195;&#30721;&#29983;&#25104;&#36741;&#21161;&#24037;&#20855;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ocassionally Secure: A Comparative Analysis of Code Generation Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22235;&#31181;&#20808;&#36827;&#30340;LLMs&#22312;9&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#21644;&#29702;&#35299;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#37096;&#32626;LLMs&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20195;&#30721;&#29983;&#25104;&#23601;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#26377;&#33021;&#21147;&#29983;&#25104;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#20294;&#25991;&#29486;&#27809;&#26377;&#32771;&#34385;&#21040;&#20160;&#20040;&#22240;&#32032;&#26377;&#21161;&#20110;&#29983;&#25104;&#23433;&#20840;&#21644;&#26377;&#25928;&#30340;&#20195;&#30721;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#26159;&#30830;&#23450;&#21644;&#29702;&#35299;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;LLMs&#33021;&#22815;&#26377;&#25928;&#21644;&#23433;&#20840;&#22320;&#37096;&#32626;&#26469;&#29983;&#25104;&#20248;&#36136;&#20195;&#30721;&#30340;&#26465;&#20214;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#8212;&#8212;&#20351;&#29992;ChatGPT&#21644;Bard&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20197;&#21450;&#26469;&#33258;Google&#30340;Gemini&#8212;&#8212;&#20351;&#29992;9&#20010;&#29420;&#31435;&#20219;&#21153;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#32622;&#20110;&#19968;&#20010;&#20856;&#22411;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#65292;&#20195;&#34920;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#24037;&#20316;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23433;&#20840;&#24847;&#35782;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#20004;&#20010;&#19981;&#21516;&#29256;&#26412;&#30340;&#24037;&#20855;&#26469;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our develop
&lt;/p&gt;</description></item><item><title>Query2Triple&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#26469;&#32534;&#30721;&#21644;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11246</link><description>&lt;p&gt;
Query2Triple: &#32479;&#19968;&#26597;&#35810;&#32534;&#30721;&#20197;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#19978;&#22810;&#26679;&#22797;&#26434;&#26597;&#35810;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs. (arXiv:2310.11246v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11246
&lt;/p&gt;
&lt;p&gt;
Query2Triple&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#26469;&#32534;&#30721;&#21644;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#12290;&#30001;&#20110;KG&#30340;&#19981;&#23436;&#25972;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#65288;QE&#65289;&#26041;&#27861;&#65292;&#23558;&#26597;&#35810;&#21644;&#23454;&#20307;&#32534;&#30721;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#31526;&#35270;&#20026;&#31070;&#32463;&#38598;&#21512;&#36816;&#31639;&#31526;&#65292;&#20197;&#33719;&#24471;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21516;&#26102;&#23545;&#31616;&#21333;&#65288;&#19968;&#36339;&#65289;&#21644;&#22797;&#26434;&#65288;&#22810;&#36339;&#21644;&#36923;&#36753;&#65289;&#26597;&#35810;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20250;&#23548;&#33268;&#31616;&#21333;&#26597;&#35810;&#24615;&#33021;&#30340;&#19979;&#38477;&#21644;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Query to Triple&#65288;Q2T&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#12290;Q2T&#23558;&#35757;&#32451;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#22312;&#31616;&#21333;&#26597;&#35810;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#65292;&#20197;&#22522;&#20110;&#22836;&#23454;&#20307;&#21644;&#20851;&#31995;&#39044;&#27979;&#23614;&#23454;&#20307;&#12290;&#65288;2&#65289;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#35757;&#32451;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#23558;&#22810;&#26679;&#30340;&#22797;&#26434;&#26597;&#35810;&#32534;&#30721;&#20026;&#32479;&#19968;&#30340;&#19977;&#20803;&#32452;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16025</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65306;&#20174;&#40657;&#30418;&#21040;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#33719;&#21462;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#23398;&#20064;&#20174;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;highD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#19982;&#24403;&#21069;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable an
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.01449</link><description>&lt;p&gt;
&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#35266;&#27979;&#25968;&#25454;&#32467;&#21512;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#36890;&#24120;&#30001;&#20110;&#26080;&#27861;&#27979;&#35797;&#30340;&#20551;&#35774;&#32780;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36739;&#36731;&#30340;&#20551;&#35774;&#19979;&#27979;&#35797;&#22806;&#37096;&#25928;&#24230;&#21644;&#21487;&#24573;&#35270;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;&#24403;&#21482;&#26377;&#19968;&#20010;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#24378;&#35843;&#20102;&#20934;&#30830;&#35782;&#21035;&#36829;&#21453;&#30340;&#20551;&#35774;&#23545;&#19968;&#33268;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13721</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#24341;&#23548;&#38382;&#31572;&#30340;&#25345;&#32493;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36866;&#24212;&#26032;&#26381;&#21153;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#26032;&#26381;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#30340;&#26381;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#37325;&#26500;&#20026;&#19968;&#32452;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#20174;&#32780;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#29305;&#23450;&#26381;&#21153;&#30340;&#35760;&#24518;&#36127;&#25285;&#65292;&#24182;&#25945;&#20250;&#27169;&#22411;&#23558;&#25152;&#32473;&#38382;&#39064;&#21644;&#31034;&#20363;&#29992;&#20110;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#21482;&#26377;6000&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20174;&#26816;&#32034;&#22120;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#33719;&#24471;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user's goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.04474</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#30528;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174; mutual information &#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#21040;&#36127;&#26679;&#26412;&#30340;&#20114;&#20449;&#24687;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#26377;&#30410;&#24433;&#21709;&#21644;&#26377;&#23475;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.13723</link><description>&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#39592;&#24178;&#23398;&#20064;&#22810;&#20010;&#30446;&#26631;&#12290;&#19982;&#21333;&#29420;&#35757;&#32451;&#20219;&#21153;&#30456;&#27604;&#65292;MTL&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#26469;&#28508;&#22312;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#20854;&#20013;&#65292;MTL&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#38598;&#20013;&#22312;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#25512;&#23548;&#20986;&#23545;&#25152;&#26377;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#32456;&#26799;&#24230;&#19979;&#38477;&#26041;&#21521;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#32780;&#19981;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#35757;&#32451;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#25439;&#22833;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
&lt;/p&gt;</description></item></channel></rss>