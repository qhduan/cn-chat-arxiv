<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11954</link><description>&lt;p&gt;
Spikingformer: &#22522;&#20110;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network. (arXiv:2304.11954v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#30001;&#20110;&#20854;&#20107;&#20214;&#39537;&#21160;&#30340;&#33033;&#20914;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33410;&#33021;&#26367;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;Spikformer&#21644;SEW ResNet&#22312;&#20869;&#30340;&#26368;&#26032;&#28145;&#24230;SNN&#23384;&#22312;&#38750;&#33033;&#20914;&#35745;&#31639;&#65288;&#25972;&#25968;&#28014;&#28857;&#20056;&#27861;&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#27531;&#24046;&#36830;&#25509;&#32467;&#26500;&#25152;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;&#38750;&#33033;&#20914;&#35745;&#31639;&#22686;&#21152;&#20102;SNN&#30340;&#21151;&#32791;&#65292;&#24182;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#21482;&#25903;&#25345;&#33033;&#20914;&#25805;&#20316;&#30340;&#20027;&#27969;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20010;&#27531;&#24046;&#35774;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Spikingformer&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;ImageNet&#12289;CIFAR10&#12289;CIFAR100&#12289;CIFAR10-DVS&#21644;DVS128 Gesture&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Spikingformer&#65292;&#24182;&#34920;&#26126;&#20316;&#20026;&#20808;&#36827;&#39592;&#24178;&#30340;&#30452;&#25509;&#35757;&#32451;&#30340;&#32431;SNN&#65292;Spikingformer&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;(75.85$\%$ top-1 accuracy on ImageNet)&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, state-of-the-art deep SNNs (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connection. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware, which only supports spike operations. In this paper, we propose a hardware-friendly spike-driven residual learning architecture for SNNs to avoid non-spike computations. Based on this residual design, we develop Spikingformer, a pure transformer-based spiking neural network. We evaluate Spikingformer on ImageNet, CIFAR10, CIFAR100, CIFAR10-DVS and DVS128 Gesture datasets, and demonstrate that Spikingformer outperforms the state-of-the-art in directly trained pure SNNs as a novel advanced backbone (75.85$\
&lt;/p&gt;</description></item></channel></rss>