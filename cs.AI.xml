<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;</title><link>https://arxiv.org/abs/2403.13501</link><description>&lt;p&gt;
VSTAR&#65306;&#29992;&#20110;&#29983;&#25104;&#38271;&#21160;&#24577;&#35270;&#39057;&#21512;&#25104;&#30340;&#26102;&#38388;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24320;&#28304;&#30340;T2V&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#21644;&#19981;&#26029;&#36827;&#21270;&#20869;&#23481;&#30340;&#36739;&#38271;&#35270;&#39057;&#12290;&#23427;&#20204;&#24448;&#24448;&#21512;&#25104;&#20934;&#38745;&#24577;&#35270;&#39057;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#25552;&#31034;&#20013;&#28041;&#21450;&#30340;&#24517;&#35201;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#23454;&#29616;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;&#21512;&#25104;&#24448;&#24448;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21363;&#26102;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GTN&#26041;&#27861;&#65292;&#21517;&#20026;VSTAR&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;1&#65289;&#35270;&#39057;&#26775;&#27010;&#25552;&#31034;&#65288;VSP&#65289;-&#22522;&#20110;&#21407;&#22987;&#21333;&#20010;&#25552;&#31034;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#65292;&#21033;&#29992;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#25991;&#26412;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13501v1 Announce Type: cross  Abstract: Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to differe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04343</link><description>&lt;p&gt;
CoTBal: &#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 &#20844;&#21578;&#31867;&#22411;: &#26032;   &#25688;&#35201;: &#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#26159;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#20851;&#38190;&#35757;&#32451;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26080;&#24046;&#21035;&#28151;&#21512;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#30340;&#26222;&#36941;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#25351;&#20196;&#26684;&#24335;&#21644;&#30693;&#35782;&#39046;&#22495;&#19981;&#21516;&#32780;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#65288;CoTBal&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;LMMs&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#25506;&#32034;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#24179;&#34913;&#30340;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;:&#65288;1&#65289;&#20219;&#21153;&#38388;&#36129;&#29486;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#21487;&#33021;&#22686;&#24378;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#24402;&#22240;&#20110;&#37325;&#21472;&#30340;&#30693;&#35782;&#39046;&#22495;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#20869;&#38590;&#24230;&#65292;&#25351;&#30340;&#26159;&#21333;&#20010;&#20219;&#21153;&#20869;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#36890;&#36807;&#29992;&#22522;&#20110;&#24615;&#33021;&#30340;&#26041;&#27861;&#37327;&#21270;&#36825;&#20004;&#20010;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 Announce Type: new  Abstract: Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based me
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.01909</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#22522;&#20110;&#35821;&#20041;&#23545;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36880;&#20687;&#32032;&#26631;&#35760;&#22270;&#20687;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#20013;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#39318;&#27425;&#32508;&#21512;&#21644;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01909v1 Announce Type: cross  Abstract: Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#27010;&#24565;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#20123;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#20026;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#37326;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.07684</link><description>&lt;p&gt;
&#36879;&#36807;&#28040;&#24687;&#20256;&#36882;&#30340;&#35270;&#35282;&#30475;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#36074;&#24615;&#19982;&#26550;&#26500;&#35774;&#35745;&#30340;&#20849;&#21516;&#35270;&#37326;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#27010;&#24565;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#20123;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#20026;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#37326;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#37096;&#20998;&#30340;&#36229;&#22270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#37117;&#26159;&#36890;&#36807;&#20174;&#22270;&#30340;&#31867;&#27604;&#20013;&#25552;&#21319;&#36807;&#26469;&#30340;&#65292;&#24573;&#30053;&#20102;&#36229;&#22270;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#23581;&#35797;&#35299;&#20915;&#19968;&#20123;&#30456;&#20851;&#30340;&#38382;&#39064;&#65306;Q1 &#21516;&#36074;&#24615;&#22312;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#21542;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65311;Q2 &#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#32454;&#33268;&#22788;&#29702;&#39640;&#38454;&#32593;&#32476;&#30340;&#29305;&#24449;&#26469;&#25913;&#21892;&#24403;&#21069;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65311;Q3 &#29616;&#26377;&#25968;&#25454;&#38598;&#26159;&#21542;&#23545;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#22522;&#20934;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#39640;&#38454;&#32593;&#32476;&#21516;&#36074;&#24615;&#30340;&#26032;&#27010;&#24565;&#21270;&#65292;&#32479;&#19968;&#20102;&#39640;&#38454;&#32593;&#32476;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#39640;&#38454;&#32467;&#26500;&#30340;&#19968;&#20123;&#33258;&#28982;&#20294;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#30340;&#31574;&#30053;&#65292;&#27604;&#22914;&#20445;&#30041;&#36229;&#36793;&#20381;&#36182;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#25110;&#26159;&#20197;&#33410;&#28857;&#21644;&#36229;&#36793;&#20849;&#21516;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, leading to overshadowing specific characteristics of hypergraphs. This paper attempts to confront some pending questions in that regard: Q1 Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HNNs)? Q2 Is there room for improving current HNN architectures by carefully addressing specific characteristics of higher-order networks? Q3 Do existing datasets provide a meaningful benchmark for HNNs? To address them, we first introduce a novel conceptualization of homophily in higher-order networks based on a Message Passing (MP) scheme, unifying both the analytical examination and the modeling of higher-order networks. Further, we investigate some natural, yet mostly unexplored, strategies for processing higher-order structures within HNNs such as keeping hyperedge-dependent node representations, or per
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13883</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain-Independent Dynamic Programming. (arXiv:2401.13883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#33539;&#20363;&#22914;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010; (MIP) &#21644;&#32422;&#26463;&#35268;&#21010; (CP) &#26088;&#22312;&#35299;&#32806;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#36807;&#31243;&#65292;&#36825;&#26159;&#22768;&#26126;&#24615;&#38382;&#39064;&#27714;&#35299;&#30340;&#8220;&#22307;&#26479;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#65288;DIDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010; (DP) &#30340;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;DP&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#36890;&#24120;&#23427;&#34987;&#20316;&#20026;&#19968;&#31181;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328; (DyPDL)&#65292;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;AI&#35268;&#21010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#29992;&#26469;&#27714;&#35299;DyPDL&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19971;&#31181;DIDP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;11&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#31867;&#21035;&#30340;&#22522;&#20934;&#23454;&#20363;&#19978;&#65292;&#23558;&#25105;&#20204;&#30340;DIDP&#27714;&#35299;&#22120;&#19982;&#21830;&#19994;MIP&#21644;CP&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65288;&#20998;&#21035;&#27714;&#35299;MIP&#21644;CP&#27169;&#22411;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;DIDP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20248;&#20110;MIP&#65292;&#20063;&#20248;&#20110;CP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and 
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11256</link><description>&lt;p&gt;
&#22312;&#27714;&#35299;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24724;&#31639;&#27861;&#22312;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#26631;&#20934;&#22411;&#28216;&#25103;&#21644;&#25193;&#23637;&#22411;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#32771;&#34385;&#20102;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#25910;&#25947;&#30340;&#26080;&#24724;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#26368;&#26377;&#21517;&#30340;&#20004;&#20010;&#31639;&#27861;&#26159;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#12290;&#28982;&#32780;&#65292;OGDA&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;OMWU&#20855;&#26377;&#36739;&#20302;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20294;&#23454;&#39564;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#23427;&#30340;&#25910;&#25947;&#20165;&#22312;&#32435;&#20160;&#22343;&#34913;&#21807;&#19968;&#26102;&#25104;&#31435;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#29992;&#20110;MWU&#65292;&#23427;&#28040;&#38500;&#20102;&#21807;&#19968;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#19982;OMWU&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;RT&#30340;&#31639;&#27861;&#22312;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#19979;&#34920;&#29616;&#19981;&#22914;OGDA&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21453;&#39304;&#20551;&#35774;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;RT&#26694;&#26550;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02000</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#34920;&#31034;&#21040;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#21512;&#31070;&#32463;&#34920;&#31034;&#19982;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#21487;&#33021;&#20351;&#31526;&#21495;&#24605;&#32500;&#20174;&#26412;&#36136;&#19978;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#36880;&#28176;&#20174;&#36890;&#36807;&#30693;&#35273;&#21644;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#31526;&#21495;&#26500;&#24314;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#65288;TDL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;EM&#31639;&#27861;&#23398;&#20064;&#25968;&#25454;&#30340;&#36807;&#28193;&#34920;&#31034;&#65292;&#23558;&#36755;&#20837;&#30340;&#39640;&#32500;&#35270;&#35273;&#37096;&#20998;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#32452;&#24352;&#37327;&#20316;&#20026;&#31070;&#32463;&#21464;&#37327;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#35270;&#20026;&#21512;&#20316;&#21338;&#24328;&#26469;&#23454;&#29616;&#26694;&#26550;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#35859;&#35789;&#65292;&#24182;&#36890;&#36807;RL&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#20197;&#34701;&#20837;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;</title><link>http://arxiv.org/abs/2301.12351</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies in Causality and Deep Generative Models: A Survey. (arXiv:2301.12351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20102;&#35299;&#21644;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65288;DGP&#65289;&#30340;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#32780;&#22240;&#26524;&#24615;&#21017;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#39537;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#26426;&#21046;&#65292;&#24182;&#31361;&#26174;&#20102;&#36825;&#20123;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#22240;&#26524;&#25928;&#24212;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#22240;&#26524;&#24615;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21364;&#38754;&#20020;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24847;&#35782;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;DGM&#30340;&#20132;&#27719;&#28857;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#24615;&#21407;&#21017;&#22312;DGM&#20013;&#30340;&#25972;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DGM&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22240;&#26524;&#24615;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#29983;&#25104;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26041;&#27861;&#35770;&#65292;&#31361;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2208.06648</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#23384;&#22312;&#19979;&#30340;&#22635;&#34917;&#31574;&#30053;&#65306;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#25552;&#20986;&#65292;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#20869;&#23481;&#20063;&#20250;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20559;&#35265;&#24050;&#32463;&#22312;&#21307;&#30103;&#21382;&#21490;&#19978;&#30041;&#19979;&#20102;&#28145;&#28145;&#30340;&#28889;&#21360;&#65292;&#23548;&#33268;&#36793;&#32536;&#21270;&#32676;&#20307;&#21463;&#21040;&#19981;&#24179;&#31561;&#30340;&#25252;&#29702;&#12290;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#36890;&#24120;&#21453;&#26144;&#20102;&#36825;&#20123;&#32676;&#20307;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#29305;&#23450;&#32676;&#20307;&#32570;&#22833;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#23613;&#31649;&#20854;&#28508;&#22312;&#24433;&#21709;&#24040;&#22823;&#65292;&#20294;&#22635;&#34917;&#24448;&#24448;&#34987;&#24573;&#35270;&#20026;&#19968;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#32780;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#37325;&#24314;&#35823;&#24046;&#30340;&#20943;&#23569;&#21644;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#24573;&#30053;&#20102;&#22635;&#34917;&#22914;&#20309;&#23545;&#19981;&#21516;&#32676;&#20307;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning risks reinforcing biases present in data, and, as we argue in this work, in what is absent from data. In healthcare, biases have marked medical history, leading to unequal care affecting marginalised groups. Patterns in missing data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is often an overlooked preprocessing step, with attention placed on the reduction of reconstruction error and overall performance, ignoring how imputation can affect groups differently. Our work studies how imputation choices affect reconstruction errors across groups and algorithmic fairness properties of downstream predictions.
&lt;/p&gt;</description></item></channel></rss>