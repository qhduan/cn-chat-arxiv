<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#20998;&#26512;&#23458;&#35266;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16902</link><description>&lt;p&gt;
&#20174;&#26597;&#35810;&#24037;&#20855;&#21040;&#22240;&#26524;&#26550;&#26500;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#32423;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data. (arXiv:2306.16902v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#20998;&#26512;&#23458;&#35266;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#12289;&#31185;&#23398;&#21644;&#27861;&#24459;&#31561;&#22810;&#20010;&#37325;&#35201;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#22312;&#27010;&#24565;&#38388;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26368;&#36817;&#23545;LLM&#22312;&#21508;&#31181;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#30340;&#30740;&#31350;&#24050;&#32463;&#20026;&#32463;&#20856;&#30340;&#19977;&#38454;&#27573;&#22240;&#26524;&#26694;&#26550;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#38454;&#26799;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#23558;&#22522;&#20110;&#30693;&#35782;&#30340;LLM&#22240;&#26524;&#20998;&#26512;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#25512;&#36827;&#20102;&#30446;&#21069;&#22522;&#20110;LLM&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20351;LLM&#19981;&#21482;&#26159;&#19968;&#20010;&#26597;&#35810;&#24037;&#20855;&#65292;&#20805;&#20998;&#21033;&#29992;&#20854;&#22312;&#21457;&#29616;&#33258;&#28982;&#21644;&#26032;&#30340;&#22240;&#26524;&#23450;&#24459;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;LLM&#23545;&#29616;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#23453;&#36149;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#23458;&#35266;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#36890;&#29992;&#30340;&#25552;&#31034;&#65292;&#26088;&#22312;&#20174;&#32473;&#23450;&#21464;&#37327;&#20013;&#25552;&#21462;&#22240;&#26524;&#22270;&#65292;&#24182;&#35780;&#20272;LLM&#20043;&#21069;&#22240;&#26524;&#24615;&#23545;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exhibit exceptional abilities for causal analysis between concepts in numerous societally impactful domains, including medicine, science, and law. Recent research on LLM performance in various causal discovery and inference tasks has given rise to a new ladder in the classical three-stage framework of causality. In this paper, we advance the current research of LLM-driven causal discovery by proposing a novel framework that combines knowledge-based LLM causal analysis with data-driven causal structure learning. To make LLM more than a query tool and to leverage its power in discovering natural and new laws of causality, we integrate the valuable LLM expertise on existing causal mechanisms into statistical analysis of objective data to build a novel and practical baseline for causal structure learning.  We introduce a universal set of prompts designed to extract causal graphs from given variables and assess the influence of LLM prior causality on recovering 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#22240;&#26524;&#22270;&#24674;&#22797;&#31639;&#27861;&#12289;&#23454;&#38469;&#24212;&#29992;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10032</link><description>&lt;p&gt;
&#22240;&#26524;&#25506;&#32034;&#32508;&#36848;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;&#65288;arXiv:2305.10032v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Discovery: Theory and Practice. (arXiv:2305.10032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#22240;&#26524;&#22270;&#24674;&#22797;&#31639;&#27861;&#12289;&#23454;&#38469;&#24212;&#29992;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25511;&#21046;&#29616;&#35937;&#30340;&#35268;&#24459;&#26159;&#31185;&#23398;&#36827;&#27493;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20197;&#22240;&#26524;&#26041;&#24335;&#24314;&#27169;&#19981;&#21516;&#26041;&#38754;&#30340;&#30456;&#20114;&#20316;&#29992;&#20026;&#30446;&#26631;&#26102;&#65292;&#36825;&#19968;&#28857;&#26356;&#20026;&#37325;&#35201;&#12290;&#20107;&#23454;&#19978;&#65292;&#22240;&#26524;&#25512;&#26029;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37327;&#21270;&#23548;&#33268;&#20854;&#25928;&#24212;&#30340;&#22522;&#26412;&#20851;&#31995;&#12290;&#22240;&#26524;&#21457;&#29616;&#26159;&#26356;&#24191;&#27867;&#30340;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#22312;&#20854;&#20013;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#22270;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25506;&#35752;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#35774;&#32622;&#19979;&#24050;&#24320;&#21457;&#31639;&#27861;&#30340;&#19968;&#33268;&#27010;&#36848;&#65292;&#25253;&#21578;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#21644;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#24212;&#29992;&#20197;&#29702;&#35299;&#36825;&#20123;&#26041;&#27861;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#24471;&#21040;&#20016;&#23500;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs is recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.
&lt;/p&gt;</description></item></channel></rss>