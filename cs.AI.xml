<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14459</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Explanations for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25200;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;LIME&#21644;SHAP&#65292;&#36890;&#24120;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#20851;&#27880;&#23427;&#20204;&#22914;&#20309;&#25193;&#23637;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#21644;&#38271;&#25991;&#26412;&#36755;&#20837;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#24402;&#22240;&#31639;&#27861;&#23454;&#20363;&#21270;&#12290;&#20026;&#20102;&#22788;&#29702;&#25991;&#26412;&#36755;&#20986;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#23454;&#25968;&#30340;&#26631;&#37327;&#21270;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#32423;&#26041;&#27861;&#65292;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#27169;&#22411;&#26597;&#35810;&#32447;&#24615;&#32553;&#25918;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#25200;&#21160;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#29992;&#20110;&#25688;&#35201;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#38382;&#31572;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#26356;&#21152;&#36148;&#36817;&#26412;&#22320;&#30340;&#29983;&#25104;&#24335;&#36755;&#20986;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14459v1 Announce Type: cross  Abstract: Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#26041;&#27861;LSVCR&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#32852;&#21512;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;</title><link>https://arxiv.org/abs/2403.13574</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#26041;&#27861;LSVCR&#65292;&#36890;&#36807;&#34701;&#21512;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#32852;&#21512;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#19978;&#65292;&#38405;&#35835;&#25110;&#25776;&#20889;&#26377;&#36259;&#35270;&#39057;&#30340;&#35780;&#35770;&#24050;&#32463;&#25104;&#20026;&#35270;&#39057;&#35266;&#30475;&#20307;&#39564;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35270;&#39057;&#25512;&#33616;&#31995;&#32479;&#20027;&#35201;&#23545;&#29992;&#25143;&#19982;&#35270;&#39057;&#30340;&#20132;&#20114;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#32570;&#20047;&#23545;&#35780;&#35770;&#22312;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#20013;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LSVCR&#30340;&#26032;&#39062;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#19982;&#35270;&#39057;&#21644;&#35780;&#35770;&#30340;&#20132;&#20114;&#21382;&#21490;&#65292;&#20849;&#21516;&#36827;&#34892;&#20010;&#24615;&#21270;&#35270;&#39057;&#21644;&#35780;&#35770;&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#21363;&#24207;&#21015;&#25512;&#33616;&#65288;SR&#65289;&#27169;&#22411;&#21644;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#33616;&#22120;&#12290;SR&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#25512;&#33616;&#39592;&#24178;&#65288;&#22312;&#37096;&#32626;&#20013;&#20445;&#30041;&#65289;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#20316;&#20026;&#19968;&#20010;&#34917;&#20805;&#32452;&#20214;&#65288;&#22312;&#37096;&#32626;&#20013;&#20002;&#24323;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#28508;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13574v1 Announce Type: new  Abstract: In online video platforms, reading or writing comments on interesting videos has become an essential part of the video watching experience. However, existing video recommender systems mainly model users' interaction behaviors with videos, lacking consideration of comments in user behavior modeling. In this paper, we propose a novel recommendation approach called LSVCR by leveraging user interaction histories with both videos and comments, so as to jointly conduct personalized video and comment recommendation. Specifically, our approach consists of two key components, namely sequential recommendation (SR) model and supplemental large language model (LLM) recommender. The SR model serves as the primary recommendation backbone (retained in deployment) of our approach, allowing for efficient user preference modeling. Meanwhile, we leverage the LLM recommender as a supplemental component (discarded in deployment) to better capture underlying 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15225</link><description>&lt;p&gt;
&#20174;DDMs&#21040;DNNs&#65306;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23478;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#23478;&#24050;&#32463;&#35748;&#35782;&#21040;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;&#20363;&#22914;&#65292;&#20915;&#31574;&#25152;&#38656;&#30340;&#26102;&#38388;&#21487;&#20197;&#25581;&#31034;&#19968;&#20010;&#20010;&#20307;&#30495;&#27491;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20915;&#31574;&#26412;&#36523;&#12290;&#31867;&#20284;&#22320;&#65292;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#65292;&#22914;&#30524;&#21160;&#25110;&#31070;&#32463;&#35760;&#24405;&#65292;&#21253;&#21547;&#20102;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#27809;&#26377;&#36798;&#25104;&#20915;&#31574;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24212;&#26356;&#21152;&#20851;&#27880;&#20915;&#31574;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#20197;&#21450;&#22914;&#20309;&#34701;&#20837;&#30456;&#20851;&#30340;&#36807;&#31243;&#25968;&#25454;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35748;&#20026;&#20915;&#31574;&#26159;&#20174;&#26434;&#38899;&#32047;&#31215;&#30340;&#35777;&#25454;&#20013;&#20135;&#29983;&#30340;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#32463;&#27982;&#23398;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agents true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.05437</link><description>&lt;p&gt;
ACMP: Allen-Cahn&#20449;&#24687;&#20256;&#36882;&#29992;&#20110;&#24102;&#26377;&#29289;&#36136;&#30456;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle Phase Transition. (arXiv:2206.05437v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACMP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#65292;&#20811;&#26381;&#20102;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#26159;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#21333;&#20803;&#65292;&#32771;&#34385;&#20174;&#19968;&#23618;&#21040;&#19979;&#19968;&#23618;&#30340;&#32593;&#32476;&#20256;&#25773;&#20013;&#30340;&#30456;&#37051;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25490;&#26021;&#21147;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#26469;&#24314;&#27169;&#36825;&#31181;&#36807;&#31243;&#65292;&#24182;&#22312;&#30456;&#21464;&#24314;&#27169;&#20013;&#24341;&#20837;Allen-Cahn&#21147;&#12290;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26159;&#19968;&#31181;&#21453;&#24212;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#31890;&#23376;&#20998;&#31163;&#32780;&#19981;&#20250;&#25193;&#25955;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#31181;Allen-Cahn&#20449;&#24687;&#20256;&#36882;(ACMP)&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#31890;&#23376;&#31995;&#32479;&#35299;&#30340;&#25968;&#20540;&#36845;&#20195;&#26500;&#25104;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#12290;ACMP&#20855;&#26377;&#31616;&#21333;&#30340;&#23454;&#29616;&#21644;&#31070;&#32463;ODE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#23558;&#32593;&#32476;&#28145;&#24230;&#25512;&#21040;100&#23618;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;Dirichlet&#33021;&#37327;&#20005;&#26684;&#27491;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#27169;&#22411;&#30340;GNN&#65292;&#36991;&#20813;&#20102;&#24120;&#35265;&#30340;GNN&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20351;&#29992;ACMP&#30340;GNN&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#23454;&#38469;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#21305;&#37197;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node class
&lt;/p&gt;</description></item></channel></rss>