<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2404.02454</link><description>&lt;p&gt;
&#34913;&#37327;&#36951;&#24536;&#31574;&#30053;&#30340;&#25512;&#29702;&#24378;&#24230;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Techniques for Measuring the Inferential Strength of Forgetting Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#21407;&#29702;&#35770;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;Problog&#24037;&#20855;&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#36951;&#24536;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#19988;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#30340;&#36951;&#24536;&#31574;&#30053;&#25110;&#19981;&#21516;&#36951;&#24536;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#21407;&#29702;&#35770;&#30340;&#25512;&#29702;&#24378;&#24230;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#26681;&#25454;&#27169;&#22411;&#35745;&#25968;&#21644;&#27010;&#29575;&#29702;&#35770;&#30340;&#30452;&#35273;&#23450;&#20041;&#29992;&#20110;&#34913;&#37327;&#25512;&#29702;&#24378;&#24230;&#21464;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#30740;&#31350;&#20102;&#27492;&#31867;&#25439;&#22833;&#24230;&#37327;&#30340;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#30693;&#35782;&#24037;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#20351;&#29992;Problog&#35745;&#31639;&#25439;&#22833;&#24230;&#37327;&#12290;&#35770;&#25991;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#21644;&#30830;&#23450;&#19981;&#21516;&#36951;&#24536;&#31574;&#30053;&#24378;&#24230;&#30340;&#24037;&#20316;&#26041;&#27861;&#65292;&#20197;&#21450;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;Problog&#24212;&#29992;&#29702;&#35770;&#32467;&#26524;&#30340;&#20855;&#20307;&#31034;&#20363;&#12290;&#34429;&#28982;&#37325;&#28857;&#26159;&#36951;&#24536;&#65292;&#20294;&#32467;&#26524;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#24212;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02454v1 Announce Type: new  Abstract: The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog. Although the focus is on forgetting, the results are much more general and should have wider applicati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;</title><link>https://arxiv.org/abs/2403.18569</link><description>&lt;p&gt;
PDNNet&#65306;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28304;&#20379;&#24212;&#32593;&#32476;&#65288;PDN&#65289;&#19978;&#30340;IR&#25481;&#30005;&#19982;PDN&#30340;&#37197;&#32622;&#21644;&#30005;&#27969;&#28040;&#32791;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#21160;&#24577;IR&#25481;&#30005;&#20223;&#30495;&#21464;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IR&#25481;&#30005;&#39044;&#27979;&#34987;&#25506;&#32034;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#32771;&#34385;&#19981;&#20165;&#22914;&#20309;&#27491;&#30830;&#34920;&#31034;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#36824;&#32771;&#34385;&#22914;&#20309;&#22312;&#29305;&#24449;&#32858;&#21512;&#36807;&#31243;&#20013;&#27169;&#25311;IR&#25481;&#30005;&#36981;&#24490;&#20854;&#29289;&#29702;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#65292;PDNGraph&#65292;&#32479;&#19968;&#20102;PDN&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#21333;&#20803;-PDN&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;&#65292;PDNNet&#65292;&#23558;&#20004;&#20010;&#24182;&#34892;&#30340;GNN-CNN&#20998;&#25903;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21033;&#20110;&#25429;&#25417;&#19978;&#36848;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12562</link><description>&lt;p&gt;
&#36890;&#36807;&#33719;&#21462;&#36171;&#26435;&#65306;&#25903;&#25345;&#23567;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Equity through Access: A Case for Small-scale Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24471;&#30410;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#35745;&#31639;&#21147;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#36164;&#28304;&#34987;&#29992;&#20110;&#35757;&#32451;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#12289;&#25968;&#25454;&#12289;&#33021;&#28304;&#21644;&#30899;&#25490;&#25918;&#26041;&#38754;&#28040;&#32791;&#24040;&#22823;&#12290;&#36825;&#20123;&#25104;&#26412;&#27491;&#22312;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#26032;&#22411;&#20934;&#20837;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22312;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#36164;&#28304;&#26377;&#38480;&#30340;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#35270;&#20102;&#29616;&#26377;&#35270;&#35273;&#20219;&#21153;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;DL&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#24615;&#33021;&#19982;&#36164;&#28304;&#21333;&#20803;&#30340;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;PePR&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#65288;&#36328;&#24230;&#20174;1M&#21040;130M&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65289;&#21644;&#19977;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#26377;&#20851;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20851;&#31995;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06601</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#24418;&#21464;&#25442;&#20013;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#35299;&#20915;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#20851;&#31995;&#39044;&#27979;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24456;&#38590;&#25214;&#21040;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#25968;&#25454;&#31232;&#30095;&#24615;&#38656;&#35201;&#24314;&#31435;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1) &#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37319;&#26679;&#26368;&#20339;&#25968;&#37327;&#30340;&#30446;&#26631;&#20851;&#31995;(&#36793;&#32536;)&#65292;(2) &#19968;&#31181;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#40784;&#19981;&#21516;&#39046;&#22495;&#30340;&#29305;&#24449;&#65292;&#21644;(3) &#19968;&#31181;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20108;&#32500;&#36755;&#20837;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#19977;&#32500;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#19979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06601v1 Announce Type: cross  Abstract: Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data. We demonstrate our method's utility in cross-domain and cross-dimension 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05152</link><description>&lt;p&gt;
&#26397;&#21521;&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Towards a Psychology of Machines: Large Language Models Predict Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#32570;&#20047;&#20154;&#31867;&#35748;&#30693;&#22522;&#30784;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#31616;&#21333;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#25552;&#20379;&#20851;&#20110;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#27934;&#35265;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25991;&#26412;&#29702;&#35299;&#29702;&#35770;&#65292;&#25105;&#20204;&#20551;&#35774;&#35782;&#21035;&#27169;&#26865;&#20004;&#21487;&#30340;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#22240;&#20026;&#27604;&#23572;&#21917;&#37202;&#65292;&#25152;&#20197;&#37202;&#20174;&#26410;&#30041;&#22312;&#25151;&#23376;&#37324;&#8221;&#65289;&#22312;&#21069;&#38754;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20250;&#24471;&#21040;&#20419;&#36827;&#12290;&#21442;&#19982;&#32773;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;ChatGPT&#65292;&#37117;&#34987;&#21576;&#29616;&#25104;&#23545;&#30340;&#21477;&#23376;&#12290;&#31532;&#20108;&#20010;&#21477;&#23376;&#24635;&#26159;&#19968;&#20010;&#26088;&#22312;&#22266;&#26377;&#22320;&#27169;&#26865;&#20004;&#21487;&#30340;&#33457;&#22253;&#36335;&#24452;&#21477;&#65292;&#32780;&#31532;&#19968;&#20010;&#21477;&#23376;&#21017;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#24739;&#26377;&#24930;&#24615;&#37202;&#31934;&#20013;&#27602;&#8221;&#65289;&#25110;&#19981;&#21512;&#36866;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#21916;&#27426;&#25171;&#39640;&#23572;&#22827;&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18908</link><description>&lt;p&gt;
&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Facility Location Games with Scaling Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18908
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20154;&#25104;&#26412;&#20989;&#25968;&#31561;&#20110;&#20182;&#20204;&#36317;&#31163;&#35774;&#26045;&#30340;&#36317;&#31163;&#20056;&#20197;&#19968;&#20010;&#30001;&#35774;&#26045;&#20301;&#32622;&#30830;&#23450;&#30340;&#27604;&#20363;&#22240;&#23376;&#12290;&#38500;&#20102;&#19968;&#33324;&#31867;&#21035;&#30340;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#30340;&#27604;&#20363;&#20989;&#25968;&#30340;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#30340;&#30446;&#26631;&#65292;&#24182;&#25551;&#36848;&#20102;&#26368;&#20248;&#35299;&#30340;&#35745;&#31639;&#12290;&#28982;&#21518;&#25105;&#20204;&#36716;&#21521;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#65292;&#35266;&#23519;&#21040;&#20195;&#29702;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#20877;&#26159;&#21333;&#23792;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#30830;&#20445;&#20195;&#29702;&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#27604;&#20363;&#20989;&#25968;&#26465;&#20214;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#33021;&#22815;&#36890;&#36807;strategyproof&#21644;anonymous me&#36798;&#21040;&#30340;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18908v1 Announce Type: cross  Abstract: We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous me
&lt;/p&gt;</description></item><item><title>DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;</title><link>https://arxiv.org/abs/2402.14123</link><description>&lt;p&gt;
DeiSAM&#65306;&#36890;&#36807;&#25351;&#31034;&#25552;&#31034;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
DeiSAM: Segment Anything with Deictic Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14123
&lt;/p&gt;
&lt;p&gt;
DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#38646;-shot&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#20855;&#20307;&#23545;&#35937;&#65292;&#20154;&#31867;&#26412;&#33021;&#22320;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#24615;&#25551;&#36848;&#65292;&#21363;&#26681;&#25454;&#19978;&#19979;&#25991;&#25351;&#31216;&#26576;&#29289;&#65292;&#27604;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#24182;&#22312;&#26479;&#23376;&#21518;&#38754;&#30340;&#29289;&#20307;&#8221;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#32570;&#20047;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#36825;&#31181;&#25351;&#31034;&#24615;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeiSAM&#8212;&#8212;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#12290;&#32473;&#23450;&#22797;&#26434;&#30340;&#25991;&#26412;&#20998;&#21106;&#25551;&#36848;&#65292;DeiSAM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#36827;&#34892;&#21487;&#21306;&#20998;&#30340;&#21069;&#21521;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;DeiSAM&#36890;&#36807;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
&lt;/p&gt;</description></item><item><title>PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11534</link><description>&lt;p&gt;
PreAct: &#22312; ReAct &#20013;&#39044;&#27979;&#26410;&#26469;&#22686;&#24378;&#26234;&#33021;&#20307;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11534
&lt;/p&gt;
&lt;p&gt;
PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#39044;&#27979;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#24120;&#24120;&#26377;&#21161;&#20110;&#20010;&#20307;&#25299;&#23637;&#24605;&#32500;&#36807;&#31243;&#24182;&#36827;&#34892;&#21453;&#24605;&#65292;&#20174;&#32780;&#20419;&#36827;&#26397;&#27491;&#30830;&#26041;&#21521;&#25512;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; PreAct &#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#39044;&#27979;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#23548;&#21521;&#30340;&#25512;&#29702;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PreAct &#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#20248;&#20110; ReAct &#26041;&#27861;&#65292;&#24182;&#19988;&#24403;&#19982;&#21453;&#24605;&#26041;&#27861;&#32467;&#21512;&#26102;&#65292;PreAct &#30340;&#25928;&#26524;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#21382;&#21490;&#39044;&#27979;&#23545;LLM&#35268;&#21010;&#26377;&#25345;&#32493;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#12290;&#23454;&#39564;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#65292;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#20026;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#25552;&#20379;&#20102;&#30452;&#35266;&#26131;&#25026;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.10101</link><description>&lt;p&gt;
&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#20197;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems. (arXiv:2401.10101v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#27010;&#29575;&#22270;&#27169;&#22411;&#36827;&#34892;&#36870;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#12290;&#23454;&#39564;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#65292;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#20026;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#25552;&#20379;&#20102;&#30452;&#35266;&#26131;&#25026;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21644;&#36870;&#21521;&#25512;&#29702;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#26032;&#20852;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#25512;&#26029;&#20986;&#20551;&#35774;&#24773;&#26223;&#12290;&#22312;&#23454;&#39564;&#25968;&#25454;&#36890;&#24120;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#65292;&#36825;&#23588;&#20854;&#26377;&#29992;&#12290;&#22312;&#29615;&#22659;&#21644;&#29983;&#24577;&#31185;&#23398;&#39046;&#22495;&#65292;&#22240;&#26524;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#29983;&#24577;&#31995;&#32479;&#23545;&#20551;&#35774;&#24178;&#39044;&#30340;&#21709;&#24212;&#12290;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#24615;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#30452;&#35266;&#30340;&#29305;&#24615;&#65292;&#22810;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#26597;&#35810;&#65292;&#31216;&#20026;&#19981;&#21487;&#36777;&#35782;&#30340;&#26597;&#35810;&#65292;&#26080;&#27861;&#20197;&#31934;&#30830;&#30340;&#26041;&#24335;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#24212;&#29992;&#19968;&#31181;&#26032;&#39062;&#12289;&#26368;&#36817;&#30340;&#25216;&#26415;&#26469;&#30028;&#23450;&#31038;&#20250;&#29983;&#24577;&#31995;&#32479;&#39046;&#22495;&#20869;&#30340;&#19981;&#21487;&#36777;&#35782;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal and counterfactual reasoning are emerging directions in data science that allow us to reason about hypothetical scenarios. This is particularly useful in domains where experimental data are usually not available. In the context of environmental and ecological sciences, causality enables us, for example, to predict how an ecosystem would respond to hypothetical interventions. A structural causal model is a class of probabilistic graphical models for causality, which, due to its intuitive nature, can be easily understood by experts in multiple fields. However, certain queries, called unidentifiable, cannot be calculated in an exact and precise manner. This paper proposes applying a novel and recent technique for bounding unidentifiable queries within the domain of socioecological systems. Our findings indicate that traditional statistical analysis, including probabilistic graphical models, can identify the influence between variables. However, such methods do not offer insights in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01951</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#33021;&#21542;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#25163;&#37096;&#22270;&#20687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36798;&#21313;&#24180;&#20043;&#20037;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19968;&#30452;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#25163;&#21644;&#25163;&#25351;&#20013;&#25152;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#12290;&#34429;&#28982;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#25351;&#21521;&#20102;&#24213;&#23618;&#32467;&#26500;&#30340;&#26681;&#26412;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#20854;&#20013;&#21253;&#21547;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#65292;&#26469;&#23637;&#31034;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
&lt;/p&gt;</description></item><item><title>ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;</title><link>http://arxiv.org/abs/2401.00741</link><description>&lt;p&gt;
ToolEyes&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00741
&lt;/p&gt;
&lt;p&gt;
ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36873;&#25321;&#30340;&#24037;&#20855;&#19982;&#26399;&#26395;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#26223;&#65292;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#31572;&#26696;&#21487;&#20197;&#20107;&#20808;&#30830;&#23450;&#65292;&#19982;&#30495;&#23454;&#38656;&#27714;&#32972;&#36947;&#32780;&#39536;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#32467;&#26524;&#24573;&#35270;&#20102;LLMs&#26377;&#25928;&#21033;&#29992;&#24037;&#20855;&#25152;&#38656;&#30340;&#22797;&#26434;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolEyes&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;LLMs&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#35814;&#32454;&#20998;&#26512;&#20102;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#65292;&#20998;&#26512;&#20102;&#23545;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20116;&#20010;&#32500;&#24230;&#65306;&#26684;&#24335;&#23545;&#40784;&#65292;&#24847;&#22270;&#29702;&#35299;&#65292;&#34892;&#20026;&#35268;&#21010;&#65292;&#24037;&#20855;&#36873;&#25321;&#21644;&#31572;&#26696;&#32452;&#32455;&#12290;&#27492;&#22806;&#65292;ToolEyes&#36824;&#21253;&#21547;&#19968;&#20010;&#25317;&#26377;&#32422;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#65292;&#20316;&#20026;LLMs&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#22312;&#28041;&#21450;&#19977;&#20010;&#31867;&#21035;&#30340;&#21313;&#20010;LLMs&#30340;&#35780;&#20272;&#20013;&#65292;ToolEyes&#21462;&#24471;&#20102;&#22914;&#19979;&#30340;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined, diverging from genuine needs. Furthermore, a sole emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories revea
&lt;/p&gt;</description></item><item><title>RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16382</link><description>&lt;p&gt;
RLLTE&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#38271;&#26399;&#28436;&#36827;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16382
&lt;/p&gt;
&lt;p&gt;
RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RLLTE&#65306;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#19982;&#24212;&#29992;&#26694;&#26550;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#27969;&#30340;&#31639;&#27861;&#23454;&#29616;&#20043;&#22806;&#65292;RLLTE&#36824;&#20316;&#20026;&#19968;&#20010;&#31639;&#27861;&#24320;&#21457;&#24037;&#20855;&#21253;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RLLTE&#23436;&#20840;&#35299;&#32806;&#20102;RL&#31639;&#27861;&#19982;&#24320;&#21457;&#31639;&#27861;&#30340;&#23454;&#36341;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#32452;&#20214;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#28436;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;RLLTE&#26159;&#31532;&#19968;&#20010;&#26500;&#24314;&#20102;&#23436;&#25972;&#20016;&#23500;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;RL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#37096;&#32626;&#12289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24515;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#30340;&#21103;&#39550;&#39542;&#12290;&#39044;&#26399;RLLTE&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#65292;&#24182;&#23545;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#20855;&#26377;&#39640;&#24230;&#21050;&#28608;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15513</link><description>&lt;p&gt;
&#35843;&#25972;&#22256;&#24785;&#24230;&#24182;&#35745;&#31639;&#22522;&#20110;&#37319;&#26679;&#30340;t-SNE&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#24120;&#29992;&#30340;&#31649;&#36947;&#21033;&#29992;&#20108;&#32500;&#21487;&#35270;&#21270;&#65292;&#20363;&#22914;&#36890;&#36807;t&#20998;&#24067;&#37051;&#36817;&#38543;&#26426;&#23884;&#20837;&#65288;t-SNE&#65289;&#12290;&#20294;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#20250;&#29983;&#25104;&#27425;&#20248;&#30340;&#23884;&#20837;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#19981;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#12290;&#23558;&#36825;&#20123;&#21442;&#25968;&#22686;&#21152;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35745;&#31639;&#23545;&#20110;&#23454;&#38469;&#24037;&#20316;&#27969;&#31243;&#26469;&#35828;&#22826;&#26114;&#36149;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#37319;&#26679;&#30340;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24517;&#39035;&#35880;&#24910;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#21462;&#20915;&#20110;&#37319;&#26679;&#29575;&#21644;&#39044;&#26399;&#30340;&#26368;&#32456;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09832</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36793;&#32536;&#36164;&#28304;&#20219;&#21153;&#37096;&#32626;&#21644;&#25193;&#23637;&#26041;&#27861;&#29992;&#20110;&#36710;&#36733;&#32593;&#32476;&#26381;&#21153;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning. (arXiv:2305.09832v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#36710;&#32852;&#32593;&#8221;&#27491;&#22788;&#20110;&#25105;&#20204;&#31038;&#20250;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#29992;&#20110;&#25552;&#20379;&#36710;&#36742;&#36890;&#32852;&#32593;&#65288;C-V2N&#65289;&#26381;&#21153;&#65292;&#35299;&#20915;&#26381;&#21153;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32852;&#21512;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#20010;&#38382;&#39064;&#30340;&#32852;&#25509;&#26041;&#24335;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#36138;&#24515;&#31639;&#27861;&#30340;&#20851;&#20110;&#20219;&#21153;&#37096;&#32626;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110; Deep Deterministic Policy Gradient (DDPG) &#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25193;&#23637;&#20195;&#29702;&#19982;&#22810;&#20010;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#25193;&#23637;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.04180</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#35299;&#32806;&#24378;&#21270;&#23398;&#20064;&#21644;&#21521;&#37327;&#22810;&#26679;&#24615;&#65292;&#19968;&#23567;&#26102;&#20869;&#35757;&#32451;&#36866;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#30340;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity. (arXiv:2305.04180v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#35299;&#20915;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DRL&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Color&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30001;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the Local Path Planning (LPP) problem. However, such application in the real world is immensely limited due to the deficient efficiency and generalization capability of DRL. To alleviate these two issues, a solution named Color is proposed, which consists of an Actor-Sharer-Learner (ASL) training framework and a mobile robot-oriented simulator Sparrow. Specifically, the ASL framework, intending to improve the efficiency of the DRL algorithm, employs a Vectorized Data Collection (VDC) mode to expedite data acquisition, decouples the data collection from model optimization by multithreading, and partially connects the two procedures by harnessing a Time Feedback Mechanism (TFM) to evade data underuse or overuse. Meanwhile, the Sparrow simulator utilizes a 2D grid-based world, simplified kinematics, and conversion-free data flow to achieve a lightweight design. The lightness facilitates vectorized diversity, allowing di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2301.07330</link><description>&lt;p&gt;
FPANet: &#22522;&#20110;&#39057;&#29575;&#30340;&#35270;&#39057;&#21435;&#33707;&#23572;&#32441;&#25216;&#26415;&#65292;&#20351;&#29992;&#24103;&#32423;&#21518;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment. (arXiv:2301.07330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#32593;&#26684;&#27169;&#24335;&#20043;&#38388;&#30340;&#24178;&#25200;&#20250;&#23548;&#33268;&#33707;&#23572;&#32441;&#65292;&#20174;&#32780;&#38477;&#20302;&#26222;&#36890;&#25968;&#30721;&#30456;&#26426;&#25429;&#25417;&#25968;&#23383;&#26174;&#31034;&#23631;&#30340;&#22270;&#20687;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#23398;&#20064;&#39057;&#29575;&#21644;&#31354;&#38388;&#22495;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#20351;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#65292;&#23398;&#20064;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#24182;&#36755;&#20986;&#26356;&#22909;&#36136;&#37327;&#30340;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interference between overlapping gird patterns creates moire patterns, degrading the visual quality of an image that captures a screen of a digital display device by an ordinary digital camera. Removing such moire patterns is challenging due to their complex patterns of diverse sizes and color distortions. Existing approaches mainly focus on filtering out in the spatial domain, failing to remove a large-scale moire pattern. In this paper, we propose a novel model called FPANet that learns filters in both frequency and spatial domains, improving the restoration quality by removing various sizes of moire patterns. To further enhance, our model takes multiple consecutive frames, learning to extract frame-invariant content features and outputting better quality temporally consistent images. We demonstrate the effectiveness of our proposed method with a publicly available large-scale dataset, observing that ours outperforms the state-of-the-art approaches, including ESDNet, VDmoire, MBCNN, 
&lt;/p&gt;</description></item></channel></rss>