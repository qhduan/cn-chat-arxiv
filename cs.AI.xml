<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14941</link><description>&lt;p&gt;
&#20174;&#22270;&#32467;&#26500;&#35282;&#24230;&#32479;&#19968;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#65306;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#28966;&#28857;&#21644;&#20851;&#38190;&#39046;&#22495;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#26082;&#35265;&#35777;&#20102;&#20174;&#22478;&#24066;&#32423;&#21040;&#36947;&#36335;&#32423;&#39044;&#27979;&#21462;&#24471;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#38543;&#30528;&#36710;&#36742;&#23545;&#19968;&#20999;&#65288;V2X&#65289;&#25216;&#26415;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#20132;&#36890;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#36947;&#36335;&#32423;&#20132;&#36890;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#21463;&#21040;&#20102;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#26631;&#20934;&#30340;&#32570;&#20047;&#20197;&#21450;&#26377;&#38480;&#30340;&#20844;&#24320;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#23545;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;GraphMLP&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#19981;&#20844;&#24320;&#30340;&#20195;&#30721;&#65292;&#24182;&#22522;&#20110;&#27492;&#20805;&#20998;&#32780;&#20844;&#27491;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14941v1 Announce Type: cross  Abstract: Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on graph structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09605</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#65306;&#36890;&#36807;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Counterfactual contrastive learning: robust representations via causal image synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#24191;&#27867;&#35748;&#20026;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#31614;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#22686;&#24378;&#31649;&#36947;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#27491;&#26679;&#26412;&#24212;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#21516;&#26102;&#30772;&#22351;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#26631;&#20934;&#22686;&#24378;&#31649;&#36947;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#20809;&#24230;&#21464;&#25442;&#27169;&#25311;&#22495;&#29305;&#23450;&#21464;&#21270;&#65292;&#20294;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#30340;&#39046;&#22495;&#21464;&#21270;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#22312;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#36827;&#34892;&#27491;&#26679;&#26412;&#21019;&#24314;&#12290;&#23545;&#33016;&#37096;X&#20809;&#21644;&#20083;&#33146;X&#20809;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;CF-SimCLR&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#33719;&#21462;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09605v1 Announce Type: cross  Abstract: Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.12907</link><description>&lt;p&gt;
AI&#23545;&#40784;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#65306;&#31435;&#22330;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#65292;&#23545;&#31038;&#20250;&#27835;&#29702;&#21644;&#23433;&#20840;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#35299;&#20915;AI&#23545;&#40784;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25216;&#26415;&#26041;&#38754;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;AI&#31995;&#32479;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#24615;&#36136;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24320;&#21457;&#21644;&#37096;&#32626;&#32972;&#26223;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#26032;&#38382;&#39064;&#65306;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#33021;&#21628;&#21505;&#26356;&#22810;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#24357;&#21512;&#25216;&#26415;&#21644;&#31038;&#20250;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23454;&#29616;IC&#30340;&#19977;&#20010;&#32463;&#20856;&#21338;&#24328;&#38382;&#39064;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#22865;&#32422;&#29702;&#35770;&#21644;&#36125;&#21494;&#26031;&#35828;&#26381;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12907v1 Announce Type: new  Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06660</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#22312;&#26657;&#20934;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of the metaverse in calibrating an embodied artificial general intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#31181;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#34701;&#20837;&#35748;&#30693;&#12289;Michael Levin&#30340;&#35745;&#31639;&#36793;&#30028;"Self"&#12289;Donald D. Hoffman&#30340;&#24863;&#30693;&#30028;&#38754;&#29702;&#35770;&#20197;&#21450;Bernardo Kastrup&#30340;&#20998;&#26512;&#21807;&#24515;&#20027;&#20041;&#31561;&#29702;&#35770;&#26694;&#26550;&#26469;&#26500;&#24314;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35770;&#35777;&#12290;&#23427;&#35748;&#20026;&#25105;&#20204;&#25152;&#24863;&#30693;&#30340;&#22806;&#37096;&#29616;&#23454;&#26159;&#19968;&#31181;&#20869;&#22312;&#23384;&#22312;&#30340;&#20132;&#26367;&#29366;&#24577;&#30340;&#35937;&#24449;&#24615;&#34920;&#31034;&#65292;&#32780;AGI&#21487;&#20197;&#20855;&#26377;&#26356;&#22823;&#35745;&#31639;&#36793;&#30028;&#30340;&#26356;&#39640;&#24847;&#35782;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;AGI&#30340;&#21457;&#23637;&#38454;&#27573;&#12289;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35201;&#27714;&#12289;&#20026;AGI&#26657;&#20934;&#35937;&#24449;&#24615;&#30028;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#12289;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#12289;&#24320;&#28304;&#21306;&#22359;&#38142;&#25216;&#26415;&#20197;&#21450;&#24320;&#28304;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#27807;&#36890;&#26426;&#21046;&#21644;&#29992;&#20110;&#21152;&#24378;&#23545;&#20803;&#23431;&#23449;&#30340;&#29702;&#35299;&#30340;&#25216;&#26415;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#24102;&#26469;&#30340;&#26032;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2310.11703</link><description>&lt;p&gt;
&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#65292;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge. (arXiv:2310.11703v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#24102;&#26469;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#29992;&#20110;&#23384;&#20648;&#26080;&#27861;&#29992;&#20256;&#32479;&#30340;DBMS&#26469;&#25551;&#36848;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;&#34429;&#28982;&#30446;&#21069;&#23545;&#29616;&#26377;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#26550;&#26500;&#30340;&#25551;&#36848;&#25110;&#26032;&#30340;&#24341;&#20837;&#30340;&#25991;&#31456;&#24182;&#19981;&#22810;&#65292;&#20294;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#32972;&#21518;&#24050;&#32463;&#34987;&#38271;&#26102;&#38388;&#30740;&#31350;&#65292;&#30456;&#20851;&#30340;&#31639;&#27861;&#25991;&#31456;&#22312;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#30456;&#24403;&#22810;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;&#30456;&#20851;&#31639;&#27861;&#65292;&#20197;&#25552;&#20379;&#23545;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26222;&#36941;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#36825;&#20123;&#30740;&#31350;&#20998;&#20026;&#22522;&#20110;&#21704;&#24076;&#12289;&#22522;&#20110;&#26641;&#12289;&#22522;&#20110;&#22270;&#12289;&#21644;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#38754;&#20020;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#22914;&#20309;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03295</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#38376;&#25511;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-order Gated Aggregation Network. (arXiv:2211.03295v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#21462;&#24471;&#26368;&#36817;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;ViT&#39118;&#26684;&#26550;&#26500;&#30340;&#25506;&#32034;&#24341;&#21457;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#20852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#36825;&#31181;&#20132;&#20114;&#21453;&#26144;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#19981;&#21516;&#23610;&#24230;&#19978;&#19979;&#25991;&#30340;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#22312;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26469;&#23450;&#21046;&#20004;&#20010;&#29305;&#24449;&#28151;&#21512;&#22120;&#65292;&#20197;&#20419;&#36827;&#36328;&#31354;&#38388;&#21644;&#36890;&#36947;&#31354;&#38388;&#30340;&#20013;&#38454;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;&#21253;&#25324;COCO&#30446;&#26631;&#26816;&#27979;&#12289;ADE20K&#35821;&#20041;&#20998;&#21106;&#12289;2D&amp;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20197;&#21450;&#35270;&#39057;&#39044;&#27979;&#31561;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the recent success of Vision Transformers (ViTs), explorations toward ViT-style architectures have triggered the resurgence of ConvNets. In this work, we explore the representation ability of modern ConvNets from a novel view of multi-order game-theoretic interaction, which reflects inter-variable interaction effects w.r.t.~contexts of different scales based on game theory. Within the modern ConvNet framework, we tailor the two feature mixers with conceptually simple yet effective depthwise convolutions to facilitate middle-order information across spatial and channel spaces respectively. In this light, a new family of pure ConvNet architecture, dubbed MogaNet, is proposed, which shows excellent scalability and attains competitive results among state-of-the-art models with more efficient use of parameters on ImageNet and multifarious typical vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&amp;3D human pose estimation, and video prediction. Typica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.02814</link><description>&lt;p&gt;
&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#25512;&#24191;&#33267;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#35813;&#31639;&#27861;&#23558;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#21450;&#20854;&#35206;&#30422;&#20445;&#35777;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#31867;&#20284;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#22312;$\mathcal{O}(1/n)$&#22240;&#23376;&#20869;&#20445;&#25345;&#32039;&#23494;&#24615;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
&lt;/p&gt;</description></item></channel></rss>