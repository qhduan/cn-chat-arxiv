<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.00081</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Molecular Generative Adversarial Network with Multi-Property Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;$de~novo$&#20998;&#23376;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#65292;&#26469;&#22788;&#29702;GANs&#20013;&#20998;&#23376;&#34920;&#31034;&#30340;&#31163;&#25955;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GANs&#21644;RL&#27169;&#22411;&#30340;&#22266;&#26377;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;MCTS&#37319;&#26679;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;MCTS RL-based GANs&#38590;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#21270;&#23398;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24102;&#21363;&#26102;&#21644;&#20840;&#23616;&#22870;&#21169;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#30340;&#26032;&#22411;GAN&#65292;&#31216;&#20026;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#34987;&#21033;&#29992;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InstGAN&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;</title><link>https://arxiv.org/abs/2403.01489</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#28335;&#28304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22522;&#20110;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#21487;&#33021;&#34987;&#28389;&#29992;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#12290;&#32473;&#23450;&#19968;&#20010;&#24453;&#24402;&#22240;&#30340;&#27979;&#35797;&#22270;&#20687;&#65292;&#39318;&#20808;&#25105;&#20204;&#21453;&#21521;&#37325;&#24314;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#37325;&#24314;&#30340;&#25552;&#31034;&#25918;&#20837;&#19981;&#21516;&#30340;&#20505;&#36873;&#27169;&#22411;&#20013;&#20197;&#20877;&#29616;&#20505;&#36873;&#20551;&#22270;&#20687;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#25490;&#21517;&#27979;&#35797;&#22270;&#20687;&#19982;&#20505;&#36873;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22270;&#20687;&#30340;&#26469;&#28304;&#12290;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#20854;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38480;&#21046;&#20505;&#36873;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01489v1 Announce Type: cross  Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image genera
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24322;&#36136;&#29305;&#24449;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#31038;&#21306;&#38887;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.01661</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#65306;&#22522;&#20110;&#30456;&#20114;&#20851;&#32852;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features. (arXiv:2311.01661v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24322;&#36136;&#29305;&#24449;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#31038;&#21306;&#38887;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38887;&#24615;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#22810;&#32500;&#30340;&#29616;&#35937;&#65292;&#20854;&#28304;&#20110;&#19981;&#21516;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#20854;&#38887;&#24615;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#31038;&#21306;&#38887;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33030;&#24369;&#24615;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#20854;&#23545;&#20110;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20869;&#37096;&#30340;&#24322;&#36136;&#29305;&#24449;&#21450;&#20854;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;&#38887;&#24615;&#30340;&#31283;&#20581;&#24615;&#12289;&#20887;&#20313;&#24615;&#21644;&#36164;&#28304;&#24615;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65288;&#31216;&#20026;Resili-Net&#65289;&#12290;&#22312;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65288;&#21363;&#35774;&#26045;&#12289;&#22522;&#30784;&#35774;&#26045;&#21644;&#31038;&#20250;&#65289;&#20013;&#65292;&#30830;&#23450;&#21644;&#35745;&#31639;&#20102;&#20855;&#20307;&#30340;12&#20010;&#21487;&#27979;&#37327;&#30340;&#38887;&#24615;&#29305;&#24449;&#65292;&#19982;&#38887;&#24615;&#30340;&#31283;&#20581;&#24615;&#12289;&#20887;&#20313;&#24615;&#21644;&#36164;&#28304;&#24615;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#30456;&#20851;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#32654;&#22269;&#22823;&#37117;&#24066;&#32479;&#35745;&#21306;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United S
&lt;/p&gt;</description></item><item><title>GameGPT&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#21644;&#35299;&#32806;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08067</link><description>&lt;p&gt;
GameGPT: &#28216;&#25103;&#24320;&#21457;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GameGPT: Multi-agent Collaborative Framework for Game Development. (arXiv:2310.08067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08067
&lt;/p&gt;
&lt;p&gt;
GameGPT&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#21644;&#35299;&#32806;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#24320;&#21457;&#20013;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26234;&#33021;&#20307;&#23637;&#31034;&#20102;&#23427;&#20204;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#28216;&#25103;&#24320;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GameGPT&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#24320;&#21457;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25351;&#20986;&#24187;&#35273;&#26159;&#37096;&#32626;LLM&#22312;&#29983;&#20135;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36824;&#23384;&#22312;&#21478;&#19968;&#20010;&#38382;&#39064;&#65306;&#20887;&#20313;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#21452;&#37325;&#21327;&#20316;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20869;&#37096;&#35789;&#24211;&#65292;&#20197;&#38477;&#20302;&#35268;&#21010;&#12289;&#20219;&#21153;&#35782;&#21035;&#21644;&#23454;&#26045;&#38454;&#27573;&#30340;&#24187;&#35273;&#21644;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#20195;&#30721;&#29983;&#25104;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.07917</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#26410;&#26469;&#36235;&#21183;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#26816;&#27979;&#32597;&#35265;&#20107;&#20214;&#19968;&#30452;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25913;&#36827;&#25968;&#25454;&#22788;&#29702;&#21644;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#38750;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#35770;&#25991;&#25910;&#38598;&#21644;&#23457;&#26597;&#20102;258&#31687;&#26469;&#33258;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#30340;&#21516;&#34892;&#35780;&#23457;&#35770;&#25991;&#65292;&#26088;&#22312;&#20174;&#25216;&#26415;&#21644;&#24212;&#29992;&#35282;&#24230;&#28145;&#20837;&#23457;&#26597;&#38750;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#20026;&#22312;&#23398;&#26415;&#30028;&#25110;&#24037;&#19994;&#30028;&#24076;&#26395;&#28145;&#20837;&#23398;&#20064;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#32508;&#36848;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item></channel></rss>