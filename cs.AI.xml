<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18599</link><description>&lt;p&gt;
Meta-Tasks: &#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Meta-Tasks: An alternative view on Meta-Learning Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#35757;&#32451;&#21644;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#22806;&#23618;&#24490;&#29615;&#20043;&#21069;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#23545;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#65292;&#23558;&#20854;&#20316;&#20026;&#8220;&#20803;&#20219;&#21153;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#39062;&#21644;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25910;&#25947;&#26356;&#24555;&#12289;&#26356;&#22909;&#65292;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#20934;&#24046;&#26356;&#20302;&#65292;&#34920;&#26126;&#20854;&#22312;FSL&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#21407;&#22411;&#32593;&#32476;&#39640;&#20986;3.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18599v1 Announce Type: cross  Abstract: Few-shot learning (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using unsupervised techniques as ``meta-tasks''. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item></channel></rss>