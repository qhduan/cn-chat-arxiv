<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.05306</link><description>&lt;p&gt;
Sym-Q&#65306;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#36827;&#34892;&#33258;&#36866;&#24212;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05306
&lt;/p&gt;
&lt;p&gt;
Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20855;&#26377;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#25968;&#23398;&#21644;&#29289;&#29702;&#20851;&#31995;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#24403;&#36755;&#20986;&#34920;&#36798;&#24335;&#19981;&#36275;&#20197;&#36866;&#24212;&#23454;&#39564;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#36866;&#24212;&#25110;&#20462;&#25913;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#32570;&#20047;&#28789;&#27963;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#20851;&#31995;&#26041;&#38754;&#12290;&#21463;&#21040;&#20154;&#31867;&#19987;&#23478;&#22914;&#20309;&#25913;&#36827;&#21644;&#35843;&#25972;&#34920;&#36798;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;Symbolic Q-network&#65288;Sym-Q&#65289;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;Sym-Q&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#24182;&#26681;&#25454;&#22870;&#21169;&#20449;&#21495;&#26469;&#25913;&#36827;&#34920;&#36798;&#24335;&#65292;&#22870;&#21169;&#20449;&#21495;&#25351;&#31034;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#12290;&#23427;&#29420;&#29305;&#30340;&#33021;&#21147;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexi
&lt;/p&gt;</description></item><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13835</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#27169;&#22411;&#21644;&#20154;&#31867;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#26657;&#20934;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#21363;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21644;&#20256;&#36798;&#23427;&#20204;&#30340;&#39044;&#27979;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;LLM&#20869;&#37096;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#26159;LLM&#33021;&#22815;&#22914;&#20309;&#23558;&#36825;&#31181;&#20869;&#37096;&#27169;&#22411;&#32622;&#20449;&#24230;&#20256;&#36798;&#32473;&#20154;&#31867;&#29992;&#25143;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#23545;LLM&#21709;&#24212;&#30340;&#22806;&#37096;&#32622;&#20449;&#24230;&#19982;&#27169;&#22411;&#20869;&#37096;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#20154;&#31867;&#29992;&#25143;&#35782;&#21035;LLM&#36755;&#20986;&#21487;&#20449;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#35780;&#20272;&#29992;&#25143;&#23545;&#30495;&#23454;LLM&#32622;&#20449;&#24230;&#30340;&#24863;&#30693;&#21644;&#65288;2&#65289;&#35843;&#26597;&#20010;&#24615;&#21270;&#35299;&#37322;&#23545;&#35813;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#40664;&#35748;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20462;&#25913;&#35299;&#37322;&#30340;&#26041;&#24335;&#21487;&#20197;&#20943;&#23567;&#36825;&#31181;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19347</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#36827;&#34892;&#23545;&#25239;&#35299;&#32806;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#25991;&#31456;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#12290;&#19982;&#20043;&#21069;&#30340;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BART&#65292;T5&#65289;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#21046;&#36896;&#24858;&#34850;&#38169;&#35823;&#26041;&#38754;&#36739;&#23569;&#65292;&#20294;&#21046;&#36896;&#20102;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#21152;&#20837;&#22240;&#26524;&#20851;&#31995;&#12289;&#28155;&#21152;&#38169;&#35823;&#32454;&#33410;&#21644;&#36807;&#24230;&#27867;&#21270;&#31561;&#12290;&#36825;&#20123;&#24187;&#35273;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#36825;&#32473;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35299;&#32806;&#26041;&#27861;&#26469;&#20998;&#31163;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65288;DECENT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#65292;&#20197;&#24357;&#34917;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLMs&#23545;&#20110;&#20462;&#39280;&#21644;&#29702;&#35299;&#30340;&#27010;&#24565;&#26356;&#21152;&#28165;&#26224;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11876</link><description>&lt;p&gt;
&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#32423;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20043;&#38388;&#30340;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#65292;&#22240;&#27492;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65288;&#20854;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#26159;&#20998;&#21106;&#65289;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#29978;&#33267;&#19981;&#22914;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JCL&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#22312;&#19968;&#38454;&#27573;&#20869;&#23545;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290; &#65288;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#32771;&#34385;&#29305;&#24449;&#32423;&#21035;&#12289;&#22270;&#20687;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#25237;&#24433;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03388</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22810;&#26680;&#22270;&#23398;&#20064;&#30340;&#33258;&#38381;&#30151;&#39044;&#27979;&#19982;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery. (arXiv:2303.03388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#21644;&#20998;&#31867;&#26159;&#30142;&#30149;&#39044;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#25269;&#28040;&#22810;&#27169;&#24577;&#38598;&#25104;&#36807;&#31243;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#22270;&#65292;&#28982;&#21518;&#25552;&#20986;&#22810;&#26680;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20174;&#22810;&#27169;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#32858;&#21512;&#22810;&#27169;&#24577;&#22270;&#20013;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its complexity, graph learning-based multi-modal integration and classification is one of the most challenging obstacles for disease prediction. To effectively offset the negative impact between modalities in the process of multi-modal integration and extract heterogeneous information from graphs, we propose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning). For the problem of negative impact between modalities, we propose a multi-modal graph embedding module to construct a multi-modal graph. Different from conventional methods that manually construct static graphs for all modalities, each modality generates a separate graph by adaptive learning, where a function graph and a supervision graph are introduced for optimization during the multi-graph fusion embedding process. We then propose a multi-kernel graph learning module to extract heterogeneous information from the multi-modal graph. The information in the multi-modal graph at different levels is aggregat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.12934</link><description>&lt;p&gt;
&#22312;&#31867;&#22411;&#21270;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#20013;&#26816;&#39564;&#27010;&#29575;&#35745;&#31639;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Checking Trustworthiness of Probabilistic Computations in a Typed Natural Deduction System. (arXiv:2206.12934v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; TPTND &#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#25512;&#23548;&#26377;&#20851;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#23646;&#24615;&#65292;&#20363;&#22914;&#24403;&#20170;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37027;&#20123;&#23646;&#24615;&#12290;TPTND &#20013;&#30340;&#25512;&#23548;&#34987;&#35299;&#37322;&#20026;&#20174;&#32473;&#23450;&#30340;&#20998;&#31867;&#20998;&#24067;&#20013;&#25552;&#21462; n &#20010;&#21487;&#33021;&#22797;&#26434;&#36755;&#20986;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36755;&#20986;&#26679;&#26412;&#30340;&#21487;&#20449;&#24615;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#20551;&#35774;&#27979;&#35797;&#65292;&#21363;&#35745;&#31639;&#20986;&#29616;&#26377;&#30340;&#39057;&#29575;&#19982;&#39044;&#26399;&#30340;&#27010;&#29575;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36825;&#20010;&#28436;&#31639;&#31995;&#32479;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#26816;&#26597;&#36825;&#31181;&#21487;&#20449;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#39033;&#25552;&#20379;&#20102;&#35745;&#31639;&#35821;&#20041;&#65292;&#24182;&#23450;&#20041;&#20102;&#36923;&#36753;&#36816;&#31639;&#31526;&#20197;&#21450;&#20449;&#20219;&#36816;&#31639;&#31526;&#30340;&#24341;&#20837;&#21644;&#28040;&#35299;&#35268;&#21017;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#20803;&#29702;&#35770;&#23646;&#24615;&#65292;&#23588;&#20854;&#26159;&#33021;&#22815;&#30830;&#23450;&#21738;&#20123;&#39033;&#28436;&#21270;&#21644;&#36923;&#36753;&#35268;&#21017;&#24212;&#29992;&#26102;&#65292;&#35745;&#31639;&#20173;&#28982;&#26159;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the probabilistic typed natural deduction calculus TPTND, designed to reason about and derive trustworthiness properties of probabilistic computational processes, like those underlying current AI applications. Derivability in TPTND is interpreted as the process of extracting $n$ samples of possibly complex outputs with a certain frequency from a given categorical distribution. We formalize trust for such outputs as a form of hypothesis testing on the distance between such frequency and the intended probability. The main advantage of the calculus is to render such notion of trustworthiness checkable. We present a computational semantics for the terms over which we reason and then the semantics of TPTND, where logical operators as well as a Trust operator are defined through introduction and elimination rules. We illustrate structural and metatheoretical properties, with particular focus on the ability to establish under which term evolutions and logical rules ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.13310</link><description>&lt;p&gt;
MonoDETR&#65306;&#28145;&#24230;&#24341;&#23548;&#30340;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#19968;&#30452;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26681;&#25454;&#20256;&#32479;&#30340;&#20108;&#32500;&#26816;&#27979;&#22120;&#39318;&#20808;&#23450;&#20301;&#30446;&#26631;&#20013;&#24515;&#65292;&#28982;&#21518;&#36890;&#36807;&#37051;&#36817;&#29305;&#24449;&#39044;&#27979;&#19977;&#32500;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20351;&#29992;&#23616;&#37096;&#35270;&#35273;&#29305;&#24449;&#26159;&#19981;&#36275;&#20197;&#29702;&#35299;&#22330;&#26223;&#32423;&#21035;&#30340;&#19977;&#32500;&#31354;&#38388;&#32467;&#26500;&#24182;&#24573;&#30053;&#20102;&#36828;&#36317;&#31163;&#30340;&#30446;&#26631;&#28145;&#24230;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#37319;&#29992;&#28145;&#24230;&#24341;&#23548;Transformer&#30340;&#21333;&#30446;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;MonoDETR&#12290;&#25105;&#20204;&#23558;&#22522;&#26412;&#30340;Transformer&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#20854;&#20855;&#26377;&#28145;&#24230;&#24863;&#30693;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#28145;&#24230;&#32447;&#32034;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25429;&#25417;&#29289;&#20307;&#22806;&#35266;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#21069;&#26223;&#28145;&#24230;&#22270;&#65292;&#24182;&#19987;&#38376;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#38750;&#23616;&#37096;&#28145;&#24230;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19977;&#32500;&#30446;&#26631;&#20505;&#36873;&#29289;&#24418;&#24335;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#30446;&#26631;-&#22330;&#26223;&#28145;&#24230;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#30446;&#26631;&#37117;&#21487;&#20197;&#24471;&#21040;&#26356;&#20840;&#38754;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#26356;&#20934;&#30830;&#30340;&#19977;&#32500;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monocular 3D object detection has long been a challenging task in autonomous driving. Most existing methods follow conventional 2D detectors to first localize object centers, and then predict 3D attributes by neighboring features. However, only using local visual features is insufficient to understand the scene-level 3D spatial structures and ignores the long-range inter-object depth relations. In this paper, we introduce the first DETR framework for Monocular DEtection with a depth-guided TRansformer, named MonoDETR. We modify the vanilla transformer to be depth-aware and guide the whole detection process by contextual depth cues. Specifically, concurrent to the visual encoder that captures object appearances, we introduce to predict a foreground depth map, and specialize a depth encoder to extract non-local depth embeddings. Then, we formulate 3D object candidates as learnable queries and propose a depth-guided decoder to conduct object-scene depth interactions. In this way, each obj
&lt;/p&gt;</description></item></channel></rss>