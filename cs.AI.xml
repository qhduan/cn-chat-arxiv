<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#26041;&#27861;&#30340;&#28789;&#27963;&#29289;&#29702;&#20266;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Flexible Physical Camouflage Generation Based on a Differential Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#25239;&#20266;&#35013;&#65292;&#22312;&#24191;&#27867;&#30340;&#19977;&#32500;&#28210;&#26579;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#24544;&#23454;&#22320;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#21644;&#26448;&#26009;&#21464;&#21270;&#65292;&#30830;&#20445;&#22312;&#19977;&#32500;&#30446;&#26631;&#19978;&#23545;&#32441;&#29702;&#36827;&#34892;&#24494;&#22937;&#32780;&#36924;&#30495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#12290;&#36825;&#28041;&#21450;&#23558;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#20445;&#20266;&#35013;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20266;&#35013;&#22312;&#36148;&#32440;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35206;&#30422;&#30446;&#26631;&#32780;&#19981;&#24433;&#21709;&#23545;&#25239;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29289;&#29702;&#23454;&#39564;&#65292;FPA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13575v1 Announce Type: cross  Abstract: This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05541</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#40065;&#26834;&#21644;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#30340;&#20652;&#21270;&#21058;&#65306;&#35299;&#23494;&#23458;&#25143;&#36129;&#29486;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#31995;&#32479;&#19978;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#32463;&#24120;&#24573;&#35270;&#32479;&#35745;&#24322;&#36136;&#24615;&#21644;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#36825;&#20123;&#22240;&#32032;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20010;&#24615;&#21270;&#30340;FL&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26469;&#36866;&#24212;&#20010;&#21035;&#23458;&#25143;&#30340;&#29305;&#28857;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#36866;&#24212;&#20248;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#23458;&#25143;&#36129;&#29486;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#22686;&#24378;&#24694;&#24847;&#23458;&#25143;&#19979;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03348</link><description>&lt;p&gt;
&#23562;&#37325;&#27169;&#22411;: &#32454;&#31890;&#24230;&#19988;&#40065;&#26834;&#30340;&#35299;&#37322;&#19982;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#33021;&#21542;&#30495;&#23454;&#38416;&#26126;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#30495;&#23454;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#29616;&#26377;&#26041;&#27861;&#20559;&#31163;&#20102;&#23545;&#27169;&#22411;&#30340;&#24544;&#23454;&#34920;&#36798;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#26041;&#27861;&#65292;&#31216;&#20026;SRD(&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;)&#65292;&#23427;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#20803;&#32423;&#21035;&#24378;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#26469;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#65292;&#31216;&#20026;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#65292;&#35753;&#25105;&#20204;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#27963;&#36291;&#21644;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SRD&#20801;&#35768;&#36882;&#24402;&#20998;&#35299;&#19968;&#20010;&#28857;&#29305;&#24449;&#21521;&#37327;(PFV)&#12290;
&lt;/p&gt;
&lt;p&gt;
The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.07450</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#26102;&#23578;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26102;&#23578;&#21512;&#25104;&#21644;&#32534;&#36753;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#23616;&#37096;&#20462;&#25913;&#35774;&#35745;&#33609;&#22270;&#65292;&#20026;&#26102;&#23578;&#35774;&#35745;&#24072;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20294;&#22312;&#20174;&#25277;&#35937;&#30340;&#35774;&#35745;&#20803;&#32032;&#20013;&#29983;&#25104;&#26102;&#23578;&#35774;&#35745;&#21644;&#31934;&#32454;&#32534;&#36753;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#65292;&#20363;&#22914;&#21150;&#20844;&#23460;&#12289;&#21830;&#21153;&#21644;&#27966;&#23545;&#65292;&#24418;&#25104;&#20102;&#25277;&#35937;&#30340;&#24863;&#23448;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#34966;&#38271;&#12289;&#39046;&#22411;&#21644;&#35044;&#38271;&#31561;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#34987;&#35270;&#20026;&#26381;&#35013;&#30340;&#20302;&#32423;&#23646;&#24615;&#12290;&#20351;&#29992;&#20887;&#38271;&#30340;&#25991;&#23383;&#25551;&#36848;&#26469;&#25511;&#21046;&#21644;&#32534;&#36753;&#26102;&#23578;&#22270;&#20687;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20849;&#20139;&#30340;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21644;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#34701;&#20837;&#21040;&#20998;&#23618;&#32467;&#26500;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#20026;&#19981;&#21516;&#30340;&#23618;&#27425;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed 
&lt;/p&gt;</description></item><item><title>OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.02422</link><description>&lt;p&gt;
OneAdapt&#65306;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02422
&lt;/p&gt;
&lt;p&gt;
OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27969;&#23186;&#20307;&#25968;&#25454;&#30340;&#25512;&#26029;&#26041;&#38754;&#24050;&#32463;&#26222;&#21450;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;LiDAR&#25968;&#25454;&#21644;&#38899;&#39057;&#27874;&#24418;&#20013;&#30340;&#25991;&#26412;&#25552;&#21462;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#32593;&#32476;&#24102;&#23485;&#26469;&#25910;&#38598;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#24182;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;GPU&#36164;&#28304;&#26469;&#36816;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#23613;&#31649;&#36890;&#36807;&#20248;&#21270;&#37197;&#32622;&#21442;&#25968;&#65288;&#22914;&#35270;&#39057;&#20998;&#36776;&#29575;&#21644;&#24103;&#29575;&#65289;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23545;&#32593;&#32476;&#24102;&#23485;&#21644;GPU&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#20294;&#30446;&#21069;&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#19977;&#20010;&#35201;&#27714;&#65306;&#65288;i&#65289;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;GPU&#25110;&#24102;&#23485;&#24320;&#38144;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#25968;&#25454;&#23545;&#26368;&#32456;DNN&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20915;&#31574;&#65307;&#65288;iii&#65289;&#38024;&#23545;&#19968;&#31995;&#21015;&#37197;&#32622;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OneAdapt&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#21442;&#25968;&#65292;&#28385;&#36275;&#20102;&#36825;&#20123;&#35201;&#27714;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20805;&#20998;&#21033;&#29992;DNN&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' different
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item></channel></rss>