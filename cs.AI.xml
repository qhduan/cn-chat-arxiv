<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>GOMA&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.11075</link><description>&lt;p&gt;
GOMA&#65306;&#36890;&#36807;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#23454;&#29616;&#20027;&#21160;&#21512;&#20316;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11075
&lt;/p&gt;
&lt;p&gt;
GOMA&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#22836;&#20132;&#27969;&#22312;&#20154;&#31867;&#21512;&#20316;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#21512;&#20316;&#20249;&#20276;&#21482;&#23545;&#20219;&#21153;&#12289;&#29615;&#22659;&#21644;&#24444;&#27492;&#30340;&#24515;&#29702;&#29366;&#24577;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#20316;&#27807;&#36890;&#26694;&#26550;&#65292;&#21363;&#38754;&#21521;&#30446;&#26631;&#30340;&#24515;&#26234;&#23545;&#40784;&#65288;GOMA&#65289;&#12290;GOMA&#23558;&#21475;&#22836;&#20132;&#27969;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26234;&#33021;&#20307;&#24515;&#26234;&#29366;&#24577;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#19968;&#20010;&#20855;&#26377;&#36523;&#20307;&#30340;&#21161;&#25163;&#33021;&#22815;&#25512;&#29702;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20197;&#33258;&#28982;&#35821;&#35328;&#20027;&#21160;&#24320;&#22987;&#19982;&#20154;&#31867;&#30340;&#21475;&#22836;&#27807;&#36890;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#20316;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;Overcooked&#65288;&#19968;&#27454;&#22810;&#20154;&#28216;&#25103;&#65289;&#21644;VirtualHome&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#22120;&#65289;&#20013;&#65292;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;&#35821;&#22659;&#30340;&#26377;&#24847;&#20041;&#27807;&#36890;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11075v1 Announce Type: cross  Abstract: Verbal communication plays a crucial role in human cooperation, particularly when the partners only have incomplete information about the task, environment, and each other's mental state. In this paper, we propose a novel cooperative communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates verbal communication as a planning problem that minimizes the misalignment between the parts of agents' mental states that are relevant to the goals. This approach enables an embodied assistant to reason about when and how to proactively initialize communication with humans verbally using natural language to help achieve better cooperation. We evaluate our approach against strong baselines in two challenging environments, Overcooked (a multiplayer game) and VirtualHome (a household simulator). Our experimental results demonstrate that large language models struggle with generating meaningful communication that is grounded in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04917</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#38598;&#22270;&#30340;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#30340;&#28151;&#21512;&#25972;&#25968;&#38181;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23547;&#25214;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;MT-TSP&#65289;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#30340;&#20844;&#24335;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#30701;&#36335;&#24452;&#65292;&#20351;&#19968;&#20010;&#20174;&#20179;&#24211;&#20986;&#21457;&#30340;&#20195;&#29702;&#35775;&#38382;&#19968;&#32452;&#31227;&#21160;&#30446;&#26631;&#65292;&#24182;&#22312;&#23427;&#20204;&#20998;&#37197;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#24688;&#22909;&#35775;&#38382;&#19968;&#27425;&#65292;&#28982;&#21518;&#36820;&#22238;&#21040;&#20179;&#24211;&#12290;&#35813;&#20844;&#24335;&#20381;&#36182;&#20110;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#21363;&#24403;&#30446;&#26631;&#27839;&#30528;&#32447;&#31227;&#21160;&#26102;&#65292;&#23427;&#20204;&#30340;&#36712;&#36857;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#21464;&#20026;&#20984;&#38598;&#12290;&#28982;&#21518;&#65292;&#38382;&#39064;&#23601;&#32553;&#20943;&#20026;&#22312;&#19968;&#20010;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#65292;&#21463;&#21040;&#19968;&#20123;&#36895;&#24230;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20844;&#24335;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#30446;&#26631;&#25968;&#37327;&#26368;&#22810;&#20026;20&#20010;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;MICP&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#32553;&#30701;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#26368;&#20248;&#24615;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;60&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#35299;&#27861;&#30340;&#25104;&#26412;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04917v1 Announce Type: cross  Abstract: This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from th
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11421</link><description>&lt;p&gt;
PastNet&#65306;&#24341;&#20837;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#29992;&#20110;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction. (arXiv:2305.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#27969;&#29983;&#25104;&#26410;&#26469;&#35270;&#39057;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#35821;&#20041;&#22320;&#22270;&#31561;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#35270;&#39057;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#35270;&#39057;&#20869;&#22266;&#26377;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23545;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29289;&#29702;&#36741;&#21161;&#26102;&#31354;&#32593;&#32476;&#65288;PastNet&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;PastNet&#26680;&#24515;&#22312;&#20110;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24341;&#20837;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#30340;&#23384;&#20648;&#22120;&#24211;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#31354;&#20449;&#21495;&#26102;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs 
&lt;/p&gt;</description></item></channel></rss>