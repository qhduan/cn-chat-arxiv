<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15509</link><description>&lt;p&gt;
&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#30340;&#21487;&#20998;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#31561;&#35768;&#22810;&#38382;&#39064;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;AEs&#34920;&#31034;&#20013;&#28151;&#21512;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TAE&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;TAE&#23558;&#28508;&#22312;&#34920;&#31034;&#30830;&#23450;&#22320;&#36716;&#25442;&#20026;&#26356;&#26131;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#21363;\textit{&#21487;&#20998;&#31163;&#34920;&#31034;}&#65292;&#24182;&#22312;&#36755;&#20986;&#31471;&#37325;&#24314;&#21487;&#20998;&#31163;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15509v1 Announce Type: cross  Abstract: Representation Learning (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent representation that better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the representation of AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent representation into a more distinguishable representation namely the \textit{separable representation} and the reconstructsuct the separable representation at the output. The output of TAE called the \textit{reconstruction represe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.14562</link><description>&lt;p&gt;
&#35821;&#20041;&#35299;&#30721;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
The Era of Semantic Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#29616;&#20102;&#22312;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#32534;&#25490;&#21327;&#20316;&#20197;&#35299;&#20915;LLM&#22266;&#26377;&#23616;&#38480;&#24615;&#30340;&#24819;&#27861;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;&#36825;&#20123;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;LLM&#27010;&#24565;&#21270;&#20026;&#25805;&#32437;&#25105;&#20204;&#31216;&#20043;&#20026;&#35821;&#20041;&#26631;&#35760;&#65288;&#24050;&#30693;&#24605;&#24819;&#65289;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#29255;&#27573;&#30340;&#35821;&#20041;&#22788;&#29702;&#22120;&#12290;LLM&#26159;&#20247;&#22810;&#20854;&#20182;&#35821;&#20041;&#22788;&#29702;&#22120;&#20043;&#19968;&#65292;&#21253;&#25324;&#20154;&#31867;&#21644;&#24037;&#20855;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#25110;&#20195;&#30721;&#25191;&#34892;&#22120;&#12290;&#35821;&#20041;&#22788;&#29702;&#22120;&#38598;&#20307;&#21442;&#19982;&#35821;&#20041;&#26631;&#35760;&#30340;&#21160;&#24577;&#20132;&#27969;&#65292;&#36880;&#27493;&#26500;&#24314;&#39640;&#25928;&#36755;&#20986;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#25628;&#32034;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#35821;&#20041;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#24050;&#24191;&#20026;&#30740;&#31350;&#30340;&#35821;&#20041;&#35299;&#30721;&#38382;&#39064;&#30452;&#25509;&#24179;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14562v1 Announce Type: cross  Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of s
&lt;/p&gt;</description></item><item><title>3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2310.18511</link><description>&lt;p&gt;
3DCoMPaT$^{++}$&#65306;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#30340;&#25913;&#36827;&#22411;&#22823;&#35268;&#27169;&#19977;&#32500;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18511
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3DCoMPaT$^{++}$&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#20197;&#19978;10&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#30340;&#28210;&#26579;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#24418;&#29366;&#22312;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#31934;&#24515;&#27880;&#37322;&#65292;&#24182;&#37197;&#26377;&#21305;&#37197;&#30340;RGB&#28857;&#20113;&#12289;3D&#32441;&#29702;&#32593;&#26684;&#12289;&#28145;&#24230;&#22270;&#21644;&#20998;&#21106;&#33945;&#29256;&#12290;3DCoMPaT$^{++}$&#28085;&#30422;&#20102;41&#20010;&#24418;&#29366;&#31867;&#21035;&#12289;275&#20010;&#32454;&#31890;&#24230;&#37096;&#20998;&#31867;&#21035;&#21644;293&#20010;&#32454;&#31890;&#24230;&#26448;&#26009;&#31867;&#21035;&#65292;&#36825;&#20123;&#31867;&#21035;&#21487;&#20197;&#32452;&#21512;&#24212;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#30340;&#21508;&#37096;&#20998;&#12290;&#25105;&#20204;&#20174;&#22235;&#20010;&#31561;&#38388;&#36317;&#35270;&#22270;&#21644;&#22235;&#20010;&#38543;&#26426;&#35270;&#22270;&#20013;&#28210;&#26579;&#20102;&#19968;&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#24418;&#29366;&#30340;&#23376;&#38598;&#65292;&#20849;&#35745;1.6&#20159;&#20010;&#28210;&#26579;&#12290;&#37096;&#20214;&#22312;&#23454;&#20363;&#32423;&#21035;&#12289;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Grounded CoMPaT Recognition (GCR)&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#20849;&#21516;&#35782;&#21035;&#21644;&#22522;&#20110;&#29289;&#20307;&#37096;&#20998;&#30340;&#26448;&#26009;&#32452;&#21512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#27963;&#21160;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18511v2 Announce Type: replace-cross  Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;</title><link>http://arxiv.org/abs/2401.10266</link><description>&lt;p&gt;
&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;: &#26041;&#27861;&#35770;&#21644;&#19981;&#30830;&#23450;&#24615;&#31649;&#29702;&#31574;&#30053;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies. (arXiv:2401.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;Tennessee Eastman Process&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#31639;&#27861;&#30340;&#20248;&#21155;&#21183;&#12290;&#36824;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#26679;&#26412;&#31561;&#25361;&#25112;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#24212;&#23545;&#12290;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;Tennessee Eastman Process&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#30417;&#27979;&#22312;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#26085;&#30410;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20851;&#27880;&#30340;&#22686;&#38271;&#20027;&#39064;&#21644;&#19968;&#31181;&#24378;&#22823;&#30340;&#25925;&#38556;&#35782;&#21035;&#26041;&#24335;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24037;&#19994;&#21378;&#25151;&#26234;&#33021;&#29366;&#24577;&#30417;&#27979;&#21644;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#24320;&#28304;&#22522;&#20934;Tennessee Eastman Process&#65288;TEP&#65289;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#24635;&#32467;&#20102;&#29992;&#20110;&#24037;&#19994;&#21378;&#25151;&#29366;&#24577;&#30417;&#27979;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#26368;&#27969;&#34892;&#21644;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#36824;&#28085;&#30422;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#12289;&#26080;&#26631;&#35760;&#26679;&#26412;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#27604;&#36739;&#20102;&#21033;&#29992;Tennessee Eastman Process&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#35268;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process 
&lt;/p&gt;</description></item></channel></rss>