<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16066</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21160;&#24577;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Temporal Graph Network Framework for Dynamic Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#29992;&#25143;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#27969;&#23186;&#20307;&#26381;&#21153;&#31561;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#30001;&#20110;&#38745;&#24577;&#25968;&#25454;&#20381;&#36182;&#65292;&#25512;&#33616;&#31995;&#32479;&#24120;&#24120;&#33853;&#21518;&#20110;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#12290;&#22312;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#34987;&#25552;&#20986;&#21518;&#65292;&#21508;&#31181;&#30740;&#31350;&#34920;&#26126;TGN&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#33410;&#28857;&#21644;&#36793;&#30340;&#29305;&#24449;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#26377;&#30528;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#36825;&#22312;&#35813;&#39046;&#22495;&#23578;&#23646;&#39318;&#27425;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31995;&#21015;&#22270;&#24418;&#21644;&#21382;&#21490;&#23884;&#20837;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TGN&#30340;&#36866;&#24212;&#24615;&#65292;&#35777;&#23454;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16066v1 Announce Type: new  Abstract: Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance. After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time. However, despite its promising capabilities, it has not been directly applied in recommender systems to date. Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field. Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.15049</link><description>&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#23548;&#33322;&#21040;&#30446;&#30340;&#22320;&#12290;&#29616;&#26377;&#30340;VLN&#20195;&#29702;&#35757;&#32451;&#26041;&#27861;&#39044;&#35774;&#22266;&#23450;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65306;&#24341;&#20837;&#26032;&#29615;&#22659;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#20445;&#30041;&#24050;&#32463;&#36935;&#21040;&#30340;&#29615;&#22659;&#30340;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;VLN&#20195;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#35780;&#20272;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15049v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#65292;&#32467;&#21512;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#65292;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12723</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;Python&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Python Fuzzing for Trustworthy Machine Learning Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12723
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#65292;&#32467;&#21512;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#65292;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#31946;&#27979;&#35797;&#26159;&#23433;&#20840;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65288;SSDLC&#65289;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#23433;&#20840;&#21644;&#20581;&#22766;&#30340;&#36719;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Sydr-Fuzz&#24037;&#20855;&#38598;&#38024;&#23545;Python&#39033;&#30446;&#30340;&#21160;&#24577;&#20998;&#26512;&#31649;&#36947;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#21253;&#25324;&#27169;&#31946;&#27979;&#35797;&#12289;&#35821;&#26009;&#24211;&#26368;&#23567;&#21270;&#12289;&#23849;&#28291;&#20998;&#31867;&#21644;&#35206;&#30422;&#29575;&#25910;&#38598;&#12290;&#23849;&#28291;&#20998;&#31867;&#21644;&#20005;&#37325;&#24615;&#35780;&#20272;&#26159;&#30830;&#20445;&#21450;&#26102;&#35299;&#20915;&#26368;&#20851;&#38190;&#28431;&#27934;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#31649;&#36947;&#38598;&#25104;&#22312;GitLab CI&#20013;&#12290;&#20026;&#20102;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#37096;&#20998;&#65292;&#25105;&#20204;&#20998;&#26512;&#23427;&#20204;&#28508;&#22312;&#30340;&#25915;&#20987;&#38754;&#65292;&#24182;&#20026;PyTorch&#12289;TensorFlow&#24320;&#21457;&#27169;&#31946;&#27979;&#35797;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12723v1 Announce Type: cross  Abstract: Ensuring the security and reliability of machine learning frameworks is crucial for building trustworthy AI-based systems. Fuzzing, a popular technique in secure software development lifecycle (SSDLC), can be used to develop secure and robust software. Popular machine learning frameworks such as PyTorch and TensorFlow are complex and written in multiple programming languages including C/C++ and Python. We propose a dynamic analysis pipeline for Python projects using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus minimization, crash triaging, and coverage collection. Crash triaging and severity estimation are important steps to ensure that the most critical vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is integrated in GitLab CI. To identify the most vulnerable parts of the machine learning frameworks, we analyze their potential attack surfaces and develop fuzz targets for PyTorch, TensorFlow, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.09498</link><description>&lt;p&gt;
&#20174;&#24576;&#30097;&#21040;&#25509;&#21463;&#65306;&#27169;&#25311;&#23545;&#34394;&#20551;&#26032;&#38395;&#24577;&#24230;&#21160;&#24577;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#34394;&#20551;&#26032;&#38395;&#21644;&#35875;&#35328;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#36805;&#36895;&#20256;&#25773;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#24433;&#21709;&#30528;&#20844;&#20247;&#33286;&#35770;&#12290;&#20256;&#32479;&#30340;&#34394;&#20551;&#26032;&#38395;&#24314;&#27169;&#36890;&#24120;&#39044;&#27979;&#19981;&#21516;&#32676;&#20307;&#30340;&#26222;&#36941;&#27969;&#34892;&#36235;&#21183;&#25110;&#25968;&#23383;&#21270;&#20195;&#34920;&#24847;&#35265;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36807;&#20110;&#31616;&#21270;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#24573;&#35270;&#20102;&#26032;&#38395;&#25991;&#26412;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#27169;&#25311;&#24494;&#22937;&#24847;&#35265;&#21160;&#24577;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65288;FPS&#65289;&#65292;&#35814;&#32454;&#30740;&#31350;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#20223;&#30495;&#20013;&#30340;&#27599;&#20010;&#20195;&#29702;&#20154;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20154;&#12290;&#20182;&#20204;&#37197;&#22791;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#21450;&#21453;&#24605;&#26426;&#21046;&#26469;&#27169;&#20223;&#31867;&#20154;&#24605;&#32500;&#12290;&#27599;&#22825;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09498v1 Announce Type: cross  Abstract: In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, the
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.05810</link><description>&lt;p&gt;
&#29992;&#20110;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05810
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#37096;&#20998;&#36712;&#36857;&#25968;&#25454;&#26469;&#35843;&#25972;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#19981;&#22826;&#21487;&#33021;&#20174;&#25152;&#26377;&#28508;&#22312;&#30340;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36712;&#36857;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#39033;&#21517;&#20026;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#36890;&#36807;&#39046;&#22495;&#23545;&#40784;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#27169;&#22359;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05810v1 Announce Type: cross  Abstract: Pedestrian trajectory prediction is a crucial component in computer vision and robotics, but remains challenging due to the domain shift problem. Previous studies have tried to tackle this problem by leveraging a portion of the trajectory data from the target domain to adapt the model. However, such domain adaptation methods are impractical in real-world scenarios, as it is infeasible to collect trajectory data from all potential target domains. In this paper, we study a task named generalized pedestrian trajectory prediction, with the aim of generalizing the model to unseen domains without accessing their trajectories. To tackle this task, we introduce a Recurrent Aligned Network~(RAN) to minimize the domain gap through domain alignment. Specifically, we devise a recurrent alignment module to effectively align the trajectory feature spaces at both time-state and time-sequence levels by the recurrent alignment strategy.Furthermore, we 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22522;&#20110;GPT-4&#30340;&#23450;&#21046;AI&#21161;&#25163;&#65292;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#25552;&#21319;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03188</link><description>&lt;p&gt;
&#36798;&#25104;&#27665;&#20027;&#21270;&#27946;&#27700;&#39118;&#38505;&#31649;&#29702;&#65306;&#22522;&#20110;GPT-4&#30340;&#20808;&#36827;AI&#21161;&#25163;&#23454;&#29616;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03188
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22522;&#20110;GPT-4&#30340;&#23450;&#21046;AI&#21161;&#25163;&#65292;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#25552;&#21319;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#20247;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#27946;&#27700;&#39044;&#27979;&#22312;&#20419;&#36827;&#21450;&#26102;&#26377;&#25928;&#30340;&#24212;&#24613;&#21709;&#24212;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#22312;&#20110;&#24357;&#21512;&#22797;&#26434;&#25968;&#23383;&#27946;&#27700;&#27169;&#22411;&#19982;&#23454;&#38469;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20915;&#31574;&#32773;&#32463;&#24120;&#20381;&#36182;&#19987;&#23478;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#27946;&#27700;&#20943;&#28798;&#31574;&#30053;&#12290;&#20844;&#20247;&#38656;&#35201;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#35843;&#26597;&#21644;&#29702;&#35299;&#31038;&#20250;&#25991;&#21270;&#21644;&#21046;&#24230;&#22240;&#32032;&#65292;&#36825;&#32463;&#24120;&#38459;&#30861;&#20102;&#20844;&#20247;&#23545;&#27946;&#27700;&#39118;&#38505;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65306;&#30001;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#23450;&#21046;AI&#21161;&#25163;&#12290;&#36825;&#20010;AI&#21161;&#25163;&#26088;&#22312;&#20419;&#36827;&#20915;&#31574;&#32773;&#12289;&#26222;&#36890;&#20844;&#20247;&#21644;&#27946;&#27700;&#39044;&#25253;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#65292;&#32780;&#26080;&#38656;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#19968;&#26032;&#26694;&#26550;&#21033;&#29992;&#20102;GPT-4&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03188v1 Announce Type: new  Abstract: Real-time flood forecasting plays a crucial role in enabling timely and effective emergency responses. However, a significant challenge lies in bridging the gap between complex numerical flood models and practical decision-making. Decision-makers often rely on experts to interpret these models for optimizing flood mitigation strategies. And the public requires complex techniques to inquiry and understand socio-cultural and institutional factors, often hinders the public's understanding of flood risks. To overcome these challenges, our study introduces an innovative solution: a customized AI Assistant powered by the GPT-4 Large Language Model. This AI Assistant is designed to facilitate effective communication between decision-makers, the general public, and flood forecasters, without the requirement of specialized knowledge. The new framework utilizes GPT-4's advanced natural language understanding and function calling capabilities to pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.10985</link><description>&lt;p&gt;
&#21033;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26816;&#27979;&#20113;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI Planning For Detecting Cloud Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#26381;&#21153;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#26222;&#21450;&#65292;&#19982;&#20854;&#23433;&#20840;&#28431;&#27934;&#30456;&#20851;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#21644;&#21202;&#32034;&#36719;&#20214;&#31561;&#22797;&#26434;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#36798;&#20113;&#31995;&#32479;&#20013;&#19981;&#21516;&#23545;&#35937;&#65288;&#22914;&#29992;&#25143;&#12289;&#25968;&#25454;&#23384;&#20648;&#12289;&#23433;&#20840;&#35282;&#33394;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#12290;&#35775;&#38382;&#25511;&#21046;&#35823;&#37197;&#32622;&#36890;&#24120;&#26159;&#20113;&#25915;&#20987;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;PDDL&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#23433;&#20840;&#28431;&#27934;&#65292;&#20363;&#22914;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#25915;&#20987;&#65288;&#22914;&#21202;&#32034;&#36719;&#20214;&#65289;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#12290;&#35268;&#21010;&#22120;&#21487;&#20197;&#29983;&#25104;&#25915;&#20987;&#20197;&#35782;&#21035;&#20113;&#20013;&#30340;&#27492;&#31867;&#28431;&#27934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;14&#20010;&#19981;&#21516;&#21830;&#19994;&#32452;&#32455;&#30340;&#30495;&#23454;&#20122;&#39532;&#36874;AWS&#20113;&#37197;&#32622;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10985v1 Announce Type: cross  Abstract: Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08328</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction. (arXiv:2310.08328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120; (H-STFormer) &#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#22686;&#37327;&#23398;&#20064;&#21644;&#36716;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#26159;&#26102;&#31354;&#25968;&#25454;&#65292;&#19981;&#20165;&#19982;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#31354;&#38388;&#20301;&#32622;&#30456;&#20851;&#65292;&#32780;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#26159;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#25152;&#26377;&#22266;&#26377;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#30340;&#22686;&#37327;&#23398;&#20064;&#20960;&#20046;&#27809;&#26377;&#23581;&#35797;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20063;&#24456;&#38590;&#36716;&#21270;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#25361;&#25112;&#21644;&#36947;&#36335;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#20132;&#36890;&#26530;&#32445;&#24863;&#30693;&#26102;&#31354;&#33258;&#36866;&#24212;&#22270;&#36716;&#25442;&#22120;&#65288;H-STFormer&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-att
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#21160;&#24577;&#31574;&#30053;&#36873;&#25321;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning. (arXiv:2310.01446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#26681;&#25454;&#38382;&#39064;&#38590;&#24230;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#36825;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#21018;&#24615;&#37319;&#29992;&#32479;&#19968;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26102;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#38382;&#39064;&#24448;&#24448;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#24615;&#12290;&#20154;&#31867;&#26412;&#33021;&#22320;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#35843;&#25972;&#20182;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21033;&#29992;LLM&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#37319;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;: &#19981;&#31649;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#65292;&#37117;&#20351;&#29992;&#19968;&#33268;&#30340;&#27169;&#22411;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#38382;&#39064;&#20998;&#35299;&#31243;&#24230;&#12290;&#36825;&#31181;&#21018;&#24615;&#21487;&#33021;&#20250;&#24102;&#26469;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#26694;&#26550;&#12290;&#23427;&#26681;&#25454;&#38382;&#39064;&#30340;&#38590;&#24230;&#31574;&#30053;&#24615;&#22320;&#35843;&#25972;&#27714;&#35299;&#31574;&#30053;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#12290;&#21021;&#22987;&#35780;&#20272;&#27169;&#22359;&#35780;&#20272;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30340;&#20805;&#20998;&#24615;&#12290;&#22914;&#26524;&#38656;&#35201;&#25913;&#36827;&#65292;&#25509;&#19979;&#26469;&#30340;&#33258;&#36866;&#24212;&#27169;&#22359;&#20250;&#20171;&#20837;&#12290;&#22312;&#36825;&#20010;&#27169;&#22359;&#20869;&#65292;&#26377;&#19977;&#20010;&#20851;&#38190;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;OED&#38382;&#39064;&#65292;&#32467;&#26524;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.10430</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational Sequential Optimal Experimental Design using Reinforcement Learning. (arXiv:2306.10430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;OED&#38382;&#39064;&#65292;&#32467;&#26524;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745; (vsOED) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#26469;&#26368;&#20248;&#22320;&#35774;&#35745;&#26377;&#38480;&#24207;&#21015;&#30340;&#23454;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#19979;&#30028;&#20272;&#35745;&#26399;&#26395;&#25928;&#29992;&#12290;&#36890;&#36807;&#21516;&#26102;&#26368;&#22823;&#21270;&#21464;&#20998;&#19979;&#30028;&#21644;&#25191;&#34892;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#26469;&#25968;&#20540;&#35299;&#20915;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#38754;&#21521;&#21442;&#25968;&#25512;&#26029;&#12289;&#27169;&#22411;&#21306;&#20998;&#21644;&#30446;&#26631;&#23548;&#21521;&#39044;&#27979;&#30340;OED&#38382;&#39064;&#12290;&#36825;&#20123;&#26696;&#20363;&#28085;&#30422;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#20284;&#28982;&#20989;&#25968;&#12289;&#40635;&#28902;&#21442;&#25968;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;vsOED&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#39034;&#24207;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#65292;&#26679;&#26412;&#25928;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#25152;&#38656;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce variational sequential Optimal Experimental Design (vsOED), a new method for optimally designing a finite sequence of experiments under a Bayesian framework and with information-gain utilities. Specifically, we adopt a lower bound estimator for the expected utility through variational approximation to the Bayesian posteriors. The optimal design policy is solved numerically by simultaneously maximizing the variational lower bound and performing policy gradient updates. We demonstrate this general methodology for a range of OED problems targeting parameter inference, model discrimination, and goal-oriented prediction. These cases encompass explicit and implicit likelihoods, nuisance parameters, and physics-based partial differential equation models. Our vsOED results indicate substantially improved sample efficiency and reduced number of forward model simulations compared to previous sequential design algorithms.
&lt;/p&gt;</description></item><item><title>LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09599</link><description>&lt;p&gt;
LEA: &#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36229;&#36234;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy. (arXiv:2304.09599v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09599
&lt;/p&gt;
&lt;p&gt;
LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#25104;&#20026;&#26114;&#36149;&#40657;&#30418;&#20248;&#21270;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#40657;&#30418;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#20851;&#38190;&#30340;&#38556;&#30861;&#26159;&#25214;&#20986;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#34920;&#24449;&#19981;&#36275;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20302;&#25928;&#20132;&#20114;&#32780;&#26174;&#24471;&#34180;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65288;LEA&#65289;&#65292;&#20197;&#23454;&#29616;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#21040;&#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36716;&#25442;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#21442;&#25968;&#21644;&#26356;&#26032;&#35268;&#21017;&#12290;&#19982;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#19981;&#21516;&#65292;LEA&#23545;&#30446;&#26631;&#20219;&#21153;&#20855;&#26377;&#39640;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LEA&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have emerged as a powerful framework for expensive black-box optimization. Obtaining better solutions with less computational cost is essential and challenging for black-box optimization. The most critical obstacle is figuring out how to effectively use the target task information to form an efficient optimization strategy. However, current methods are weak due to the poor representation of the optimization strategy and the inefficient interaction between the optimization strategy and the target task. To overcome the above limitations, we design a learned EA (LEA) to realize the move from hand-designed optimization strategies to learned optimization strategies, including not only hyperparameters but also update rules. Unlike traditional EAs, LEA has high adaptability to the target task and can obtain better solutions with less computational cost. LEA is also able to effectively utilize the low-fidelity information of the target task to form an efficient op
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item></channel></rss>