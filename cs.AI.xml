<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.10586</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#32467;&#26524;&#65306;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#26159;&#33521;&#22269;&#27599;&#22825;&#36896;&#25104;15&#20154;&#27515;&#20129;&#30340;&#39046;&#20808;&#27852;&#23615;&#36947;&#30284;&#30151;&#12290;&#36825;&#31181;&#30284;&#30151;&#20027;&#35201;&#34920;&#29616;&#20026;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#65288;NMIBC&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#32959;&#30244;&#36824;&#26410;&#28183;&#36879;&#21040;&#33152;&#33009;&#22721;&#30340;&#32908;&#32905;&#23618;&#12290; NMIBC&#30340;&#22797;&#21457;&#29575;&#38750;&#24120;&#39640;&#65292;&#36798;&#21040;70-80&#65285;&#65292;&#22240;&#27492;&#27835;&#30103;&#25104;&#26412;&#26368;&#39640;&#12290;&#30446;&#21069;&#29992;&#20110;&#39044;&#27979;&#22797;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#35780;&#20998;&#31995;&#32479;&#26469;&#39640;&#20272;&#39118;&#38505;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22797;&#21457;&#30340;&#19981;&#20934;&#30830;&#21644;&#24310;&#36831;&#39044;&#27979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#22797;&#21457;&#23545;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31649;&#29702;&#21644;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20986;&#29616;&#30340;&#22320;&#26041;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23376;&#21644;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;NMIBC&#22797;&#21457;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#27425;&#23457;&#26597;&#23545;&#39044;&#27979;NMIBC&#22797;&#21457;&#30340;ML&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09567</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#65306;&#19968;&#31181;&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09567
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#22312;&#28041;&#21450;&#20154;&#31867;&#20114;&#21160;&#30340;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#24341;&#36215;&#23433;&#20840;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#20107;&#20214;&#32972;&#21518;&#30340;&#24773;&#20917;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#21521;&#38750;&#19987;&#23478;&#29992;&#25143;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#35299;&#37322;&#22312;&#25552;&#39640;&#21487;&#20449;&#24230;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#20316;&#20026;&#38450;&#33539;&#22833;&#36133;&#12289;&#38169;&#35823;&#21644;&#35823;&#35299;&#30340;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#65292;&#24357;&#21512;&#20195;&#29702;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#30340;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#22522;&#20110;ROS&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23454;&#26045;&#30340;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31867;&#20284;&#40657;&#30418;&#30340;&#20803;&#32032;&#29992;&#20110;&#25552;&#20379;&#38382;&#36131;&#21046;&#65292;&#20855;&#26377;&#36890;&#36807;&#21306;&#22359;&#38142;&#25216;&#26415;&#23454;&#29616;&#30340;&#38450;&#31713;&#25913;&#23646;&#24615;&#12290;&#20854;&#27425;&#65292;&#19968;&#20010;&#36127;&#36131;&#30340;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09567v1 Announce Type: cross  Abstract: The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of 
&lt;/p&gt;</description></item><item><title>&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09549</link><description>&lt;p&gt;
&#23558;&#21435;&#22122;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#20197;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09549
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;3D&#21407;&#23376;&#20307;&#31995;&#20013;&#30340;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#20174;&#22836;&#31639;&#35745;&#31639;&#65292;&#22240;&#27492;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#38750;&#24179;&#34913;&#32467;&#26500;&#65288;DeNS&#65289;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;DeNS&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21521;&#20854;3D&#22352;&#26631;&#28155;&#21152;&#22122;&#22768;&#26469;&#30772;&#22351;3D&#32467;&#26500;&#65292;&#28982;&#21518;&#39044;&#27979;&#22122;&#22768;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#20165;&#38480;&#20110;&#24179;&#34913;&#32467;&#26500;&#30340;&#21435;&#22122;&#24037;&#20316;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#21435;&#22122;&#27867;&#21270;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#38750;&#24179;&#34913;&#32467;&#26500;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#38750;&#24179;&#34913;&#32467;&#26500;&#19981;&#23545;&#24212;&#20110;&#23616;&#37096;&#33021;&#37327;&#26368;&#23567;&#20540;&#65292;&#20855;&#26377;&#38750;&#38646;&#21147;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#21407;&#23376;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02262</link><description>&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#24456;&#37325;&#35201;&#65306;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Data Quality Matters: Suicide Intention Detection on Social Media Posts Using a RoBERTa-CNN Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;RoBERTa-CNN&#27169;&#22411;&#26469;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#33258;&#26432;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;RoBERTa-CNN&#36890;&#36807;&#22312;RoBERTa&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;&#25552;&#39640;&#20102;&#23545;&#37325;&#35201;&#27169;&#24335;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#27880;&#28966;&#28857;&#65292;&#24613;&#38656;&#21019;&#26032;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#35782;&#21035;SuicideWatch Reddit&#24086;&#23376;&#20013;&#30340;&#33258;&#26432;&#24847;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#31471;&#30340;RoBERTa-CNN&#27169;&#22411;&#36827;&#34892;&#33258;&#26432;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;RoBERTa-CNN&#26159;RoBERTa&#65288;&#40065;&#26834;&#24615;&#20248;&#21270;BERT&#26041;&#27861;&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;RoBERTa&#34987;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;RoBERTa&#30340;&#26377;&#25928;&#24615;&#22312;&#20110;&#23427;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#20449;&#24687;&#24182;&#24418;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#28155;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23618;&#65292;RoBERTa&#22686;&#24378;&#20102;&#20174;&#24222;&#22823;&#25968;&#25454;&#38598;&#20013;&#25429;&#25417;&#37325;&#35201;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#26432;&#21644;&#25233;&#37057;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;RoBERTa-CNN&#65292;&#24182;&#33719;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#65292;RoBERTa-CNN&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#19978;&#33719;&#24471;&#20102;98&#65285;&#65292;&#26631;&#20934;&#24046;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Suicide remains a global health concern for the field of health, which urgently needs innovative approaches for early detection and intervention. In this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit posts and present a novel approach to suicide detection using the cutting-edge RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach). RoBERTa is used for various Natural Language Processing (NLP) tasks, including text classification and sentiment analysis. The effectiveness of the RoBERTa lies in its ability to capture textual information and form semantic relationships within texts. By adding the Convolution Neural Network (CNN) layer to the original model, the RoBERTa enhances its ability to capture important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we experimented on the Suicide and Depression Detection dataset and obtained solid results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard deviation (ST
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10889</link><description>&lt;p&gt;
&#20351;&#29992;3D&#25511;&#21046;&#21512;&#25104;&#31227;&#21160;&#20154;&#29289;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;3D&#36816;&#21160;&#24207;&#21015;&#29983;&#25104;&#20154;&#29289;&#21160;&#30011;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;a) &#23398;&#20064;&#20851;&#20110;&#20154;&#20307;&#21644;&#26381;&#35013;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;b) &#20197;&#36866;&#24403;&#30340;&#26381;&#35013;&#21644;&#32441;&#29702;&#28210;&#26579;&#26032;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#23545;&#20110;&#31532;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#22635;&#20805;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#32473;&#23450;&#21333;&#24352;&#22270;&#20687;&#29983;&#25104;&#20154;&#29289;&#30340;&#19981;&#21487;&#35265;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#32441;&#29702;&#26144;&#23556;&#31354;&#38388;&#19978;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#23039;&#21183;&#21644;&#35270;&#35282;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#28210;&#26579;&#27969;&#27700;&#32447;&#65292;&#30001;3D&#20154;&#20307;&#23039;&#21183;&#25511;&#21046;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#20154;&#29289;&#26032;&#23039;&#21183;&#30340;&#28210;&#26579;&#22270;&#20687;&#65292;&#21253;&#25324;&#26381;&#35013;&#12289;&#22836;&#21457;&#21644;&#26410;&#30693;&#21306;&#22495;&#30340;&#21512;&#29702;&#34917;&#20805;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#26082;&#31526;&#21512;3D&#23039;&#21183;&#20013;&#30340;&#30446;&#26631;&#36816;&#21160;&#65292;&#20063;&#31526;&#21512;&#35270;&#35273;&#19978;&#19982;&#36755;&#20837;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.13524</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#37327;&#23376;&#35745;&#31639;&#65288;MBQC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#29420;&#29305;&#30340;&#33539;&#20363;&#26469;&#35774;&#35745;&#37327;&#23376;&#31639;&#27861;&#12290;&#22312;MBQC&#20013;&#65292;&#30001;&#20110;&#37327;&#23376;&#27979;&#37327;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#33258;&#28982;&#30340;&#25805;&#20316;&#19981;&#26159;&#30830;&#23450;&#24615;&#21644;&#24186;&#27491;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#27010;&#29575;&#38468;&#24102;&#30340;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;MBQC&#30340;&#20027;&#35201;&#31639;&#27861;&#24212;&#29992;&#26159;&#23436;&#20840;&#25269;&#28040;&#36825;&#31181;&#27010;&#29575;&#24615;&#36136;&#65292;&#20197;&#27169;&#25311;&#34920;&#36798;&#22312;&#30005;&#36335;&#27169;&#22411;&#20013;&#30340;&#24186;&#27491;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;MBQC&#31639;&#27861;&#30340;&#24605;&#36335;&#65292;&#35813;&#31639;&#27861;&#25509;&#21463;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#23558;MBQC&#20013;&#30340;&#38543;&#26426;&#38468;&#24102;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#24615;&#26377;&#30410;&#30340;&#33258;&#28982;&#24212;&#29992;&#65292;&#21363;&#29983;&#25104;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29983;&#25104;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25511;&#21046;&#21442;&#25968;&#30340;&#21464;&#20998;MBQC&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#35843;&#25972;&#20801;&#35768;&#22312;&#35745;&#31639;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07064</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07064
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#20986;&#19968;&#20123;&#31034;&#20363;&#21644;&#20013;&#38388;&#27493;&#39588;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;LLM&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#25552;&#31034;&#26041;&#27861;&#22312;&#38544;&#24335;&#30693;&#35782;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#19981;&#19968;&#33268;&#26102;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#20551;&#35774;&#21040;&#29702;&#35770;" (HtT) &#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#12290;HtT&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#24402;&#32435;&#38454;&#27573;&#21644;&#28436;&#32462;&#38454;&#27573;&#12290;&#22312;&#24402;&#32435;&#38454;&#27573;&#65292;&#39318;&#20808;&#35201;&#27714;LLM&#26681;&#25454;&#19968;&#32452;&#35757;&#32451;&#31034;&#20363;&#29983;&#25104;&#21644;&#39564;&#35777;&#35268;&#21017;&#12290;&#20986;&#29616;&#24182;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#35268;&#21017;&#23558;&#34987;&#25910;&#38598;&#24418;&#25104;&#19968;&#20010;&#35268;&#21017;&#24211;&#12290;&#22312;&#28436;&#32462;&#38454;&#27573;&#65292;&#28982;&#21518;&#35201;&#27714;LLM&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#21017;&#24211;&#36827;&#34892;&#25512;&#29702;&#20197;&#22238;&#31572;&#27979;&#35797;&#38382;&#39064;&#12290;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20851;&#31995;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HtT&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01930</link><description>&lt;p&gt;
&#23398;&#20064;ECG&#20449;&#21495;&#29305;&#24449;&#30340;&#38750;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning ECG signal features without backpropagation. (arXiv:2307.01930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#26088;&#22312;&#21457;&#29616;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#21407;&#22987;&#25968;&#25454;&#30340;&#26377;&#25928;&#29305;&#24449;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#36825;&#20010;&#26032;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23646;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#29305;&#24449;&#30340;&#32447;&#24615;&#35268;&#24459;&#12290;&#36890;&#36807;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#35268;&#24459;&#22312;&#21069;&#21521;&#26041;&#24335;&#19979;&#29983;&#25104;&#19968;&#20010;&#19982;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.08996</link><description>&lt;p&gt;
EDO-Net: &#20174;&#22270;&#21160;&#21147;&#23398;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics. (arXiv:2209.08996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08996
&lt;/p&gt;
&lt;p&gt;
EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#22270;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#29289;&#29702;&#23646;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20363;&#22914;&#20174;&#25289;&#20280;&#20132;&#20114;&#20013;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDO-Net&#65288;&#24377;&#24615;&#21487;&#21464;&#24418;&#23545;&#35937;-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20855;&#26377;&#19981;&#21516;&#24377;&#24615;&#23646;&#24615;&#30340;&#22823;&#37327;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#23646;&#24615;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;EDO-Net&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#21644;&#19968;&#20010;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22359;&#12290;&#21069;&#32773;&#36127;&#36131;&#25552;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#39044;&#27979;&#20197;&#22270;&#24418;&#34920;&#31034;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#20102;EDO-Net&#30340;&#33021;&#21147;&#65306;1&#65289;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;2&#65289;&#36716;&#31227;&#23398;&#20064;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned rep
&lt;/p&gt;</description></item></channel></rss>