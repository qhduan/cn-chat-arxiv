<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>LLM&#21644;MLLM&#30340;&#36827;&#27493;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#65292;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12289;&#26041;&#27861;&#35770;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;</title><link>https://arxiv.org/abs/2404.02039</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model-Based Game Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02039
&lt;/p&gt;
&lt;p&gt;
LLM&#21644;MLLM&#30340;&#36827;&#27493;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#65292;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12289;&#26041;&#27861;&#35770;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#22312;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;LLM&#21450;&#20854;&#22810;&#27169;&#24577;&#23545;&#24212;&#29289;&#65288;MLLM&#65289;&#30340;&#36827;&#23637;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#30340;&#30005;&#33041;&#28216;&#25103;&#29615;&#22659;&#20013;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#20174;&#25972;&#20307;&#35270;&#35282;&#20840;&#38754;&#27010;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#24863;&#30693;&#12289;&#35760;&#24518;&#12289;&#24605;&#32500;&#12289;&#35282;&#33394;&#25198;&#28436;&#12289;&#34892;&#21160;&#21644;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;LLM&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;&#20195;&#34920;&#24615;LLM&#28216;&#25103;&#26234;&#33021;&#20307;&#65292;&#28041;&#21450;&#21040;&#20845;&#31867;&#28216;&#25103;&#20013;&#30340;&#26041;&#27861;&#35770;&#21644;&#36866;&#24212;&#33021;&#21147;&#65292;&#21253;&#25324;&#20882;&#38505;&#12289;&#27807;&#36890;&#12289;&#31454;&#20105;&#12289;&#21512;&#20316;&#12289;&#27169;&#25311;&#20197;&#21450;&#21019;&#36896;&#19982;&#25506;&#32034;&#28216;&#25103;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02039v1 Announce Type: new  Abstract: The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting &amp; exploration games. Finally, we present an outlook of future research
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#65292;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.09184</link><description>&lt;p&gt;
&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Algorithms for Verification of Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#65292;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#22522;&#20110;Br\'azdil, T.&#31561;&#20154;&#65288;2014&#65289;&#30340;&#24819;&#27861;&#12290;&#35813;&#26694;&#26550;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#26159;&#20381;&#38752;&#21551;&#21457;&#24335;&#12290;&#26412;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23545;&#22522;&#30784;&#29702;&#35770;&#30340;&#20960;&#20010;&#32454;&#33410;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#38169;&#35823;&#20462;&#27491;&#12290;&#31532;1.3&#33410;&#25552;&#20379;&#20102;&#25152;&#26377;&#24046;&#24322;&#30340;&#27010;&#36848;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#27010;&#29575;&#21487;&#36798;&#24615;&#65292;&#36825;&#26159;&#39564;&#35777;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24182;&#20855;&#20307;&#21270;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#31532;&#19968;&#20010;&#20551;&#35774;&#23436;&#20840;&#20102;&#35299;MDP&#65292;&#23588;&#20854;&#26159;&#31934;&#30830;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#23427;&#25191;&#34892;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#27169;&#22411;&#37096;&#20998;&#25506;&#32034;&#65292;&#20135;&#29983;&#31934;&#20934;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09184v1 Announce Type: cross  Abstract: We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\'azdil, T. et al. (2014). Verification of Markov Decision Processes Using Learning Algorithms. The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics. This approach is significantly extended in this work. Several details of the base theory are refined and errors are fixed. Section 1.3 provides an overview of all differences.   The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding preci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.11498</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#22797;&#26434;&#26426;&#22120;&#20154;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Verifiably Following Complex Robot Instructions with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20154;&#20204;&#24076;&#26395;&#22312;&#25351;&#23548;&#26426;&#22120;&#20154;&#26102;&#33021;&#22815;&#28789;&#27963;&#34920;&#36798;&#32422;&#26463;&#65292;&#25351;&#21521;&#20219;&#24847;&#22320;&#26631;&#24182;&#39564;&#35777;&#34892;&#20026;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#23558;&#20154;&#31867;&#25351;&#20196;&#28040;&#38500;&#27495;&#20041;&#65292;&#23558;&#25351;&#20196;&#21442;&#29031;&#29289;&#32852;&#31995;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;&#19982;&#20808;&#21069;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LIMP&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25351;&#20196;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#20154;&#19982;&#25351;&#23548;&#32773;&#39044;&#26399;&#21160;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11498v1 Announce Type: cross  Abstract: Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.02182</link><description>&lt;p&gt;
&#22312;&#38750;&#38745;&#24577;&#19978;&#19979;&#25991;&#39537;&#21160;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Non-Stationary Context-Driven Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#29983;&#19978;&#19979;&#25991;&#36807;&#31243;&#24433;&#21709;&#30528;&#29615;&#22659;&#21160;&#24577;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#38543;&#30528;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26032;&#32463;&#39564;&#22686;&#21152;&#65292;&#20195;&#29702; tend to forget &#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26631;&#31614;&#65288;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#19981;&#23384;&#22312;&#30340;&#65289;&#25110;&#32773;&#20351;&#29992;&#33073;&#26426;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Locally Constrained Policy Optimization (LCPO) &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#30340;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#38170;&#23450;&#65292;LCPO&#20351;&#29992;&#26469;&#33258;&#24403;&#21069;&#19978;&#19979;&#25991;&#20998;&#24067;&#20043;&#22806;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#23616;&#37096;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;Mujoco&#12289;&#32463;&#20856;&#25511;&#21046;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19978;&#19979;&#25991;&#36319;&#36394;&#65292;&#35780;&#20272;&#20102;LCPO&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14906</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#21644;&#25439;&#22833;&#20989;&#25968;&#25193;&#23637;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30005;&#21830;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#38598;&#25104;&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRON&#65292;&#19968;&#31181;&#20351;&#29992;&#20248;&#21270;&#30340;&#36127;&#37319;&#26679;&#30340;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;Transformer&#25512;&#33616;&#31995;&#32479;&#12290;&#21463;&#21040;SASRec&#21644;GRU4Rec+&#31561;&#29616;&#26377;&#27169;&#22411;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;TRON&#38598;&#25104;&#20102;top-k&#36127;&#37319;&#26679;&#21644;&#21015;&#34920;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#20854;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#22312;&#30456;&#20851;&#30340;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;TRON&#22312;&#20445;&#25345;&#19982;SASRec&#31867;&#20284;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#19968;&#39033;&#23454;&#26102;&#30340;A/B&#27979;&#35797;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;SASRec&#65292;TRON&#30340;&#28857;&#20987;&#29575;&#22686;&#21152;&#20102;18.14%&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.02868</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#29609;&#25991;&#23383;&#28216;&#25103;&#65311;&#29616;&#29366;&#21644;&#26410;&#26469;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#29992;&#25143;&#36890;&#20449;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#26088;&#22312;&#35843;&#26597;&#23427;&#20204;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#29609;&#23478;&#36890;&#36807;&#19982;&#28216;&#25103;&#19990;&#30028;&#30340;&#23545;&#35805;&#26469;&#29702;&#35299;&#29615;&#22659;&#24182;&#23545;&#24773;&#20917;&#20570;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;ChatGPT&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;ChatGPT&#26080;&#27861;&#36890;&#36807;&#29609;&#28216;&#25103;&#25110;&#38405;&#35835;&#28216;&#25103;&#25163;&#20876;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#65307;&#23427;&#21487;&#33021;&#26080;&#27861;&#21033;&#29992;&#23427;&#24050;&#32463;&#25317;&#26377;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#23427;&#26080;&#27861;&#25512;&#26029;&#20986;&#38543;&#30528;&#28216;&#25103;&#36827;&#23637;&#30340;&#27599;&#19968;&#27493;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20132;&#21449;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item></channel></rss>