<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.11714</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#25442;&#22120;&#65306;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#38388;&#29305;&#24615;&#65292;&#21253;&#25324;&#26412;&#22320;&#30456;&#20851;&#24615;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;AAE'&#65292;&#23427;&#30001;&#19968;&#20010;&#23545;&#25239;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#21644;&#19968;&#20010;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;'&#30340;&#26032;&#35774;&#35745;&#26550;&#26500;&#32452;&#25104;&#12290;&#26102;&#38388;&#21464;&#25442;&#22120;&#39318;&#20808;&#36890;&#36807;&#23618;&#27425;&#24182;&#34892;&#35774;&#35745;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;Transformer&#30340;&#33021;&#21147;&#65292;&#20998;&#21035;&#25552;&#21462;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22312;&#20004;&#20010;&#20998;&#25903;&#20043;&#38388;&#25552;&#20379;&#20114;&#34917;&#30340;&#24341;&#23548;&#65292;&#24182;&#23454;&#29616;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#21512;&#36866;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
&lt;/p&gt;</description></item></channel></rss>