<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.10860</link><description>&lt;p&gt;
&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation for Endoscopic Visual Odometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#37324;&#31243;&#35745;&#22312;&#20869;&#31397;&#38236;&#25104;&#20687;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#32570;&#20047;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#24615;&#30340;&#22270;&#20687;&#23545;&#20110;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#20026;&#36830;&#25509;&#26415;&#21069;&#35268;&#21010;&#39046;&#22495;&#21644;&#26415;&#20013;&#23454;&#38469;&#39046;&#22495;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#23384;&#22312;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#12290;&#20026;&#20102;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 Announce Type: cross  Abstract: Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30284;&#30151;&#30740;&#31350;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10102</link><description>&lt;p&gt;
&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#30284;&#30151;&#30740;&#31350;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#12289;&#20998;&#24067;&#24335;&#30340;&#12289;&#21512;&#20316;&#30340;&#22522;&#20110;FCM&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30284;&#30151;&#30740;&#31350;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#27169;&#31946;&#35748;&#30693;&#22270;&#12290;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#21327;&#20316;FCM&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#31526;&#21512;&#24403;&#21069;&#35268;&#23450;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30284;&#30151;&#26816;&#27979;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;&#31867;&#20284;&#20110;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10102v1 Announce Type: new  Abstract: Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the Federated Learning process, and obtaining similar results to the ones that can be found in the literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09444</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multimodal Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#26159;&#35780;&#20272;&#21160;&#20316;&#25191;&#34892;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20165;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24573;&#35270;&#20102;&#38899;&#39057;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;AQA&#39640;&#24230;&#20381;&#36182;&#35270;&#35273;&#20449;&#24687;&#65292;&#20294;&#38899;&#39057;&#20063;&#26159;&#25552;&#39640;&#35780;&#20998;&#22238;&#24402;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#34917;&#20805;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#32972;&#26223;&#38899;&#20048;&#30340;&#36816;&#21160;&#39033;&#30446;&#20013;&#65292;&#22914;&#33457;&#26679;&#28369;&#20912;&#21644;&#38901;&#24459;&#20307;&#25805;&#12290;&#20026;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;AQA&#65292;&#21363;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;PAMFN&#65289;&#65292;&#23427;&#20998;&#21035;&#23545;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#21644;&#19968;&#20010;&#28151;&#21512;&#27169;&#24577;&#20998;&#25903;&#32452;&#25104;&#65292;&#29420;&#31435;&#22320;&#25506;&#32034;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#28176;&#36827;&#22320;&#32858;&#21512;&#26469;&#33258;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09444v1 Announce Type: cross  Abstract: Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01669</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#30340;&#25913;&#36827;&#65306;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#20064;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#26657;&#20013;&#65292;&#22823;&#35268;&#27169;&#30340;&#35838;&#22530;&#35268;&#27169;&#32473;&#20010;&#24615;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#25945;&#32946;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#36827;&#23637;&#20551;&#35774;&#65288;LPH&#65289;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;ZPDES&#31639;&#27861;&#23545;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65288;LP&#65289;&#30340;&#32451;&#20064;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#31639;&#27861;&#22312;&#20043;&#21069;&#30340;&#23454;&#22320;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#23398;&#20064;&#34920;&#29616;&#25552;&#21319;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#35838;&#31243;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20854;&#21160;&#26426;&#24433;&#21709;&#23578;&#26410;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;ZPDES&#19981;&#20801;&#35768;&#23398;&#29983;&#21457;&#34920;&#36873;&#25321;&#24847;&#35265;&#12290;&#36825;&#31181;&#32570;&#20047;&#26426;&#26500;&#30340;&#38480;&#21046;&#19982;&#20851;&#27880;&#24314;&#27169;&#22909;&#22855;&#39537;&#21160;&#23398;&#20064;&#30340;LPH&#29702;&#35770;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30740;&#31350;&#20102;&#36825;&#31181;&#36873;&#25321;&#21487;&#33021;&#24615;&#30340;&#24341;&#20837;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#12290;&#32473;&#23450;&#30340;&#36873;&#25321;&#19982;&#32451;&#20064;&#38590;&#24230;&#27491;&#20132;&#30340;&#32500;&#24230;&#26377;&#20851;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14688</link><description>&lt;p&gt;
&#19987;&#23478;&#25552;&#31034;&#65306;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. (arXiv:2305.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#8220;&#19987;&#23478;&#25552;&#31034;&#8221;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26480;&#20986;&#30340;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#24182;&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#36825;&#20123;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#65292;&#35813;&#21161;&#25163;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;96&#65285;&#30340;ChatGPT&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20197;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#31572;&#36136;&#37327;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#25552;&#31034;&#65292;&#20197;&#24341;&#21457;LLMs&#20316;&#20026;&#26480;&#20986;&#19987;&#23478;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#27599;&#20010;&#29305;&#23450;&#25351;&#20196;&#30340;&#35814;&#32454;&#21644;&#23450;&#21046;&#30340;&#19987;&#23478;&#36523;&#20221;&#25551;&#36848;&#65292;&#28982;&#21518;&#35201;&#27714;LLMs&#26681;&#25454;&#36825;&#31181;&#20195;&#29702;&#20154;&#32972;&#26223;&#25552;&#20379;&#31572;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#22686;&#24378;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#24320;&#28304;&#32842;&#22825;&#21161;&#25163;ExpertLLaMA&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;GPT4&#30340;&#35780;&#20272;&#26174;&#31034;&#65306;1&#65289;&#19987;&#23478;&#25968;&#25454;&#30340;&#36136;&#37327;&#26174;&#33879;&#39640;&#20110;&#26222;&#36890;&#31572;&#26696;&#65292;2&#65289;ExpertLLaMA&#32988;&#36807;&#29616;&#26377;&#30340;&#24320;&#28304;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;ChatGPT&#33021;&#21147;&#30340;96&#65285;&#12290;&#25152;&#26377;&#25968;&#25454;&#21644;ExpertLLaMA&#27169;&#22411;&#23558;&#22312;\url{https://github.com/OFA-Sys/Exp}&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \url{https://github.com/OFA-Sys/Exp
&lt;/p&gt;</description></item></channel></rss>