<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20280</link><description>&lt;p&gt;
&#24102;&#26377;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#31232;&#30095;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sparse multimodal fusion with modal channel attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20280
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#20316;&#20026;&#27169;&#24577;&#31232;&#30095;&#24230;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#20013;&#24341;&#20837;&#20102;&#27169;&#24577;&#19981;&#23436;&#20840;&#36890;&#36947;&#65292;&#31216;&#20026;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#12290;&#20351;&#29992;&#20102;&#21253;&#21547;4&#31181;&#27169;&#24577;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CMU-MOSEI&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;TCGA&#29992;&#20110;&#22810;&#32452;&#23398;&#12290;&#27169;&#22411;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#26679;&#26412;&#20013;&#21482;&#29992;&#20102;&#22235;&#31181;&#27169;&#24577;&#20013;&#30340;&#20004;&#31181;&#23601;&#23398;&#20064;&#20986;&#32479;&#19968;&#19988;&#23545;&#40784;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#27169;&#24577;&#31232;&#30095;&#65292;&#25152;&#25552;&#20986;&#30340;MCA&#26426;&#21046;&#20063;&#33021;&#25552;&#39640;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#65292;&#21484;&#22238;&#25351;&#26631;&#65292;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20177</link><description>&lt;p&gt;
&#20154;&#24037;&#24847;&#35782;&#12290;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;
&lt;/p&gt;
&lt;p&gt;
Artificial consciousness. Some logical and conceptual preliminaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20177
&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#24847;&#35782;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#33021;&#65311;&#26159;&#21542;&#21512;&#20046;&#24773;&#29702;&#65311;&#22914;&#26524;&#26159;&#65292;&#37027;&#20040;&#25216;&#26415;&#19978;&#21487;&#34892;&#21527;&#65311;&#35201;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26377;&#24517;&#35201;&#22880;&#23450;&#19968;&#20123;&#22522;&#30784;&#65292;&#38416;&#26126;&#20154;&#24037;&#24847;&#35782;&#20135;&#29983;&#30340;&#36923;&#36753;&#21644;&#32463;&#39564;&#26465;&#20214;&#20197;&#21450;&#28041;&#21450;&#30340;&#30456;&#20851;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;&#24847;&#35782;&#26159;&#19968;&#20010;&#22810;&#20041;&#35789;&#65306;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#31070;&#32463;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21746;&#23398;&#31561;&#65292;&#26377;&#26102;&#20250;&#20351;&#29992;&#19981;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#30456;&#21516;&#29616;&#35937;&#65292;&#25110;&#32773;&#20351;&#29992;&#30456;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#19981;&#21516;&#29616;&#35937;&#12290;&#20107;&#23454;&#19978;&#65292;&#22914;&#26524;&#25105;&#20204;&#24819;&#25506;&#35752;&#20154;&#24037;&#24847;&#35782;&#65292;&#23601;&#38656;&#35201;&#24688;&#24403;&#30028;&#23450;&#20851;&#38190;&#27010;&#24565;&#12290;&#22312;&#27492;&#65292;&#32463;&#36807;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;&#24037;&#20316;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#36827;&#34892;&#24179;&#34913;&#35752;&#35770;&#65292;&#25506;&#35752;&#23427;&#20204;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#21487;&#33021;&#23454;&#20363;&#21270;&#25110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 Announce Type: new  Abstract: Is artificial consciousness theoretically possible? Is it plausible? If so, is it technically feasible? To make progress on these questions, it is necessary to lay some groundwork clarifying the logical and empirical conditions for artificial consciousness to arise and the meaning of relevant terms involved. Consciousness is a polysemic word: researchers from different fields, including neuroscience, Artificial Intelligence, robotics, and philosophy, among others, sometimes use different terms in order to refer to the same phenomena or the same terms to refer to different phenomena. In fact, if we want to pursue artificial consciousness, a proper definition of the key concepts is required. Here, after some logical and conceptual preliminaries, we argue for the necessity of using dimensions and profiles of consciousness for a balanced discussion about their possible instantiation or realisation in artificial systems. Our primary goal in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#20013;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#65292;&#26088;&#22312;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17234</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#20572;&#36710;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;MCTS&#20013;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#20013;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#65292;&#26088;&#22312;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#36807;&#31243;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#35813;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#25972;&#21512;&#21040;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20013;&#65292;&#20197;&#25552;&#21319;&#22312;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#36827;&#34892;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#30340;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#12290;&#22312;&#39640;&#32500;&#31354;&#38388;&#19979;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#32791;&#26102;&#38271;&#30340;&#29305;&#28857;&#12290;&#29366;&#24577;&#35780;&#20272;&#26041;&#27861;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#20110;&#25628;&#32034;&#27493;&#39588;&#20013;&#65292;&#20351;&#23454;&#26102;&#31995;&#32479;&#20013;&#30340;&#36807;&#31243;&#26356;&#24555;&#36895;&#12290;&#37492;&#20110;&#33258;&#21160;&#20572;&#36710;&#20219;&#21153;&#36890;&#24120;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25191;&#34892;&#65292;&#20256;&#32479;&#20998;&#26512;&#26041;&#24335;&#38590;&#20197;&#26500;&#24314;&#22362;&#23454;&#20294;&#36731;&#37327;&#32423;&#30340;&#21551;&#21457;&#24335;&#25351;&#23548;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36335;&#24452;&#35268;&#21010;&#26694;&#26550;&#19979;&#20855;&#26377;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24378;&#21270;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#23398;&#20064;&#29366;&#24577;&#30340;&#20215;&#20540;&#20197;&#21450;&#26368;&#20339;&#21160;&#20316;&#65292;&#22312;&#21069;&#19968;&#20010;&#21608;&#26399;&#32467;&#26524;&#30340;&#26679;&#26412;&#20013;&#36873;&#25321;&#26368;&#20339;&#21160;&#20316;&#65292;&#25105;&#20204;&#33021;&#22815;&#24314;&#27169;&#19968;&#20010;&#20540;&#20272;&#35745;&#22120;&#20197;&#21450;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17234v1 Announce Type: new  Abstract: In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15474</link><description>&lt;p&gt;
EC-IoU: &#36890;&#36807;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#35843;&#25972;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#65288;EC-IoU&#65289;&#24230;&#37327;&#26469;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#26469;&#20248;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;IoU&#24230;&#37327;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#33258;&#25105;&#20195;&#29702;&#20154;&#30340;&#35270;&#35282;&#35206;&#30422;&#26356;&#36817;&#30340;&#22320;&#38754;&#30495;&#23454;&#23545;&#35937;&#28857;&#30340;&#39044;&#27979;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;EC-IoU&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20856;&#22411;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#36873;&#25321;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#34920;&#29616;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#36824;&#21487;&#20197;&#38598;&#25104;&#21040;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#23613;&#31649;&#38754;&#21521;&#23433;&#20840;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;EC-IoU&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22343;&#20540;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20248;&#20110;&#20351;&#29992;IoU&#35757;&#32451;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18205</link><description>&lt;p&gt;
Lemur: &#20351;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18205
&lt;/p&gt;
&lt;p&gt;
Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#20135;&#29983;&#30340;&#26085;&#24535;&#23545;&#30417;&#35270;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#26085;&#24535;&#20998;&#26512;&#26377;&#21161;&#20110;&#26816;&#27979;&#12289;&#25253;&#35686;&#21644;&#35786;&#26029;&#31995;&#32479;&#25925;&#38556;&#12290;&#26085;&#24535;&#35299;&#26512;&#26159;&#26085;&#24535;&#20998;&#26512;&#33258;&#21160;&#21270;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23427;&#28041;&#21450;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#27169;&#26495;&#12290;&#29616;&#26377;&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#30001;&#20110;&#20381;&#36182;&#20110;&#20154;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#32780;&#26080;&#27861;&#35782;&#21035;&#27491;&#30830;&#30340;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#32479;&#35745;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#26085;&#24535;&#28040;&#24687;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65288;Lemur&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#21160;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29109;&#21551;&#21457;&#30340;&#26032;&#22411;&#25277;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20856;&#22411;&#26085;&#24535;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#26085;&#24535;&#27169;&#26495;&#30340;&#21512;&#24182;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24605;&#32500;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18205v1 Announce Type: cross  Abstract: Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method f
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08112</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;microRTS&#20013;&#33719;&#22870;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Competition Winning Deep Reinforcement Learning Agent in microRTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08112
&lt;/p&gt;
&lt;p&gt;
&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CIG&#21644;CoG&#20030;&#21150;&#30340;IEEE microRTS&#65288;$\mu$RTS&#65289;&#31454;&#36187;&#30340;&#20116;&#23626;&#20013;&#65292;&#33050;&#26412;&#20195;&#29702;&#20027;&#23548;&#20102;&#27604;&#36187;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#22312;&#23454;&#26102;&#31574;&#30053;&#65288;RTS&#65289;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#36164;&#28304;&#20197;&#21450;&#21019;&#24314;&#21644;&#35843;&#35797;&#27492;&#31867;&#20195;&#29702;&#25152;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20027;&#35201;&#26159;&#23398;&#26415;&#31454;&#36187;&#20013;&#30340;&#37319;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;RAISocketAI&#26159;&#31532;&#19968;&#20010;&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#33719;&#32988;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#19968;&#20010;&#27809;&#26377;&#24615;&#33021;&#38480;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RAISocketAI&#32463;&#24120;&#20987;&#36133;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#12290;&#36825;&#20010;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;DRL&#25552;&#20132;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;microRTS&#31454;&#36187;&#30340;&#22522;&#20934;&#65292;&#24182;&#25104;&#20026;&#26410;&#26469;DRL&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#23545;&#29305;&#23450;&#22320;&#22270;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#23545;RAISocketAI&#30340;&#33719;&#32988;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#32463;&#27982;&#35757;&#32451;&#26410;&#26469;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2311.05546</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20351;&#29992;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#26234;&#33021;&#20135;&#19994;&#24212;&#29992;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#37319;&#29992;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#36139;&#30240;&#24179;&#21488;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#24615;&#33021;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;Coin Game&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36951;&#20256;&#21464;&#31181;&#65292;&#24182;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#26041;&#27861;&#30456;&#27604;&#20110;&#20855;&#26377;&#31867;&#20284;&#21442;&#25968;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01571</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#65292;&#22914;&#20837;&#38498;&#35760;&#24405;&#12289;&#20986;&#38498;&#23567;&#32467;&#21644;&#36827;&#23637;&#35760;&#24405;&#65292;&#21253;&#21547;&#20016;&#23500;&#32780;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20020;&#24202;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#21333;&#29420;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#25110;&#25991;&#26412;&#32858;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#22312;&#20004;&#20010;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#65288;&#27515;&#20129;&#39044;&#27979;&#21644;&#20303;&#38498;&#22825;&#25968;&#39044;&#27979;&#65289;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#32447;&#12289;&#29420;&#31435;&#30340;&#38598;&#25104;&#21644;&#32858;&#21512;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.17091</link><description>&lt;p&gt;
&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#30340;&#38544;&#34109;&#32593;&#32476;&#25915;&#20987;&#30340;&#26816;&#27979;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach. (arXiv:2310.17091v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37197;&#22791;&#20102;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#20854;&#20182;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#30340;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38024;&#23545;&#36825;&#20123;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#32593;&#32476;&#25915;&#20987;&#28508;&#22312;&#39118;&#38505;&#20063;&#20986;&#29616;&#20102;&#12290;&#34429;&#28982;&#24378;&#21046;&#36710;&#36742;&#21457;&#29983;&#30896;&#25758;&#30340;&#26126;&#26174;&#25915;&#20987;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20294;&#26356;&#38544;&#34109;&#30340;&#25915;&#20987;&#65292;&#21482;&#30053;&#24494;&#25913;&#21464;&#34892;&#39542;&#34892;&#20026;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#33539;&#22260;&#20869;&#25317;&#22581;&#12289;&#29123;&#27833;&#28040;&#32791;&#22686;&#21152;&#65292;&#29978;&#33267;&#22686;&#21152;&#30896;&#25758;&#39118;&#38505;&#65292;&#20294;&#24456;&#38590;&#34987;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#33021;&#30340;&#19977;&#31181;&#32593;&#32476;&#25915;&#20987;&#31867;&#22411;&#65306;&#24694;&#24847;&#25805;&#32437;&#36710;&#36742;&#25511;&#21046;&#21629;&#20196;&#12289;&#23545;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#34394;&#20551;&#27880;&#20837;&#25915;&#20987;&#21644;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;&#20010;&#20307;&#36710;&#36742;&#65288;&#24494;&#35266;&#65289;&#21644;&#20132;&#36890;&#27969;&#65288;&#23439;&#35266;&#65289;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of vehicles equipped with advanced driver-assistance systems, such as adaptive cruise control (ACC) and other automated driving features, the potential for cyberattacks on these automated vehicles (AVs) has emerged. While overt attacks that force vehicles to collide may be easily identified, more insidious attacks, which only slightly alter driving behavior, can result in network-wide increases in congestion, fuel consumption, and even crash risk without being easily detected. To address the detection of such attacks, we first present a traffic model framework for three types of potential cyberattacks: malicious manipulation of vehicle control commands, false data injection attacks on sensor measurements, and denial-of-service (DoS) attacks. We then investigate the impacts of these attacks at both the individual vehicle (micro) and traffic flow (macro) levels. A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.04522</link><description>&lt;p&gt;
&#36830;&#25509;NTK&#21644;NNGP&#65306;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#22312;&#26680;&#21306;&#22495;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#32570;&#20047;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#23545;&#20110;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#25551;&#36848;&#32593;&#32476;&#30340;&#36755;&#20986;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26694;&#26550;&#65292;&#20551;&#35774;&#20102;&#32447;&#24615;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#65307;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21644;&#23398;&#20064;&#21518;&#30340;&#32593;&#32476;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#30340;&#31934;&#30830;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#31070;&#32463;&#21160;&#24577;&#26680;&#65288;NDK&#65289;&#65292;&#36825;&#20010;&#26680;&#21487;&#20197;&#21516;&#26102;&#20135;&#29983;NTK&#21644;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2308.04399</link><description>&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models. (arXiv:2308.04399v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#24320;&#21457;&#21644;&#21457;&#24067;&#36890;&#29992;&#27169;&#22411;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#30001;&#20854;&#20182;&#20225;&#19994;&#21644;&#26426;&#26500;&#36827;&#34892;&#36866;&#24212;&#65292;&#20197;&#25191;&#34892;&#29305;&#23450;&#30340;&#39046;&#22495;&#19987;&#29992;&#21151;&#33021;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#36866;&#24212;&#25110;&#24494;&#35843;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20301;&#36890;&#29992;&#19987;&#23478;&#23558;&#25216;&#26415;&#20135;&#21697;&#65288;&#21363;ML&#27169;&#22411;&#65289;&#25552;&#21319;&#21040;&#19968;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#19968;&#20301;&#25110;&#22810;&#20301;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#35843;&#25972;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#36825;&#20004;&#20010;&#23454;&#20307;&#37117;&#26159;&#36861;&#27714;&#21033;&#28070;&#30340;&#65292;&#24403;&#20182;&#20204;&#25237;&#36164;&#20110;&#25216;&#26415;&#26102;&#20250;&#20135;&#29983;&#25104;&#26412;&#65292;&#22312;&#25216;&#26415;&#36827;&#20837;&#24066;&#22330;&#21069;&#65292;&#20182;&#20204;&#24517;&#39035;&#23601;&#22914;&#20309;&#20998;&#20139;&#25910;&#20837;&#36798;&#25104;&#35848;&#21028;&#21327;&#35758;&#12290;&#23545;&#20110;&#30456;&#23545;&#19968;&#33324;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#24494;&#35843;&#21338;&#24328;&#20135;&#29983;&#21033;&#28070;&#20998;&#20139;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20219;&#20309;&#28508;&#22312;&#30340;&#39046;&#22495;&#19987;&#19994;&#21270;&#37117;&#20250;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03340</link><description>&lt;p&gt;
&#23450;&#21521;&#28436;&#21270;&#21644;&#29983;&#24577;&#36827;&#21270;&#21160;&#21147;&#23398;&#30340;&#29983;&#29289;&#29289;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics. (arXiv:2305.03340v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#21160;&#21147;&#23398;&#20013;&#30340;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#21487;&#20197;&#22312;&#21338;&#24328;&#29702;&#35770;&#32972;&#26223;&#19979;&#20197;&#38543;&#26426;&#36712;&#36857;&#20998;&#26512;&#30340;&#26041;&#24335;&#26377;&#24847;&#20041;&#22320;&#26144;&#23556;&#21040;&#20998;&#26512;&#20013;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#23569;&#37327;&#19981;&#21516;&#30340;&#32676;&#20307;&#21644;/&#25110;&#20551;&#35774;&#21160;&#21147;&#23398;&#21457;&#29983;&#22312;&#20154;&#21475;&#35268;&#27169;&#22823;&#21040;&#36275;&#20197;&#20351;&#30830;&#23450;&#24615;&#36712;&#36857;&#25104;&#20026;&#29616;&#23454;&#30340;&#21306;&#22495;&#12290;&#34987;&#31216;&#20026;&#8220;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#8221;&#30340;&#29983;&#24577;&#22240;&#32032;&#30340;&#28155;&#21152;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#21160;&#21147;&#23398;&#65292;&#24182;&#23548;&#33268;&#35768;&#22810;&#38382;&#39064;&#38590;&#20197;&#22788;&#29702;&#25110;&#24403;&#21069;&#30340;&#29702;&#35770;&#26041;&#27861;&#38590;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#19968;&#31181;&#31867;&#20284;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#26041;&#27861;&#26159;&#23558;&#37325;&#28857;&#25918;&#22312;&#27169;&#22411;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#19978;&#65292;&#20381;&#25454;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21644;&#30456;&#37051;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#35821;&#35328;&#65292;&#32780;&#34987;&#31216;&#20026;&#8220;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#20598;&#24615;&#65292;&#23558;&#21516;&#26102;&#32771;&#34385;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#30340;&#22797;&#26434;&#24615;&#26144;&#23556;&#21040;&#19968;&#20010;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many major questions in the theory of evolutionary dynamics can in a meaningful sense be mapped to analyses of stochastic trajectories in game theoretic contexts. Often the approach is to analyze small numbers of distinct populations and/or to assume dynamics occur within a regime of population sizes large enough that deterministic trajectories are an excellent approximation of reality. The addition of ecological factors, termed "eco-evolutionary dynamics", further complicates the dynamics and results in many problems which are intractable or impractically messy for current theoretical methods. However, an analogous but underexplored approach is to analyze these systems with an eye primarily towards uncertainty in the models themselves. In the language of researchers in Reinforcement Learning and adjacent fields, a Partially Observable Markov Process. Here we introduce a duality which maps the complexity of accounting for both ecology and individual genotypic/phenotypic types onto a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04957</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#29983;&#25104;&#33021;&#22815;&#19982;&#26410;&#30693;&#21512;&#20316;&#20249;&#20276;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#21327;&#21516;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26234;&#33021;&#20307;&#26292;&#38706;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#20249;&#20276;&#26102;&#28041;&#21450;&#33258;&#25105;&#23545;&#24328;&#65292;&#38544;&#24335;&#22320;&#20551;&#35774;&#20219;&#21153;&#26159;&#21516;&#36136;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26159;&#24322;&#26500;&#30340;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#36807;&#31243;&#65306;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#65292;&#23545;&#20004;&#20010;&#26234;&#33021;&#20307;&#21644;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21327;&#21516;&#36827;&#21270;&#12290;&#23545;&#19981;&#21516;&#24322;&#26500;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#35299;&#20915;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
&lt;/p&gt;</description></item></channel></rss>