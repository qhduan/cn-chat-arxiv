<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10559</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#25506;&#32034;&#20132;&#36890;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10559
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35843;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#20004;&#31181;&#25512;&#21160;&#25216;&#26415;&#21644;&#20132;&#36890;&#36827;&#27493;&#30340;&#31361;&#30772;&#24615;&#21147;&#37327;&#30340;&#21382;&#21490;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#20851;&#27880;&#29983;&#25104;&#27169;&#22411;&#22312;CAVs&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#31181;&#25972;&#21512;&#22914;&#20309;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#20132;&#36890;&#39046;&#22495;&#25972;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;CAV&#25216;&#26415;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#26088;&#22312;&#24378;&#35843;&#21462;&#24471;&#30340;&#36827;&#23637;&#12289;&#21097;&#20313;&#30340;&#38556;&#30861;&#20197;&#21450;&#22312;&#23433;&#20840;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.16894</link><description>&lt;p&gt;
PFB-Diff: &#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#25193;&#25955;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing. (arXiv:2306.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16894
&lt;/p&gt;
&lt;p&gt;
PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20854;&#21512;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#32534;&#36753;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24120;&#24120;&#22240;&#20026;&#30446;&#26631;&#22270;&#20687;&#21644;&#25193;&#25955;&#28508;&#22312;&#21464;&#37327;&#30340;&#20687;&#32032;&#32423;&#28151;&#21512;&#32780;&#20135;&#29983;&#19981;&#26399;&#26395;&#30340;&#20266;&#24433;&#65292;&#32570;&#20047;&#32500;&#25345;&#22270;&#20687;&#19968;&#33268;&#24615;&#25152;&#24517;&#38656;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PFB-Diff&#65292;&#19968;&#31181;&#36880;&#27493;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;PFB-Diff&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#23558;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#30446;&#26631;&#22270;&#20687;&#26080;&#32541;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#28145;&#23618;&#29305;&#24449;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#35821;&#20041;&#21644;&#20174;&#39640;&#21040;&#20302;&#32423;&#21035;&#30340;&#28176;&#36827;&#28151;&#21512;&#26041;&#26696;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#29305;&#23450;&#35789;&#35821;&#23545;&#32534;&#36753;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have showcased their remarkable capability to synthesize diverse and high-quality images, sparking interest in their application for real image editing. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the pixel-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;</title><link>http://arxiv.org/abs/2303.09823</link><description>&lt;p&gt;
Transformers&#21644;Ensemble&#26041;&#27861;&#65306;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#21152;CERIST NLP&#25361;&#25112;&#36187;2022&#20013;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#23454;&#39564;&#36807;&#31243;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;6&#20010;Transformer&#27169;&#22411;&#21450;&#20854;&#32452;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;2&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#35757;&#32451;&#38598;&#19978;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#20026;F1&#20998;&#25968;&#20026;0.60&#65292;&#20934;&#30830;&#24615;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
&lt;/p&gt;</description></item></channel></rss>