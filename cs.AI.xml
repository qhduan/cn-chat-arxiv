<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#22797;&#26434;&#24615;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;</title><link>https://arxiv.org/abs/2401.17805</link><description>&lt;p&gt;
&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Biospheric AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17805
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20197;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#20026;&#22522;&#30784;&#65292;&#26088;&#22312;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#22797;&#26434;&#24615;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#20215;&#20540;&#35843;&#25972;&#39046;&#22495;&#20013;&#65292;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#36825;&#20123;&#23398;&#31185;&#30340;&#20851;&#27880;&#37325;&#28857;&#20165;&#38480;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#23548;&#33268;&#20854;&#27934;&#23519;&#21147;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#21463;&#38480;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23398;&#32773;&#24050;&#24320;&#22987;&#23581;&#35797;&#25193;&#22823;&#21040;&#20197;&#24863;&#30693;&#32773;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20004;&#31181;&#35266;&#28857;&#37117;&#19981;&#36275;&#20197;&#25429;&#25417;&#29983;&#29289;&#22280;&#30340;&#23454;&#38469;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#23545;&#20854;&#36896;&#25104;&#25439;&#23475;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#8212;&#8212;&#29983;&#29289;&#22280;&#20154;&#24037;&#26234;&#33021;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;&#29983;&#24577;&#20013;&#24515;&#20027;&#20041;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#34987;&#35774;&#35745;&#30340;&#20551;&#35774;&#24615;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19982;&#29983;&#29289;&#22280;&#21033;&#30410;&#19968;&#33268;&#30340;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#24037;&#20316;&#35797;&#22270;&#36808;&#20986;&#39318;&#35201;&#27493;&#39588;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#29983;&#29289;&#22280;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm in AI ethics and value alignment is highly anthropocentric. The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights. Recently, attempts to expand to a sentientist perspective have been initiated. We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it. Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss hypothetical ways in which such an AI might be designed. Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests. All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2204.05798</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#36229;&#22797;&#25968;&#23398;&#20064;&#29992;&#20110;&#20083;&#33146;&#30284;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-View Hypercomplex Learning for Breast Cancer Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25191;&#34892;&#21333;&#35270;&#22270;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20083;&#33146;X-ray&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#21516;&#26102;&#20998;&#26512;&#32452;&#25104;&#20083;&#25151;X&#20809;&#25668;&#24433;&#26816;&#26597;&#30340;&#25152;&#26377;&#22235;&#20010;&#35270;&#22270;&#65292;&#36825;&#20026;&#35782;&#21035;&#32959;&#30244;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25552;&#20986;&#22810;&#35270;&#22270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#30340;&#29616;&#26377;&#26550;&#26500;&#20013;&#65292;&#20083;&#25151;X&#20809;&#22270;&#20687;&#34987;&#29420;&#31435;&#30340;&#21367;&#31215;&#20998;&#25903;&#22788;&#29702;&#20026;&#29420;&#31435;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#22833;&#21435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#12290;&#30001;&#20110;&#36229;&#22797;&#25968;&#20195;&#25968;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#24314;&#27169;&#24182;&#21033;&#29992;&#32452;&#25104;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#29616;&#26377;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#27169;&#25311;&#38405;&#29255;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05798v3 Announce Type: replace-cross  Abstract: Traditionally, deep learning methods for breast cancer classification perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper, we propose a methodological approach for multi-view breast cancer classification based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram, thus mimicking the reading process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04615</link><description>&lt;p&gt;
&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#20998;&#35299;&#22312;&#22522;&#20110;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Value Decomposition Framework with Disentangled World Model&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#21516;&#30446;&#26631;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26080;&#27169;&#22411;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21253;&#25324;&#21160;&#20316;&#26465;&#20214;&#12289;&#26080;&#21160;&#20316;&#21644;&#38745;&#24577;&#20998;&#25903;&#65292;&#26469;&#35299;&#24320;&#29615;&#22659;&#21160;&#24577;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20135;&#29983;&#24819;&#35937;&#20013;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#30495;&#23454;&#29615;&#22659;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#23558;&#20854;&#19982;&#22522;&#20110;&#20540;&#30340;&#26694;&#26550;&#21512;&#24182;&#65292;&#20197;&#39044;&#27979;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#25972;&#20307;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item></channel></rss>