<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#23545;&#28040;&#36153;&#32773;&#21033;&#30410;&#26377;&#23475;&#65292;&#20854;&#35828;&#26126;&#20070;&#20013;&#30340;&#30701;&#35821;&#21464;&#21270;&#21487;&#33021;&#22686;&#21152;&#21246;&#32467;&#12290;</title><link>https://arxiv.org/abs/2404.00806</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collusion by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00806
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#23545;&#28040;&#36153;&#32773;&#21033;&#30410;&#26377;&#23475;&#65292;&#20854;&#35828;&#26126;&#20070;&#20013;&#30340;&#30701;&#35821;&#21464;&#21270;&#21487;&#33021;&#22686;&#21152;&#21246;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00806v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#31639;&#27861;&#23450;&#20215;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#31639;&#27861;&#21246;&#32467;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29305;&#21035;&#26159;GPT-4&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#23450;&#20215;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#65288;2&#65289;&#22522;&#20110;LLM&#30340;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#25439;&#23475;&#28040;&#36153;&#32773;&#21033;&#30410;&#65292;&#65288;3&#65289;LLM&#35828;&#26126;&#20070;&#20013;&#30475;&#20284;&#26080;&#23475;&#30701;&#35821;("&#25552;&#31034;")&#30340;&#21464;&#21270;&#21487;&#33021;&#20250;&#22686;&#21152;&#21246;&#32467;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#25293;&#21334;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26377;&#20851;&#31639;&#27861;&#23450;&#20215;&#30340;&#21453;&#22404;&#26029;&#30417;&#31649;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#23450;&#20215;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00806v1 Announce Type: cross  Abstract: The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ("prompts") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.14494</link><description>&lt;p&gt;
&#23398;&#20064;&#25237;&#24433;&#20197;&#36827;&#34892;&#36328;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Learning to Project for Cross-Task Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;(KD)&#20381;&#36182;&#20110;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#29087;&#32451;&#25945;&#24072;&#65292;&#32780;&#36825;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#36328;&#20219;&#21153;&#33976;&#39311;&#65292;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#20219;&#20309;&#25945;&#24072;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#36825;&#31181;&#36328;&#20219;&#21153;&#35774;&#32622;&#26102;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#65306;&#20351;&#29992;&#21453;&#21521;&#25237;&#24433;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23545;&#26631;&#20934;&#25237;&#24433;&#30340;&#25554;&#20837;&#24335;&#26367;&#20195;&#26159;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#23398;&#20064;&#25490;&#38500;&#21487;&#33021;&#38477;&#20302;&#23398;&#29983;&#34920;&#29616;&#30340;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#36275;&#20197;&#23558;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#25193;&#23637;&#21040;&#36328;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#19981;&#21516;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#22312;&#36328;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30456;&#27604;&#20110;&#20256;&#32479;&#25237;&#24433;&#65292;&#21487;&#33719;&#24471;&#26368;&#39640;1.9%&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14494v1 Announce Type: cross  Abstract: Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14708</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#27450;&#35784;&#23545;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#33410;&#28857;&#30340;&#26412;&#22320;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#8212;&#8212;CaT-GNN&#65288;Causal Temporal Graph Neural Networks&#65289;&#65292;&#21033;&#29992;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#26469;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21457;&#29616;&#21644;&#24178;&#39044;&#38454;&#27573;&#65292;CaT-GNN&#30830;&#23450;&#20132;&#26131;&#22270;&#20013;&#30340;&#22240;&#26524;&#33410;&#28857;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;CaT-GNN&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Causal-Inspector&#21644;Causal-Intervener&#12290;Causal-Inspector&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#35782;&#21035;&#22240;&#26524;&#21644;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14708v1 Announce Type: cross  Abstract: Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environm
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14244</link><description>&lt;p&gt;
MENTOR&#65306;&#22312;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14244
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#20026;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#24182;&#20381;&#27425;&#23436;&#25104;&#30340;&#23618;&#27425;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#23376;&#30446;&#26631;&#26469;&#30830;&#20445;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#25972;&#21512;&#21040;&#20854;&#20013;&#65288;MENTOR&#65289;&#12290;MENTOR&#20805;&#24403;&#8220;&#23548;&#24072;&#8221;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#39640;&#23618;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#20197;&#25214;&#21040;&#26356;&#22909;&#30340;&#23376;&#30446;&#26631;&#12290;&#33267;&#20110;&#20302;&#23618;&#31574;&#30053;&#65292;MENTOR&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#31574;&#30053;&#20197;&#20998;&#21035;&#36827;&#34892;&#25506;&#32034;-&#24320;&#21457;&#35299;&#32806;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20154;&#31867;&#21487;&#20197;&#31616;&#21333;&#22320;&#23558;&#20219;&#21153;&#25286;&#20998;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12050</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Semantic Framework for Neural-Symbolic Computing. (arXiv:2212.12050v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#31995;&#32479;&#65292;&#23545;&#20110;&#19968;&#31995;&#21015;AI&#38382;&#39064;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#22343;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#25152;&#38656;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#20204;&#35748;&#20026;&#36825;&#26159;&#27599;&#31181;&#26041;&#27861;&#20869;&#22312;&#24369;&#28857;&#25152;&#33268;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#20123;&#24369;&#28857;&#20284;&#20046;&#26159;&#20114;&#34917;&#30340;&#65292;&#31526;&#21495;&#31995;&#32479;&#25797;&#38271;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#30340;&#20107;&#29289;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#35797;&#22270;&#21033;&#29992;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#12290;&#36890;&#24120;&#36825;&#26159;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#27809;&#26377;&#20844;&#20849;&#30340;&#32534;&#30721;&#23450;&#20041;&#21487;&#20379;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#31526;&#21495;AI&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#35777;&#26126;&#23427;&#36275;&#20197;&#35299;&#37322;&#22823;&#37327;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#31526;&#21495;&#31995;&#32479;&#26893;&#26681;&#20110;&#20854;&#39046;&#22495;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#21508;&#31181;&#31526;&#21495;&#31995;&#32479;&#22312;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#30340;&#31070;&#32463;&#34920;&#24449;&#21644;&#20351;&#29992;&#22266;&#23450;&#31070;&#32463;&#34920;&#24449;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two approaches to AI, neural networks and symbolic systems, have been proven very successful for an array of AI problems. However, neither has been able to achieve the general reasoning ability required for human-like intelligence. It has been argued that this is due to inherent weaknesses in each approach. Luckily, these weaknesses appear to be complementary, with symbolic systems being adept at the kinds of things neural networks have trouble with and vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry by combining neural networks and symbolic AI into integrated systems. Often this has been done by encoding symbolic knowledge into neural networks. Unfortunately, although many different methods for this have been proposed, there is no common definition of an encoding to compare them. We seek to rectify this problem by introducing a semantic framework for neural-symbolic AI, which is then shown to be general enough to account for a large family of neural-symb
&lt;/p&gt;</description></item></channel></rss>