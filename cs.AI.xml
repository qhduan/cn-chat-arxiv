<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.09606</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21327;&#20316;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09606
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#21508;&#31181;NLP&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#35843;&#26597;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;LLMs&#30340;&#22240;&#26524;&#35270;&#35282;&#65292;&#22312;&#20197;&#19979;&#39046;&#22495;&#23637;&#24320;&#65306;&#29702;&#35299;&#21644;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#20026;LLMs&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#21516;&#26102;&#65292;LLMs&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21453;&#36807;&#26469;&#21487;&#20197;&#36890;&#36807;&#24110;&#21161;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26469;&#20419;&#36827;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19982;LLMs&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#38598;&#20307;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.01827</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#22522;&#20110;&#20840; memristor &#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#30340;&#20648;&#23618;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#20648;&#23618;&#35745;&#31639;&#65288;RC&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#20449;&#21495;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#26694;&#26550;&#12290;RC&#20197;&#20854;&#26102;&#38388;&#22788;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#19982;&#20256;&#32479;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20854;&#30828;&#20214;&#37096;&#32626;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33021;&#22815;&#29983;&#25104;&#21160;&#24577;&#20648;&#23618;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#31181;&#22522;&#20110; WOx &#30340; memristor &#30340;&#30701;&#26399;&#23384;&#20648;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616; 16 &#20010;&#19981;&#21516;&#29366;&#24577;&#30340;&#32534;&#30721;&#36229;&#36807; 4 &#20010;&#27604;&#29305;&#65292;&#24182;&#22312;&#35835;&#20986;&#23618;&#20013;&#20351;&#29992; TiOx-based memristor &#30340;&#38271;&#26399;&#23384;&#20648;&#22120;&#32452;&#20214;&#12290;&#25105;&#20204;&#24443;&#24213;&#30740;&#31350;&#20102;&#20004;&#31181; memristor &#31867;&#22411;&#65292;&#24182;&#21033;&#29992; RC &#31995;&#32479;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340; RC &#31995;&#32479;&#30340;&#24615;&#33021;&#36890;&#36807;&#20004;&#20010;&#22522;&#20934;&#20219;&#21153;&#36827;&#34892;&#20102;&#39564;&#35777;: &#23545;&#20855;&#26377;&#19981;&#23436;&#25972;&#36755;&#20837;&#30340;&#23396;&#31435;&#21475;&#36848;&#25968;&#23383;&#35782;&#21035;&#21644; Mackey-Glass &#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340; 98.84% &#20934;&#30830;&#29575;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 Announce Type: cross  Abstract: Reservoir computing (RC) offers a neuromorphic framework that is particularly effective for processing spatiotemporal signals. Known for its temporal processing prowess, RC significantly lowers training costs compared to conventional recurrent neural networks. A key component in its hardware deployment is the ability to generate dynamic reservoir states. Our research introduces a novel dual-memory RC system, integrating a short-term memory via a WOx-based memristor, capable of achieving 16 distinct states encoded over 4 bits, and a long-term memory component using a TiOx-based memristor within the readout layer. We thoroughly examine both memristor types and leverage the RC system to process temporal data sets. The performance of the proposed RC system is validated through two benchmark tasks: isolated spoken digit recognition with incomplete inputs and Mackey-Glass time series prediction. The system delivered an impressive 98.84% accu
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.01788</link><description>&lt;p&gt;
LitLLM&#65306;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LitLLM: A Toolkit for Scientific Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01788
&lt;/p&gt;
&lt;p&gt;
"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#23545;&#20110;&#29702;&#35299;&#30740;&#31350;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#26500;&#24314;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#33258;&#21160;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#22120;&#21464;&#24471;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#27492;&#31867;&#32508;&#36848;&#30340;&#29616;&#26377;&#24037;&#20316;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#38750;&#23454;&#38469;&#20449;&#24687;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#26410;&#21463;&#36807;&#35757;&#32451;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#22312;LLM&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25688;&#35201;&#36716;&#21270;&#20026;&#20851;&#38190;&#35789;&#26469;&#36827;&#34892;&#32593;&#32476;&#25628;&#32034;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#12290;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#26469;&#25913;&#36827;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#26681;&#25454;-
&lt;/p&gt;
&lt;p&gt;
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#20559;&#24046;&#35780;&#20272;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bias Assessment and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14345
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#20013;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#32534;&#30721;&#36807;&#31243;&#30340;&#29983;&#20135;&#21147;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;LLM&#22312;&#36719;&#20214;&#32534;&#30721;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#24050;&#32463;&#20986;&#29616;&#65306;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65311;&#36825;&#20010;&#38382;&#39064;&#20851;&#31995;&#21040;&#20381;&#36182;&#20110;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#36719;&#20214;&#24212;&#29992;&#30340;&#23436;&#25972;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#36947;&#24503;&#22522;&#30784;&#65292;&#28982;&#32780;&#22312;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#26032;&#39062;&#20559;&#24046;&#35780;&#20272;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;31.45\%&#21040;79.93\%&#30340;&#20195;&#30721;&#20989;&#25968;&#20855;&#26377;&#20559;&#35265;&#65292;9.68\%&#21040;37.37\%&#30340;&#20195;&#30721;&#20989;&#25968;&#30340;&#21151;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
Utilizing state-of-the-art Large Language Models (LLMs), automatic code generation models play a pivotal role in enhancing the productivity and efficiency of software development coding procedures. As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social biases, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models, yet is under-explored in the literature. This paper presents a novel bias assessment framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive evaluation on the bias of nine state-of-the-art LLM-based code generation models. Our findings reveal that first, 31.45\% to 79.93\% code functions generated by our evaluated code generation models are biased, and 9.68\% to 37.37\% code functions' funct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.15793</link><description>&lt;p&gt;
&#27010;&#35201;&#12289;&#20142;&#28857;&#21644;&#34892;&#21160;&#39033;&#30446;&#65306;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system. (arXiv:2307.15793v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#20250;&#35758;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35758;&#22312;&#24037;&#20316;&#21327;&#35843;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#30340;&#22522;&#30784;&#35774;&#26045;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#21521;&#28151;&#21512;&#21644;&#36828;&#31243;&#24037;&#20316;&#30340;&#36716;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20250;&#35758;&#27491;&#22312;&#36716;&#31227;&#21040;&#22312;&#32447;&#35745;&#31639;&#26426;&#23186;&#20307;&#31354;&#38388;&#12290;&#36825;&#23548;&#33268;&#20102;&#26032;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#22312;&#26356;&#19981;&#21560;&#24341;&#20154;&#30340;&#20250;&#35758;&#19978;&#33457;&#36153;&#26356;&#22810;&#30340;&#26102;&#38388;&#65289;&#21644;&#26032;&#30340;&#26426;&#20250;&#65288;&#20363;&#22914;&#33258;&#21160;&#36716;&#24405;/&#23383;&#24149;&#21644;&#24635;&#32467;&#25903;&#25345;&#65289;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24635;&#32467;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#20943;&#23569;&#20010;&#20154;&#30340;&#20250;&#35758;&#36127;&#25285;&#21644;&#22686;&#21152;&#20250;&#35758;&#36755;&#20986;&#30340;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#20250;&#35758;&#20307;&#39564;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#38271;&#31687;&#36716;&#24405;&#21644;&#26080;&#27861;&#26681;&#25454;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#25429;&#25417;&#21040;&#22810;&#26679;&#30340;&#24635;&#32467;&#38656;&#27714;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#25216;&#26415;&#38480;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20102;&#19968;&#31181;&#20250;&#35758;&#24635;&#32467;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24605;&#20102;&#20004;&#20010;&#26126;&#26174;&#30340;&#24635;&#32467;&#34920;&#31034;&#26041;&#24335;&#8212;&#8212;&#37325;&#35201;&#20142;&#28857;&#21644;&#32467;&#26500;&#21270;&#30340;&#20998;&#32423;&#20250;&#35758;&#32426;&#35201;&#35270;&#22270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#20123;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the rep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2306.15503</link><description>&lt;p&gt;
&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65306;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20063;&#31216;&#20026;&#31163;&#32447;RL&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20855;&#26377;&#25552;&#21319;&#22312;&#32447;RL&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#31163;&#32447;RL&#20013;&#30340;&#25968;&#25454;&#37319;&#26679;&#25216;&#26415;&#30340;&#20316;&#29992;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#37319;&#26679;&#25216;&#26415;&#24212;&#29992;&#20110;&#29366;&#24577;&#36716;&#25442;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#39640;&#31163;&#32447;RL&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#25216;&#26415;&#8212;&#8212;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;TR/PTR&#65289;&#65292;&#23427;&#23558;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;TR&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#20248;&#21270;&#21518;&#32493;&#29366;&#24577;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#22312;TR&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#65292;&#20197;&#36991;&#20813;&#22312;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;PTR&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36712;&#36857;&#37319;&#26679;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36712;&#36857;&#20248;&#20808;&#24230;&#25351;&#26631;&#36827;&#34892;&#20248;&#20808;&#35774;&#32622;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
&lt;/p&gt;</description></item></channel></rss>