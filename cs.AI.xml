<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14606</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Elements of Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14606
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#27169;&#22411;&#12289;&#24222;&#22823;&#25968;&#25454;&#38598;&#12289;&#21152;&#36895;&#30828;&#20214;&#65292;&#20197;&#21450;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#21464;&#38761;&#24615;&#21147;&#37327;&#12290;&#36825;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#20351;&#22797;&#26434;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#21253;&#25324;&#20855;&#26377;&#25511;&#21046;&#27969;&#21644;&#25968;&#25454;&#32467;&#26500;&#30340;&#31243;&#24207;&#65289;&#33021;&#22815;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#24494;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#31243;&#24207;&#21442;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#19981;&#20165;&#20165;&#26159;&#31243;&#24207;&#30340;&#24494;&#20998;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#20063;&#21253;&#25324;&#20102;&#31243;&#24207;&#20248;&#21270;&#12289;&#27010;&#29575;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;&#26412;&#20070;&#20171;&#32461;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#25152;&#38656;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#37319;&#29992;&#20102;&#20248;&#21270;&#21644;&#27010;&#29575;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#36827;&#34892;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14606v1 Announce Type: new  Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08785</link><description>&lt;p&gt;
DeltaSpace:&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#35821;&#20041;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing. (arXiv:2310.08785v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#30528;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#25991;&#29486;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#26631;&#27880;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#19968;&#20123;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#25910;&#38598;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#30340;&#20248;&#21270;&#25110;&#25512;&#29702;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#30830;&#23450;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31354;&#38388;&#65292;&#31216;&#20026;CLIP DeltaSpace&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#22270;&#20687;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;CLIP&#25991;&#26412;&#29305;&#24449;&#24046;&#24322;&#22312;&#35821;&#20041;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;&#22522;&#20110;DeltaSpace&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DeltaEdit&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.16075</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#23545;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65306;&#23545;&#33521;&#21360;&#35821;&#35328;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#26412;&#30740;&#31350;&#21017;&#32771;&#23519;&#20102;&#23558;&#22270;&#20687;&#29305;&#24449;&#28155;&#21152;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22270;&#20687;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#12290;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#12290;&#23454;&#39564;&#23558;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35270;&#35273;&#32972;&#26223;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#65306;&#23545;&#20110;&#38750;&#22122;&#22768;&#32763;&#35793;&#65292;&#19981;&#20351;&#29992;&#35270;&#35273;&#32972;&#26223;&#25928;&#26524;&#26368;&#22909;&#65307;&#23545;&#20110;&#20302;&#22122;&#22768;&#65292;&#35009;&#21098;&#30340;&#22270;&#20687;&#29305;&#24449;&#26368;&#20339;&#65307;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#22270;&#20687;&#29305;&#24449;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
&lt;/p&gt;</description></item><item><title>DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05889</link><description>&lt;p&gt;
DF2: &#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DF2: Distribution-Free Decision-Focused Learning. (arXiv:2308.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05889
&lt;/p&gt;
&lt;p&gt;
DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#23450;&#21046;&#21040;&#19968;&#20010;&#19979;&#28216;&#20248;&#21270;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;DFL&#26041;&#27861;&#21463;&#21040;&#19977;&#20010;&#37325;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#12290;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#28304;&#20110;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#20998;&#24067;&#19982;&#30495;&#23454;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#12290;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#26159;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#26469;&#36817;&#20284;&#26399;&#26395;&#20248;&#21270;&#30446;&#26631;&#26102;&#20135;&#29983;&#30340;&#12290;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#21457;&#29983;&#22312;DFL&#20381;&#38752;KKT&#26465;&#20214;&#36827;&#34892;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#26102;&#65292;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#36817;&#20284;&#26799;&#24230;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DF2 - &#31532;&#19968;&#20010;&#26126;&#30830;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#29942;&#39048;&#30340;&#26080;&#20998;&#24067;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending 
&lt;/p&gt;</description></item></channel></rss>