<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11021</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Video Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#35270;&#39057;&#25968;&#25454;&#29983;&#20135;&#30340;&#31354;&#21069;&#28608;&#22686;&#38656;&#27714;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#24103;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290; &#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#26159;&#24103;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290; &#34429;&#28982; VideoLLaMA &#21644; ViCLIP &#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#30701;&#26399;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#24103;&#30340;&#38271;&#26399;&#25512;&#29702;&#26041;&#38754;&#21364;&#20196;&#20154;&#24778;&#35766;&#22320;&#22833;&#36133;&#12290; &#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#23427;&#20204;&#23558;&#36880;&#24103;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#20132;&#32455;&#25104;&#21333;&#20010;&#28145;&#24230;&#32593;&#32476;&#12290; &#22240;&#27492;&#65292;&#35299;&#32806;&#20294;&#20849;&#21516;&#35774;&#35745;&#35821;&#20041;&#29702;&#35299;&#21644;&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#20010;&#24103;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#20294;&#26377;&#25928;&#22320;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#65288;TL&#65289;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20123;&#20844;&#24335;&#22312;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16886</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20316;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#65292;&#20197;&#21307;&#30103;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using text embedding models and vector databases as text classifiers with the example of medical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16886
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#25214;&#21040;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#20934;&#35201;&#27714;&#38750;&#24120;&#39640;&#12290;&#19982;LLMs&#37197;&#21512;&#20351;&#29992;&#65292;&#21521;&#37327;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#21508;&#31181;&#25968;&#25454;&#27169;&#24335;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24335;&#23481;&#26131;&#34987;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#29702;&#35299;&#12290;&#38500;&#20102;&#26041;&#20415;&#22320;&#21521;&#36825;&#20123;&#21521;&#37327;&#25968;&#25454;&#24211;&#28155;&#21152;&#20449;&#24687;&#12289;&#30693;&#35782;&#21644;&#25968;&#25454;&#22806;&#65292;&#23427;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#21363;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#24120;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#26816;&#32034;&#20449;&#24687;&#20219;&#21153;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;Google&#30340;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#26367;&#20195;&#27169;&#22411;Med-PaLM&#65292;&#19987;&#38376;&#26088;&#22312;&#19982;&#20020;&#24202;&#21307;&#24072;&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#21305;&#37197;&#12290;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#24320;&#21457;&#27169;&#22411;&#26102;&#65292;&#20445;&#25345;&#20107;&#23454;&#21644;&#20943;&#23569;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
&lt;/p&gt;</description></item><item><title>ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12317</link><description>&lt;p&gt;
ARKS&#65306;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARKS: Active Retrieval in Knowledge Soup for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12317
&lt;/p&gt;
&lt;p&gt;
ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#23427;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21033;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;(ARKS)&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#29992;&#20110;&#27867;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20381;&#38752;&#21333;&#19968;&#26469;&#28304;&#19981;&#21516;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;&#32593;&#39029;&#25628;&#32034;&#12289;&#25991;&#26723;&#12289;&#25191;&#34892;&#21453;&#39304;&#21644;&#36827;&#21270;&#20195;&#30721;&#29255;&#27573;&#25972;&#21512;&#22312;&#19968;&#36215;&#30340;&#30693;&#35782;&#27748;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31215;&#26497;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#24182;&#26356;&#26032;&#30693;&#35782;&#27748;&#12290;&#20026;&#20102;&#35780;&#20272;ARKS&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#12290;&#22312;ChatGPT&#21644;CodeLlama&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARKS&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20135;&#29983;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22238;&#31572;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#24314;&#31435;&#20102;&#24418;&#24335;&#35299;&#37322;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03048</link><description>&lt;p&gt;
&#20174;&#40065;&#26834;&#24615;&#21040;&#21487;&#35299;&#37322;&#24615;&#20877;&#21040;&#40065;&#26834;&#24615;&#12290;(arXiv:2306.03048v2 [cs.AI] &#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
From Robustness to Explainability and Back Again. (arXiv:2306.03048v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22238;&#31572;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#24314;&#31435;&#20102;&#24418;&#24335;&#35299;&#37322;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20020;&#26102;&#26041;&#27861;&#30456;&#27604;&#65292;&#24418;&#24335;&#35299;&#37322;&#25552;&#20379;&#20102;&#20005;&#23494;&#24615;&#30340;&#37325;&#35201;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#35299;&#37322;&#22312;&#26576;&#20123;&#20998;&#31867;&#22120;&#23478;&#26063;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#25285;&#24515;&#24418;&#24335;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#20854;&#20182;&#26041;&#27861;&#30340;&#34917;&#20805;&#65292;&#20197;&#25552;&#20379;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#24418;&#24335;&#35299;&#37322;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#24418;&#24335;&#35299;&#37322;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#31181;&#26032;&#31639;&#27861;&#36890;&#36807;&#22238;&#31572;&#19968;&#31995;&#21015;&#40065;&#26834;&#24615;&#26597;&#35810;&#26469;&#35745;&#31639;&#35299;&#37322;&#65292;&#24182;&#19988;&#36825;&#20123;&#26597;&#35810;&#30340;&#25968;&#37327;&#26368;&#22810;&#19982;&#29305;&#24449;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#24314;&#31435;&#20102;&#30452;&#25509;&#20851;&#31995;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#25991;&#25512;&#24191;&#20102;&#24418;&#24335;&#35299;&#37322;&#30340;&#23450;&#20041;&#65292;&#20174;&#32780;&#20801;&#35768;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#26469;&#25913;&#36827;&#35299;&#37322;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast with ad-hoc methods for eXplainable Artificial Intelligence (XAI), formal explainability offers important guarantees of rigor. However, formal explainability is hindered by poor scalability for some families of classifiers, the most significant being neural networks. As a result, there are concerns as to whether formal explainability might serve to complement other approaches in delivering trustworthy AI. This paper addresses the limitation of scalability of formal explainability, and proposes novel algorithms for computing formal explanations. The novel algorithm computes explanations by answering instead a number of robustness queries, and such that the number of such queries is at most linear on the number of features. Consequently, the proposed algorithm establishes a direct relationship between the practical complexity of formal explainability and that of robustness. More importantly, the paper generalizes the definition of formal explanation, thereby allowing the use 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item></channel></rss>