<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.16952</link><description>&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65306;&#36890;&#36807;&#39044;&#27979;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#26469;&#20248;&#21270;&#25968;&#25454;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#22810;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#23398;&#26415;&#35770;&#25991;&#12289;&#20195;&#30721;&#65289;&#65292;&#20854;&#28151;&#21512;&#27604;&#20363;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#23450;&#24615;&#31574;&#30053;&#26469;&#35843;&#25972;&#27604;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23450;&#37327;&#21487;&#39044;&#27979;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#12290;&#22312;&#26679;&#26412;&#28151;&#21512;&#19978;&#25311;&#21512;&#36825;&#31181;&#20989;&#25968;&#25581;&#31034;&#20102;&#26410;&#35265;&#28151;&#21512;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#24341;&#23548;&#36873;&#25321;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#27493;&#39588;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25105;&#20204;&#30340;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#30340;&#32553;&#25918;&#35268;&#24459;&#30340;&#23884;&#22871;&#20351;&#29992;&#65292;&#20197;&#20351;&#24471;&#20165;&#36890;&#36807;&#23567;&#35268;&#27169;&#35757;&#32451;&#23601;&#33021;&#22815;&#39044;&#27979;&#22312;&#21508;&#31181;&#28151;&#21512;&#25968;&#25454;&#19979;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20248;&#21270;&#20102;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;</title><link>https://arxiv.org/abs/2403.02901</link><description>&lt;p&gt;
&#20851;&#20110;&#36807;&#31243;&#23548;&#21521;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25506;&#35752;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#65288;ATS&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#21019;&#24314;&#31616;&#27905;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22788;&#29702;&#22823;&#37327;&#25991;&#26412;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;ATS&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#36807;&#21435;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35843;&#26597;ATS&#30340;&#26041;&#27861;; &#20294;&#26159;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#23454;&#38469;&#23454;&#26045;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#20197;&#24448;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;ATS&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312; 1&#65289;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8221;&#35270;&#35282;&#25552;&#20379;ATS&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#26368;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;; 2) &#20840;&#38754;&#23457;&#35270;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;; &#20197;&#21450; 3&#65289;&#25552;&#20379;&#20851;&#20110;ATS&#30340;&#26368;&#26032;&#35843;&#26597;&#65292;&#24357;&#34917;&#25991;&#29486;&#20013;&#20004;&#24180;&#38388;&#38548;&#20043;&#22788;&#12290;&#20196;&#20154;&#24863;&#21040;&#28385;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 Announce Type: new  Abstract: Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.16013</link><description>&lt;p&gt;
SERL: &#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#35266;&#23519;&#65292;&#23454;&#38469;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#31034;&#33539;&#21644;&#20808;&#21069;&#32463;&#39564;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#20351;&#29992;&#12290;&#20174;&#23454;&#36341;&#32773;&#20013;&#35748;&#35782;&#21040;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24120;&#24120;&#19982;&#31639;&#27861;&#36873;&#25321;&#21516;&#26679;&#37325;&#35201;&#65288;&#22914;&#26524;&#19981;&#26159;&#26356;&#37325;&#35201;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#36827;&#19968;&#27493;&#21457;&#23637;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#23545;&#38590;&#20197;&#33719;&#21462;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#24515;&#23454;&#29616;&#30340;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#39640;&#25928;&#26679;&#26412;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#20154;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17178</link><description>&lt;p&gt;
&#22270;&#24418;&#21270;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#37319;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#33021;&#22815;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22635;&#34917;&#20102;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21487;&#20197;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;3D&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#19968;&#20010;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#30340;2D&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#35745;&#31639;&#26159;&#19968;&#31181;&#33021;&#22815;&#35843;&#25972;&#31995;&#32479;&#35774;&#35745;&#32467;&#26524;&#36136;&#37327;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#21560;&#24341;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#35843;&#26597;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.11128</link><description>&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#35843;&#26597;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximate Computing Survey, Part II: Application-Specific &amp; Architectural Approximation Techniques and Applications. (arXiv:2307.11128v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11128
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#26159;&#19968;&#31181;&#33021;&#22815;&#35843;&#25972;&#31995;&#32479;&#35774;&#35745;&#32467;&#26524;&#36136;&#37327;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#21560;&#24341;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#35843;&#26597;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#37096;&#32626;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#65292;&#36843;&#20351;&#35745;&#31639;&#31995;&#32479;&#30028;&#25506;&#32034;&#26032;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#36817;&#20284;&#35745;&#31639;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#35843;&#25972;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;/&#25110;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#31181;&#26681;&#26412;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20852;&#36259;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#35745;&#23618;&#38754;&#65288;&#20174;&#31995;&#32479;&#21040;&#38598;&#25104;&#30005;&#36335;&#65289;&#19978;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#36817;&#20284;&#25216;&#26415;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#21463;&#36817;&#20284;&#35745;&#31639;&#22312;&#36807;&#21435;10&#24180;&#30340;&#24191;&#27867;&#21560;&#24341;&#21147;&#30340;&#39537;&#20351;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20004;&#37096;&#20998;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#20851;&#38190;&#26041;&#38754;&#65288;&#22914;&#26415;&#35821;&#21644;&#24212;&#29992;&#65289;&#24182;&#22238;&#39038;&#20102;&#20256;&#32479;&#35745;&#31639;&#22534;&#26632;&#30340;&#21508;&#20010;&#23618;&#38754;&#30340;&#26368;&#26032;&#36817;&#20284;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#23545;&#24212;&#29992;&#29305;&#23450;&#21644;&#26550;&#26500;&#36817;&#20284;&#25216;&#26415;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#21644;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenging deployment of compute-intensive applications from domains such Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces the community of computing systems to explore new design approaches. Approximate Computing appears as an emerging solution, allowing to tune the quality of results in the design of a system in order to improve the energy efficiency and/or performance. This radical paradigm shift has attracted interest from both academia and industry, resulting in significant research on approximation techniques and methodologies at different design layers (from system down to integrated circuits). Motivated by the wide appeal of Approximate Computing over the last 10 years, we conduct a two-part survey to cover key aspects (e.g., terminology and applications) and review the state-of-the art approximation techniques from all layers of the traditional computing stack. In Part II of our survey, we classify and present the technical details of application-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.11311</link><description>&lt;p&gt;
BELLA: &#36890;&#36807;&#26412;&#22320;&#32447;&#24615;&#36924;&#36817;&#36827;&#34892;&#40657;&#30418;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
BELLA: Black box model Explanations by Local Linear Approximations. (arXiv:2305.11311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29702;&#35299;&#40657;&#30418;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#20165;&#25104;&#20026;&#27861;&#24459;&#35201;&#27714;&#65292;&#20063;&#25104;&#20026;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#21478;&#19968;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#24182;&#21487;&#33021;&#25439;&#23475;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204; tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;
&lt;p&gt;
In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13773</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65306;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26356;&#26377;&#25928;&#22320;&#35843;&#24230;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#12290;&#22312;&#31163;&#32447;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#65288;ONTS&#65289;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#36712;&#36947;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26368;&#20339;&#23433;&#25490;&#65292;&#21516;&#26102;&#32771;&#34385;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#20248;&#20808;&#32423;&#65292;&#26368;&#23567;&#21644;&#26368;&#22823;&#28608;&#27963;&#20107;&#20214;&#65292;&#25191;&#34892;&#26102;&#38388;&#26694;&#26550;&#65292;&#21608;&#26399;&#21644;&#25191;&#34892;&#31383;&#21475;&#65292;&#20197;&#21450;&#21355;&#26143;&#30005;&#21147;&#36164;&#28304;&#21644;&#33021;&#37327;&#25910;&#38598;&#21644;&#31649;&#29702;&#30340;&#22797;&#26434;&#24615;&#30340;&#32422;&#26463;&#12290;ONTS&#38382;&#39064;&#24050;&#32463;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#31934;&#30830;&#26041;&#27861;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#35843;&#24230;&#38382;&#39064;&#21644;&#35774;&#26045;&#25918;&#32622;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ONTS&#38382;&#39064;&#30340;MILP&#23454;&#20363;&#23436;&#20840;&#34920;&#31034;&#25104;&#20108;&#20998;&#22270;&#32593;&#32476;&#32467;&#26500;&#26469;&#24212;&#29992;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
&lt;/p&gt;</description></item></channel></rss>