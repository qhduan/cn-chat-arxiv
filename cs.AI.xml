<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.09919</link><description>&lt;p&gt;
&#36947;&#36335;&#22270;&#29983;&#25104;&#22120;&#65306;&#20174;GPS&#25968;&#25454;&#20013;&#29983;&#25104;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Road Graph Generator: Mapping roads at construction sites from GPS data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;GPS&#36712;&#36857;&#26469;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#36947;&#36335;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#30340;&#20132;&#21449;&#21475;&#24182;&#36830;&#25509;&#23427;&#20204;&#65292;&#29983;&#25104;&#36947;&#36335;&#22270;&#65292;&#20026;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;GPS&#36712;&#36857;&#20013;&#25512;&#27979;&#36947;&#36335;&#20197;&#32472;&#21046;&#24314;&#31569;&#24037;&#22320;&#22320;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#20219;&#21153;&#30001;&#20110;&#24314;&#31569;&#26426;&#26800;&#30340;&#19981;&#35268;&#21017;&#21644;&#38750;&#26631;&#20934;&#36816;&#21160;&#27169;&#24335;&#19982;&#24050;&#24314;&#31435;&#36947;&#36335;&#19978;&#30340; typcial &#36710;&#36742;&#20132;&#36890;&#26174;&#33879;&#19981;&#21516;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35782;&#21035;&#36947;&#36335;&#32593;&#32476;&#20013;&#20316;&#20026;&#20851;&#38190;&#20915;&#31574;&#28857;&#30340;&#20132;&#21449;&#21475;&#65292;&#28982;&#21518;&#36830;&#25509;&#23427;&#20204;&#20197;&#24418;&#25104;&#19968;&#20010;&#22270;&#65292;&#38543;&#21518;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#21644;&#20219;&#21153;&#20998;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25386;&#23041;&#30340;&#19968;&#20010;&#23454;&#38469;&#24314;&#31569;&#24037;&#22320;&#32472;&#21046;&#36947;&#36335;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09919v1 Announce Type: new  Abstract: We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a graph, which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.06827</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06827
&lt;/p&gt;
&lt;p&gt;
KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#34892;&#20026;&#26469;&#25512;&#26029;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#28857;&#20272;&#35745;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#21487;&#33021;&#26377;&#22810;&#20010;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#25551;&#36848;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#27169;&#25311;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;Q&#20540;&#20989;&#25968;&#20195;&#26367;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#21518;&#39564;&#35745;&#31639;&#37327;&#22823;&#65292;&#29702;&#35770;&#20445;&#35777;&#23569;&#65292;&#24182;&#19988;Q&#20540;&#20989;&#25968;&#36890;&#24120;&#23545;&#20284;&#28982;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;KD-BIRL&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#26680;&#23494;&#24230;&#20272;&#35745;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#25913;&#36827;&#30340;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#19979;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
&lt;/p&gt;</description></item></channel></rss>