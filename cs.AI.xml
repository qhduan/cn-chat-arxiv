<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.15426</link><description>&lt;p&gt;
&#25945;&#32946;&#29615;&#22659;&#19979;&#38598;&#25104;&#24378;&#20808;&#39564;&#27169;&#22359;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#30340;&#19977;&#38454;&#27573;SFT&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20808;&#39564;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#35777;&#26126;&#27604;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#26356;&#26377;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22120;&#21644;&#37325;&#21472;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20581;&#22766;&#30340;&#20998;&#31867;&#65292;&#23558;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#20998;&#19977;&#25209;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;LORA&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#27169;&#22359;&#65292;&#23558;&#31995;&#32479;&#25552;&#31034;&#12289;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#20219;&#21153;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;&#20808;&#39564;&#30340;&#24494;&#35843;&#27169;&#22411;&#24212;&#29992;&#20102;&#21387;&#32553;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#38543;&#21518;&#22312;&#36755;&#20986;&#31471;&#36827;&#34892;&#25991;&#26412;&#36807;&#28388;&#20197;&#33719;&#24471;&#22686;&#37327;&#24341;&#23548;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#30495;&#27491;&#20197;&#20016;&#23500;&#30340;&#25945;&#32946;&#30693;&#35782;&#12289;&#20998;&#27493;&#25351;&#23548;&#30340;&#29305;&#28857;&#20307;&#29616;&#23548;&#24072;&#35282;&#33394;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
&lt;/p&gt;</description></item><item><title>CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#22270;&#26368;&#22823;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Clustering Method with Graph Maximum Decoding Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13846
&lt;/p&gt;
&lt;p&gt;
CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20854;&#33021;&#22815;&#19982;&#20854;&#20182;&#30456;&#20851;&#24212;&#29992;&#26080;&#32541;&#38598;&#25104;&#30340;&#36866;&#24212;&#24615;&#36171;&#20104;&#20102;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#20998;&#26512;&#33021;&#21147;&#65292;&#21487;&#20197;&#24378;&#22823;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#8220;&#33258;&#28982;&#20851;&#32852;&#8221;&#25110;&#8220;&#22270;&#32467;&#26500;&#8221;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#24403;&#21069;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#24573;&#30053;&#20102;&#33410;&#28857;&#20043;&#38388;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#20197;&#21450;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20869;&#26368;&#22823;&#21270;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CMDI&#12290;CMDI&#21019;&#26032;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#32435;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22270;&#32467;&#26500;&#25552;&#21462;&#21644;&#22270;&#39030;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2401.13334</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#39046;&#22495;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21442;&#25968;&#35843;&#20248;&#30340;&#25511;&#21046;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36817;&#20284;&#35823;&#24046;&#21644;&#31616;&#21270;&#30446;&#26631;&#65292;BO&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20559;&#31163;&#20154;&#31867;&#19987;&#23478;&#30340;&#30495;&#23454;&#30446;&#26631;&#65292;&#38656;&#35201;&#21518;&#32493;&#35843;&#25972;&#12290;BO&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#21327;&#20316;&#35843;&#20248;&#36807;&#31243;&#65292;&#22240;&#20026;&#19987;&#23478;&#19981;&#20449;&#20219;BO&#30340;&#24314;&#35758;&#12290;&#30446;&#21069;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#27492;&#38388;&#38553;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TNTRules&#65288;TUNE-NOTUNE&#35268;&#21017;&#65289;&#65292;&#19968;&#31181;&#20107;&#21518;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#38469;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TNTRules&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;BO&#21644;XAI&#30340;&#20132;&#21449;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable opt
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20860;&#39038;&#24211;&#23384;&#24377;&#24615;&#21644;&#25913;&#21892;&#24179;&#22343;&#24615;&#33021;&#65292;&#26469;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.12183</link><description>&lt;p&gt;
&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories. (arXiv:2310.12183v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20860;&#39038;&#24211;&#23384;&#24377;&#24615;&#21644;&#25913;&#21892;&#24179;&#22343;&#24615;&#33021;&#65292;&#26469;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#21644;&#20813;&#20998;&#24067;&#20048;&#35266;-&#40065;&#26834;&#21452;&#27169;&#24335;&#24211;&#23384;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#20998;&#37197;&#38144;&#21806;&#38142;&#19978;&#30340;&#24211;&#23384;&#65292;&#20197;&#28385;&#36275;&#26102;&#21464;&#30340;&#12289;&#19981;&#30830;&#23450;&#30340;&#20840;&#28192;&#36947;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#26356;&#21152;&#27880;&#37325;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#24615;&#38656;&#27714;&#65292;&#32780;&#21452;&#27169;&#24335;&#31574;&#30053;&#19981;&#20165;&#32771;&#34385;&#20102;&#20445;&#25345;&#20687;&#40065;&#26834;&#20248;&#21270;&#19968;&#26679;&#30340;&#24377;&#24615;&#65292;&#36824;&#36890;&#36807;&#20811;&#26381;&#20869;&#29983;&#22855;&#24322;&#20540;&#30340;&#23384;&#22312;&#32780;&#33719;&#24471;&#20102;&#25913;&#21892;&#24179;&#22343;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#22238;&#25253;&#12290;&#36825;&#31181;&#21452;&#27169;&#24335;&#31574;&#30053;&#22312;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#38754;&#29305;&#21035;&#26377;&#20215;&#20540;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#24211;&#23384;&#20248;&#21270;&#27169;&#22411;&#30340;&#26680;&#24515;&#25152;&#22312;&#12290;&#30001;&#20110;&#28192;&#36947;&#30340;&#24322;&#36136;&#34892;&#20026;&#65292;&#36825;&#20123;&#22240;&#32032;&#26159;&#38750;&#23545;&#31216;&#30340;&#65292;&#21069;&#32773;&#22312;&#22833;&#38144;&#21806;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#20559;&#24046;&#65292;&#32780;&#21518;&#32773;&#21017;&#20381;&#36182;&#20110;&#32593;&#32476;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO while also reaping the rewards of improved average-case performance by overcoming the presence of endogenous outliers. This bimodal strategy is particularly valuable for balancing the tradeoff between lost sales at the store and the costs of cross-channel e-commerce fulfillment, which is at the core of our inventory optimization model. These factors are asymmetric due to the heterogenous behavior of the channels, with a bias towards the former in terms of lost-sales cost and a dependence on network effects for the latter. We provide structural insights about the BIO solution and how it can be tu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item></channel></rss>