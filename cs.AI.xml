<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#20114;&#34917;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;&#30340;&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.00029</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#30340;&#20114;&#34917;&#24615;&#65306;&#27010;&#24565;&#12289;&#26469;&#28304;&#21644;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#20114;&#34917;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#25552;&#39640;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#20154;&#31867;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#21644;AI&#20043;&#38388;&#30340;&#21327;&#20316;&#24212;&#35813;&#23548;&#33268;&#20114;&#34917;&#30340;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;--&#19968;&#31181;&#26082;&#19981;&#26159;&#20182;&#20204;&#20010;&#20307;&#25152;&#33021;&#36798;&#21040;&#30340;&#32489;&#25928;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#35266;&#23519;&#21040;CTP&#65292;&#36825;&#34920;&#26126;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#30340;&#20114;&#34917;&#32452;&#20998;&#30340;&#29702;&#35299;&#19981;&#22815;&#65292;&#36825;&#20123;&#32452;&#20998;&#21487;&#20197;&#20419;&#36827;&#20915;&#31574;&#20013;&#30340;CTP&#12290;&#26412;&#30740;&#31350;&#20026;&#29702;&#35299;&#21644;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#22880;&#23450;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#24418;&#24335;&#21270;&#20114;&#34917;&#24615;&#28508;&#21147;&#21450;&#20854;&#23454;&#29616;&#30340;&#27010;&#24565;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#27010;&#36848;&#20102;&#35299;&#37322;CTP&#30340;&#26469;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#24212;&#29992;&#25105;&#20204;&#30340;&#27010;&#24565;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#27010;&#24565;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20449;&#24687;&#19981;&#23545;&#31216;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00029v1 Announce Type: cross  Abstract: Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.01909</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#22522;&#20110;&#35821;&#20041;&#23545;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36880;&#20687;&#32032;&#26631;&#35760;&#22270;&#20687;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#20013;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#39318;&#27425;&#32508;&#21512;&#21644;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01909v1 Announce Type: cross  Abstract: Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;</title><link>https://arxiv.org/abs/2402.15270</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32541;&#25509;&#36817;&#24230;&#25972;&#21512;&#30340;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothed Graph Contrastive Learning via Seamless Proximity Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15270
&lt;/p&gt;
&lt;p&gt;
SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#36890;&#36807;&#23558;&#33410;&#28857;&#23545;&#24402;&#31867;&#20026;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#36873;&#25321;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#20004;&#20010;&#22686;&#24378;&#22270;&#20013;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;GCL&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#32479;&#19968;&#22320;&#34701;&#20837;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#36127;&#33410;&#28857;&#34987;&#24179;&#31561;&#23545;&#24453;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#19982;&#30495;&#27491;&#27491;&#26679;&#26412;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;SGCL&#65289;&#65292;&#21033;&#29992;&#22686;&#24378;&#22270;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#27880;&#20837;&#19982;&#27491;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25509;&#36817;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#35268;&#33539;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;SGCL&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.07130</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#25552;&#20379;&#35768;&#22810;&#21019;&#26032;&#26381;&#21153;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36947;&#20041;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#20844;&#20849;TTI&#26381;&#21153;&#37319;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26469;&#38450;&#27490;&#24847;&#22806;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#65292;&#20197;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;TTI&#27169;&#22411;&#65288;&#21253;&#25324;DALL-E 3&#21644;Midjourney&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21033;&#29992;LLMs&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#26469;&#21019;&#24314;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;LLMs&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22240;&#20026;&#21482;&#26377;&#24403;&#25152;&#26377;&#20010;&#20307;&#20803;&#32032;&#37117;&#34987;&#32472;&#21046;&#22312;&#19968;&#36215;&#26102;&#65292;&#28508;&#22312;&#30340;&#26377;&#23475;&#21547;&#20041;&#25165;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (TTI) models offer many innovative services but also raise ethical concerns due to their potential to generate unethical images. Most public TTI services employ safety filters to prevent unintended images. In this work, we introduce the Divide-and-Conquer Attack to circumvent the safety filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our attack leverages LLMs as text transformation agents to create adversarial prompts. We design attack helper prompts that effectively guide LLMs to break down an unethical drawing intent into multiple benign descriptions of individual image elements, allowing them to bypass safety filters while still generating unethical images. Because the latent harmful meaning only becomes apparent when all individual elements are drawn together. Our evaluation demonstrates that our attack successfully circumvents multiple strong closed-box safety filters. The comprehensive success rate of DACA bypassing the safety filters of t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08276</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#22312;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#28982;&#32780;&#22312;&#36965;&#24863;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#30528;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#36825;&#23548;&#33268;&#20102;&#38750;&#35821;&#20041;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#38169;&#35823;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#26469;&#25366;&#25496;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;ROAM&#65289;&#65292;&#22312;&#28508;&#22312;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#26681;&#25454;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65288;DTGA&#65289;&#65292;&#29992;&#36739;&#23569;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#26469;&#25193;&#23637;&#21487;&#22788;&#29702;&#30340;&#25991;&#26412;&#34920;&#31034;&#33539;&#22260;&#65292;&#22686;&#24378;&#20840;&#23616;&#35789;&#32423;&#35821;&#20041;&#36830;&#25509;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#35270;&#35273;-&#35821;&#20041;&#32422;&#26463;&#26469;&#20943;&#23569;&#21333;&#19968;&#35270;&#35273;&#20381;&#36182;&#65292;&#24182;&#20026;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#25552;&#20379;&#22806;&#37096;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;</title><link>http://arxiv.org/abs/2308.11914</link><description>&lt;p&gt;
&#36808;&#21521;&#22240;&#26524;GPT&#65306;&#36890;&#36807;&#20419;&#36827;LLMs&#20013;&#30340;&#22240;&#26524;&#19968;&#33268;&#24615;&#65292;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#23454;&#29616;&#24544;&#23454;&#30340;&#30693;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs. (arXiv:2308.11914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#30693;&#35782;&#22238;&#24518;&#21644;&#25512;&#29702;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#40723;&#21169;LLMs&#33258;&#20027;&#35745;&#21010;&#21644;&#35299;&#20915;&#38382;&#39064;&#25110;&#24191;&#27867;&#37319;&#26679;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26410;&#33021;&#35299;&#20915;&#27010;&#24565;&#21644;&#25512;&#29702;&#35884;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#25512;&#29702;&#35884;&#35823;&#65292;&#25105;&#20204;&#20174;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#24471;&#21040;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#22686;&#21152;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22810;&#20010;&#26234;&#33021;&#20307;&#65288;&#21363;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#65289;&#22312;&#25512;&#29702;&#21644;&#19968;&#33268;&#24615;&#33539;&#24335;&#20013;&#21327;&#20316;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#25512;&#29702;&#22120;&#19987;&#27880;&#20110;&#25552;&#20379;&#20855;&#26377;&#20154;&#31867;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22240;&#26524;&#35780;&#20272;&#22120;&#20195;&#29702;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#31572;&#26696;&#26159;&#21542;&#20174;&#38382;&#39064;&#20013;&#22240;&#26524;&#25512;&#23548;&#20986;&#26469;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#24182;&#29992;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#31572;&#26696;&#26469;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2209.14295</link><description>&lt;p&gt;
Conformal Prediction&#23545;&#20998;&#25955;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#27491;&#30830;&#35206;&#30422;&#26410;&#35266;&#23519;&#21040;&#30340;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#27491;&#30830;&#25511;&#21046;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#20551;&#38452;&#24615;&#27604;&#20363;&#65289;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#65292;&#38500;&#20102;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#23545;Conformal Prediction&#31639;&#27861;&#36827;&#34892;&#26377;&#30028;&#23610;&#23544;&#30340;&#22122;&#22768;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#20998;&#25968;&#25110;&#25968;&#25454;&#30340;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
&lt;/p&gt;</description></item></channel></rss>