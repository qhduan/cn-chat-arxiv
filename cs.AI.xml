<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#31995;&#32479;&#35843;&#26597;&#65306;&#25216;&#26415;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#24050;&#25104;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#21147;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#22312;&#19981;&#20462;&#25913;&#26680;&#24515;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25552;&#31034;&#20801;&#35768;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#25552;&#31034;&#24341;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#25552;&#31034;&#21487;&#20197;&#26159;&#25552;&#20379;&#19978;&#19979;&#25991;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20063;&#21487;&#20197;&#26159;&#35843;&#29992;&#30456;&#20851;&#30693;&#35782;&#30340;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20174;&#38382;&#31572;&#21040;&#24120;&#35782;&#25512;&#29702;&#37117;&#26377;&#28041;&#21450;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#26679;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#21644;&#25216;&#26415;&#32570;&#20047;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#29702;&#35299;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#23545;&#26368;&#36817;&#36827;&#23637;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04398</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Learning from Time Series under Temporal Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20998;&#31867;&#20219;&#21153;&#21463;&#21040;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#22122;&#22768;&#21487;&#33021;&#20250;&#23548;&#33268;&#26631;&#31614;&#36136;&#37327;&#38543;&#26102;&#38388;&#25913;&#21892;&#12289;&#24694;&#21270;&#25110;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#21644;&#31995;&#32479;&#21270;&#20102;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#20998;&#31867;&#30340;&#19968;&#20010;&#26410;&#32463;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#22810;&#20010;&#26631;&#31614;&#36830;&#32493;&#35760;&#24405;&#65292;&#21516;&#26102;&#21463;&#21040;&#19968;&#20010;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#20989;&#25968;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24314;&#27169;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#30340;&#25345;&#32493;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#23545;&#22122;&#22768;&#20855;&#26377;&#23481;&#24525;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#65292;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.08957</link><description>&lt;p&gt;
SWBT&#65306;&#20855;&#26377;&#19981;&#23436;&#32654;&#28436;&#31034;&#30340;&#30456;&#20284;&#24615;&#21152;&#26435;&#34892;&#20026;&#36716;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
SWBT: Similarity Weighted Behavior Transformer with the Imperfect Demonstration for Robotic Manipulation. (arXiv:2401.08957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#20165;&#20351;&#29992;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#24182;&#24573;&#30053;&#19981;&#23436;&#32654;&#30340;&#28436;&#31034;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#20174;&#22312;&#32447;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Similarity Weighted Behavior Transformer&#65288;SWBT&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;SWBT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26131;&#33719;&#21462;&#30340;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#22914;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#20449;&#24687;&#26174;&#33879;&#22686;&#24378;&#20102;&#32593;&#32476;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#12290;&#22312;ManiSkill2 bench&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL), aiming to learn optimal control policies from expert demonstrations, has been an effective method for robot manipulation tasks. However, previous IL methods either only use expensive expert demonstrations and omit imperfect demonstrations or rely on interacting with the environment and learning from online experiences. In the context of robotic manipulation, we aim to conquer the above two challenges and propose a novel framework named Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from both expert and imperfect demonstrations without interaction with environments. We reveal that the easy-to-get imperfect demonstrations, such as forward and inverse dynamics, significantly enhance the network by learning fruitful information. To the best of our knowledge, we are the first to attempt to integrate imperfect demonstrations into the offline imitation learning setting for robot manipulation tasks. Extensive experiments on the ManiSkill2 bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.04928</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#20316;&#29615;&#22659;&#20013;&#65292;&#22914;&#20250;&#35758;&#23433;&#25490;&#12289;&#21512;&#20316;&#21644;&#39033;&#30446;&#35268;&#21010;&#20013;&#65292;&#38598;&#20307;&#20915;&#31574;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20559;&#22909;&#22810;&#26679;&#24615;&#12289;&#24037;&#20316;&#28966;&#28857;&#19981;&#21516;&#21644;&#25104;&#21592;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#31561;&#22240;&#32032;&#65292;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20419;&#36827;&#32676;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20010;&#20307;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#28385;&#36275;&#25104;&#21592;&#20559;&#22909;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#27492;&#31995;&#32479;&#24212;&#29992;&#20110;&#20225;&#19994;&#20250;&#35758;&#23433;&#25490;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#21019;&#24314;&#20102;&#21512;&#25104;&#21592;&#24037;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#35780;&#20272;&#31995;&#32479;&#34920;&#29616;&#26469;&#20316;&#20026;&#24320;&#23637;&#29992;&#25143;&#30740;&#31350;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#23454;&#29616;&#25104;&#21592;&#19982;LLM&#31995;&#32479;&#20043;&#38388;&#30340;&#39640;&#25928;&#21327;&#35843;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23545;&#20854;&#25552;&#20986;&#30340;&#36873;&#39033;&#36827;&#34892;&#25913;&#36827;&#21644;&#23436;&#21892;&#65292;&#30830;&#20445;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from conversations and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.19806</link><description>&lt;p&gt;
&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#31561;&#20215;&#24615;&#23646;&#24615;&#30340;&#33258;&#21160;&#39564;&#35777;-&#23398;&#22763;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#24037;&#19994;&#24212;&#29992;&#22686;&#21152;&#65292;&#23545;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20851;&#38190;&#24212;&#29992;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#31243;&#24207;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#24076;&#26395;&#26377;&#19968;&#31181;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#20248;&#21270;&#30340;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#12290;&#20174;&#24418;&#24335;&#19978;&#35762;&#65292;&#36825;&#23545;&#24212;&#20110;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20102;&#32763;&#35793;&#24037;&#20855;anthem&#12290;&#23427;&#21487;&#20197;&#19982;&#29992;&#20110;&#32463;&#20856;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#26159;&#21542;&#24378;&#31561;&#20215;&#12290;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;anthem&#20013;&#65292;&#21482;&#33021;&#39564;&#35777;&#20855;&#26377;&#21463;&#38480;&#36755;&#20837;&#35821;&#35328;&#30340;&#27491;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;&#36825;&#26159;anthem&#20013;&#23454;&#29616;&#30340;&#32763;&#35793;&#964;*&#30340;&#32467;&#26524;&#65292;&#23427;&#29983;&#25104;&#20102;here-and-there&#36923;&#36753;&#20013;&#30340;&#20844;&#24335;&#65292;&#35813;&#36923;&#36753;&#21482;&#23545;&#27491;&#31243;&#24207;&#19982;&#32463;&#20856;&#36923;&#36753;&#30456;&#19968;&#33268;&#12290;&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;anthem&#65292;&#20197;&#20415;&#21487;&#20197;&#39564;&#35777;&#26356;&#24191;&#27867;&#30340;&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.05999</link><description>&lt;p&gt;
&#29983;&#21629;&#21551;&#21457;&#30340;&#33258;&#20027;&#21644;&#36866;&#24212;&#26234;&#33021;&#20026;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents. (arXiv:2309.05999v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#26412;&#25991;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building autonomous --- i.e., choosing goals based on one's needs -- and adaptive -- i.e., surviving in ever-changing environments -- agents has been a holy grail of artificial intelligence (AI). A living organism is a prime example of such an agent, offering important lessons about adaptive autonomy. Here, we focus on interoception, a process of monitoring one's internal environment to keep it within certain bounds, which underwrites the survival of an organism. To develop AI with interoception, we need to factorize the state variables representing internal environments from external environments and adopt life-inspired mathematical properties of internal environment states. This paper offers a new perspective on how interoception can help build autonomous and adaptive agents by integrating the legacy of cybernetics with recent advances in theories of life, reinforcement learning, and neuroscience.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.12794</link><description>&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#22522;&#20934;&#65306;&#29992;&#20110;&#23398;&#20064;&#21644;&#38750;&#23398;&#20064;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;JSP&#65289;&#65292;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#65288;FSP&#65289;&#65292;&#28789;&#27963;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;FJSP&#65289;&#65292;&#20855;&#26377;&#35013;&#37197;&#32422;&#26463;&#30340;FJSP&#65288;FAJSP&#65289;&#65292;&#20855;&#26377;&#24207;&#21015;&#20381;&#36182;&#35774;&#32622;&#26102;&#38388;&#30340;FJSP&#65288;FJSP-SDST&#65289;&#21644;&#22312;&#32447;FJSP&#65288;&#22312;&#32447;&#20316;&#19994;&#21040;&#36798;&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#26426;&#22120;&#35843;&#24230;&#25361;&#25112;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20174;&#19994;&#32773;&#21644;&#29233;&#22909;&#32773;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.07207</link><description>&lt;p&gt;
Valley: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35270;&#39057;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#21331;&#36234;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25104;&#20026;&#24378;&#22823;&#30340;AI&#21161;&#25163;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26500;&#24314;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#24212;&#29992;AI&#21161;&#25163;&#65311;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#27169;&#22359;&#26469;&#23545;&#40784;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#65292;&#28982;&#21518;&#22312;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Valley&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#35270;&#39057;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17473</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#19982;&#27604;&#36739;&#20998;&#26512;&#65306;CNN&#12289;RNN&#12289;LSTM&#12289;GRU&#12290;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24378;&#22823;&#23376;&#38598;&#65292;&#29305;&#21035;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#12290;&#20854;&#24433;&#21709;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#39044;&#27979;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#32473;&#35774;&#35745;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#29983;&#25104;&#27169;&#22411;&#12289;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#24212;&#29992;&#12289;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33021;&#21516;&#26102;&#31363;&#21462;&#22810;&#20986;&#21475;&#32593;&#32476;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#21644;&#24615;&#33021;&#25439;&#22833;&#12289;&#31574;&#30053;&#25439;&#22833;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13584</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20986;&#21475;&#32593;&#32476;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Model Stealing Attack against Multi-Exit Networks. (arXiv:2305.13584v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33021;&#21516;&#26102;&#31363;&#21462;&#22810;&#20986;&#21475;&#32593;&#32476;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#21644;&#24615;&#33021;&#25439;&#22833;&#12289;&#31574;&#30053;&#25439;&#22833;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20855;&#26377;&#21333;&#20010;&#20986;&#21475;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#22810;&#20986;&#21475;&#32593;&#32476;&#20855;&#26377;&#22810;&#20010;&#20986;&#21475;&#65292;&#36825;&#20123;&#20986;&#21475;&#20801;&#35768;&#20174;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#26089;&#26399;&#36755;&#20986;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#31867;&#20284;&#35782;&#21035;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#24403;&#20351;&#29992;&#20256;&#32479;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#26041;&#27861;&#23581;&#35797;&#31363;&#21462;&#36825;&#20123;&#26377;&#20215;&#20540;&#30340;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26041;&#27861;&#21482;&#33021;&#31363;&#21462;&#27169;&#22411;&#30340;&#20998;&#31867;&#20989;&#25968;&#65292;&#32780;&#19981;&#33021;&#25429;&#25417;&#20854;&#36755;&#20986;&#31574;&#30053;&#12290;&#36825;&#23548;&#33268;&#31363;&#21462;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#38477;&#20302;&#65292;&#22833;&#21435;&#22810;&#20986;&#21475;&#32593;&#32476;&#30340;&#20248;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31363;&#21462;&#27169;&#22411;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#21462;&#27169;&#22411;&#20989;&#25968;&#21644;&#36755;&#20986;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#21464;&#28857;&#26816;&#27979;&#26469;&#20998;&#26512;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#24615;&#33021;&#25439;&#22833;&#21644;&#31574;&#30053;&#25439;&#22833;&#26469;&#25351;&#23548;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36755;&#20986;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20351;&#26367;&#20195;&#27169;&#22411;&#36824;&#21407;&#31363;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional neural networks with a single exit, a multi-exit network has multiple exits that allow for early output from intermediate layers of the model, thus bringing significant improvement in computational efficiency while maintaining similar recognition accuracy. When attempting to steal such valuable models using traditional model stealing attacks, we found that conventional methods can only steal the model's classification function while failing to capture its output strategy. This results in a significant decrease in computational efficiency for the stolen substitute model, thereby losing the advantages of multi-exit networks.In this paper, we propose the first model stealing attack to extract both the model function and output strategy. We employ bayesian changepoint detection to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the substitute model. Furthermore, we designed a novel output strategy search
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item></channel></rss>