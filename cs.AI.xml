<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item></channel></rss>