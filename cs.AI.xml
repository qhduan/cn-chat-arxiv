<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13572</link><description>&lt;p&gt;
&#35770;&#19968;&#31181;&#21464;&#31181;Looped Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of a Variant of the Looped Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13572
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22312;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#31185;&#23398;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#26041;&#38754;&#65292;Transformer&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#20174;&#34920;&#36798;&#33021;&#21147;&#21644;&#21151;&#33021;&#24615;&#35282;&#24230;&#35299;&#37322;&#65292;&#26631;&#20934;&#30340;Transformer&#33021;&#22815;&#25191;&#34892;&#19968;&#20123;&#31639;&#27861;&#12290;&#20026;&#20102;&#36171;&#20104;Transformer&#31639;&#27861;&#33021;&#21147;&#65292;&#24182;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;Looped Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#22359;&#65292;&#21517;&#20026;Algorithm Transformer&#65288;&#31616;&#31216;AlgoFormer&#65289;&#12290;&#19982;&#26631;&#20934;Transformer&#21644;&#32431;&#31929;&#30340;Looped Transformer&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AlgoFormer&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#31034;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21463;&#20154;&#31867;&#35774;&#35745;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;Transformer&#22359;&#21253;&#25324;&#19968;&#20010;&#36127;&#36131;&#36827;&#34892;ta
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.03056</link><description>&lt;p&gt;
LitSumm&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
LitSumm: Large language models for literature summarisation of non-coding RNAs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03056
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Motivation: &#22312;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#25972;&#29702;&#24037;&#20316;&#20013;&#65292;&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;&#21457;&#24067;&#36895;&#24230;&#30340;&#25345;&#32493;&#22686;&#21152;&#65292;&#20877;&#21152;&#19978;&#20840;&#29699;&#22266;&#23450;&#25968;&#37327;&#30340;&#31574;&#23637;&#20154;&#21592;&#65292;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#24211;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#24456;&#23569;&#26377;&#30693;&#35782;&#24211;&#26377;&#36164;&#28304;&#21487;&#20197;&#25193;&#23637;&#21040;&#25152;&#26377;&#30456;&#20851;&#25991;&#29486;&#65292;&#32780;&#25152;&#26377;&#30693;&#35782;&#24211;&#37117;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#33258;&#24049;&#30340;&#21162;&#21147;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#38750;&#32534;&#30721;RNA&#29983;&#25104;&#25991;&#29486;&#25688;&#35201;&#65292;&#39318;&#27425;&#20943;&#36731;&#20102;RNA&#31185;&#23398;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21830;&#19994;LLM&#21644;&#19968;&#31995;&#21015;&#25552;&#31034;&#21644;&#26816;&#26597;&#20174;&#25991;&#29486;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#20107;&#23454;&#20934;&#30830;&#30340;&#25688;&#35201;&#21450;&#20934;&#30830;&#30340;&#24341;&#29992;&#12290;&#20154;&#24037;&#35780;&#20272;&#38024;&#23545;&#25688;&#35201;&#23376;&#38598;&#36827;&#34892;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#34987;&#35780;&#20026;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#26368;&#24120;&#29992;&#30340;&#33258;&#21160;&#21270;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03056v2 Announce Type: replace-cross  Abstract: Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15720</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems. (arXiv:2308.15720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;(RandNLA)&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35745;&#31639;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32463;&#39564;&#24615;&#33021;&#20197;&#21450;&#24378;&#22823;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#19968;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#25152;&#38480;&#21046;&#65292;&#21363;&#29992;&#25143;&#38656;&#35201;&#35774;&#32622;&#21508;&#31181;&#19981;&#21516;&#20110;&#20256;&#32479;NLA&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#29305;&#23450;&#35843;&#21442;&#21442;&#25968;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#26469;&#35299;&#20915;RandNLA&#31639;&#27861;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#33609;&#22270;&#21644;&#39044;&#22788;&#29702;(SAP)&#30340;&#38543;&#26426;&#21270;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#36827;&#34892;&#20102;&#26367;&#20195;&#27169;&#22411;&#33258;&#21160;&#35843;&#20248;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#22312;&#29616;&#20195;RandNLA&#20013;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20197;&#27604;&#38543;&#26426;&#25628;&#32034;&#23569;&#32422;4&#20493;&#30340;&#35797;&#39564;&#25104;&#26412;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be effective in handling high-dimensional computational problems, providing high-quality empirical performance as well as strong probabilistic guarantees. However, their practical application is complicated by the fact that the user needs to set various algorithm-specific tuning parameters which are different than those used in traditional NLA. This paper demonstrates how a surrogate-based autotuning approach can be used to address fundamental problems of parameter selection in RandNLA algorithms. In particular, we provide a detailed investigation of surrogate-based autotuning for sketch-and-precondition (SAP) based randomized least squares methods, which have been one of the great success stories in modern RandNLA. Empirical results show that our surrogate-based autotuning approach can achieve near-optimal performance with much less tuning cost than a random search (up to about 4x fewer trials of different para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13969</link><description>&lt;p&gt;
&#27880;&#37325;&#27880;&#24847;&#21147;&#65306;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#20173;&#28982;&#38656;&#35201;&#20381;&#36182;&#20154;&#31867;&#21028;&#26029;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#31867;&#35270;&#35273;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#30524;&#21160;&#20202;&#25910;&#38598;&#21040;&#30340;&#27880;&#35270;&#28857;&#65292;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#21644;Vision Transformer&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#27880;&#35270;&#21306;&#22495;&#22312;&#24038;&#21491;&#39550;&#39542;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#27880;&#35270;&#22270;&#21644;ViT&#27880;&#24847;&#21147;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21333;&#20010;&#22836;&#37096;&#21644;&#23618;&#20043;&#38388;&#30340;&#37325;&#21472;&#21160;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#37325;&#21472;&#21160;&#24577;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#21098;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#19982;&#27880;&#35270;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#8220;&#32852;&#21512;&#31354;&#38388;-&#27880;&#35270;&#8221;&#65288;JSF&#65289;&#30340;&#27880;&#24847;&#21147;&#35774;&#32622;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GUTS &#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#37096;&#32626;&#65292;&#35299;&#20915;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#30340;&#20027;&#21160;&#25628;&#32034;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#12289;&#36974;&#25377;&#38382;&#39064;&#12289;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;</title><link>http://arxiv.org/abs/2304.02075</link><description>&lt;p&gt;
GUTS&#65306;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#30340;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693; Thompson Sampling&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent Active Search. (arXiv:2304.02075v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GUTS &#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#37096;&#32626;&#65292;&#35299;&#20915;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#30340;&#20027;&#21160;&#25628;&#32034;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#12289;&#36974;&#25377;&#38382;&#39064;&#12289;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#28798;&#38590;&#21709;&#24212;&#30340;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#23545;&#30830;&#20445;&#26368;&#23567;&#29983;&#21629;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#25628;&#32034;&#21306;&#22495;&#23545;&#20110;&#20154;&#31867;&#25937;&#25588;&#32773;&#32780;&#35328;&#36807;&#20110;&#21361;&#38505;&#25110;&#36807;&#20110;&#24191;&#38420;&#26102;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#24322;&#27493;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#26088;&#22312;&#39640;&#25928;&#22320;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65288;OOI&#65289;&#12290;&#36825;&#31181;&#34920;&#36848;&#35299;&#20915;&#20102;&#25628;&#32034;&#20219;&#21153;&#24212;&#35813;&#19987;&#27880;&#20110;&#24555;&#36895;&#24674;&#22797;OOI&#32780;&#19981;&#26159;&#23545;&#25628;&#32034;&#21306;&#22495;&#36827;&#34892;&#20840;&#35206;&#30422;&#30340;&#35201;&#27714;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#65292;&#32771;&#34385;&#21040;&#30001;&#20110;&#26893;&#34987;&#25110;&#22320;&#24418;&#30340;&#36974;&#25377;&#32780;&#23548;&#33268;&#30340;&#36974;&#25377;&#38382;&#39064;&#65292;&#25110;&#32773;&#32771;&#34385;&#21040;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;Thompson&#25277;&#26679;&#65288;GUTS&#65289;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#22312;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20027;&#21160;&#25628;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#34920;&#26126;&#65292;GUTS&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OOIs rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17155</link><description>&lt;p&gt;
&#22522;&#20110;&#21028;&#21035;&#24615;&#31867;&#26631;&#30340;&#25991;&#26412;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#29255;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#24120;&#24120;&#26080;&#27861;&#25551;&#32472;&#20986;&#24494;&#22937;&#30340;&#32454;&#33410;&#19988;&#26131;&#20110;&#20986;&#38169;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#22312;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#65306;&#65288;i&#65289;&#19982;&#29992;&#20110;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#29228;&#21462;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22270;&#29255;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20250;&#20005;&#37325;&#21463;&#24433;&#21709;&#65292;&#25110;&#65288;ii&#65289;&#36755;&#20837;&#26159;&#30828;&#32534;&#30721;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#33258;&#30001;&#25991;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#24615;&#20449;&#21495;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26082;&#21457;&#25381;&#20102;&#33258;&#30001;&#25991;&#26412;&#30340;&#34920;&#36798;&#28508;&#21147;&#65292;&#21448;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16045</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31354;&#38388;&#21435;&#21367;&#31215;&#21644;&#20449;&#24687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#65292;&#24182;&#35745;&#31639;&#20986;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#21487;&#20197;&#25506;&#31350;&#38750;&#38543;&#26426;&#25968;&#25454;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#21495;&#25110;&#20449;&#24687;&#30340;&#32500;&#24230;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#19982;&#21487;&#35745;&#31639;&#25110;&#21322;&#21487;&#35745;&#31639;&#30340;&#36817;&#20284;&#26041;&#27861;&#25110;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#30456;&#20851;&#20294;&#19981;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#23545;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26377;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.10227</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#35299;&#20915;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38598;&#25104;&#12290;&#35813;&#38598;&#25104;&#20351;&#29992;Walsh&#31995;&#25968;&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#22815;&#36924;&#36817;&#24067;&#23572;&#20989;&#25968;&#24182;&#25511;&#21046;&#38598;&#25104;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#20551;&#35774;&#26159;&#39640;&#26354;&#29575;&#30340;&#20915;&#31574;&#36793;&#30028;&#20801;&#35768;&#25214;&#21040;&#23545;&#25239;&#25200;&#21160;&#65292;&#20294;&#20250;&#25913;&#21464;&#20915;&#31574;&#36793;&#30028;&#30340;&#26354;&#29575;&#65292;&#32780;&#19982;&#28165;&#26224;&#22270;&#20687;&#30456;&#27604;&#65292;&#20351;&#29992;Walsh&#31995;&#25968;&#23545;&#20854;&#36827;&#34892;&#36924;&#36817;&#30340;&#26041;&#24335;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#21487;&#29992;&#20110;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;DNN&#30340;&#23398;&#20064;&#21644;&#21487;&#36801;&#31227;&#24615;&#29305;&#24615;&#12290;&#23613;&#31649;&#26412;&#25991;&#30340;&#23454;&#39564;&#20351;&#29992;&#22270;&#20687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06433</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#20135;&#29983;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35266;&#23519;&#23545;&#35937;&#21644;&#22330;&#26223;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#26102;&#38388;&#29702;&#35299;&#30340;&#29305;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#38745;&#24577;&#22270;&#20687;&#39044;&#35757;&#32451;&#20173;&#28982;&#26159;&#23398;&#20064;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#19981;&#21305;&#37197;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#38382;&#26159;&#21542;&#35270;&#39057;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65306;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#12289;&#23545;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31579;&#36873;&#35270;&#39057;&#30340;&#26032;&#39062;&#31243;&#24207;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#27604;&#24615;&#26694;&#26550;&#65292;&#20174;&#20854;&#20013;&#30340;&#22797;&#26434;&#36716;&#25442;&#20013;&#23398;&#20064;&#12290;&#36825;&#31181;&#20174;&#35270;&#39057;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#31616;&#21333;&#33539;&#24335;&#34987;&#31216;&#20026;VITO&#65292;&#23427;&#20135;&#29983;&#30340;&#19968;&#33324;&#34920;&#31034;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#19978;&#36828;&#36828;&#20248;&#20110;&#20808;&#21069;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;VITO&#34920;&#31034;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#24418;&#21464;&#30340;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
&lt;/p&gt;</description></item></channel></rss>