<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>UrbanVLP&#26159;&#19968;&#31181;&#22810;&#31890;&#24230;&#20449;&#24687;&#38598;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#30446;&#21069;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#24230;</title><link>https://arxiv.org/abs/2403.16831</link><description>&lt;p&gt;
UrbanVLP&#65306;&#29992;&#20110;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#30340;&#22810;&#31890;&#24230;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16831
&lt;/p&gt;
&lt;p&gt;
UrbanVLP&#26159;&#19968;&#31181;&#22810;&#31890;&#24230;&#20449;&#24687;&#38598;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#30446;&#21069;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#26088;&#22312;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#25512;&#26029;&#19981;&#21516;&#22478;&#24066;&#26223;&#35266;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#21355;&#26143;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#38754;&#20020;&#30528;&#21452;&#37325;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20165;&#38598;&#20013;&#22312;&#21355;&#26143;&#25968;&#25454;&#20013;&#30340;&#23439;&#35266;&#32423;&#21035;&#27169;&#24335;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#65292;&#22312;&#24494;&#35266;&#32423;&#21035;&#32570;&#20047;&#32454;&#33268;&#30340;&#32454;&#33410;&#65292;&#20363;&#22914;&#26576;&#22320;&#30340;&#24314;&#31569;&#32454;&#33410;&#12290;&#20854;&#27425;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25552;&#20379;&#22478;&#24066;&#35268;&#21010;&#36879;&#26126;&#35777;&#25454;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Language Pre-Trained Model&#65288;UrbanVLP&#65289;&#12290;&#25105;&#20204;&#30340;UrbanVLP&#26080;&#32541;&#25972;&#21512;&#26469;&#33258;&#23439;&#35266;&#65288;&#21355;&#26143;&#65289;&#21644;&#24494;&#35266;&#65288;&#34903;&#26223;&#65289;&#32423;&#21035;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#21644;&#26657;&#20934;&#65292;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16831v1 Announce Type: cross  Abstract: Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel Vision-Language Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic text generation and calibration, elevating interpretability in downstream application
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16501</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#20154;&#31867;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning To Guide Human Decision Makers With Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#20197;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#34920;&#29616;&#20986;&#20852;&#36259;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#65292;&#26088;&#22312;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#21644;&#20943;&#23569;&#35748;&#30693;&#36127;&#25285;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#23558;&#19987;&#23478;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21512;&#20316;&#65292;&#23558;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#19979;&#25918;&#65292;&#35753;&#21069;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#20182;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#8220;&#36131;&#20219;&#20998;&#24037;&#8221;&#35774;&#32622;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16501v1 Announce Type: new  Abstract: There is increasing interest in developing AIs for assisting human decision making in \textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   %   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   %   This \textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine's decisions due to \textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   %   As a remedy, we introduce \textit{learning to guide} (LTG), an alternative framewo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13847</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Domain Adaptation through Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31561;&#20215;&#30340;&#31163;&#25955;&#38382;&#39064;&#35299;&#20915;&#36830;&#32493;&#26368;&#20248;&#36755;&#36816;&#12290;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#26041;&#26696;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#21305;&#37197;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22495;&#20043;&#38388;&#26144;&#23556;&#25968;&#25454;&#28857;&#65292;&#25110;&#32773;&#23558;&#26631;&#31614;&#20174;&#28304;&#22495;&#32452;&#20214;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#22312;&#22833;&#25928;&#35786;&#26029;&#30340;&#20004;&#20010;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13847v1 Announce Type: cross  Abstract: In this paper we explore domain adaptation through optimal transport. We propose a novel approach, where we model the data distributions through Gaussian mixture models. This strategy allows us to solve continuous optimal transport through an equivalent discrete problem. The optimal transport solution gives us a matching between source and target domain mixture components. From this matching, we can map data points between domains, or transfer the labels from the source domain components towards the target domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis, showing that our methods have state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65292;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23558;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#25512;&#24191;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.14174</link><description>&lt;p&gt;
&#21010;&#20998;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Boundaries of Tractability in Hierarchical Task Network Planning. (arXiv:2401.14174v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65292;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23558;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#25512;&#24191;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65306;&#25552;&#20379;&#35745;&#21010;&#30340;&#39564;&#35777;&#65292;&#26159;&#21542;&#23384;&#22312;&#21487;&#25191;&#34892;&#35745;&#21010;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26576;&#20010;&#35745;&#21010;&#36798;&#21040;&#32473;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24120;&#37327;&#20559;&#24207;&#23485;&#24230;&#30340;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#65288;&#20197;&#21450;&#20854;&#25512;&#24191;&#24418;&#24335;&#65289;&#19978;&#65292;&#36825;&#19977;&#20010;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#32780;&#23545;&#20110;&#21518;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#20165;&#22312;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#21487;&#35777;&#26126;&#30340;&#24517;&#35201;&#38480;&#21046;&#19979;&#25165;&#25104;&#31435;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#20803;&#23450;&#29702;&#21450;&#30456;&#24212;&#30340;&#19979;&#30028;&#65292;&#20197;&#30830;&#23450;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#19968;&#33324;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#21487;&#20197;&#34987;&#25552;&#21319;&#30340;&#20005;&#26684;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#26469;&#20016;&#23500;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;(1)&#36890;&#36807;&#23558;&#20559;&#24207;&#23485;&#24230;&#26367;&#25442;&#20026;t&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan. We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space. Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks. Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13112</link><description>&lt;p&gt;
DISCOUNT: &#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35299;&#37322;&#26159;&#22312;&#40657;&#30418;&#20915;&#31574;&#27169;&#22411;&#20013;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36755;&#20837;&#23454;&#20363;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#20174;&#20010;&#20307;&#25968;&#25454;&#28857;&#25193;&#22823;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#21629;&#21517;&#20026;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#12290;&#22312;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20998;&#26512;&#20107;&#23454;&#21644;&#23545;&#25239;&#30340;&#20998;&#24067;&#23646;&#24615;&#65292;&#31867;&#20284;&#20110;&#35780;&#20272;&#20010;&#20307;&#23454;&#20363;&#21450;&#20854;&#32467;&#26524;&#20915;&#31574;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#26500;&#24314;&#19968;&#20010;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23548;&#20986;&#19982;&#20107;&#23454;&#23545;&#24212;&#30340;&#23545;&#25239;&#20998;&#24067;&#65292;&#20197;&#32479;&#35745;&#32622;&#20449;&#24230;&#20570;&#25903;&#25745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;DISCOUNT&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#24179;&#34913;&#36825;&#31181;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MaPeT&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22270;&#20687;&#22359;&#20869;&#30340;&#20381;&#36182;&#20851;&#31995;&#24182;&#20943;&#23569;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07346</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#21644;&#32622;&#25442;&#35270;&#35273;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training. (arXiv:2306.07346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MaPeT&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22270;&#20687;&#22359;&#20869;&#30340;&#20381;&#36182;&#20851;&#31995;&#24182;&#20943;&#23569;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#24050;&#25104;&#20026;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#31561;&#35270;&#35273;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#37325;&#26500;&#19982;&#38543;&#26426;&#25513;&#30721;&#22270;&#20687;&#22359;&#30456;&#20851;&#32852;&#30340;&#35270;&#35273;&#20196;&#29260;&#26469;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25513;&#34109;&#26041;&#27861;&#20250;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#22768;&#36827;&#20837;&#36755;&#20837;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#25513;&#34109;&#24573;&#30053;&#20102;&#21463;&#25439;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22686;&#21152;&#20102;&#19979;&#28216;&#24494;&#35843;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#25513;&#34109;&#21644;&#32622;&#25442;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;MaPeT&#65289;&#65292;&#23427;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22359;&#20869;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;MaPeT&#20351;&#29992;&#36741;&#21161;&#20301;&#32622;&#20449;&#24687;&#26469;&#20943;&#23569;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of self-supervised pre-training has emerged as a promising approach to enhance the performance of visual tasks such as image classification. In this context, recent approaches have employed the Masked Image Modeling paradigm, which pre-trains a backbone by reconstructing visual tokens associated with randomly masked image patches. This masking approach, however, introduces noise into the input data during pre-training, leading to discrepancies that can impair performance during the fine-tuning phase. Furthermore, input masking neglects the dependencies between corrupted patches, increasing the inconsistencies observed in downstream fine-tuning tasks. To overcome these issues, we propose a new self-supervised pre-training approach, named Masked and Permuted Vision Transformer (MaPeT), that employs autoregressive and permuted predictions to capture intra-patch dependencies. In addition, MaPeT employs auxiliary positional information to reduce the disparity between the pre-trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13604</link><description>&lt;p&gt;
&#24102;&#26377;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#36172;&#24466;&#21453;&#39304;&#30340;&#38543;&#26426;&#27425;&#27169;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26399;&#26395;&#19979;&#30340;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#65292;&#24310;&#36831;&#21453;&#39304;&#34987;&#20551;&#23450;&#20026;&#32452;&#21512;&#21644;&#21311;&#21517;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24310;&#36831;&#21453;&#39304;&#26159;&#30001;&#36807;&#21435;&#34892;&#21160;&#30340;&#22870;&#21169;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#22870;&#21169;&#30001;&#23376;&#32452;&#20214;&#26500;&#25104;&#65292;&#20854;&#26410;&#30693;&#30340;&#20998;&#37197;&#26041;&#24335;&#12290;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#65306;&#26377;&#30028;&#23545;&#25239;&#27169;&#22411;&#12289;&#38543;&#26426;&#29420;&#31435;&#27169;&#22411;&#21644;&#38543;&#26426;&#26465;&#20214;&#29420;&#31435;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#27599;&#31181;&#24310;&#36831;&#27169;&#22411;&#23548;&#20986;&#20102;&#21518;&#24724;&#30028;&#12290;&#24573;&#30053;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#24310;&#36831;&#27169;&#22411;&#30340;&#21518;&#24724;&#30028;&#20026; $\tilde{O}(T^{2/3} + T^{1/3} \nu)$&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#33539;&#22260;&#65292;$\nu$ &#26159;&#19977;&#31181;&#24773;&#20917;&#19979;&#19981;&#21516;&#23450;&#20041;&#30340;&#24310;&#36831;&#21442;&#25968;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#24102;&#26377;&#24310;&#36831;&#30340;&#34917;&#20607;&#39033;&#12290;&#25152;&#32771;&#34385;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;&#32771;&#34385;&#20102;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#30340;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
&lt;/p&gt;</description></item></channel></rss>