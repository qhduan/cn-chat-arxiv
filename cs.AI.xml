<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;</title><link>https://arxiv.org/abs/2403.17467</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
A Unified Kernel for Neural Network Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20869;&#26680;&#23398;&#20064;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#32852;&#31995;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#36830;&#25509;&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#36827;&#23637;&#12290;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#27969;&#26041;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;(NNGP)&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#12290;&#21069;&#32773;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20195;&#34920;&#20102;&#38646;&#38454;&#26680;&#65292;&#32780;&#21518;&#32773;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20999;&#21521;&#31354;&#38388;&#65292;&#26159;&#31532;&#19968;&#38454;&#26680;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#35813;&#20869;&#26680;&#34920;&#24449;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26799;&#24230;&#19979;&#38477;&#21644;&#21442;&#25968;&#21021;&#22987;&#21270;&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;UNK&#20869;&#26680;&#20445;&#25345;&#20102;NNGP&#21644;NTK&#30340;&#26497;&#38480;&#29305;&#24615;&#65292;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#20294;&#26377;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#24182;&#19988;&#24403;&#23398;&#20064;&#27493;&#39588;&#25509;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#23545;UNK&#20869;&#26680;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17467v1 Announce Type: cross  Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoreticall
&lt;/p&gt;</description></item></channel></rss>