<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.07573</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#21487;&#36866;&#24212;&#24615;&#35745;&#31639;&#21644;&#32593;&#32476;&#34701;&#21512;&#30340;&#21160;&#24577;&#26410;&#26469;&#65288;ACNC&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;6G&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#35745;&#20250;&#20986;&#29616;&#23454;&#36136;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#31361;&#20986;&#20102;&#30001;&#22823;&#37327;&#36830;&#25509;&#21644;&#20005;&#26684;&#36981;&#23432;&#26381;&#21153;&#36136;&#37327;/&#20307;&#39564;&#65288;QoS/E&#65289;&#20808;&#20915;&#26465;&#20214;&#25152;&#29305;&#24449;&#21270;&#30340;&#20840;&#38754;&#30340;&#19968;&#20999;&#23545;&#19968;&#20999;&#20132;&#20114;&#12290;&#21363;&#23558;&#38754;&#20020;&#30340;&#25361;&#25112;&#28304;&#20110;&#36164;&#28304;&#31232;&#32570;&#65292;&#20419;&#20351;&#26377;&#24847;&#35782;&#22320;&#21521;&#35745;&#31639;-&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#36807;&#28193;&#65292;&#20316;&#20026;&#32852;&#21512;&#36164;&#28304;&#32534;&#25490;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;CNC&#30340;&#26426;&#21046;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#29616;&#26410;&#26469;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;Metaverse&#30340;&#20351;&#29992;&#24773;&#26223;&#20013;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#29992;&#25143;&#12289;&#26381;&#21153;&#21644;&#36164;&#28304;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18853</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24102;&#26377;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#30340;&#22810;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Multi-domain Generalization with A General Learning Objective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#27867;&#21270;&#65288;mDG&#65289;&#30340;&#26222;&#36941;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#22686;&#24378;&#36793;&#38469;&#21040;&#26631;&#31614;&#20998;&#24067;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;mDG&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#24120;&#23545;&#38745;&#24577;&#30446;&#26631;&#36793;&#38469;&#20998;&#24067;&#26045;&#21152;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#19968;&#20010;$Y$-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;mDG&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#26469;&#35299;&#37322;&#21644;&#20998;&#26512;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;mDG&#26234;&#24935;&#12290;&#36825;&#20010;&#36890;&#29992;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#21327;&#21516;&#30340;&#30446;&#26631;&#65306;&#23398;&#20064;&#19982;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#19968;&#20010;&#21518;&#39564;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#36825;&#20123;&#39033;&#32467;&#21512;&#20102;&#20808;&#39564;&#20449;&#24687;&#24182;&#25233;&#21046;&#20102;&#26080;&#25928;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20943;&#36731;&#20102;&#25918;&#26494;&#32422;&#26463;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20026;&#22495;&#23545;&#40784;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18853v1 Announce Type: cross  Abstract: Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.13270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#23454;&#29616;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;
&lt;/p&gt;
&lt;p&gt;
Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#26159;&#19968;&#20010;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#22330;&#26223;&#36827;&#34892;&#20934;&#30830;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#20197;&#20272;&#35745;&#28784;&#24230;&#22270;&#20687;&#30340;&#21512;&#29702;&#39068;&#33394;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20132;&#20114;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#19978;&#33394;&#26469;&#35828;&#65292;&#25512;&#26029;&#20986;&#36924;&#30495;&#21644;&#20934;&#30830;&#30340;&#39068;&#33394;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#23545;&#28784;&#24230;&#22330;&#26223;&#30340;&#35821;&#20041;&#29702;&#35299;&#38590;&#24230;&#65292;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#30456;&#24212;&#30340;&#38899;&#39057;&#65292;&#38899;&#39057;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#20851;&#20110;&#21516;&#19968;&#22330;&#26223;&#30340;&#39069;&#22806;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#27880;&#20837;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#65288;AIAIC&#65289;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#20316;&#20026;&#26725;&#26753;&#65292;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#24341;&#23548;&#39044;&#35757;&#32451;&#19978;&#33394;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#38899;&#39057;&#19982;&#35270;&#39057;&#30340;&#33258;&#28982;&#20849;&#29616;&#26469;&#23398;&#20064;&#38899;&#39057;&#21644;&#35270;&#35273;&#22330;&#26223;&#20043;&#38388;&#30340;&#39068;&#33394;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#31532;&#19977;&#65292;&#38544;&#24335;&#38899;&#39057;&#35821;&#20041;&#34920;&#31034;&#34987;&#21033;&#29992;&#20197;&#24341;&#23548;&#22270;&#20687;&#19978;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.08669</link><description>&lt;p&gt;
&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#19968;&#20123;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#30340;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#21644;&#38750;&#24120;&#22797;&#26434;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#23578;&#26410;&#35777;&#26126;&#26377;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#31181;&#36825;&#26679;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#36742;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#38656;&#35201;&#27839;&#30528;&#33410;&#28857;&#24207;&#21015;&#31227;&#21160;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#36215;&#28857;&#21040;&#32456;&#28857;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#20026;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#35268;&#27169;&#30340;&#20379;&#24212;&#38142;&#29289;&#27969;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#22810;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#19979;&#36827;&#34892;&#65292;&#24182;&#33021;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00074</link><description>&lt;p&gt;
SocREval&#65306;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35780;&#20272;&#23427;&#20204;&#30340;&#36880;&#27493;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#35780;&#20272;&#27169;&#22411;&#23548;&#20986;&#30340;&#25512;&#29702;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20154;&#24037;&#32534;&#20889;&#30340;&#25512;&#29702;&#38142;&#21487;&#33021;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;&#33719;&#21462;&#36890;&#24120;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25512;&#29702;&#25351;&#26631;&#28040;&#38500;&#20102;&#20154;&#24037;&#21046;&#20316;&#25512;&#29702;&#38142;&#30340;&#38656;&#27714;&#20316;&#20026;&#21442;&#32771;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#20154;&#24037;&#25512;&#29702;&#38142;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#22797;&#26434;&#21270;&#20102;&#27969;&#31243;&#24182;&#24341;&#21457;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;&#25512;&#29702;&#38142;&#36136;&#37327;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#21046;&#20316;&#21442;&#32771;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#25552;&#31034;&#26469;&#22686;&#24378;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#36825;&#23601;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;SocREval&#65288;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic metho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17166</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#22312;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#33039;&#27963;&#26816;&#26159;&#32958;&#33039;&#30142;&#30149;&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#12290;&#19987;&#23478;&#32958;&#33039;&#30149;&#29702;&#23398;&#23478;&#21046;&#23450;&#30340;&#30149;&#21464;&#35780;&#20998;&#26159;&#21322;&#23450;&#37327;&#30340;&#65292;&#24182;&#19988;&#23384;&#22312;&#39640;&#30340;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23545;&#20998;&#21106;&#30340;&#35299;&#21078;&#23545;&#35937;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#36825;&#31181;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#27963;&#26816;&#30340;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;a&#65289;&#24179;&#22343;&#25968;&#37327;&#36739;&#22823;&#65288;&#32422;300&#33267;1000&#20010;&#65289;&#23494;&#38598;&#25509;&#35302;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#65288;b&#65289;&#20855;&#26377;&#22810;&#20010;&#31867;&#21035;&#65288;&#33267;&#23569;3&#20010;&#65289;&#65292;&#65288;c&#65289;&#23610;&#23544;&#21644;&#24418;&#29366;&#21508;&#24322;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#19981;&#33021;&#20197;&#39640;&#25928;&#36890;&#29992;&#30340;&#26041;&#24335;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#38170;&#28857;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25193;&#25955;&#27169;&#22411;&#12289;&#21464;&#25442;&#22120;&#27169;&#22359;&#21644;RCNN&#65288;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#21488;NVIDIA GeForce RTX 3090 GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#21487;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2210.14164</link><description>&lt;p&gt;
3D&#28857;&#20113;&#20998;&#31867;&#30340;&#26080;&#30418;&#23376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21508;&#31181;&#36755;&#20837;&#20449;&#21495;&#30340;&#20998;&#26512;&#65292;&#23545;&#25239;&#25915;&#20987;&#26500;&#25104;&#20102;&#20005;&#37325;&#25361;&#25112;&#12290;&#22312;3D&#28857;&#20113;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35782;&#21035;&#22312;&#32593;&#32476;&#20915;&#31574;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28857;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#26174;&#33879;&#24615;&#22270;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#25239;&#25915;&#20987;&#20250;&#26174;&#33879;&#24433;&#21709;&#32593;&#32476;&#20915;&#31574;&#30340;&#28857;&#12290;&#36890;&#24120;&#65292;&#35782;&#21035;&#23545;&#25239;&#28857;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#28857;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#26080;&#30418;&#23376;&#8221;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;14&#20010;&#28857;&#20113;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#26469;&#26816;&#26597;&#36825;&#20123;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#20197;&#21450;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2106.16046</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65306;&#22522;&#20934;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Context Generalizability in Spatiotemporal Crowd Flow Prediction: Benchmark and Guideline. (arXiv:2106.16046v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.16046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#20026;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#26159;&#26500;&#24314;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#65288;STCFP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#22312;&#20110;&#19981;&#21516;&#22330;&#26223;&#20013;&#19978;&#19979;&#25991;&#29305;&#24449;&#65288;&#20363;&#22914;&#22825;&#27668;&#12289;&#20551;&#26085;&#21644;&#20852;&#36259;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#26410;&#30693;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#30001;&#22823;&#35268;&#27169;&#26102;&#31354;&#20154;&#32676;&#27969;&#37327;&#25968;&#25454;&#12289;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#26368;&#20808;&#36827;&#30340;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22478;&#24066;&#20154;&#32676;&#27969;&#37327;&#39044;&#27979;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#23450;&#37327;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#29305;&#24449;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#27867;&#21270;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#27969;&#34892;&#30740;&#31350;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24320;&#21457;&#20102;&#19978;&#19979;&#25991;&#24314;&#27169;&#25216;&#26415;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#25968;&#30334;&#31181;&#27169;&#22411;&#20197;&#25429;&#25417;&#19978;&#19979;&#25991;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;STCFP&#20013;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#24314;&#27169;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual features are important data sources for building spatiotemporal crowd flow prediction (STCFP) models. However, the difficulty of applying context lies in the unknown generalizability of both contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we build a benchmark composed of large-scale spatiotemporal crowd flow data, contextual data, and state-of-the-art spatiotemporal prediction models. We conduct a comprehensive experimental study to quantitatively investigate the generalizability of different contextual features and modeling techniques in several urban crowd flow prediction scenarios (including bike flow, metro passenger flow, electric vehicle charging demand and so on). In particular, we develop a general taxonomy of context modeling techniques based on extensive investigations in prevailing research. With millions of records and rich context data, we have trained and tested hun
&lt;/p&gt;</description></item></channel></rss>