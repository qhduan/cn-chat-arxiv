<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05842</link><description>&lt;p&gt;
Hufu&#65306;&#19968;&#31181;&#36890;&#36807;&#32622;&#25442;&#31561;&#21464;&#24615;&#23545;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05842
&lt;/p&gt;
&lt;p&gt;
Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26381;&#21153;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#20445;&#25252;&#23453;&#36149;&#30340;&#27169;&#22411;&#21442;&#25968;&#20813;&#21463;&#30423;&#31363;&#24050;&#25104;&#20026;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#27700;&#21360;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#23450;&#21046;&#65292;&#38590;&#20197;&#20316;&#20026;&#38598;&#25104;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hufu&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#20381;&#36182;&#20110;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#12290;Hufu&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#32622;&#25442;&#30340;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#19978;&#23884;&#20837;&#27700;&#21360;&#65292;&#23884;&#20837;&#30340;&#27169;&#22411;&#22522;&#26412;&#19978;&#21253;&#21547;&#20004;&#32452;&#26435;&#37325; -- &#19968;&#32452;&#29992;&#20110;&#27491;&#24120;&#20351;&#29992;&#65292;&#21478;&#19968;&#32452;&#29992;&#20110;&#27700;&#21360;&#25552;&#21462;&#65292;&#35302;&#21457;&#26465;&#20214;&#26159;&#32463;&#36807;&#32622;&#25442;&#30340;&#36755;&#20837;&#12290;&#32622;&#25442;&#31561;&#21464;&#24615;&#30830;&#20445;&#36825;&#20004;&#32452;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#26368;&#23567;&#24178;&#25200;&#65292;&#20174;&#32780;&#22312;&#27700;&#21360;&#25552;&#21462;&#26102;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05842v1 Announce Type: cross  Abstract: With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers. Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downst
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15288</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Active teacher selection for reinforcement learning from human feedback. (arXiv:2310.15288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#21453;&#39304;&#37117;&#26469;&#33258;&#19968;&#20010;&#21333;&#19968;&#30340;&#20154;&#31867;&#25945;&#24072;&#65292;&#23613;&#31649;&#38656;&#35201;&#35810;&#38382;&#19981;&#21516;&#25945;&#24072;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Hidden Utility Bandit"&#65288;HUB&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#25945;&#24072;&#22312;&#29702;&#24615;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#20174;&#22810;&#20010;&#25945;&#24072;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#65306;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;"Active Teacher Selection"&#65288;ATS&#65289;&#31639;&#27861;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#20309;&#26102;&#20197;&#21450;&#36873;&#25321;&#21738;&#20010;&#25945;&#24072;&#26469;&#26597;&#35810;&#65292;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;HUB&#26694;&#26550;&#21644;ATS&#31639;&#27861;&#23637;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#40065;&#26834;&#22870;&#21169;&#24314;&#27169;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item></channel></rss>