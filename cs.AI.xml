<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19723</link><description>&lt;p&gt;
HGT&#65306;&#21033;&#29992;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19723
&lt;/p&gt;
&lt;p&gt;
HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#29702;&#35299; (TU) &#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38754;&#20020;&#25163;&#21160;&#26631;&#35760;&#34920;&#26684;&#30340;&#31232;&#32570;&#24615;&#21644;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HGT &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#24322;&#36136;&#22270; (HG) &#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412; TU &#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#25351;&#23548;&#36716;&#25442;&#23558;&#34920;&#26684;&#35821;&#20041;&#19982;LLM&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#19977;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#26696;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;HGT&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#23569;&#26679;&#26412;&#22797;&#26434;TU&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.07573</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#21487;&#36866;&#24212;&#24615;&#35745;&#31639;&#21644;&#32593;&#32476;&#34701;&#21512;&#30340;&#21160;&#24577;&#26410;&#26469;&#65288;ACNC&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;6G&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#35745;&#20250;&#20986;&#29616;&#23454;&#36136;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#31361;&#20986;&#20102;&#30001;&#22823;&#37327;&#36830;&#25509;&#21644;&#20005;&#26684;&#36981;&#23432;&#26381;&#21153;&#36136;&#37327;/&#20307;&#39564;&#65288;QoS/E&#65289;&#20808;&#20915;&#26465;&#20214;&#25152;&#29305;&#24449;&#21270;&#30340;&#20840;&#38754;&#30340;&#19968;&#20999;&#23545;&#19968;&#20999;&#20132;&#20114;&#12290;&#21363;&#23558;&#38754;&#20020;&#30340;&#25361;&#25112;&#28304;&#20110;&#36164;&#28304;&#31232;&#32570;&#65292;&#20419;&#20351;&#26377;&#24847;&#35782;&#22320;&#21521;&#35745;&#31639;-&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#36807;&#28193;&#65292;&#20316;&#20026;&#32852;&#21512;&#36164;&#28304;&#32534;&#25490;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;CNC&#30340;&#26426;&#21046;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#29616;&#26410;&#26469;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;Metaverse&#30340;&#20351;&#29992;&#24773;&#26223;&#20013;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#29992;&#25143;&#12289;&#26381;&#21153;&#21644;&#36164;&#28304;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;</title><link>https://arxiv.org/abs/2403.05050</link><description>&lt;p&gt;
DyRoNet&#65306;&#19968;&#31181;&#20302;&#31209;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65292;&#29992;&#20110;&#27969;&#23186;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05050
&lt;/p&gt;
&lt;p&gt;
DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#24863;&#30693;&#26469;&#24212;&#23545;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65288;DyRoNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#20197;&#22686;&#24378;&#27969;&#23186;&#20307;&#24863;&#30693;&#12290;&#36890;&#36807;&#38598;&#25104;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#38024;&#23545;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#24494;&#35843;&#65292;DyRoNet&#22312;&#24310;&#36831;&#21644;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#20854;&#26680;&#24515;&#29305;&#24449;&#26159;&#36895;&#24230;&#36335;&#30001;&#27169;&#22359;&#65292;&#26234;&#33021;&#22320;&#23558;&#36755;&#20837;&#25968;&#25454;&#24341;&#23548;&#21040;&#26368;&#36866;&#21512;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;DyRoNet&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#65292;&#20026;&#21508;&#31181;&#22330;&#26223;&#24615;&#33021;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#26438;&#12290;DyRoNet&#19981;&#20165;&#20026;&#27969;&#23186;&#20307;&#24863;&#30693;&#24314;&#31435;&#20102;&#26032;&#30340;&#26631;&#26438;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24037;&#31243;&#27934;&#35265;&#12290;&#26377;&#20851;&#26356;&#22810;&#39033;&#30446;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382; https://tastevision.github.io/DyRoNet/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01002</link><description>&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01002
&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#24635;&#32467;&#20020;&#24202;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#29983;&#25104;&#20934;&#30830;&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#19982;&#22522;&#30784;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#32467;&#26500;&#21270;&#65288;AS&#65289;&#20316;&#20026;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#26500;&#21270;&#20102;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#12290;&#23427;&#23558;&#35780;&#20272;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#20010;&#22522;&#20110;LLM&#25191;&#34892;&#30456;&#23545;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#21644;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#32508;&#21512;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AS&#22987;&#32456;&#25913;&#21892;&#20102;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#20013;&#20154;&#31867;&#27880;&#37322;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;AS&#36890;&#36807;&#30701;&#25991;&#26412;&#24418;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01002v1 Announce Type: cross  Abstract: Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short te
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.16313</link><description>&lt;p&gt;
Chain-of-Discussion&#65306;&#22797;&#26434;&#35777;&#25454;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#27169;&#22411;&#25214;&#21040;&#36866;&#24403;&#30340;&#35777;&#25454;&#26469;&#24418;&#25104;&#21512;&#29702;&#12289;&#20840;&#38754;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#21442;&#19982;&#23545;&#19982;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#30340;&#28508;&#22312;&#22330;&#26223;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#12290;&#22312;&#26816;&#32034;&#27169;&#22359;&#30340;&#22686;&#24378;&#19979;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20294;&#22312;&#21487;&#38752;&#35777;&#25454;&#36873;&#25321;&#21644;&#28145;&#20837;&#38382;&#39064;&#20998;&#26512;&#26041;&#38754;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Chain-of-Discussion&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#24320;&#25918;&#24335;QA&#25552;&#20379;&#26356;&#27491;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20010;&#20307;&#19978;&#36824;&#19981;&#22815;&#24378;&#22823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#35752;&#35770;&#23545;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;\url{https://github.com/kobaya}&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12928</link><description>&lt;p&gt;
&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20998;&#25955;&#30340;&#30693;&#35782;&#65292;&#25991;&#29486;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#25152;&#30740;&#31350;&#20027;&#39064;&#30340;&#20840;&#38754;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#65288;PAMI&#65289;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36807;&#22810;&#30340;&#32508;&#36848;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#35770;&#32773;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#20851;&#27880;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#20840;&#38754;&#23457;&#35270;PAMI&#39046;&#22495;&#30340;&#32508;&#36848;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12928v1 Announce Type: cross  Abstract: By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12121</link><description>&lt;p&gt;
&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Image Review Ability of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26159;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LVLM&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;LVLM&#23545;&#22270;&#20687;&#30340;&#35780;&#20215;&#33021;&#21147;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#31361;&#26174;&#20102;&#23545;&#20854;&#35780;&#20215;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#19982;&#22270;&#20687;&#26631;&#39064;&#19981;&#21516;&#65292;&#35780;&#20215;&#25991;&#26412;&#21487;&#20197;&#20174;&#22270;&#20687;&#26500;&#22270;&#21644;&#26333;&#20809;&#31561;&#19981;&#21516;&#35270;&#35282;&#25776;&#20889;&#12290;&#36825;&#31181;&#35780;&#20215;&#35282;&#24230;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21807;&#19968;&#30830;&#23450;&#22270;&#20687;&#30340;&#27491;&#30830;&#35780;&#20215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;LVLM&#23545;&#35780;&#20215;&#25991;&#26412;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#27979;&#37327;&#36825;&#20123;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#26368;&#26032;LVLM&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05813</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#25512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#36951;&#24536;&#25216;&#26415;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#12290;&#19982;&#20197;&#24448;&#23436;&#20840;&#30456;&#21453;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#25935;&#24863;&#20449;&#24687;&#25552;&#21462;&#21487;&#33021;&#24615;&#65288;S-EL&#65289;&#21644;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20934;&#30830;&#24615;&#65288;S-MA&#65289;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21152;&#24378;&#36951;&#24536;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#12290;&#22312;&#32447;&#36873;&#25321;&#26426;&#21046;&#21033;&#29992;&#35821;&#35328;&#27010;&#29575;&#24471;&#20998;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#31163;&#32447;&#31574;&#30053;&#21017;&#21033;&#29992;&#22522;&#20110;&#36317;&#31163;&#30340;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03247</link><description>&lt;p&gt;
HEANA: &#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#25968;&#25454;&#27969;&#30340;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#33021;&#37327;&#39640;&#25928;&#30340;CNN&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03247
&lt;/p&gt;
&lt;p&gt;
HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEANA&#30340;&#26032;&#22411;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#21152;&#36895;&#25972;&#25968;&#37327;&#21270;CNN&#30340;&#25512;&#29702;&#12290;HEANA&#37319;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#65292;&#22686;&#24378;&#20102;HEANA&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#25903;&#25345;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#35889;&#26080;&#25439;&#30340;TAOMs&#25490;&#21015;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#23384;&#22312;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#21508;&#31181;&#20018;&#25200;&#24433;&#21709;&#12289;&#26080;&#27861;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.19180</link><description>&lt;p&gt;
JEN-1 Composer: &#19968;&#20010;&#29992;&#20110;&#39640;&#20445;&#30495;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19180
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#21512;&#25104;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#38899;&#36712;&#29983;&#25104;&#30340;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#21407;&#22987;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#21333;&#29420;&#32452;&#25104;&#21644;&#32452;&#21512;&#22810;&#38899;&#36712;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#20316;&#26354;&#23478;&#30340;&#20856;&#22411;&#24037;&#20316;&#27969;&#31243;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEN-1 Composer&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#39640;&#25928;&#22320;&#24314;&#27169;&#22810;&#38899;&#36712;&#38899;&#20048;&#30340;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#12290;JEN-1 Composer&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#22320;&#25972;&#21512;&#20219;&#20309;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#20363;&#22914;Jen-1&#65292;&#22686;&#24378;&#20854;&#22810;&#21151;&#33021;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35838;&#31243;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#36880;&#27493;&#25351;&#23548;&#27169;&#22411;&#20174;&#21333;&#38899;&#36712;&#29983;&#25104;&#21040;&#28789;&#27963;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#20851;&#20110;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#22312;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#31639;&#27861;&#21487;&#20197;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.14548</link><description>&lt;p&gt;
&#31639;&#27861;&#21246;&#32467;&#36824;&#26159;&#31454;&#20105;&#65306;&#24179;&#21488;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collusion or Competition: the Role of Platforms' Recommender Systems. (arXiv:2309.14548v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#20851;&#20110;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#22312;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#31639;&#27861;&#21487;&#20197;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#26415;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#23548;&#33268;&#30340;&#31639;&#27861;&#21246;&#32467;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20351;&#29992;&#25512;&#33616;&#31639;&#27861;&#26469;&#20998;&#37197;&#19981;&#21516;&#20135;&#21697;&#30340;&#26333;&#20809;&#65292;&#32780;&#36825;&#19968;&#37325;&#35201;&#26041;&#38754;&#22312;&#20808;&#21069;&#30340;&#31639;&#27861;&#21246;&#32467;&#30740;&#31350;&#20013;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#24182;&#26816;&#39564;&#20102;&#25512;&#33616;&#31639;&#27861;&#22914;&#20309;&#20915;&#23450;&#22522;&#20110;AI&#30340;&#23450;&#20215;&#31639;&#27861;&#30340;&#31454;&#20105;&#25110;&#21246;&#32467;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;&#25512;&#33616;&#31639;&#27861;&#65306;(i)&#20197;&#26368;&#22823;&#21270;&#21334;&#23478;&#24635;&#21033;&#28070;&#20026;&#30446;&#26631;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;(ii)&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#19978;&#20135;&#21697;&#38656;&#27714;&#20026;&#30446;&#26631;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#37325;&#22797;&#21338;&#24328;&#26694;&#26550;&#65292;&#23558;&#21334;&#23478;&#30340;&#23450;&#20215;&#31639;&#27861;&#21644;&#24179;&#21488;&#30340;&#25512;&#33616;&#31639;&#27861;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent academic research has extensively examined algorithmic collusion resulting from the utilization of artificial intelligence (AI)-based dynamic pricing algorithms. Nevertheless, e-commerce platforms employ recommendation algorithms to allocate exposure to various products, and this important aspect has been largely overlooked in previous studies on algorithmic collusion. Our study bridges this important gap in the literature and examines how recommendation algorithms can determine the competitive or collusive dynamics of AI-based pricing algorithms. Specifically, two commonly deployed recommendation algorithms are examined: (i) a recommender system that aims to maximize the sellers' total profit (profit-based recommender system) and (ii) a recommender system that aims to maximize the demand for products sold on the platform (demand-based recommender system). We construct a repeated game framework that incorporates both pricing algorithms adopted by sellers and the platform's recom
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#35299;&#37322;&#33021;&#21147;&#19968;&#30452;&#26159;&#38480;&#21046;&#20854;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#22914;&#21307;&#23398;&#12289;&#32593;&#32476;&#23433;&#20840;&#25110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21463;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#33021;&#37327;&#26223;&#35266;&#30740;&#31350;&#26041;&#27861;&#21551;&#21457;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#20419;&#36827;&#27169;&#22411;&#20915;&#31574;&#12290;&#36890;&#36807;&#35782;&#21035;&#25439;&#22833;&#26223;&#35266;&#23616;&#37096;&#26497;&#23567;&#20540;&#32452;&#20013;&#30340;&#23432;&#24658;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#23384;&#22312;&#31867;&#20284;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22352;&#26631;&#19981;&#21464;&#37327;&#25110;&#26377;&#24207;&#21442;&#25968;&#26469;&#30830;&#23450;&#20998;&#23376;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#26223;&#35266;&#20013;&#27809;&#26377;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
&lt;/p&gt;</description></item></channel></rss>