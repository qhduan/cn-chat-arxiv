<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.18140</link><description>&lt;p&gt;
Juru: &#26469;&#33258;&#21487;&#38752;&#26469;&#28304;&#30340;&#24052;&#35199;&#27861;&#24459;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Juru: Legal Brazilian Large Language Model from Reputable Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18140
&lt;/p&gt;
&lt;p&gt;
Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#30456;&#20851;&#30740;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39046;&#22495;&#19987;&#38376;&#21270;&#21644;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20026;&#25506;&#32034;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21487;&#38752;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#19987;&#38376;&#21270;&#20102;Sabi\'a-2 Small&#27169;&#22411;&#65292;&#24182;&#22312;&#27861;&#24459;&#21644;&#19968;&#33324;&#30693;&#35782;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Juru&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19987;&#38376;&#21270;&#26159;&#20197;&#22312;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22686;&#21152;&#30340;&#31185;&#23398;&#35777;&#25454;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21487;&#33021;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.14410</link><description>&lt;p&gt;
GLC++: &#20840;&#23616;&#23616;&#37096;&#32858;&#31867;&#21644;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22312;&#21327;&#21464;&#37327;&#21644;&#31867;&#21035;&#36716;&#31227;&#19979;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;SFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#23553;&#38381;&#38598;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26088;&#22312;&#20934;&#30830;&#20998;&#31867;&#23646;&#20110;&#24120;&#35265;&#31867;&#21035;&#30340;&#8220;&#24050;&#30693;&#8221;&#25968;&#25454;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#19987;&#26377;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#38548;&#31163;&#24320;&#26469;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#29699;&#21644;&#23616;&#37096;&#32858;&#31867;&#65288;GLC&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#30340;&#19968;&#23545;&#20840;&#23616;&#32858;&#31867;&#31639;&#27861;&#26469;&#21306;&#20998;&#30446;&#26631;&#31867;&#21035;&#65292;&#36741;&#20197;&#26412;&#22320;k-NN&#32858;&#31867;&#31574;&#30053;&#20197;&#20943;&#36731;&#36127;&#38754;&#36716;&#31227;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22266;&#26377;&#30340;&#23553;&#38381;&#28304;&#26550;&#26500;&#23548;&#33268;&#23545;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#8220;&#26410;&#30693;&#8221;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GLC&#21457;&#23637;&#21040;GLC++&#65292;&#25972;&#21512;&#20102;&#23545;&#27604;&#20146;&#21644;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14410v1 Announce Type: cross  Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.03328</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;AI&#27169;&#22411;&#32570;&#20047;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large-scale Generative AI Models Lack Visual Number Sense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#36731;&#26494;&#21028;&#26029;&#29289;&#20307;&#30340;&#25968;&#37327;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#35745;&#25968;&#65292;&#32780;&#19988;&#36825;&#31181;&#25216;&#33021;&#22312;&#21508;&#31181;&#21160;&#29289;&#29289;&#31181;&#21644;&#35821;&#35328;&#21457;&#23637;&#21644;&#27491;&#24335;&#23398;&#26657;&#25945;&#32946;&#20043;&#21069;&#30340;&#23156;&#20799;&#20013;&#37117;&#26377;&#35760;&#24405;&#12290;&#23545;&#20110;&#23567;&#30340;&#29289;&#20307;&#38598;&#65292;&#25968;&#23383;&#21028;&#26029;&#26159;&#26080;&#35823;&#30340;&#65292;&#32780;&#23545;&#20110;&#26356;&#22823;&#30340;&#38598;&#21512;&#65292;&#22238;&#24212;&#21464;&#24471;&#36817;&#20284;&#65292;&#24182;&#19988;&#21464;&#24322;&#24615;&#19982;&#30446;&#26631;&#25968;&#23383;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#23613;&#31649;&#29289;&#20307;&#29305;&#24449;&#65288;&#22914;&#39068;&#33394;&#25110;&#24418;&#29366;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#31181;&#22238;&#24212;&#27169;&#24335;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#29289;&#20307;&#19978;&#35266;&#23519;&#21040;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#20381;&#36182;&#20110;&#25968;&#23383;&#25968;&#37327;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21487;&#38752;&#22320;&#21629;&#21517;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#20013;&#30340;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#29289;&#21697;&#25968;&#37327;&#30340;&#22270;&#20687;&#65288;1-10&#33539;&#22260;&#20869;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#32771;&#34385;&#30340;&#25152;&#26377;&#22522;&#30784;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#19968;&#26679;&#30340;&#26041;&#24335;&#34920;&#29616;&#20986;&#26469;&#65306;&#21363;&#20351;&#26159;&#20855;&#26377;&#36739;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#29359;&#19979;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with sm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02339</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#21463;&#21040;&#22495;&#38388;&#24046;&#24322;&#30340;&#38480;&#21046;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#25972;&#20307;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#32422;&#26463;&#65292;&#36825;&#20165;&#20165;&#30830;&#20445;&#20102;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#23545;&#40784;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270; (UAO) &#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;2D&#21040;3D&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#30456;&#24212;&#30340;3D&#23039;&#21183;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;3D&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27979;&#35797;&#26102;&#30340;&#20248;&#21270;&#65292;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20165;&#20248;&#21270;&#23569;&#37327;&#20851;&#38190;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
&lt;/p&gt;</description></item><item><title>ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.16744</link><description>&lt;p&gt;
ShaRP&#65306;&#29992;Shapley&#20540;&#35299;&#37322;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ShaRP: Explaining Rankings with Shapley Values. (arXiv:2401.16744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16744
&lt;/p&gt;
&lt;p&gt;
ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25307;&#32856;&#12289;&#22823;&#23398;&#25307;&#29983;&#21644;&#36151;&#27454;&#31561;&#37325;&#35201;&#39046;&#22495;&#30340;&#31639;&#27861;&#20915;&#31574;&#24120;&#24120;&#26159;&#22522;&#20110;&#25490;&#21517;&#30340;&#12290;&#30001;&#20110;&#36825;&#20123;&#20915;&#31574;&#23545;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#20154;&#32676;&#30340;&#24433;&#21709;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#23427;&#20204;&#65306;&#20102;&#35299;&#20915;&#31574;&#26159;&#21542;&#36981;&#23432;&#27861;&#24459;&#65292;&#24110;&#21161;&#20010;&#20154;&#25552;&#39640;&#20182;&#20204;&#30340;&#25490;&#21517;&#65292;&#24182;&#35774;&#35745;&#26356;&#22909;&#30340;&#25490;&#21517;&#31243;&#24207;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ShaRP&#65288;Shapley for Rankings and Preferences&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#29305;&#24449;&#23545;&#25490;&#21517;&#32467;&#26524;&#19981;&#21516;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20351;&#29992;ShaRP&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#31639;&#27861;&#25490;&#21517;&#22120;&#20351;&#29992;&#30340;&#35780;&#20998;&#20989;&#25968;&#26159;&#24050;&#30693;&#30340;&#19988;&#26159;&#32447;&#24615;&#30340;&#65292;&#27599;&#20010;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#12290;&#36129;&#29486;&#21462;&#20915;&#20110;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#24494;&#22937;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;ShaRP&#22522;&#20110;&#37327;&#21270;&#36755;&#20837;&#24433;&#21709;&#26694;&#26550;&#65292;&#24182;&#21487;&#20197;&#35745;&#31639;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Because of the impact these decisions have on individuals, organizations, and population groups, there is a need to understand them: to know whether the decisions are abiding by the law, to help individuals improve their rankings, and to design better ranking procedures.  In this paper, we present ShaRP (Shapley for Rankings and Preferences), a framework that explains the contributions of features to different aspects of a ranked outcome, and is based on Shapley values. Using ShaRP, we show that even when the scoring function used by an algorithmic ranker is known and linear, the weight of each feature does not correspond to its Shapley value contribution. The contributions instead depend on the feature distributions, and on the subtle local interactions between the scoring features. ShaRP builds on the Quantitative Input Influence framework, and can compute the contri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.00132</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#20195;&#29702;&#19982;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#12289;&#34892;&#20026;&#25110;&#31574;&#30053;&#30340;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#25110;&#31454;&#20105;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35774;&#35745;&#33258;&#36866;&#24212;&#31574;&#30053;&#26102;&#65292;&#20195;&#29702;&#24314;&#27169;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#36825;&#26159;&#33258;&#25105;&#20195;&#29702;&#29702;&#35299;&#20854;&#20182;&#20195;&#29702;&#34892;&#20026;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#25110;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#31574;&#30053;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#20197;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#65288;&#24314;&#27169;&#20195;&#29702;&#65289;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#24182;&#25552;&#39640;&#20195;&#29702;&#24314;&#27169;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#65288;CLAM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#33258;&#25105;&#20195;&#29702;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems often require agents to collaborate with or compete against other agents with diverse goals, behaviors, or strategies. Agent modeling is essential when designing adaptive policies for intelligent machine agents in multiagent systems, as this is the means by which the ego agent understands other agents' behavior and extracts their meaningful policy representations. These representations can be used to enhance the ego agent's adaptive policy which is trained by reinforcement learning. However, existing agent modeling approaches typically assume the availability of local observations from other agents (modeled agents) during training or a long observation trajectory for policy adaption. To remove these constrictive assumptions and improve agent modeling performance, we devised a Contrastive Learning-based Agent Modeling (CLAM) method that relies only on the local observations from the ego agent during training and execution. With these observations, CLAM is capable of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.01536</link><description>&lt;p&gt;
&#19968;&#20010;&#20195;&#29702;&#22312;&#19990;&#30028;&#34920;&#31034;&#20013;&#34892;&#21160;&#30340;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Algebras of actions in an agent's representations of the world. (arXiv:2310.01536v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#19968;&#20010;&#20195;&#29702;&#30340;&#35270;&#35282;&#25552;&#21462;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#23545;&#31216;&#24615;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;(SBDRL)&#30340;&#35282;&#24230;&#22797;&#29616;&#20102;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#30340;&#24037;&#20316;[1]&#65292;&#21482;&#26377;&#24418;&#25104;&#32676;&#30340;&#19990;&#30028;&#36716;&#25442;&#20195;&#25968;&#25165;&#33021;&#29992;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#31616;&#21333;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#20855;&#26377;&#29305;&#24449;&#30340;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#24320;&#21457;&#30340;&#35745;&#31639;&#26041;&#27861;&#25552;&#21462;&#20102;&#36825;&#20123;&#19990;&#30028;&#36716;&#25442;&#30340;&#20195;&#25968;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#30340;&#23646;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;SBDRL&#30340;&#20004;&#20010;&#37325;&#35201;&#32467;&#26524; - &#31561;&#21464;&#26465;&#20214;&#21644;&#20998;&#31163;&#23450;&#20041; - &#20174;&#20165;&#36866;&#29992;&#20110;&#23545;&#31216;&#24615;&#22522;&#30784;&#34920;&#31034;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#25429;&#25417;&#19990;&#30028;&#36716;&#25442;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a framework to extract the algebra of the transformations of worlds from the perspective of an agent. As a starting point, we use our framework to reproduce the symmetry-based representations from the symmetry-based disentangled representation learning (SBDRL) formalism proposed by [1]; only the algebra of transformations of worlds that form groups can be described using symmetry-based representations. We then study the algebras of the transformations of worlds with features that occur in simple reinforcement learning scenarios. Using computational methods, that we developed, we extract the algebras of the transformations of these worlds and classify them according to their properties. Finally, we generalise two important results of SBDRL - the equivariance condition and the disentangling definition - from only working with symmetry-based representations to working with representations capturing the transformation properties of worlds with transformations for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#65292;&#19968;&#20010;&#35757;&#32451;-free&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#19982;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#25581;&#31034;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13697</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Analyzing Generative Data for Visual Recognition. (arXiv:2307.13697v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13697
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#22522;&#20934;&#65292;&#19968;&#20010;&#35757;&#32451;-free&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#19982;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#25581;&#31034;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#25193;&#22823;&#20102;&#23427;&#20204;&#20316;&#20026;&#26377;&#25928;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#24433;&#21709;&#65292;&#20027;&#35201;&#27604;&#36739;&#20102;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#65288;&#22914;&#29983;&#25104;&#25968;&#25454;&#12289;&#26816;&#32034;&#25968;&#25454;&#12289;&#21407;&#22987;&#25968;&#25454;&#65289;&#30340;&#33539;&#20363;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;1) GenBench&#26500;&#24314;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;GenBench&#65292;&#19968;&#20010;&#21253;&#21547;22&#20010;&#25968;&#25454;&#38598;&#21644;2548&#20010;&#31867;&#21035;&#30340;&#24191;&#27867;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;2) CLER&#20998;&#25968;&#65306;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;FID&#12289;CLIP&#20998;&#25968;&#65289;&#19982;&#19979;&#28216;&#35782;&#21035;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#36275;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLER&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#25351;&#31034;&#35782;&#21035;&#20219;&#21153;&#35757;&#32451;&#20043;&#21069;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;3) &#26032;&#30340;&#22522;&#20934;&#32447;&#65306;&#23558;&#29983;&#25104;&#25968;&#25454;&#19982;&#26469;&#33258;&#30456;&#21516;&#22806;&#37096;&#27744;&#30340;&#26816;&#32034;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#26377;&#21161;&#20110;&#38416;&#26126;&#29983;&#25104;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;4) &#22806;&#37096;&#30693;&#35782;&#27880;&#20837;&#65306;&#36890;&#36807;&#27880;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25552;&#39640;&#29983;&#25104;&#25968;&#25454;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition. This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\ie generative \vs retrieval \vs original).  Our key contributions are: \textbf{1) GenBench Construction:} We devise \textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks. \textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\eg, FID, CLIP score) with downstream recognition performance, we propose \textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training. \textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data. \textbf{4) External Knowledge Injection:} By 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00046</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32954;&#30284;&#35786;&#26029;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;
&lt;/p&gt;
&lt;p&gt;
An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules. (arXiv:2305.00046v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30103;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20351;&#29992;&#25913;&#36827;&#30340;3D Res-U-Net&#36827;&#34892;&#32954;&#20998;&#21106;&#12289;&#20351;&#29992;YOLO-v5&#36827;&#34892;&#32467;&#33410;&#26816;&#27979;&#12289;&#20351;&#29992;&#22522;&#20110;Vision Transformer&#30340;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#30340;&#25968;&#25454;&#38598;LUNA16&#19978;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#26159;&#20351;&#29992;&#21508;&#39046;&#22495;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#30340;&#12290;&#35813;&#26694;&#26550;&#22312;&#32954;&#37096;&#20998;&#21106;dice&#31995;&#25968;&#19978;&#36798;&#21040;&#20102;98.82&#65285;&#65292;&#21516;&#26102;&#26816;&#27979;&#32954;&#32467;&#33410;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.76 mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP
&lt;/p&gt;</description></item></channel></rss>