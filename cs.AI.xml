<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;p&gt;
ViGoR&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25913;&#36827;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06118
&lt;/p&gt;
&lt;p&gt;
ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24191;&#27867;&#30693;&#35782;&#19982;&#22270;&#20687;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#23545;&#25509;&#65292;&#23548;&#33268;&#38169;&#35823;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#23384;&#22312;&#22330;&#26223;&#20803;&#32032;&#12289;&#36951;&#28431;&#37325;&#35201;&#30340;&#22330;&#26223;&#37096;&#20998;&#65292;&#20197;&#21450;&#25512;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ViGoR&#65288;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#36827;&#34892;&#35270;&#35273;&#23545;&#25509;&#65289;&#65292;&#23427;&#21033;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#26469;&#26174;&#33879;&#25552;&#21319;&#22522;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;LVLMs&#30340;&#35270;&#35273;&#23545;&#25509;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36890;&#36807;&#20351;&#29992;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#20415;&#23452;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#20010;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
&lt;/p&gt;</description></item><item><title>CodeTF&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Transformer&#24211;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#26631;&#20934;&#21270;&#25509;&#21475;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.00029</link><description>&lt;p&gt;
CodeTF&#65306;&#19968;&#31449;&#24335;Transformer&#24211;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;LLM
&lt;/p&gt;
&lt;p&gt;
CodeTF: One-stop Transformer Library for State-of-the-art Code LLM. (arXiv:2306.00029v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00029
&lt;/p&gt;
&lt;p&gt;
CodeTF&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Transformer&#24211;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#26631;&#20934;&#21270;&#25509;&#21475;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#26234;&#33021;&#22312;&#36716;&#22411;&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#21644;&#32534;&#31243;&#35821;&#35328;&#29305;&#24449;&#30340;Transformer-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#36890;&#24120;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#36719;&#20214;&#24037;&#31243;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#24212;&#29992;&#24102;&#26469;&#20102;&#19968;&#23450;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CodeTF&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;Code LLM&#21644;&#20195;&#30721;&#26234;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#26694;&#26550;&#30340;&#21407;&#21017;&#65292;&#35774;&#35745;CodeTF&#24182;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#65292;&#20197;&#20415;&#24555;&#36895;&#35775;&#38382;&#21644;&#24320;&#21457;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24211;&#25903;&#25345;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26631;&#20934;&#21270;&#25509;&#21475;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#21644;&#26381;&#21153;&#20195;&#30721;LLMs&#65292;&#24182;&#25903;&#25345;&#21452;GPU&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#20351;&#29992;CodeTF&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#65292;&#20943;&#23569;&#35757;&#32451;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and d
&lt;/p&gt;</description></item></channel></rss>