<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01897</link><description>&lt;p&gt;
&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Spiking Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01897
&lt;/p&gt;
&lt;p&gt;
COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#22240;&#24341;&#20837;&#36830;&#32493;&#21160;&#21147;&#23398;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#33021;&#22815;&#25512;&#24191;&#29616;&#26377;&#30340;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#23427;&#20204;&#36890;&#24120;&#21463;&#25193;&#25955;&#31867;&#26041;&#27861;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;CGNNs&#30340;&#23454;&#29616;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37096;&#32626;&#22312;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#19978;&#12290;&#21463;&#26368;&#36817;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#21551;&#21457;&#65292;SNNs&#27169;&#25311;&#29983;&#29289;&#25512;&#29702;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#31181;&#33410;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23558;SNNs&#19982;CGNNs&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21629;&#21517;&#20026;&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;COS-GNN&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20351;&#29992;SNNs&#36827;&#34892;&#22270;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01897v1 Announce Type: cross  Abstract: Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.04963</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#38169;&#35823;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#28145;&#20837;&#35780;&#20272;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#31616;&#21270;&#26159;&#19968;&#31181;&#37325;&#20889;&#21477;&#23376;&#20197;&#20415;&#26356;&#26131;&#38405;&#35835;&#21644;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#24110;&#21161;&#26377;&#21508;&#31181;&#38405;&#35835;&#38590;&#39064;&#30340;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#21464;&#24471;&#36843;&#22312;&#30473;&#30571;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26469;&#35780;&#20272;LLMs&#30340;&#31616;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#23545;LLMs&#22312;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#22312;LLMs&#30340;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38519;&#20837;&#20004;&#20010;&#26497;&#31471;&#65306;&#35201;&#20040;&#36807;&#20110;&#32932;&#27973;&#65292;&#26080;&#27861;&#28165;&#26224;&#29702;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#35201;&#20040;&#36807;&#20110;&#35814;&#32454;&#65292;&#20351;&#27880;&#37322;&#36807;&#31243;&#22797;&#26434;&#19988;&#23481;&#26131;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04963v1 Announce Type: cross  Abstract: Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced large language models (LLMs), evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliabil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#21512;&#20316;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#30456;&#20851;&#12290;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.09750</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#21019;&#20316;&#20013;&#30340;&#28508;&#21147;&#65306;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#20013;&#30340;&#21019;&#24847;&#32534;&#31243;&#21644;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09750
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#21512;&#20316;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#30456;&#20851;&#12290;&#36890;&#36807;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#25506;&#32034;LLMs&#22312;&#33402;&#26415;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#21019;&#36896;&#24615;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#22312;&#36825;&#31181;&#21512;&#20316;&#36807;&#31243;&#20013;&#33402;&#26415;&#23478;&#30340;&#21453;&#24605;&#31867;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#21512;&#20316;&#26041;&#24335;&#65306;&#35843;&#29992;&#25972;&#20010;&#31243;&#24207;&#21644;&#22810;&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#33402;&#26415;&#23478;&#22312;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#20013;&#19981;&#21516;&#30340;&#21050;&#28608;&#24615;&#21453;&#24605;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#26174;&#31034;&#20102;&#21453;&#24605;&#31867;&#22411;&#19982;&#29992;&#25143;&#34920;&#29616;&#12289;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20027;&#35266;&#20307;&#39564;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#36827;&#34892;&#20004;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#23454;&#39564;&#25968;&#25454;&#21644;&#23450;&#24615;&#35775;&#35848;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;LLM&#22312;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#33402;&#26415;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20174;&#33402;&#26415;&#23478;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;&#65292;&#24182;&#38416;&#36848;&#20102;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09750v1 Announce Type: cross  Abstract: Recently, the potential of large language models (LLMs) has been widely used in assisting programming. However, current research does not explore the artist potential of LLMs in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists' different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of LLM in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists' perspective and expound design sugg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.14890</link><description>&lt;p&gt;
Boosting&#29992;&#20110;&#30028;&#23450;&#26368;&#24046;&#20998;&#31867;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#20934;&#35823;&#24046;&#29575;&#30340;&#24179;&#22343;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#19977;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#21508;&#31867;&#21035;&#30340;&#35823;&#24046;&#29575;&#20998;&#21035;&#20026;10&#65285;&#65292;10&#65285;&#21644;40&#65285;&#65292;&#20854;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#20026;40&#65285;&#65292;&#32780;&#22312;&#31867;&#21035;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#24179;&#22343;&#35823;&#24046;&#29575;&#20026;20&#65285;&#12290;&#26368;&#24046;&#31867;&#21035;&#38169;&#35823;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#24694;&#24615;&#32959;&#30244;&#31867;&#21035;&#20855;&#26377;40&#65285;&#30340;&#38169;&#35823;&#29575;&#32780;&#33391;&#24615;&#21644;&#20581;&#24247;&#31867;&#21035;&#20855;&#26377;10&#65285;&#30340;&#38169;&#35823;&#29575;&#26159;&#19981;&#33021;&#34987;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#19978;&#30028;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#27867;&#21270;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#35757;&#32451;&#38598;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\%, 10\%, and 40\% has a worst-class error rate of 40\%, whereas the average is 20\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\% error rate, while the benign and healthy classes have 10\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item></channel></rss>