<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17064</link><description>&lt;p&gt;
&#22312;T2I&#27169;&#22411;&#20013;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;&#26041;&#21521;&#23454;&#29616;&#36830;&#32493;&#12289;&#20027;&#39064;&#29305;&#23450;&#30340;&#23646;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#38480;&#21046;&#65288;&#20363;&#22914;&#8220;&#20154;&#8221;&#21644;&#8220;&#32769;&#24180;&#20154;&#8221;&#20043;&#38388;&#19981;&#23384;&#22312;&#36830;&#32493;&#30340;&#20013;&#38388;&#25551;&#36848;&#30340;&#38598;&#21512;&#65289;&#65292;&#23454;&#29616;&#23545;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#25110;&#29983;&#25104;&#36807;&#31243;&#20197;&#23454;&#29616;&#36825;&#31181;&#25511;&#21046;&#65292;&#20294;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21551;&#29992;&#20840;&#23616;&#32454;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#31895;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#20860;&#39038;&#20004;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#30340;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#23384;&#22312;&#21487;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#30340;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
&lt;/p&gt;</description></item><item><title>&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08215</link><description>&lt;p&gt;
LIX&#65306;&#23558;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08215
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#34701;&#21512;&#32593;&#32476;&#22312;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#32570;&#20047;&#31354;&#38388;&#20960;&#20309;&#25968;&#25454;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#21464;&#24471;&#26080;&#25928;&#12290;&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#19981;&#22826;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Learning to Infuse "X" (LIX) &#26694;&#26550;&#65292;&#22312;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#38754;&#36827;&#34892;&#20102;&#26032;&#39062;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35777;&#26126;&#65292;&#24378;&#35843;&#22312;&#35299;&#32806;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#21333;&#19968;&#22266;&#23450;&#26435;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;logit&#26234;&#33021;&#21160;&#24577;&#26435;&#37325;&#25511;&#21046;&#22120;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#26032;&#26657;&#20934;&#30340;&#29305;&#24449;&#33976;&#39311;&#31639;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.12988</link><description>&lt;p&gt;
&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#19982;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#21644;&#21307;&#30103;&#30693;&#35782;&#27880;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#25361;&#25112;&#65292;&#23454;&#29616;&#24930;&#24615;&#30149;&#31649;&#29702;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#24930;&#24615;&#30149;&#31649;&#29702;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#26469;&#26816;&#27979;&#21508;&#31181;&#31934;&#31070;&#38556;&#30861;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#20840;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#27880;&#37322;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#27599;&#31181;&#30142;&#30149;&#30340;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#36807;&#31243;&#65292;&#20197;&#21450;&#38656;&#35201;&#20026;&#27599;&#20010;&#38382;&#39064;&#35774;&#35745;&#19987;&#38376;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;Prompt&#24037;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25968;&#25454;&#39537;&#21160;&#24930;&#24615;&#30149;&#31649;&#29702;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65306;&#65288;1&#65289;&#24320;&#21457;&#20010;&#24615;&#21270;&#30340;&#25552;&#31034;&#26469;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;&#30340;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;&#21307;&#30103;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#20026;&#24930;&#24615;&#30149;&#26816;&#27979;&#25552;&#20379;&#19978;&#19979;&#25991;&#65292;&#25351;&#23548;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#23454;&#29616;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#31181;&#31934;&#31070;&#38556;&#30861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study harnesses state-of-the-art AI technology for chronic disease management, specifically in detecting various mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each disease and the need to design specialized deep learning architectures for each problem. To address such challenges, we propose a novel framework that leverages advanced AI techniques, including large language models and multi-prompt engineering. Specifically, we address two key technical challenges in data-driven chronic disease management: (1) developing personalized prompts to represent each user's uniqueness and (2) incorporating medical knowledge into prompts to provide context for chronic disease detection, instruct learning objectives, and operationalize prediction goals. We evaluate our method using four mental disorders, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07263</link><description>&lt;p&gt;
CoPAL:&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CoPAL: Corrective Planning of Robot Actions with Large Language Models. (arXiv:2310.07263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#25509;&#31649;&#20154;&#31867;&#20256;&#32479;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#20026;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#65292;&#21327;&#35843;&#22810;&#20010;&#35748;&#30693;&#23618;&#27425;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#21160;&#20316;&#29983;&#25104;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#22788;&#29702;&#29983;&#25104;&#30340;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#26032;&#22411;&#20877;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#20004;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#65288;&#26041;&#22359;&#19990;&#30028;&#12289;&#37202;&#21543;&#21644;&#27604;&#33832;&#21046;&#20316;&#65289;&#20013;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21453;&#39304;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#21487;&#25191;&#34892;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07462</link><description>&lt;p&gt;
&#29609;&#24324;&#25991;&#23383;&#65306;&#27604;&#36739;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#21644;ChatGPT&#31561;&#24037;&#20855;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#21487;&#20197;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#36825;&#23545;&#35835;&#32773;&#30340;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22521;&#35757;&#26159;&#21542;&#20250;&#20135;&#29983;&#24433;&#21709;&#20855;&#26377;&#35768;&#22810;&#21547;&#20041;&#65311;&#23427;&#26159;&#21542;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#28436;&#21464;&#65311;&#25105;&#20204;&#20851;&#27880;&#35821;&#35328;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65306;&#35789;&#35821;&#65307;&#22312;&#32534;&#20889;&#32473;&#23450;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#22686;&#21152;&#25110;&#20943;&#23569;&#20351;&#29992;&#30340;&#35789;&#27719;&#37327;&#25110;&#35789;&#27719;&#20016;&#23500;&#24230;&#65288;&#29702;&#35299;&#20026;&#20070;&#38754;&#25110;&#21475;&#22836;&#34920;&#36798;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#35789;&#27719;&#25968;&#37327;&#65289;&#65311;&#36825;&#23545;&#35789;&#35821;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#26410;&#21253;&#21547;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#30340;&#35789;&#35821;&#24448;&#24448;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21463;&#27426;&#36814;&#65292;&#24182;&#26368;&#32456;&#21487;&#33021;&#28040;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20197;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.07124</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diverse Projection Ensembles for Distributional Reinforcement Learning. (arXiv:2306.07124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20197;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#19981;&#21516;&#65292;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#23398;&#20064;&#22238;&#25253;&#30340;&#20998;&#24067;&#32780;&#19981;&#26159;&#20854;&#26399;&#26395;&#20540;&#12290;&#30001;&#20110;&#22238;&#25253;&#20998;&#24067;&#30340;&#24615;&#36136;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#36807;&#20110;&#22797;&#26434;&#65292;&#22240;&#27492;&#36890;&#24120;&#37319;&#29992;&#23558;&#26410;&#32422;&#26463;&#30340;&#20998;&#24067;&#25237;&#24433;&#21040;&#21487;&#34920;&#31034;&#30340;&#21442;&#25968;&#20998;&#24067;&#38598;&#21512;&#20013;&#30340;&#26041;&#27861;&#36827;&#34892;&#36924;&#36817;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#23558;&#36825;&#31181;&#25237;&#24433;&#27493;&#39588;&#19982;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#19979;&#38477;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#25237;&#24433;&#27493;&#39588;&#20250;&#20135;&#29983;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20174;&#32780;&#28145;&#21051;&#24433;&#21709;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#20026;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#38598;&#21512;&#20013;&#22810;&#20010;&#19981;&#21516;&#30340;&#25237;&#24433;&#21644;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#31181;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to classical reinforcement learning, distributional reinforcement learning algorithms aim to learn the distribution of returns rather than their expected value. Since the nature of the return distribution is generally unknown a priori or arbitrarily complex, a common approach finds approximations within a set of representable, parametric distributions. Typically, this involves a projection of the unconstrained distribution onto the set of simplified distributions. We argue that this projection step entails a strong inductive bias when coupled with neural networks and gradient descent, thereby profoundly impacting the generalization behavior of learned models. In order to facilitate reliable uncertainty estimation through diversity, this work studies the combination of several different projections and representations in a distributional ensemble. We establish theoretical properties of such projection ensembles and derive an algorithm that uses ensemble disagreement, measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07499</link><description>&lt;p&gt;
&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#35774;&#22791;&#40065;&#26834;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Device-Robust Acoustic Scene Classification via Impulse Response Augmentation. (arXiv:2305.07499v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#32780;&#35328;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#24405;&#38899;&#35774;&#22791;&#26159;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#40614;&#20811;&#39118;&#29305;&#24615;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#39057;&#29575;&#21709;&#24212;&#65292;&#20250;&#24341;&#20837;&#25968;&#23383;&#21270;&#38899;&#39057;&#20449;&#21495;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#22914;&#26524;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#32771;&#34385;&#27492;&#39046;&#22495;&#20559;&#31227;&#65292;&#37027;&#20040;&#24403;&#23427;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#35774;&#22791;&#35760;&#24405;&#38899;&#39057;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#23569;&#25968;&#19981;&#21516;&#40614;&#20811;&#39118;&#19978;&#24405;&#21046;&#38899;&#39057;&#20449;&#21495;&#30340;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#20250;&#20351;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#30340;&#35774;&#22791;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#24405;&#21046;&#35774;&#22791;&#33033;&#20914;&#21709;&#24212;(DIR)&#21367;&#31215;&#35757;&#32451;&#38598;&#20013;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#20174;&#32780;&#20154;&#24037;&#22686;&#21152;&#24405;&#38899;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;CNN&#21644;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#36827;&#34892;Acoustic Scene Classification&#20219;&#21153;&#30340;DIR&#22686;&#24378;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;DIR&#22686;&#24378;&#23601;&#33021;&#25552;&#21319;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.02731</link><description>&lt;p&gt;
&#23548;&#33322;&#30340;&#20013;&#23618;&#34920;&#31034;&#8212;&#8212;&#34394;&#25311;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#22320;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#25351;&#24341;&#32473;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#23548;&#33322;&#20449;&#24687;&#26159;&#22810;&#27169;&#24577;&#30340;&#26102;&#20505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#20197;&#35270;&#35273;&#26041;&#24335;&#21576;&#29616;&#38750;&#35270;&#35273;&#25351;&#20196;&#20449;&#21495;&#12290;&#36825;&#20123;&#35270;&#35273;&#25351;&#24341;&#20197;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#21472;&#21152;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#65292;&#20316;&#20026;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#34394;&#25311;&#23548;&#33322;&#22312;&#22810;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#34394;&#25311;&#23548;&#33322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23558;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#25351;&#20196;&#36716;&#25442;&#20026;&#29992;&#20110;&#30495;&#23454;&#29615;&#22659;&#23454;&#39564;&#30340;&#30452;&#35266;&#35270;&#35273;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#34394;&#25311;&#23548;&#33322;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.09806</link><description>&lt;p&gt;
Logit-Q&#21160;&#21147;&#23398;&#23545;&#20110;&#38543;&#26426;&#22242;&#38431;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Logit-Q Dynamics for Efficient Learning in Stochastic Teams. (arXiv:2302.09806v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#27604;&#21644;&#37327;&#21270;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#21487;&#20197;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;Logit-Q&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#23558;&#32463;&#20856;&#21644;&#29420;&#31435;&#30340;&#23545;&#25968;&#32447;&#24615;&#23398;&#20064;&#26356;&#26032;&#19982;&#19968;&#20010;&#22312;&#25919;&#31574;&#19978;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;Logit-Q&#21160;&#21147;&#23398;&#22312;&#38543;&#26426;&#22242;&#38431;&#20013;&#36798;&#21040;&#65288;&#25509;&#36817;&#65289;&#39640;&#25928;&#22343;&#34913;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Logit-Q&#21160;&#21147;&#23398;&#23545;&#32431;&#23450;&#24577;&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21160;&#21147;&#23398;&#22312;&#22870;&#21169;&#20989;&#25968;&#23548;&#33268;&#28508;&#22312;&#21338;&#24328;&#30340;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#28982;&#32780;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#29366;&#24577;&#36716;&#25442;&#36229;&#20986;&#38543;&#26426;&#22242;&#38431;&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#21160;&#21147;&#23398;&#19982;&#19968;&#20010;&#34394;&#26500;&#30340;&#22330;&#26223;&#36817;&#20284;&#65292;&#20854;&#20013;Q&#20989;&#25968;&#20272;&#35745;&#20165;&#22312;&#26377;&#38480;&#38271;&#24230;&#30340;&#32426;&#20803;&#20013;&#26159;&#23450;&#24577;&#30340;&#65292;&#20165;&#29992;&#20110;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#35201;&#22330;&#26223;&#21644;&#34394;&#26500;&#22330;&#26223;&#20013;&#30340;&#21160;&#21147;&#23398;&#32806;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#36825;&#20004;&#20010;&#22330;&#26223;&#30001;&#20110;&#36880;&#27493;&#20943;&#23567;&#30340;&#27493;&#38271;&#32780;&#36234;&#26469;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two logit-Q learning dynamics combining the classical and independent log-linear learning updates with an on-policy value iteration update for efficient learning in stochastic games. We show that the logit-Q dynamics presented reach (near) efficient equilibrium in stochastic teams. We quantify a bound on the approximation error. We also show the rationality of the logit-Q dynamics against agents following pure stationary strategies and the convergence of the dynamics in stochastic games where the reward functions induce potential games, yet only a single agent controls the state transitions beyond stochastic teams. The key idea is to approximate the dynamics with a fictional scenario where the Q-function estimates are stationary over finite-length epochs only for analysis. We then couple the dynamics in the main and fictional scenarios to show that these two scenarios become more and more similar across epochs due to the vanishing step size.
&lt;/p&gt;</description></item></channel></rss>