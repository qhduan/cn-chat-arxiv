<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16101</link><description>&lt;p&gt;
&#36328;&#22269;&#30028;&#35780;&#20272;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65306;&#26469;&#33258;&#20154;&#31867;&#24863;&#30693;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fairness Metrics Across Borders from Human Perceptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#36866;&#29992;&#20110;&#24744;&#30340;&#22330;&#26223;&#65311;&#21363;&#20351;&#32467;&#26524;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#20063;&#21487;&#33021;&#23384;&#22312;&#20851;&#20110;&#20844;&#24179;&#24863;&#30693;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#12290;&#24050;&#36827;&#34892;&#20102;&#22810;&#39033;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#19982;&#20154;&#20204;&#23545;&#20844;&#24179;&#30340;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#26597;&#33539;&#22260;&#26377;&#38480;&#65292;&#20165;&#21253;&#25324;&#21333;&#20010;&#22269;&#23478;&#20013;&#25968;&#30334;&#21517;&#21442;&#19982;&#32773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22269;&#38469;&#35843;&#26597;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#22312;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20998;&#21035;&#20174;&#20013;&#22269;&#12289;&#27861;&#22269;&#12289;&#26085;&#26412;&#21644;&#32654;&#22269;&#30340;&#27599;&#20010;&#22269;&#23478;&#25910;&#38598;&#20102;1,000&#21517;&#21442;&#19982;&#32773;&#30340;&#22238;&#24212;&#65292;&#24635;&#35745;&#24471;&#21040;&#20102;4,000&#20010;&#22238;&#24212;&#65292;&#20197;&#20998;&#26512;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#65292;&#37197;&#22791;&#20102;&#22235;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#36873;&#25321;&#20854;&#21916;&#22909;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16101v1 Announce Type: new  Abstract: Which fairness metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics. Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness. However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics. Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case. This investigation explores the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;</title><link>https://arxiv.org/abs/2403.01695</link><description>&lt;p&gt;
DyCE&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#21644;&#25193;&#23637;&#30340;&#21160;&#24577;&#21487;&#37197;&#32622;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01695
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#26377;&#25928;&#37096;&#32626;&#26102;&#65292;&#20351;&#29992;&#32553;&#25918;&#21644;&#21387;&#32553;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21160;&#24577;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#25552;&#21069;&#36864;&#20986;&#65289;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#26679;&#26412;&#30340;&#22256;&#38590;&#31243;&#24230;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#35745;&#31639;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#21160;&#24577;&#26041;&#27861;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#19982;&#38745;&#24577;&#26041;&#27861;&#20849;&#23384;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#29616;&#19978;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21160;&#24577;&#37096;&#20998;&#30340;&#20219;&#20309;&#21464;&#21270;&#37117;&#20250;&#24433;&#21709;&#21518;&#32493;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21160;&#24577;&#21387;&#32553;&#35774;&#35745;&#37117;&#26159;&#21333;&#29255;&#30340;&#65292;&#19982;&#22522;&#30784;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#65292;&#20174;&#32780;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#26032;&#39062;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#31181;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#35774;&#35745;&#32771;&#34385;&#30456;&#20114;&#35299;&#32806;&#20197;&#21450;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.04522</link><description>&lt;p&gt;
&#36830;&#25509;NTK&#21644;NNGP&#65306;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#22312;&#26680;&#21306;&#22495;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#32570;&#20047;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#23545;&#20110;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#25551;&#36848;&#32593;&#32476;&#30340;&#36755;&#20986;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26694;&#26550;&#65292;&#20551;&#35774;&#20102;&#32447;&#24615;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#65307;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21644;&#23398;&#20064;&#21518;&#30340;&#32593;&#32476;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#30340;&#31934;&#30830;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#31070;&#32463;&#21160;&#24577;&#26680;&#65288;NDK&#65289;&#65292;&#36825;&#20010;&#26680;&#21487;&#20197;&#21516;&#26102;&#20135;&#29983;NTK&#21644;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00046</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32954;&#30284;&#35786;&#26029;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;
&lt;/p&gt;
&lt;p&gt;
An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules. (arXiv:2305.00046v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30103;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20351;&#29992;&#25913;&#36827;&#30340;3D Res-U-Net&#36827;&#34892;&#32954;&#20998;&#21106;&#12289;&#20351;&#29992;YOLO-v5&#36827;&#34892;&#32467;&#33410;&#26816;&#27979;&#12289;&#20351;&#29992;&#22522;&#20110;Vision Transformer&#30340;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#30340;&#25968;&#25454;&#38598;LUNA16&#19978;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#26159;&#20351;&#29992;&#21508;&#39046;&#22495;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#30340;&#12290;&#35813;&#26694;&#26550;&#22312;&#32954;&#37096;&#20998;&#21106;dice&#31995;&#25968;&#19978;&#36798;&#21040;&#20102;98.82&#65285;&#65292;&#21516;&#26102;&#26816;&#27979;&#32954;&#32467;&#33410;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.76 mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.12484</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#30340;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12484
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#32791;&#26102;&#12290;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MIA&#65289;&#39046;&#22495;&#65292;&#25968;&#25454;&#26377;&#38480;&#65292;&#26631;&#31614;&#24456;&#38590;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#38750;&#26631;&#35760;&#21644;&#24369;&#26631;&#35760;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#36817;300&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#20197;&#20840;&#38754;&#27010;&#36848;&#26368;&#26032;&#36827;&#23637;&#30340;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#31574;&#30053;&#22312;MIA&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#24182;&#23558;&#19981;&#21516;&#26041;&#26696;&#30340;&#26041;&#27861;&#24402;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#27599;&#31181;&#26041;&#26696;&#35814;&#32454;&#30740;&#31350;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#35206;&#30422;&#20102;&#19981;&#20165;&#26159;&#26631;&#20934;&#31574;&#30053;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#21518;&#22788;&#29702;&#21644;&#38598;&#21512;&#26041;&#27861;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to be acquired. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonic
&lt;/p&gt;</description></item></channel></rss>