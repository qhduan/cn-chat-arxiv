<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.15049</link><description>&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#23548;&#33322;&#21040;&#30446;&#30340;&#22320;&#12290;&#29616;&#26377;&#30340;VLN&#20195;&#29702;&#35757;&#32451;&#26041;&#27861;&#39044;&#35774;&#22266;&#23450;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65306;&#24341;&#20837;&#26032;&#29615;&#22659;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#20445;&#30041;&#24050;&#32463;&#36935;&#21040;&#30340;&#29615;&#22659;&#30340;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;VLN&#20195;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#35780;&#20272;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15049v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based meth
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;</title><link>https://arxiv.org/abs/2403.02901</link><description>&lt;p&gt;
&#20851;&#20110;&#36807;&#31243;&#23548;&#21521;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25506;&#35752;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#21512;&#35843;&#26597;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8220;&#35270;&#35282;&#25552;&#20379;&#20102;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#20840;&#38754;&#23457;&#35270;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;ATS&#26368;&#26032;&#30340;&#35843;&#26597;&#65292;&#24357;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#20004;&#24180;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#65288;ATS&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#65292;&#26088;&#22312;&#21019;&#24314;&#31616;&#27905;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22788;&#29702;&#22823;&#37327;&#25991;&#26412;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;ATS&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#36807;&#21435;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#26469;&#35843;&#26597;ATS&#30340;&#26041;&#27861;; &#20294;&#26159;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#23454;&#38469;&#23454;&#26045;&#30340;&#23454;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#20197;&#24448;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;ATS&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312; 1&#65289;&#20174;&#8220;&#36807;&#31243;&#23548;&#21521;&#27169;&#24335;&#8221;&#35270;&#35282;&#25552;&#20379;ATS&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#26368;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;; 2) &#20840;&#38754;&#23457;&#35270;&#26368;&#26032;&#30340;&#22522;&#20110;LLM&#30340;ATS&#24037;&#20316;; &#20197;&#21450; 3&#65289;&#25552;&#20379;&#20851;&#20110;ATS&#30340;&#26368;&#26032;&#35843;&#26597;&#65292;&#24357;&#34917;&#25991;&#29486;&#20013;&#20004;&#24180;&#38388;&#38548;&#20043;&#22788;&#12290;&#20196;&#20154;&#24863;&#21040;&#28385;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02901v1 Announce Type: new  Abstract: Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#25928;&#29992;&#29702;&#35770;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#31574;&#30053;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#26368;&#20248;&#25928;&#29992;&#21644;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.09445</link><description>&lt;p&gt;
&#20102;&#35299;&#25928;&#29992;&#29702;&#35770;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey. (arXiv:2306.09445v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#25928;&#29992;&#29702;&#35770;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#31574;&#30053;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#26368;&#20248;&#25928;&#29992;&#21644;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#32463;&#27982;&#23398;&#12289;&#21338;&#24328;&#35770;&#21644;&#36816;&#31609;&#23398;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#27010;&#24565;&#65292;&#25928;&#29992;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#34987;&#29992;&#26469;&#35780;&#20272;&#20010;&#20307;&#38656;&#27714;&#12289;&#20559;&#22909;&#21644;&#21033;&#30410;&#27700;&#24179;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;/&#26426;&#22120;&#20154;&#31995;&#32479;&#65288;MAS/MRS&#65289;&#30340;&#20915;&#31574;&#21644;&#23398;&#20064;&#20013;&#65292;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#21487;&#20197;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#20854;&#24403;&#21069;&#38656;&#27714;&#24182;&#23398;&#20250;&#21512;&#20316;&#21644;&#32452;&#32455;&#20854;&#34892;&#20026;&#65292;&#20248;&#21270;&#31995;&#32479;&#30340;&#25928;&#29992;&#65292;&#24314;&#31435;&#31283;&#23450;&#21487;&#38752;&#30340;&#20851;&#31995;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#31038;&#20250;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#30340;&#22797;&#26434;&#12289;&#22823;&#35268;&#27169;&#21644;&#38271;&#26399;&#30340;&#34892;&#20026;&#24456;&#22823;&#31243;&#24230;&#19978;&#30001;&#24213;&#23618;&#20851;&#31995;&#30340;&#22522;&#26412;&#29305;&#24615;&#20915;&#23450;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#23545;&#26426;&#21046;&#30340;&#29702;&#35770;&#26041;&#38754;&#21644;&#24212;&#29992;&#39046;&#22495;&#30340;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#25928;&#29992;&#20026;&#23548;&#21521;&#30340;&#38656;&#27714;&#33539;&#24335;&#65292;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer rela
&lt;/p&gt;</description></item></channel></rss>