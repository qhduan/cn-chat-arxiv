<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2202.03482</link><description>&lt;p&gt;
&#39046;&#33322;&#31070;&#32463;&#31354;&#38388;&#65306;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#20197;&#20811;&#26381;&#26041;&#21521;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31574;&#30053;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#36890;&#24120;&#65292;CAVs&#26159;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#35745;&#31639;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#20248;&#21270;&#20855;&#26377;&#32473;&#23450;&#27010;&#24565;&#21644;&#26080;&#32473;&#23450;&#27010;&#24565;&#30340;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20197;&#21487;&#20998;&#31163;&#24615;&#20026;&#23548;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20250;&#23548;&#33268;&#19982;&#31934;&#30830;&#24314;&#27169;&#27010;&#24565;&#26041;&#21521;&#30340;&#23454;&#38469;&#30446;&#26631;&#21457;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20998;&#25955;&#26041;&#21521;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21363;&#19982;&#27010;&#24565;&#26080;&#20851;&#30340;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#32447;&#24615;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#65288;&#21363;&#26435;&#37325;&#65289;&#25429;&#33719;&#20197;&#20248;&#21270;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#65292;&#20165;&#20851;&#27880;&#27010;&#24565;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;CAV&#26041;&#27861;&#19982;&#30495;&#23454;&#27010;&#24565;&#26041;&#21521;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08751</link><description>&lt;p&gt;
&#22810;&#26679;&#30340;&#31070;&#32463;&#38899;&#39057;&#23884;&#20837; - &#24674;&#22797;&#29305;&#24449;&#65281;
&lt;/p&gt;
&lt;p&gt;
Diverse Neural Audio Embeddings -- Bringing Features back !. (arXiv:2309.08751v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#24320;&#22987;&#27969;&#34892;&#12290;&#36825;&#31181;&#36716;&#21464;&#23548;&#33268;&#20102;&#31070;&#32463;&#26550;&#26500;&#22312;&#27809;&#26377;&#39046;&#22495;&#29305;&#23450;&#20559;&#35265;/&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#26681;&#25454;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65288;&#22312;&#26412;&#20363;&#20013;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#65289;&#23398;&#20064;&#38899;&#39057;&#23884;&#20837;&#12290;&#23545;&#20110;&#28041;&#21450;&#25968;&#30334;&#31181;&#22768;&#38899;&#20998;&#31867;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23398;&#20064;&#20998;&#21035;&#38024;&#23545;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#31561;&#22810;&#26679;&#30340;&#38899;&#39057;&#23646;&#24615;&#24314;&#31435;&#31283;&#20581;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#36890;&#36807;&#31471;&#21040;&#31471;&#26550;&#26500;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25163;&#24037;&#21046;&#20316;&#30340;&#23884;&#20837;&#65292;&#20363;&#22914;&#22522;&#20110;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#23884;&#20837;&#65292;&#34429;&#28982;&#21333;&#29420;&#20351;&#29992;&#26102;&#26080;&#27861;&#20987;&#36133;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#34920;&#31034;&#65292;&#20294;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#31471;&#21040;&#31471;&#23884;&#20837;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#22312;&#31471;&#21040;&#31471;&#27169;&#22411;&#20013;&#24341;&#20837;&#19968;&#20123;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#36947;&#36335;&#65292;&#24182;&#36229;&#36234;&#20165;&#35757;&#32451;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training 
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15022</link><description>&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15022
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#23481;&#26131;&#36951;&#24536;&#37325;&#35201;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#25110;&#24635;&#32467;&#22120;&#20174;&#36807;&#21435;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#39640;&#24230;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36882;&#24402;&#29983;&#25104;&#24635;&#32467;/&#35760;&#24518;&#65292;&#20197;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21050;&#28608;LLMs&#35760;&#20303;&#23567;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#20351;&#29992;&#20043;&#21069;&#30340;&#35760;&#24518;&#21644;&#38543;&#21518;&#30340;&#23545;&#35805;&#20869;&#23481;&#20135;&#29983;&#26032;&#30340;&#35760;&#24518;&#12290;&#26368;&#21518;&#65292;LLM&#21487;&#20197;&#22312;&#26368;&#26032;&#35760;&#24518;&#30340;&#24110;&#21161;&#19979;&#36731;&#26494;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;text-davinci-003&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23454;&#29616;LLM&#24314;&#27169;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
&lt;/p&gt;</description></item><item><title>JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04729</link><description>&lt;p&gt;
JEN-1&#65306;&#20855;&#26377;&#20840;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models. (arXiv:2308.04729v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04729
&lt;/p&gt;
&lt;p&gt;
JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#38899;&#20048;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#20048;&#65288;&#21363;&#25991;&#26412;&#21040;&#38899;&#20048;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#38899;&#20048;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#21644;&#39640;&#37319;&#26679;&#29575;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#38899;&#20048;&#36136;&#37327;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JEN-1&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#36890;&#29992;&#39640;&#20445;&#30495;&#27169;&#22411;&#12290;JEN-1&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;JEN-1&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#20197;&#21450;&#24310;&#32493;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JEN-1&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#22312;&#27492;&#32593;&#22336;&#33719;&#21462;&#65306;http://URL
&lt;/p&gt;
&lt;p&gt;
Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at this http URL
&lt;/p&gt;</description></item></channel></rss>