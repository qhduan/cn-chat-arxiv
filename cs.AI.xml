<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20197;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#65292;&#20174;&#32780;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04321</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#20041;&#36890;&#35759;&#65306;&#36229;&#36234;&#27604;&#29305;&#24674;&#22797;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Semantic Communication: Diffusion Models Beyond Bit Recovery. (arXiv:2306.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20197;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#65292;&#20174;&#32780;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#35759;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36890;&#35759;&#20013;&#30340;&#26680;&#24515;&#20043;&#19968;&#12290;&#35821;&#20041;&#36890;&#35759;&#30340;&#19968;&#20010;&#21487;&#33021;&#24615;&#26159;&#65292;&#22312;&#19981;&#24517;&#24674;&#22797;&#20256;&#36755;&#27604;&#29305;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#30446;&#26631;&#31471;&#37325;&#24314;&#19982;&#20256;&#36755;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#35821;&#20041;&#31561;&#25928;&#30340;&#21103;&#26412;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#20174;&#25509;&#25910;&#21040;&#30340;&#37096;&#20998;&#20449;&#24687;&#20013;&#26500;&#24314;&#22797;&#26434;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#25193;&#25955;&#24341;&#23548;&#26694;&#26550;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#22810;&#23186;&#20307;&#20869;&#23481;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#20165;&#21457;&#36865;&#39640;&#24230;&#21387;&#32553;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#38477;&#20302;&#24102;&#23485;&#20351;&#29992;&#12290;&#28982;&#21518;&#25509;&#25910;&#31471;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35821;&#20041;&#20445;&#30041;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication is expected to be one of the cores of next-generation AI-based communications. One of the possibilities offered by semantic communication is the capability to regenerate, at the destination side, images or videos semantically equivalent to the transmitted ones, without necessarily recovering the transmitted sequence of bits. The current solutions still lack the ability to build complex scenes from the received partial information. Clearly, there is an unmet need to balance the effectiveness of generation methods and the complexity of the transmitted information, possibly taking into account the goal of communication. In this paper, we aim to bridge this gap by proposing a novel generative diffusion-guided framework for semantic communication that leverages the strong abilities of diffusion models in synthesizing multimedia content while preserving semantic features. We reduce bandwidth usage by sending highly-compressed semantic information only. Then, the diffus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.13372</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Optimization With Delayed Feedback for Neural Networks. (arXiv:2304.13372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;F^3&#65292;&#20351;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#26469;&#32553;&#25918;&#26799;&#24230;&#20174;&#32780;&#25552;&#39640;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#29983;&#29289;&#23398;&#19978;&#30340;&#25209;&#35780;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#33258;&#28982;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21487;&#34892;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;&#26435;&#37325;&#20256;&#36755;&#21644;&#26356;&#26032;&#38145;&#23450;&#65292;&#20197;&#23454;&#29616;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24310;&#36831;&#21453;&#39304;&#30340;&#21069;&#39304;&#65288;F^3&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#24310;&#36831;&#30340;&#35823;&#24046;&#20449;&#24687;&#20316;&#20026;&#26679;&#26412;&#32423;&#32553;&#25918;&#22240;&#23376;&#26469;&#26356;&#20934;&#30830;&#22320;&#36817;&#20284;&#26799;&#24230;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;F^3&#23558;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#31639;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#39044;&#27979;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;96&#65285;&#12290;&#36825;&#35777;&#26126;&#20102;&#29983;&#29289;&#21487;&#34892;&#24615;&#35757;&#32451;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20026;&#20302;&#33021;&#37327;&#35757;&#32451;&#21644;&#24182;&#34892;&#21270;&#24320;&#36767;&#20102;&#26377; promising &#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation has long been criticized for being biologically implausible, relying on concepts that are not viable in natural learning processes. This paper proposes an alternative approach to solve two core issues, i.e., weight transport and update locking, for biological plausibility and computational efficiency. We introduce Feed-Forward with delayed Feedback (F$^3$), which improves upon prior work by utilizing delayed error information as a sample-wise scaling factor to approximate gradients more accurately. We find that F$^3$ reduces the gap in predictive performance between biologically plausible training algorithms and backpropagation by up to 96%. This demonstrates the applicability of biologically plausible training and opens up promising new avenues for low-energy training and parallelization.
&lt;/p&gt;</description></item></channel></rss>