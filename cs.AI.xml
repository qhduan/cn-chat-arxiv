<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#23039;&#21183;&#20808;&#39564;&#65292;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#23558;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08930</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36827;&#34892;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
3D Human Pose Analysis via Diffusion Synthesis. (arXiv:2401.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#23039;&#21183;&#20808;&#39564;&#65292;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#23558;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PADS&#65288;&#36890;&#36807;&#25193;&#25955;&#21512;&#25104;&#36827;&#34892;&#23039;&#21183;&#20998;&#26512;&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#31243;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;PADS&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#29420;&#29305;&#30340;&#31574;&#30053;&#65306;i&#65289;&#20351;&#29992;&#25193;&#25955;&#21512;&#25104;&#36807;&#31243;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#30340;&#23039;&#21183;&#20808;&#39564;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25429;&#25417;&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#20013;&#30340;&#36816;&#21160;&#32422;&#26463;&#65307;ii&#65289;&#23558;&#20272;&#35745;&#12289;&#34917;&#20840;&#12289;&#21435;&#22122;&#31561;&#22810;&#20010;&#23039;&#21183;&#20998;&#26512;&#20219;&#21153;&#32479;&#19968;&#20026;&#36870;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#23398;&#20064;&#21040;&#30340;&#23039;&#21183;&#20808;&#39564;&#23558;&#34987;&#35270;&#20026;&#23545;&#20219;&#21153;&#29305;&#23450;&#32422;&#26463;&#30340;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#26465;&#20214;&#21435;&#22122;&#27493;&#39588;&#24341;&#23548;&#20248;&#21270;&#36807;&#31243;&#12290;PADS&#20195;&#34920;&#20102;&#39318;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#26694;&#26550;&#20869;&#30340;&#36890;&#29992;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#12290;&#20854;&#24615;&#33021;&#24050;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#26174;&#31034;&#20986;&#20854;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable success in generative modeling. In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel framework designed to address various challenges in 3D human pose analysis through a unified pipeline. Central to PADS are two distinctive strategies: i) learning a task-agnostic pose prior using a diffusion synthesis process to effectively capture the kinematic constraints in human pose data, and ii) unifying multiple pose analysis tasks like estimation, completion, denoising, etc, as instances of inverse problems. The learned pose prior will be treated as a regularization imposing on task-specific constraints, guiding the optimization process through a series of conditional denoising steps. PADS represents the first diffusion-based framework for tackling general 3D human pose analysis within the inverse problem framework. Its performance has been validated on different benchmarks, signaling the adaptability and robustness of this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#24341;&#26426;&#22120;&#20154;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#36825;&#23558;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20195;&#34920;&#24615;&#26435;&#21147;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2210.00858</link><description>&lt;p&gt;
&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach. (arXiv:2210.00858v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#24341;&#26426;&#22120;&#20154;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#36825;&#23558;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20195;&#34920;&#24615;&#26435;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#25512;&#29702;&#19982;&#26426;&#22120;&#20154;&#25805;&#20316;&#30456;&#32467;&#21512;&#12290;&#38750;&#19987;&#19994;&#20154;&#22763;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#26426;&#22120;&#20154;&#65292;&#25552;&#20379;&#25351;&#20195;&#34920;&#36798;&#24335;&#65288;REF&#65289;&#12289;&#38382;&#39064;&#65288;VQA&#65289;&#25110;&#25235;&#25569;&#21160;&#20316;&#25351;&#20196;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#27599;&#20010;&#21407;&#22987;&#25216;&#33021;&#37117;&#22788;&#29702;&#19968;&#20010;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#29702;&#35270;&#35273;&#23646;&#24615;&#12289;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#12289;&#36923;&#36753;&#21644;&#26522;&#20030;&#20197;&#21450;&#25163;&#33218;&#25511;&#21046;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#36755;&#20837;&#26597;&#35810;&#26144;&#23556;&#21040;&#30001;&#36825;&#20123;&#21407;&#35821;&#32452;&#25104;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#19978;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#12290;&#23613;&#31649;&#26377;&#20123;&#21407;&#35821;&#26159;&#32431;&#31526;&#21495;&#25805;&#20316;&#65288;&#20363;&#22914;&#35745;&#25968;&#65289;&#65292;&#20294;&#21478;&#19968;&#20123;&#26159;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#20989;&#25968;&#65288;&#20363;&#22914;&#35270;&#35273;&#25509;&#22320;&#65289;&#65292;&#22240;&#27492;&#34701;&#21512;&#20102;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20877;&#29616;&#24615;&#30340;&#20195;&#34920;&#24615;&#26435;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power o
&lt;/p&gt;</description></item></channel></rss>