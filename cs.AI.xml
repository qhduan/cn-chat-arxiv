<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16021</link><description>&lt;p&gt;
TMT: &#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#35270;&#20026;&#19981;&#21516;&#35821;&#35328;&#26469;&#23454;&#29616;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19977;&#27169;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16021
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20849;&#21516;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#27491;&#22312;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#37197;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27169;&#32763;&#35793;&#65288;TMT&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#24847;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#24182;&#23558;&#22810;&#27169;&#24577;&#32763;&#35793;&#35270;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#26631;&#35760;&#20026;&#31163;&#25955;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#36328;&#27169;&#24577;&#30340;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25552;&#20986;&#30340;TMT&#20013;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#36827;&#34892;&#26680;&#24515;&#32763;&#35793;&#65292;&#32780;&#27169;&#24577;&#29305;&#23450;&#22788;&#29702;&#20165;&#22312;&#26631;&#35760;&#21270;&#21644;&#21435;&#26631;&#35760;&#21270;&#38454;&#27573;&#20869;&#36827;&#34892;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#20845;&#31181;&#27169;&#24577;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;TMT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;&#65292;&#20026;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#21644;&#35299;&#37322;&#20854;&#19982;&#23884;&#20837;&#31354;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26041;&#20415;&#30340;&#35821;&#35328;&#21644;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.06379</link><description>&lt;p&gt;
Vehicle: &#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;
Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs. (arXiv:2401.06379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;&#65292;&#20026;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#21644;&#35299;&#37322;&#20854;&#19982;&#23884;&#20837;&#31354;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26041;&#20415;&#30340;&#35821;&#35328;&#21644;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#26159;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#21644;&#20256;&#32479;&#31526;&#21495;&#21270;&#20195;&#30721;&#30340;&#31243;&#24207;&#65292;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#36825;&#20123;&#31243;&#24207;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#8220;&#23884;&#20837;&#32570;&#21475;&#8221;&#8212;&#8212;&#20197;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#8220;&#38382;&#39064;&#31354;&#38388;&#8221;&#23646;&#24615;&#19982;&#31561;&#25928;&#30340;&#8220;&#23884;&#20837;&#31354;&#38388;&#8221;&#23646;&#24615;&#20043;&#38388;&#32570;&#20047;&#25216;&#26415;&#30340;&#38382;&#39064;&#8212;&#8212;&#35270;&#20026;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20419;&#36827;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#30340;&#31471;&#21040;&#31471;&#39564;&#35777;&#12290;Vehicle&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#38382;&#39064;&#31354;&#38388;&#8221;&#23646;&#24615;&#24182;&#22768;&#26126;&#23427;&#20204;&#19982;&#8220;&#23884;&#20837;&#31354;&#38388;&#8221;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19968;&#20010;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#36825;&#20123;&#23646;&#24615;&#35299;&#37322;&#20026;&#25152;&#36873;&#25321;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#29615;&#22659;&#12289;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#25903;&#25345;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic programs -- programs containing both machine learning components and traditional symbolic code -- are becoming increasingly widespread. However, we believe that there is still a lack of a general methodology for verifying these programs whose correctness depends on the behaviour of the machine learning components. In this paper, we identify the ``embedding gap'' -- the lack of techniques for linking semantically-meaningful ``problem-space'' properties to equivalent ``embedding-space'' properties -- as one of the key issues, and describe Vehicle, a tool designed to facilitate the end-to-end verification of neural-symbolic programs in a modular fashion. Vehicle provides a convenient language for specifying ``problem-space'' properties of neural networks and declaring their relationship to the ``embedding-space", and a powerful compiler that automates interpretation of these properties in the language of a chosen machine-learning training environment, neural network verifie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item></channel></rss>