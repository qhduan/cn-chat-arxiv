<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01499</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating a Design Method for Positive Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01499
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#26041;&#38754;&#26085;&#30410;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#8220;AI for good&#8221;&#22312;&#19982;&#22797;&#26434;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#32570;&#20047;&#25104;&#29087;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#35813;&#26041;&#27861;&#30340;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#24773;&#22659;&#21270;&#12289;&#25805;&#20316;&#21270;&#12289;&#20248;&#21270;&#21270;&#21644;&#23454;&#29616;&#31119;&#31049;&#65292;&#21516;&#26102;&#25903;&#25345;&#25345;&#32493;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#21021;&#23398;&#32773;&#35774;&#35745;&#24072;&#24212;&#29992;&#20102;&#35813;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#19982;&#25928;&#21147;&#21644;&#21487;&#29992;&#24615;&#30456;&#20851;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#39033;&#19987;&#23478;&#35780;&#20272;&#30740;&#31350;&#35780;&#20272;&#20102;&#25152;&#24471;&#27010;&#24565;&#30340;&#36136;&#37327;&#65292;&#23558;&#20854;&#35780;&#20026;&#20013;&#31561;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing "AI for good" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them modera
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2404.01349</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#19968;&#20010;&#20998;&#31867;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fairness in Large Language Models: A Taxonomic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#26576;&#20123;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#20419;&#20351;&#23545;&#20844;&#24179;&#30340;LLMs&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#30456;&#21453;&#65292;&#22312;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#28041;&#21450;&#29420;&#29305;&#30340;&#32972;&#26223;&#12289;&#20998;&#31867;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#35813;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#20844;&#24179;LLMs&#30340;&#29616;&#26377;&#25991;&#29486;&#30740;&#31350;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;LLMs&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#25509;&#30528;&#20998;&#26512;&#20102;&#23548;&#33268;LLMs&#20559;&#35265;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#35752;&#35770;&#20102;LLMs&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#24635;&#32467;&#20102;&#35780;&#20272;LLMs&#20559;&#35265;&#30340;&#25351;&#26631;&#21644;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01349v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15249</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#36816;&#21160;&#36716;&#31227;&#30340;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Spectral Motion Alignment for Video Motion Transfer using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#35270;&#39057;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65288;VDMs&#65289;&#26174;&#33879;&#20419;&#36827;&#20102;&#23558;&#36755;&#20837;&#35270;&#39057;&#23450;&#21046;&#20026;&#30446;&#26631;&#22806;&#35266;&#12289;&#36816;&#21160;&#31561;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#25552;&#21462;&#35270;&#39057;&#24103;&#30340;&#36816;&#21160;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#36830;&#32493;&#24103;&#27531;&#24046;&#20316;&#20026;&#30446;&#26631;&#36816;&#21160;&#21521;&#37327;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#32570;&#20047;&#20840;&#23616;&#36816;&#21160;&#32972;&#26223;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#36880;&#24103;&#22833;&#30495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;SMA&#36890;&#36807;&#25972;&#21512;&#39057;&#22495;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20419;&#36827;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;SMA&#22312;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15249v1 Announce Type: cross  Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while main
&lt;/p&gt;</description></item><item><title>Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15083</link><description>&lt;p&gt;
&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15083
&lt;/p&gt;
&lt;p&gt;
Hands-Free VR &#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#37096;&#25805;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#38899;&#21629;&#20196;&#23454;&#29616;&#65292;&#20855;&#26377;&#33521;&#35821;&#21475;&#38899;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23545;&#25991;&#26412;&#30340;&#36716;&#25442;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Hands-Free VR&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#33258;&#28982;&#35821;&#35328;&#34394;&#25311;&#29616;&#23454;&#30028;&#38754;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#21457;&#20986;&#21629;&#20196;&#65292;&#20854;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#32463;&#36807;&#19968;&#20010;&#38024;&#23545;&#21333;&#35789;&#38899;&#32032;&#30456;&#20284;&#24615;&#21644;&#33521;&#35821;&#21475;&#38899;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#38899;&#35782;&#21035;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#19968;&#20010;&#23545;&#33258;&#28982;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25991;&#26412;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#30340;&#34394;&#25311;&#29616;&#23454;&#21629;&#20196;&#12290;Hands-Free VR&#22312;&#19968;&#20010;&#21463;&#25511;&#30340;&#34987;&#35797;&#30740;&#31350;&#20013;&#65288;N = 22&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#25214;&#21040;&#29305;&#23450;&#29289;&#20307;&#24182;&#20197;&#21508;&#31181;&#37197;&#32622;&#25918;&#32622;&#23427;&#20204;&#12290;&#22312;&#23545;&#29031;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#30028;&#38754;&#36890;&#36807;&#25163;&#25345;&#25511;&#21046;&#22120;&#25235;&#21462;&#12289;&#25644;&#36816;&#21644;&#23450;&#20301;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;Hands-Free VR&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;Hands-Free VR&#23545;&#33521;&#35821;&#21475;&#38899;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#22312;&#25105;&#20204;&#30340;20&#21517;&#21442;&#19982;&#32773;&#20013;&#65292;&#33521;&#35821;&#19981;&#26159;&#20182;&#20204;&#30340;&#39318;&#36873;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15083v1 Announce Type: cross  Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their f
&lt;/p&gt;</description></item><item><title>XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02258</link><description>&lt;p&gt;
XTSFormer: &#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02258
&lt;/p&gt;
&lt;p&gt;
XTSFormer&#26159;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#39044;&#27979;&#30340;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#24207;&#21015;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#36830;&#32493;&#20107;&#20214;&#20043;&#38388;&#26102;&#38388;&#38388;&#38548;&#30340;&#19981;&#35268;&#21017;&#24615;&#12289;&#24490;&#29615;&#12289;&#21608;&#26399;&#24615;&#21644;&#22810;&#23610;&#24230;&#20107;&#20214;&#20132;&#20114;&#65292;&#20197;&#21450;&#38271;&#20107;&#20214;&#24207;&#21015;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#20107;&#20214;&#20132;&#20114;&#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#65292;&#32780;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20020;&#24202;&#20107;&#20214;&#25968;&#25454;&#65289;&#24456;&#24120;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26102;&#31354;&#23610;&#24230;&#30340;Transformer&#65288;XTSFormer&#65289;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#20107;&#20214;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29305;&#24449;&#30340;&#24490;&#29615;&#24863;&#30693;&#26102;&#38388;&#20301;&#32622;&#32534;&#30721;&#65288;FCPE&#65289;&#65292;&#33021;&#22815;&#28789;&#27963;&#25429;&#25417;&#26102;&#38388;&#30340;&#24490;&#29615;&#24615;&#36136;&#65292;&#20197;&#21450;&#19968;&#20010;&#20998;&#23618;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#20123;&#23610;&#24230;&#30001;&#33258;&#24213;&#21521;&#19978;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event prediction aims to forecast the time and type of a future event based on a historical event sequence. Despite its significance, several challenges exist, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, and multi-scale event interactions, as well as the high computational costs for long event sequences. Existing neural temporal point processes (TPPs) methods do not capture the multi-scale nature of event interactions, which is common in many real-world applications such as clinical event data. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), designed specifically for irregularly timed event data. Our model comprises two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism. These scales are determined by a bottom-up clustering algorithm. Extens
&lt;/p&gt;</description></item><item><title>EASRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#32467;&#21512;&#65292;&#20197;&#21450;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00390</link><description>&lt;p&gt;
EASRec&#65306;&#29992;&#20110;&#39640;&#25928;&#38271;&#26399;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00390
&lt;/p&gt;
&lt;p&gt;
EASRec&#26159;&#19968;&#20010;&#38024;&#23545;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#24377;&#24615;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#32467;&#21512;&#65292;&#20197;&#21450;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#20174;&#28023;&#37327;&#20449;&#24687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#24403;&#21069;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65288;SRSs&#65289;&#22312;&#35745;&#31639;&#21644;&#36164;&#28304;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;&#22914;SASRec&#65289;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#25512;&#33616;&#65292;&#20174;&#30005;&#23376;&#21830;&#21153;&#21040;&#31038;&#20132;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#25512;&#29702;&#38454;&#27573;&#20250;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#21160;&#21098;&#26525;&#25216;&#26415;&#21644;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#27969;&#34892;&#30340;&#36164;&#28304;&#21463;&#38480;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#20197;&#35843;&#25972;&#27169;&#22411;&#20197;&#20943;&#23569;FLOPs&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential. Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from. especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks. However, such systems suffer from substantial computational costs and resource consumption during the inference stage. To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures. We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy. The main contribution of our work is developing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2401.17981</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#26816;&#27979;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38598;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#32454;&#33410;&#35270;&#35273;&#20803;&#32032;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;MLLMs&#32467;&#21512;&#65292;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#65292;&#24182;&#20943;&#23569;&#22238;&#31572;&#20013;&#30340;&#38169;&#35823;&#25554;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#27979;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#36825;&#31181;&#34701;&#21512;&#23545;MLLMs&#30340;&#21407;&#22987;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#20114;&#25442;&#24615;&#12290;&#25105;&#20204;&#23545;LLaVA-1.5&#12289;DINO&#21644;PaddleOCRv2&#31561;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;MLLMs&#22312;&#29305;&#23450;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20445;&#25345;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#20248;&#21183;&#12290;&#36890;&#36807;&#22312;10&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22686;&#24378;&#30340;MLLMs&#22312;9&#20010;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#26631;&#20934;&#21270;&#24179;&#22343;&#24471;&#20998;&#25552;&#21319;&#39640;&#36798;12.99%&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.00284</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Selective Uncertainty Propagation in Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#27573;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#65292;&#30446;&#26631;&#22312;&#20110;&#24212;&#23545;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20013;&#27599;&#19968;&#27493;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35780;&#20272;&#31163;&#24320;&#34892;&#20026;&#31574;&#30053;&#22312;&#31532;h&#27493;&#26102;&#30340;&#22788;&#29702;&#25928;&#26524;&#65292;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#36825;&#19968;&#27493;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#27599;&#19968;&#27493;&#31574;&#30053;&#37117;&#20250;&#24433;&#21709;&#19979;&#19968;&#29366;&#24577;&#30340;&#20998;&#24067;&#65292;&#30456;&#20851;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#19978;&#27604;&#38543;&#26426;&#24773;&#22659;&#25361;&#25112;&#19979;&#30340;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#26356;&#21152;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#38590;&#24230;&#20171;&#20110;&#36825;&#20004;&#31181;&#24773;&#22659;&#20043;&#38388;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#29992;&#20110;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#38590;&#24230;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13008</link><description>&lt;p&gt;
LoBaSS&#65306;&#22312;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#20013;&#27979;&#37327;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#23545;&#40784;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#24494;&#35843;&#25968;&#25454;&#30340;&#36873;&#25321;&#28145;&#21051;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20256;&#32479;&#19978;&#20197;&#25968;&#25454;&#36136;&#37327;&#21644;&#20998;&#24067;&#20026;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SFT&#25968;&#25454;&#36873;&#25321;&#30340;&#19968;&#20010;&#26032;&#32500;&#24230;&#65306;&#21487;&#23398;&#20064;&#24615;&#12290;&#36825;&#20010;&#26032;&#32500;&#24230;&#30340;&#21160;&#26426;&#26159;&#30001;LLM&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#36866;&#21512;&#19968;&#20010;&#27169;&#22411;&#30340;SFT&#25968;&#25454;&#21487;&#33021;&#19981;&#36866;&#21512;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#33021;&#21147;&#36825;&#20010;&#26415;&#35821;&#26469;&#23450;&#20041;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#36866;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25439;&#22833;&#30340;SFT&#25968;&#25454;&#36873;&#25321;&#65288;LoBaSS&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;SFT&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#22266;&#26377;&#30340;&#27169;&#22411;&#33021;&#21147;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07675</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#34920;&#31034;&#32780;&#33719;&#30410;&#33391;&#22810;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#20197;&#36827;&#34892;&#39640;&#25928;&#21644;&#21487;&#20256;&#36882;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20381;&#36182;&#31526;&#21495;&#25512;&#29702;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(HRL)&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#38656;&#35201;&#25163;&#21160;&#35774;&#32622;&#30446;&#26631;&#34920;&#31034;&#12290;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#30340;&#25361;&#25112;&#22312;&#20110;&#23427;&#24517;&#39035;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20363;&#22914;&#29615;&#22659;&#21160;&#21147;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#20986;&#29616;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#30446;&#26631;&#21457;&#29616;&#30340;&#21457;&#23637;&#26426;&#21046;&#65292;&#35813;&#34920;&#31034;&#23558;&#20855;&#26377;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#36827;&#34892;&#25277;&#35937;&#65288;&#21363;&#20998;&#32452;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#30340;&#23553;&#24314;HRL&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#21487;&#36798;&#24615;&#20998;&#26512;&#26469;&#36817;&#20284;&#29366;&#24577;&#38598;&#21512;&#20043;&#38388;&#30340;&#36807;&#28193;&#20851;&#31995;&#65292;&#24182;&#25913;&#36827;&#30446;&#26631;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We eval
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03579</link><description>&lt;p&gt;
DTW+S: &#20351;&#29992;&#26377;&#24207;&#23616;&#37096;&#36235;&#21183;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;&#26102;&#38388;&#24207;&#21015;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26159;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#23616;&#37096;&#36235;&#21183;&#65288;&#24418;&#29366;&#65289;&#32780;&#26080;&#27861;&#25429;&#25417;&#21040;&#30456;&#20284;&#20043;&#22788;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23547;&#25214;&#22312;&#30456;&#20284;&#26102;&#38388;&#21608;&#22260;&#21457;&#29983;&#30340;&#30456;&#20284;&#36235;&#21183;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#26377;&#24207;&#30340;&#26377;&#24847;&#20041;&#30340;&#23616;&#37096;&#36235;&#21183;&#24207;&#21015;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#20013;&#65288;&#20174;&#22686;&#38271;&#21040;&#23792;&#20540;&#20877;&#21040;&#20943;&#23569;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;DTW+S&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#20445;&#25345;&#25509;&#36817;&#24615;&#8221;&#30340;&#30697;&#38453;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#19968;&#21015;&#20195;&#34920;&#23616;&#37096;&#36235;&#21183;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#31181;&#34920;&#31034;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DTW+S&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.09423</link><description>&lt;p&gt;
&#22312;NetHack&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#27169;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064; (IL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#23427;&#24448;&#24448;&#19981;&#33021;&#23436;&#20840;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#22823;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#22312;&#37027;&#37324;&#8220;&#25193;&#22823;&#35268;&#27169;&#8221;&#24050;&#32463;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20180;&#32454;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#24102;&#26469;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312; NetHack &#28216;&#25103;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31243;&#24207;&#29983;&#25104;&#12289;&#38543;&#26426;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#21457;&#29616; IL &#30340;&#25439;&#22833;&#21644;&#24179;&#22343;&#22238;&#25253;&#38543;&#30528;&#35745;&#31639;&#39044;&#31639;&#30340;&#21464;&#21270;&#32780;&#24179;&#28369;&#21464;&#21270;&#19988;&#24378;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#26679;&#26412;&#25968;&#37327;&#26041;&#38754;&#20026;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;&#30340; IL &#20195;&#29702;&#20154;&#30340;&#35745;&#31639;&#39044;&#31639;&#24314;&#31435;&#20102;&#24130;&#24459;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#20855;&#26377; IL &#30340;NetHack&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09574</link><description>&lt;p&gt;
UOR&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#21518;&#38376;&#21487;&#20197;&#20256;&#36882;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#23545;&#23433;&#20840;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#22823;&#37117;&#26159;&#38750;&#30446;&#26631;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#12290;&#24456;&#23569;&#26377;&#38024;&#23545;&#30446;&#26631;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#24615;&#30340;&#26041;&#27861;&#20351;&#29992;&#25163;&#21160;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#21644;&#36755;&#20986;&#34920;&#31034;&#65292;&#36825;&#20351;&#24471;&#25915;&#20987;&#25928;&#26524;&#19981;&#22815;&#24378;&#22823;&#21644;&#26222;&#36866;&#12290;&#26412;&#25991;&#39318;&#20808;&#24635;&#32467;&#20102;&#19968;&#20010;&#26356;&#20855;&#23041;&#32961;&#24615;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#25915;&#20987;&#24212;&#28385;&#36275;&#30340;&#35201;&#27714;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#36873;&#25321;&#21464;&#25104;&#33258;&#21160;&#20248;&#21270;&#65292;&#25171;&#30772;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#34987;&#27745;&#26579;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35302;&#21457;&#22120;&#30340;&#26356;&#21152;&#22343;&#21248;&#21644;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25628;&#32034;&#36873;&#21462;&#36866;&#24403;&#30340;&#35302;&#21457;&#35789;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#27719;&#34920;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UOR&#21487;&#20197;&#22312;&#21508;&#31181;PLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#21518;&#38376;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;99.3&#65285;&#65289;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;UOR&#36824;&#21487;&#20197;&#31361;&#30772;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experi
&lt;/p&gt;</description></item></channel></rss>