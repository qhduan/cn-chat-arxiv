<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2404.01663</link><description>&lt;p&gt;
CMAT: &#29992;&#20110;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01663
&lt;/p&gt;
&lt;p&gt;
CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#25805;&#20316;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#30830;&#24341;&#23548;&#23545;&#35805;&#27969;&#31243;&#65292;&#26234;&#33021;&#20307;&#35843;&#25972;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#28041;&#21450;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#21709;&#24212;&#36825;&#31181;&#24341;&#23548;&#12290;&#38024;&#23545;&#36825;&#19968;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Collaborative Multi-Agent Tuning&#65288;CMAT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26469;&#22686;&#24378;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#23454;&#26102;&#36866;&#24212;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14664</link><description>&lt;p&gt;
&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#36125;&#21494;&#26031;&#31163;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#65292;&#21160;&#20316;&#32463;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#20026;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#26356;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#23398;&#20064;&#65288;OPL&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#26469;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sDM&#65292;&#19968;&#20010;&#20026;OPE&#21644;OPL&#35774;&#35745;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26082;&#26377;&#31639;&#27861;&#22522;&#30784;&#21448;&#26377;&#29702;&#35770;&#22522;&#30784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;sDM&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#21463;&#22312;&#32447;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#24615;&#33021;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20559;&#31163;&#20256;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#30340;&#34920;&#29616;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#22909;&#22788;&#12290;&#23454;&#35777;&#35777;&#25454;&#23637;&#31034;&#20102;sDM&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;</title><link>http://arxiv.org/abs/2305.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Oil Spill Segmentation using Deep Encoder-Decoder models. (arXiv:2305.01386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#27833;&#26159;&#29616;&#20195;&#19990;&#30028;&#32463;&#27982;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#38543;&#30528;&#21407;&#27833;&#24191;&#27867;&#24212;&#29992;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#24847;&#22806;&#30340;&#27833;&#27745;&#27844;&#28431;&#20063;&#38590;&#20197;&#36991;&#20813;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#20960;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#27169;&#22411;&#32452;&#21512;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#19982;&#24403;&#21069;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#22312;&#8220;&#27833;&#27745;&#8221;&#31867;&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#19978;&#23454;&#29616;&#20102;64.868%&#30340;&#32467;&#26524;&#21644;61.549%&#30340;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;
Crude oil is an integral component of the modern world economy. With the growing demand for crude oil due to its widespread applications, accidental oil spills are unavoidable. Even though oil spills are in and themselves difficult to clean up, the first and foremost challenge is to detect spills. In this research, the authors test the feasibility of deep encoder-decoder models that can be trained effectively to detect oil spills. The work compares the results from several segmentation models on high dimensional satellite Synthetic Aperture Radar (SAR) image data. Multiple combinations of models are used in running the experiments. The best-performing model is the one with the ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over Union (IoU) of 64.868% and a class IoU of 61.549% for the "oil spill" class when compared with the current benchmark model, which achieved a mean IoU of 65.05% and a class IoU of 53.38% for the "oil spill" class.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;</title><link>http://arxiv.org/abs/2304.14765</link><description>&lt;p&gt;
LostPaw: &#20351;&#29992;&#24102;&#35270;&#35273;&#36755;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064; Transformer &#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;
&lt;/p&gt;
&lt;p&gt;
LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input. (arXiv:2304.14765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#21435;&#23456;&#29289;&#21487;&#33021;&#20250;&#35753;&#23456;&#29289;&#20027;&#20154;&#20493;&#24863;&#30171;&#33510;&#65292;&#32780;&#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23547;&#25214;&#20002;&#22833;&#23456;&#29289;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20415;&#20110;&#36825;&#26679;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#29616;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#21306;&#20998;&#19981;&#21516;&#23456;&#29289;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#30340;&#29399;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807; 3 &#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312; 350 &#20010;&#35757;&#32451;&#21608;&#26399;&#21518;&#65292;&#27169;&#22411;&#21462;&#24471;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27979;&#35797;&#20934;&#30830;&#24615;&#25509;&#36817;&#35757;&#32451;&#20934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#23450;&#20301;&#22833;&#36394;&#23456;&#29289;&#30340;&#24037;&#20855;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20854;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#20687;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Losing pets can be highly distressing for pet owners, and finding a lost pet is often challenging and time-consuming. An artificial intelligence-based application can significantly improve the speed and accuracy of finding lost pets. In order to facilitate such an application, this study introduces a contrastive neural network model capable of accurately distinguishing between images of pets. The model was trained on a large dataset of dog images and evaluated through 3-fold cross-validation. Following 350 epochs of training, the model achieved a test accuracy of 90%. Furthermore, overfitting was avoided, as the test accuracy closely matched the training accuracy. Our findings suggest that contrastive neural network models hold promise as a tool for locating lost pets. This paper provides the foundation for a potential web application that allows users to upload images of their missing pets, receiving notifications when matching images are found in the application's image database. Thi
&lt;/p&gt;</description></item></channel></rss>