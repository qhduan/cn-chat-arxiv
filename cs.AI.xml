<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.19002</link><description>&lt;p&gt;
GoalNet: &#38754;&#21521;&#30446;&#26631;&#21306;&#22495;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19002
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36947;&#36335;&#19978;&#34892;&#20154;&#26410;&#26469;&#30340;&#36712;&#36857;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#21463;&#22330;&#26223;&#36335;&#24452;&#12289;&#34892;&#20154;&#24847;&#22270;&#21644;&#20915;&#31574;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22823;&#22810;&#20351;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#21508;&#31181;&#28508;&#22312;&#30340;&#26410;&#26469;&#36712;&#36857;&#20998;&#24067;&#65292;&#36825;&#24182;&#26410;&#32771;&#34385;&#22330;&#26223;&#32972;&#26223;&#21644;&#34892;&#20154;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30452;&#25509;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#20351;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#30446;&#26631;&#28857;&#65292;&#28982;&#21518;&#37325;&#22797;&#20351;&#29992;&#30446;&#26631;&#28857;&#26469;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#12290;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#65292;&#36825;&#20123;&#21306;&#22495;&#20195;&#34920;&#20102;&#34892;&#20154;&#30340;&#8220;&#30446;&#26631;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GoalNet&#65292;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#26032;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19002v1 Announce Type: cross  Abstract: Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the "goals" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can pr
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02984</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#38656;&#35201;&#23545;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#26088;&#22312;&#23545;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#29616;&#26377;&#21457;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#23427;&#20204;&#30340;&#25104;&#21151;&#65292;&#24182;&#35782;&#21035;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;2023&#24180;11&#26376;&#65292;&#22312;PubMed&#12289;Web of Science&#12289;Google Scholar&#12289;arXiv&#12289;medRxiv&#21644;PsyArXiv&#20845;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#25628;&#32034;&#65292;&#36981;&#24490;2020&#24180;&#29256;&#30340;&#8220;&#31995;&#32479;&#35780;&#20215;&#21644;Meta&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#8221;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290;&#26368;&#21021;&#35782;&#21035;&#20102;313&#31687;&#20986;&#29256;&#29289;&#65292;&#25353;&#29031;&#30740;&#31350;&#32435;&#20837;&#26631;&#20934;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;34&#31687;&#20986;&#29256;&#29289;&#36827;&#34892;&#32508;&#36848;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#20102;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#27835;&#30103;&#12289;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#21457;&#29616;&#23558;&#34987;&#24635;&#32467;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The growing use of large language models (LLMs) stimulates a need for a comprehensive review of their applications and outcomes in mental health care contexts. This scoping review aims to critically analyze the existing development and applications of LLMs in mental health care, highlighting their successes and identifying their challenges and limitations in these specialized fields. Materials and Methods: A broad literature search was conducted in November 2023 using six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 313 publications were initially identified, and after applying the study inclusion criteria, 34 publications were selected for the final review. Results: We identified diverse applications of LLMs in mental health care, including diagnosis, therapy, patient engagement enhancement, etc. Key challen
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item></channel></rss>