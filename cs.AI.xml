<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17768</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#22797;&#26434;&#24615;&#21040;&#20844;&#20247;&#21465;&#20107;&#65306;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17768
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24039;&#22937;&#22320;&#23558;&#22797;&#26434;&#30340;&#30740;&#31350;&#25991;&#31456;&#32763;&#35793;&#25104;&#19982;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247; resonant &#30340;&#25253;&#36947;&#12290;&#36825;&#31181;&#21465;&#20107;&#30340;&#33258;&#21160;&#29983;&#25104;&#22686;&#24378;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#26469;&#20419;&#36827;&#36825;&#31181;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#20061;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#23398;&#26415;&#20986;&#29256;&#29289;&#21450;&#20854;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#24179;&#34892;&#32534;&#35793;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#31185;&#23398;&#26032;&#38395;&#21465;&#20107;&#21644;&#23398;&#26415;&#25991;&#31295;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#21644;&#31616;&#27905;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#36807;&#31243;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
&lt;/p&gt;</description></item><item><title>S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17643</link><description>&lt;p&gt;
S+t-SNE - &#23558;&#38477;&#32500;&#24341;&#20837;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
S+t-SNE - Bringing dimensionality reduction to data streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17643
&lt;/p&gt;
&lt;p&gt;
S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;S+t-SNE&#65292;&#36825;&#26159;t-SNE&#31639;&#27861;&#30340;&#19968;&#31181;&#25913;&#36827;&#65292;&#26088;&#22312;&#22788;&#29702;&#26080;&#38480;&#25968;&#25454;&#27969;&#12290;S+t-SNE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#36880;&#27493;&#26356;&#26032;t-SNE&#23884;&#20837;&#65292;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#22788;&#29702;&#27969;&#24335;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#27599;&#19968;&#27493;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#28857;&#65292;&#35813;&#31639;&#27861;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;&#37319;&#29992;&#30450;&#30446;&#26041;&#27861;&#36827;&#34892;&#28418;&#31227;&#31649;&#29702;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#65292;&#20419;&#36827;&#19981;&#26029;&#21487;&#35270;&#21270;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;S+t-SNE&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#25429;&#25417;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#23454;&#26102;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09472</link><description>&lt;p&gt;
&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65306;&#36229;&#36234;&#20154;&#31867;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#21028;&#26029;&#65292;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;AI&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#33021;&#21147;&#23558;&#21463;&#21040;&#20154;&#31867;&#33021;&#21147;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#36825;&#23601;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24403;&#31995;&#32479;&#30340;&#33021;&#21147;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#32487;&#32493;&#25913;&#36827;&#36825;&#20123;&#31995;&#32479;&#65311;&#26412;&#25991;&#22312;&#35299;&#20915;&#38590;&#24230;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;4-5&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#30340;&#32972;&#26223;&#19979;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#65288;&#22914;1-3&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#20013;&#23398;&#20064;&#20154;&#31867;&#27880;&#37322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#8221;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19968;&#20010;&#22312;&#26356;&#31616;&#21333;&#20219;&#21153;&#30340;&#30417;&#30563;&#19979;&#35757;&#32451;&#30340;&#35780;&#20272;&#22120;&#65288;&#22870;&#21169;&#27169;&#22411;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35780;&#20998;&#26356;&#38590;&#20219;&#21153;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#19981;&#21516;&#38590;&#24230;&#20219;&#21153;&#38388;&#30340;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#22788;&#29702;&#30563;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15861</link><description>&lt;p&gt;
BPDec: &#25581;&#31034;BERT&#39044;&#35757;&#32451;&#20013;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#22686;&#24378;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#35774;&#35745;&#21450;&#30740;&#31350;&#22312;BERT&#39044;&#35757;&#32451;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#65288;&#26469;&#33258;Transformer&#30340;&#21452;&#21521;&#32534;&#30721;&#34920;&#31034;&#65289;&#36890;&#36807;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#38598;&#20013;&#22312;&#19982;&#27169;&#22411;&#32467;&#26500;&#30456;&#20851;&#30340;&#22686;&#24378;&#65292;&#20363;&#22914;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36824;&#26377;&#19968;&#20123;&#20154;&#28145;&#20837;&#30740;&#31350;&#20102;&#19982;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#24039;&#65292;&#21253;&#25324;&#25972;&#35789;&#25513;&#30721;&#12290;DeBERTa&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;BERT&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#35777;&#26126;&#25928;&#26524;&#38750;&#24120;&#26174;&#33879;&#12290;&#25105;&#20204;&#35748;&#20026;&#22260;&#32469;&#22686;&#24378;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#21644;&#30740;&#31350;&#24182;&#26410;&#24471;&#21040;&#24212;&#26377;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;BPDec&#65288;BERT&#39044;&#35757;&#32451;&#35299;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20250;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15861v2 Announce Type: replace-cross  Abstract: BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the field of natural language processing through its exceptional performance on numerous tasks. Yet, the majority of researchers have mainly concentrated on enhancements related to the model structure, such as relative position embedding and more efficient attention mechanisms. Others have delved into pretraining tricks associated with Masked Language Modeling, including whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's encoder model for pretraining, proving to be highly effective. We argue that the design and research around enhanced masked language modeling decoders have been underappreciated. In this paper, we propose several designs of enhanced decoders and introduce BPDec (BERT Pretraining Decoder), a novel method for modeling training. Typically, a pretrained BERT model is fine-tuned for specific Natural Language 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18403</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#22270;&#20687;&#36716;&#25442;&#30772;&#22351;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18403
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#20250;&#36890;&#36807;&#21521;&#24178;&#20928;&#35757;&#32451;&#38598;&#24341;&#20837;&#31934;&#24515;&#35774;&#35745;&#19988;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;JPEG&#21387;&#32553;&#21644;&#23545;&#25239;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#22522;&#20110;&#33539;&#25968;&#32422;&#26463;&#30340;&#38468;&#21152;&#22122;&#22768;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#35753;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26080;&#25928;&#65292;&#32473;&#38450;&#24481;&#32773;&#24102;&#26469;&#26356;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#24773;&#26223;&#20013;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;&#34920;&#36798;&#20026;&#23558;&#30697;&#38453;&#20056;&#20197;&#24178;&#20928;&#26679;&#26412;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#31867;&#20869;&#30697;&#38453;&#19981;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imi}$&#65292;&#23558;&#31867;&#38388;&#30697;&#38453;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imc}$&#20197;&#30740;&#31350;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#25512;&#27979;&#22686;&#21152;&#36825;&#20004;&#20010;&#24230;&#37327;&#23558;&#26377;&#21161;&#20110;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18403v2 Announce Type: replace-cross  Abstract: Unlearnable datasets lead to a drastic drop in the generalization performance of models trained on them by introducing elaborate and imperceptible perturbations into clean training sets. Many existing defenses, e.g., JPEG compression and adversarial training, effectively counter UDs based on norm-constrained additive noise. However, a fire-new type of convolution-based UDs have been proposed and render existing defenses all ineffective, presenting a greater challenge to defenders. To address this, we express the convolution-based unlearnable sample as the result of multiplying a matrix by a clean sample in a simplified scenario, and formalize the intra-class matrix inconsistency as $\Theta_{imi}$, inter-class matrix consistency as $\Theta_{imc}$ to investigate the working mechanism of the convolution-based UDs. We conjecture that increasing both of these metrics will mitigate the unlearnability effect. Through validation experi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14707</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#29305;&#24449;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#24494;&#35843;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#24050;&#32463;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24335;&#24494;&#35843;&#26469;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#19968;&#20123;&#28508;&#22312;&#29305;&#24449;&#34987;&#23545;&#25239;&#25200;&#21160;&#25152;&#28151;&#28102;&#65292;&#24182;&#23548;&#33268;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#22312;&#26368;&#21518;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#29305;&#24449;&#20043;&#38388;&#20986;&#29616;&#24847;&#22806;&#22686;&#21152;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32544;&#30340;&#26041;&#27861;&#26469;&#26126;&#30830;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#35299;&#32544;&#22120;&#65292;&#23558;&#23545;&#25239;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#28040;&#38500;&#28508;&#22312;&#29305;&#24449;&#26469;&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#36827;&#19968;&#27493;&#20174;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#20013;&#33719;&#30410;&#65292;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.11480</link><description>&lt;p&gt;
&#23545;&#24191;&#27867;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#26399;&#26395;&#65306;&#26399;&#26395;&#20043;&#22806;&#30340;&#26410;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#28041;&#21450;&#24320;&#21457;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#29421;&#31364;&#22320;&#20851;&#27880;&#35757;&#32451;&#38598;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#38480;&#21046;&#38477;&#20302;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#31995;&#32479;&#20250;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#24322;&#24120;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#27599;&#19968;&#31181;&#20998;&#24067;&#21464;&#21270;&#19978;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;BROAD&#65288;Benchmarking Resilience Over Anomaly Diversity&#65289;&#30340;&#21517;&#20041;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#21482;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#23427;&#20204;&#29305;&#21035;&#35774;&#35745;&#26469;&#39044;&#26399;&#30340;&#24847;&#22806;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.07799</link><description>&lt;p&gt;
ChatGPT&#19982;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#65306;&#21487;&#25511;&#25991;&#26412;&#25688;&#35201;&#21644;&#21477;&#23376;&#39118;&#26684;&#36716;&#31227;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#20174;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#24341;&#36215;&#20102;&#23186;&#20307;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#65288;&#19987;&#23478;&#19982;&#19968;&#33324;&#20154;&#65289;&#21644;&#20889;&#20316;&#39118;&#26684;&#65288;&#27491;&#24335;&#19982;&#38750;&#27491;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#35832;&#22914;&#21333;&#35789;&#31867;&#22411;&#20998;&#24067;&#31561;&#20960;&#20010;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403; ChatGPT &#23558;&#25991;&#26412;&#36866;&#24212;&#29305;&#23450;&#39118;&#26684;&#26102;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.06805</link><description>&lt;p&gt;
&#29992;&#24133;&#24230;&#21463;&#38480;&#21046;&#20248;&#21270;&#35299;&#38145;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization. (arXiv:2306.06805v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;Olah&#31561;&#20154;2017&#24180;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#24037;&#20316;&#20043;&#21518;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340; popularity&#65292;&#23558;&#20854;&#30830;&#31435;&#20026;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#22270;&#20687;&#30340;&#25216;&#24039;&#20197;&#21450;&#22312;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MACO&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#35299;&#37322;&#20301;&#20110;&#33258;&#28982;&#22270;&#20687;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#24182;&#20026;&#22823;&#22411;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#23646;&#24615;&#26426;&#21046;&#65292;&#21487;&#20197;&#22686;&#24378;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#31354;&#38388;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spectrum while keeping the magnitude constant to ensure that generated explanations lie in the space of natural images. Our approach yields significantly better results (both qualitatively and quantitatively) and unlocks efficient and interpretable feature visualizations for large state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing us to augment feature visualizations with spatial importance. We validate our method on a novel benchmark for compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16784</link><description>&lt;p&gt;
&#32467;&#21512;&#35805;&#35821;&#32467;&#26500;&#20998;&#24067;&#30340;&#38271;&#25991;&#26412;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#25688;&#35201;&#65292;&#35805;&#35821;&#32467;&#26500;&#22312;&#36776;&#35782;&#25991;&#26412;&#26680;&#24515;&#20869;&#23481;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#20043;&#21069;&#23558;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#26680;&#24515;&#37096;&#20998;&#30340;&#27880;&#37322;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35805;&#35821;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#26032;&#22411;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;RST-attention&#26426;&#21046;&#26159;&#22522;&#20110;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#30340;Longformer&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20984;&#26174;&#20986;&#20854;&#22312;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.14513</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation. (arXiv:2211.14513v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;(UISS)&#26088;&#22312;&#23558;&#20302;&#23618;&#35270;&#35273;&#29305;&#24449;&#19982;&#35821;&#20041;&#32423;&#21035;&#30340;&#34920;&#31034;&#21305;&#37197;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#30417;&#31649;&#12290;&#26412;&#25991;&#20174;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#30340;&#35282;&#24230;&#25506;&#31350;&#20102;UISS&#27169;&#22411;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#24182;&#23558;UISS&#19982;&#25972;&#24133;&#22270;&#20687;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#35748;&#20026;UISS&#20013;&#29616;&#26377;&#30340;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#23384;&#22312;&#34920;&#31034;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Semantic Attention Network(SAN)&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22359;Semantic Attention(SEAT)&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#19968;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised image semantic segmentation(UISS) aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods.
&lt;/p&gt;</description></item></channel></rss>