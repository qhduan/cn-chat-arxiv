<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12527</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12527
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#65292;&#30001;&#27492;&#24102;&#26469;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21363;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#34892;&#20026;&#30340;&#20215;&#20540;&#12290;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#20013;&#36827;&#34892;&#23637;&#24320;&#36827;&#34892;&#25910;&#38598;&#39069;&#22806;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#23436;&#20840;&#22833;&#36133;&#12290;&#36825;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#35843;&#26597;&#21457;&#29616;&#65292;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;&#36807;&#31243;&#23548;&#33268;&#23384;&#22312;&#19968;&#32452;&#35302;&#21457;&#30149;&#24577;&#20540;&#36807;&#39640;&#30340;&#36793;&#32536;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12527v1 Announce Type: cross  Abstract: Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overes
&lt;/p&gt;</description></item><item><title>DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09427</link><description>&lt;p&gt;
DoorINet: &#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09427
&lt;/p&gt;
&lt;p&gt;
DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;&#24212;&#29992;&#20351;&#29992;&#20302;&#25104;&#26412;&#30340;&#24494;&#22411;&#30005;&#21160;&#26426;&#26800;&#24815;&#24615;&#20256;&#24863;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#26041;&#21521;&#20272;&#35745;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20219;&#21153;&#65292;&#24212;&#29992;&#23039;&#24577;&#21644;&#33322;&#21521;&#21442;&#32771;&#31995;&#32479;&#31639;&#27861;&#12290;&#21033;&#29992;&#38464;&#34746;&#20202;&#35835;&#25968;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35835;&#25968;&#26356;&#26032;&#23039;&#24577;&#35282;&#24230;&#65292;&#21033;&#29992;&#30913;&#21147;&#35745;&#27979;&#37327;&#26356;&#26032;&#33322;&#21521;&#35282;&#24230;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;&#30913;&#21147;&#35745;&#21463;&#21040;&#24178;&#25200;&#65292;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24433;&#21709;&#21040;&#20272;&#35745;&#33322;&#21521;&#35282;&#24230;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#25214;&#21040;&#34915;&#26588;&#25110;&#20912;&#31665;&#38376;&#30340;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoorINet&#65292;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#20302;&#25104;&#26412;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#19968;&#20010;&#21253;&#21547;391&#20998;&#38047;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#27979;&#37327;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09427v1 Announce Type: cross  Abstract: Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;</title><link>http://arxiv.org/abs/2401.17133</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#30340;&#20027;&#21160;&#24615;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65306;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;(SVC)&#36890;&#36807;&#23558;&#19968;&#20010;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#30446;&#26631;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#27468;&#35789;&#21644;&#26059;&#24459;&#65292;&#33258;&#21160;&#21270;&#20102;&#27468;&#26354;&#32763;&#21809;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#29256;&#26435;&#21644;&#20844;&#27665;&#26435;&#21033;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; SongBsAb&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20027;&#21160;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110; SVC &#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;SongBsAb &#22312;&#21457;&#24067;&#27468;&#21809;&#22768;&#38899;&#20043;&#21069;&#24341;&#20837;&#20102;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#36825;&#26679;&#24403;&#23427;&#20204;&#34987;&#20351;&#29992;&#26102;&#65292;SVC &#30340;&#29983;&#25104;&#36807;&#31243;&#23558;&#34987;&#24178;&#25200;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#27468;&#21809;&#22768;&#38899;&#12290; SongBsAb &#20855;&#26377;&#21452;&#37325;&#39044;&#38450;&#25928;&#26524;&#65292;&#24341;&#36215;&#27468;&#25163;&#36523;&#20221;&#21644;&#27468;&#35789;&#30340;&#28151;&#20081;&#65292;&#21363; SVC &#35206;&#30422;&#30340;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;&#20026;&#20102;&#25552;&#39640;&#25200;&#21160;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20197;&#20276;&#22863;&#26354;&#20316;&#20026;&#39069;&#22806;&#25513;&#34109;&#32773;&#30340;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.12850</link><description>&lt;p&gt;
&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization. (arXiv:2401.12850v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28436;&#35762;&#32773;&#20998;&#21106;&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#22270;&#32858;&#31867;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#35762;&#32773;&#20998;&#21106;&#26159;&#22522;&#20110;&#35828;&#35805;&#32773;&#36523;&#20221;&#23545;&#38899;&#39057;&#24405;&#38899;&#36827;&#34892;&#20998;&#21106;&#30340;&#37325;&#35201;&#35821;&#38899;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#20998;&#21106;&#26041;&#27861;&#28041;&#21450;&#22810;&#27425;&#23884;&#20837;&#25552;&#21462;&#21644;&#32858;&#31867;&#27493;&#39588;&#65292;&#36890;&#24120;&#20197;&#23396;&#31435;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#12290;&#34429;&#28982;&#31471;&#21040;&#31471;&#30340;&#20998;&#21106;&#31995;&#32479;&#35797;&#22270;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#20294;&#36890;&#24120;&#35757;&#32451;&#22797;&#26434;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#31471;&#21040;&#31471;&#30417;&#30563;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;E-SHARC&#12290;E-SHARC&#26041;&#27861;&#20351;&#29992;&#21069;&#31471;mel-filterbank&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#23884;&#20837;&#25552;&#21462;&#22120;&#21644;GNN&#32858;&#31867;&#27169;&#22359;&#65292;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12289;&#24230;&#37327;&#23398;&#20064;&#21644;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;E-SHARC&#36824;&#36890;&#36807;&#22806;&#37096;&#37325;&#21472;&#26816;&#27979;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization, the task of segmenting an audio recording based on speaker identity, constitutes an important speech pre-processing step for several downstream applications. The conventional approach to diarization involves multiple steps of embedding extraction and clustering, which are often optimized in an isolated fashion. While end-to-end diarization systems attempt to learn a single model for the task, they are often cumbersome to train and require large supervised datasets. In this paper, we propose an end-to-end supervised hierarchical clustering algorithm based on graph neural networks (GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The E-SHARC approach uses front-end mel-filterbank features as input and jointly learns an embedding extractor and the GNN clustering module, performing representation learning, metric learning, and clustering with end-to-end optimization. Further, with additional inputs from an external overlap detector, the E-SHARC app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item><item><title>Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03468</link><description>&lt;p&gt;
&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#23545;&#20110;Bongard&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03468
&lt;/p&gt;
&lt;p&gt;
Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;Bongard&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#8220;&#25903;&#25345;&#8221;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#23545;&#20110;&#26032;&#30340;&#26597;&#35810;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#23427;&#26159;&#21542;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#30340;&#26234;&#21147;&#27979;&#35797;&#12290;&#22312;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;Bongard&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20165;&#36798;&#21040;&#20102;66%&#65288;&#20598;&#28982;&#20934;&#30830;&#29575;&#20026;50%&#65289;&#12290;&#20302;&#20934;&#30830;&#29575;&#36890;&#24120;&#24402;&#22240;&#20110;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21457;&#29616;&#31867;&#20284;&#20154;&#31867;&#31526;&#21495;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#32780;&#22833;&#21435;&#20102;&#20934;&#30830;&#24615;&#65306;&#23427;&#20204;&#27809;&#26377;&#23558;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21152;&#20837;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20174;&#21333;&#20010;&#25903;&#25345;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#19982;&#28041;&#21450;&#23545;&#35937;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;Bongard&#38382;&#39064;&#20013;&#30340;&#8220;&#20851;&#38190;&#27010;&#24565;&#8221;&#21482;&#33021;&#20351;&#29992;&#22810;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#21453;&#20363;&#26469;&#21306;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.05037</link><description>&lt;p&gt;
&#23558;&#20219;&#20309;&#20320;&#25551;&#36848;&#30340;&#20107;&#29289;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Separate Anything You Describe. (arXiv:2308.05037v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;AudioSep&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26597;&#35810;&#38899;&#39057;&#28304;&#20998;&#31163;&#65288;LASS&#65289;&#26159;&#35745;&#31639;&#21548;&#35273;&#22330;&#26223;&#20998;&#26512;&#65288;CASA&#65289;&#20013;&#30340;&#19968;&#31181;&#26032;&#33539; Paradigm&#12290;LASS&#26088;&#22312;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20174;&#38899;&#39057;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#30446;&#26631;&#22768;&#38899;&#65292;&#20026;&#25968;&#23383;&#38899;&#39057;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#30340;&#30028;&#38754;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;LASS&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#31163;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#20048;&#22120;&#65292;&#26377;&#38480;&#31867;&#21035;&#30340;&#38899;&#39057;&#20107;&#20214;&#65289;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#22312;&#24320;&#25918;&#22495;&#20013;&#20998;&#31163;&#38899;&#39057;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudioSep&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#24320;&#25918;&#39046;&#22495;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;AudioSep&#65292;&#24182;&#23545;&#20854;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;&#38899;&#39057;&#20107;&#20214;&#20998;&#31163;&#65292;&#20048;&#22120;&#20998;&#31163;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;AudioSep&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#20998;&#31163;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#38899;&#39057;&#26631;&#39064;&#25110;&#25991;&#23383;&#26631;&#31614;&#20316;&#20026;&#26597;&#35810;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-queried audio source separation (LASS) is a new paradigm for computational auditory scene analysis (CASA). LASS aims to separate a target sound from an audio mixture given a natural language query, which provides a natural and scalable interface for digital audio applications. Recent works on LASS, despite attaining promising separation performance on specific sources (e.g., musical instruments, limited classes of audio events), are unable to separate audio concepts in the open domain. In this work, we introduce AudioSep, a foundation model for open-domain audio source separation with natural language queries. We train AudioSep on large-scale multimodal datasets and extensively evaluate its capabilities on numerous tasks including audio event separation, musical instrument separation, and speech enhancement. AudioSep demonstrates strong separation performance and impressive zero-shot generalization ability using audio captions or text labels as queries, substantially outperfor
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#31181;&#32908;&#32905;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#65292;&#20026;&#21046;&#23450;&#24247;&#22797;&#21644;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.17614</link><description>&lt;p&gt;
&#35780;&#20272;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#25345;&#32493;&#32908;&#32905;&#30130;&#21171;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Estimating Continuous Muscle Fatigue For Multi-Muscle Coordinated Exercise: A Pilot Study. (arXiv:2303.17614v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#31181;&#32908;&#32905;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#65292;&#20026;&#21046;&#23450;&#24247;&#22797;&#21644;&#35757;&#32451;&#35745;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26085;&#24120;&#38203;&#28860;&#20013;&#32908;&#32905;&#30130;&#21171;&#31243;&#24230;&#20026;&#31934;&#30830;&#23450;&#21046;&#24247;&#22797;&#21644;&#20010;&#24615;&#21270;&#35757;&#32451;&#21058;&#37327;&#25552;&#20379;&#37325;&#35201;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;Metaverse&#30340;&#32972;&#26223;&#19979;&#12290;&#35780;&#20272;&#28041;&#21450;&#22810;&#32908;&#32905;&#21327;&#35843;&#36816;&#21160;&#30340;&#30130;&#21171;&#38656;&#35201;&#34920;&#31034;&#22810;&#32908;&#32905;&#26102;&#31354;&#36866;&#24212;&#30340;&#30130;&#21171;&#29305;&#24449;&#21644;&#25429;&#25417;&#30130;&#21171;&#26102;&#38388;&#28436;&#21464;&#36827;&#31243;&#30340;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32908;&#32905;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32908;&#32905;&#34917;&#20607;&#21644;&#33034;&#39635;&#27169;&#22359;&#28608;&#27963;&#21464;&#21270;&#30340;&#29305;&#24449;&#26469;&#25551;&#36848;&#30130;&#21171;&#65292;&#24182;&#36890;&#36807;&#29983;&#29702;&#22522;&#30784;&#27169;&#22411;&#20272;&#35745;&#25345;&#32493;&#24615;&#30130;&#21171;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#32908;&#32905;&#21327;&#21516;&#20998;&#25968;&#21644;&#33034;&#39635;&#27169;&#22359;&#23574;&#23792;&#20540;&#26041;&#24046;&#20316;&#20026;&#30130;&#21171;&#35825;&#23548;&#31070;&#32463;&#32908;&#32905;&#36866;&#24212;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#35270;&#20026;&#35266;&#27979;&#20540;&#65292;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#25429;&#25417;&#26102;&#38388;&#28436;&#21464;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#26080;&#30417;&#30563;&#20272;&#35745;&#31574;&#30053;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#32570;&#20047;&#30417;&#30563;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;8&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21327;&#21516;&#25260;&#33151;&#32451;&#20064;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26681;&#25454;&#31070;&#32463;&#32908;&#32905;&#29305;&#24449;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#25345;&#32493;&#24615;&#30130;&#21171;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the progression of muscle fatigue for daily exercises provides vital indicators for precise rehabilitation, personalized training dose, especially under the context of Metaverse. Assessing fatigue of multi-muscle coordination-involved daily exercises requires the neuromuscular features that represent the fatigue-induced characteristics of spatiotemporal adaptions of multiple muscles and the estimator that captures the time-evolving progression of fatigue. In this paper, we propose to depict fatigue by the features of muscle compensation and spinal module activation changes and estimate continuous fatigue by a physiological rationale model. First, we extract muscle synergy fractionation and the variance of spinal module spikings as features inspired by the prior of fatigue-induced neuromuscular adaptations. Second, we treat the features as observations and develop a Bayesian Gaussian process to capture the time-evolving progression. Third, we solve the issue of lacking supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03106</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21387;&#32553;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#12289;&#28040;&#32791;&#20869;&#23384;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#20869;&#23384;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;NN&#27169;&#22411;&#21387;&#32553;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#37327;&#21270;&#25972;&#20010;NN&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#36895;&#29575;&#65292;&#21363;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26059;&#36716;&#19981;&#21464;&#37327;&#26041;&#27861;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;RIQ&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;RIQ&#22312;&#39044;&#35757;&#32451;&#30340;VGG&#31264;&#23494;&#21644;&#20462;&#21098;&#27169;&#22411;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;19.4&#20493;&#21644;52.9&#20493;&#30340;&#21387;&#32553;&#27604;&#65292;&#31934;&#24230;&#38477;&#20302;&#23567;&#20110;0.4%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;\url{https://github.com/ehaleva/RIQ}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $&lt;0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;</description></item></channel></rss>