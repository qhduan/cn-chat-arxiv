<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19652</link><description>&lt;p&gt;
InterDreamer&#65306;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19652
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#24191;&#27867;&#30340;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#25991;&#26412;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#21160;&#20102;&#25991;&#26412;&#26465;&#20214;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#24310;&#20280;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#29983;&#25104;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#20132;&#20114;&#25968;&#25454;&#21644;&#19982;&#36825;&#20123;&#20132;&#20114;&#19968;&#33268;&#30340;&#20840;&#38754;&#25551;&#36848;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#34892;&#21160;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#36825;&#19968;&#28857;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#21487;&#20197;&#35299;&#32806;&#12290;&#26080;&#27861;&#36890;&#36807;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#20132;&#20114;&#35821;&#20041;&#65292;&#25105;&#20204;&#36716;&#32780;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#36816;&#21160;&#27169;&#22411;&#30340;&#30693;&#35782;&#30456;&#36741;&#30456;&#25104;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#23545;&#20132;&#20114;&#35821;&#20041;&#30340;&#39640;&#32423;&#25511;&#21046;&#65292;&#20294;&#19981;&#33021;&#25552;&#20379;&#21040;&#19981;&#25104;&#23545;&#20132;&#20114;&#25991;&#26412;&#30340;&#30452;&#25509;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 Announce Type: cross  Abstract: Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14151</link><description>&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#35843;&#26597;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25277;&#35937;&#65306;&#36712;&#36857;&#35745;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#21644;&#25366;&#25496;&#65292;&#22240;&#20854;&#22312;&#35832;&#22914;&#20301;&#32622;&#26381;&#21153;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#38754;&#20020;&#22797;&#26434;&#35745;&#31639;&#12289;&#26377;&#38480;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#36275;&#20197;&#36866;&#24212;&#29616;&#23454;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36712;&#36857;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65288;DL4Traj&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#31616;&#35201;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#31649;&#29702;&#65288;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65289;&#21644;&#25366;&#25496;&#65288;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#39044;&#27979;&#12289;&#36712;&#36857;&#30456;&#20851;&#30340;&#25512;&#33616;&#12289;&#36712;&#36857;&#20998;&#31867;&#12289;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#12289;&#24322;&#24120;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 Announce Type: cross  Abstract: Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detecti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13327</link><description>&lt;p&gt;
&#20026;&#38544;&#31169;&#20445;&#25252;&#21487;&#31359;&#25140;&#21387;&#21147;&#26816;&#27979;&#29983;&#25104;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection. (arXiv:2401.13327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#34920;&#30340;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26234;&#33021;&#20581;&#24247;&#24212;&#29992;&#21644;&#24739;&#32773;&#30417;&#27979;&#20013;&#36234;&#26469;&#36234;&#34987;&#20351;&#29992;&#65292;&#21253;&#25324;&#21387;&#21147;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#21307;&#30103;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#20197;&#36827;&#34892;&#30740;&#31350;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#27880;&#38544;&#31169;&#30340;&#21512;&#25104;&#22810;&#20256;&#24863;&#22120;&#26234;&#33021;&#25163;&#34920;&#20581;&#24247;&#35835;&#25968;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#20197;&#20445;&#25252;&#24739;&#32773;&#20449;&#24687;&#12290;&#20026;&#20102;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#36136;&#37327;&#35780;&#20272;&#65292;&#24182;&#30417;&#27979;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#20854;&#26377;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24120;&#29992;&#20294;&#35268;&#27169;&#36739;&#23567;&#30340;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#31169;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#22522;&#30784;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16146</link><description>&lt;p&gt;
T-COL: &#20026;&#21487;&#21464;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#29983;&#25104;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems. (arXiv:2309.16146v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;T-COL&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#33021;&#22815;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#30340;&#21407;&#22240;&#65292;&#36824;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#36890;&#36807;&#23558;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#65292;&#20197;&#21450;&#37319;&#29992;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;T-COL&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#25361;&#25112;&#24182;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#12290;CEs&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#20204;&#19981;&#20165;&#35299;&#37322;&#20026;&#20160;&#20040;&#20250;&#39044;&#27979;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#65292;&#36824;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;CEs&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#21644;&#21487;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#29992;&#25143;&#20559;&#22909;&#24448;&#24448;&#26159;&#19968;&#33324;&#24615;&#30340;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;CEs&#38656;&#35201;&#26681;&#25454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#21464;&#24615;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;&#36825;&#20123;&#39564;&#35777;&#27169;&#22411;&#21457;&#29983;&#21464;&#21270;&#26102;&#20173;&#28982;&#20445;&#25345;&#20581;&#22766;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21487;&#33021;&#39564;&#35777;&#30340;&#19968;&#33324;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;CEs&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;T-COL&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#20004;&#31181;&#21487;&#36873;&#32467;&#26500;&#21644;&#20960;&#32452;&#21327;&#21516;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based systems have been suffering a lack of interpretability. To address this problem, counterfactual explanations (CEs) have been proposed. CEs are unique as they provide workable suggestions to users, in addition to explaining why a certain outcome was predicted. However, the application of CEs has been hindered by two main challenges, namely general user preferences and variable ML systems. User preferences, in particular, tend to be general rather than specific feature values. Additionally, CEs need to be customized to suit the variability of ML models, while also maintaining robustness even when these validation models change. To overcome these challenges, we propose several possible general user preferences that have been validated by user research and map them to the properties of CEs. We also introduce a new method called \uline{T}ree-based \uline{C}onditions \uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and several groups of co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.11434</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;&#38544;&#31169;&#65306;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;COVID-19&#26816;&#27979;&#30340;&#31169;&#26377;&#21270;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version). (arXiv:2211.11434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;COVID-19&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#38544;&#31169;&#30340;&#35780;&#20272;&#65292;&#32467;&#35770;&#34920;&#26126;&#25152;&#38656;&#30340;&#38544;&#31169;&#31561;&#32423;&#21487;&#33021;&#22240;&#21463;&#21040;&#23454;&#38469;&#23041;&#32961;&#30340;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21487;&#20197;&#36890;&#36807;&#20351;&#22823;&#37327;&#22270;&#20687;&#24555;&#36895;&#31579;&#36873;&#26469;&#24110;&#21161;&#25239;&#20987;COVID-19&#31561;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#24739;&#32773;&#38544;&#31169;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35201;&#27714;&#30340;ML&#27169;&#22411;&#12290;&#20197;&#24448;&#25506;&#32034;&#31169;&#26377;COVID-19&#27169;&#22411;&#30340;&#30740;&#31350;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#22522;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#36739;&#24369;&#25110;&#19981;&#26126;&#30830;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#19988;&#27809;&#26377;&#30740;&#31350;&#23454;&#38469;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#25913;&#36827;&#25514;&#26045;&#20197;&#35299;&#20915;&#36825;&#20123;&#31354;&#32570;&#12290;&#25105;&#20204;&#32771;&#34385;&#22825;&#29983;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#35780;&#20272;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#20197;&#21450;&#26356;&#20005;&#26684;&#30340;&#38544;&#31169;&#39044;&#31639;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24471;&#21040;&#40657;&#30418;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#23454;&#38469;&#38544;&#31169;&#20272;&#35745;&#25903;&#25345;&#12290;&#24341;&#20837;&#30340;DP&#24212;&#26377;&#21161;&#20110;&#38480;&#21046;MIAs&#24102;&#26469;&#30340;&#27844;&#28431;&#23041;&#32961;&#65292;&#32780;&#25105;&#20204;&#30340;&#23454;&#38469;&#20998;&#26512;&#26159;&#31532;&#19968;&#20010;&#22312;COVID-19&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat 
&lt;/p&gt;</description></item></channel></rss>