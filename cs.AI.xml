<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01095</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20351;&#29992;&#22810;&#23569;&#20010;&#35270;&#22270;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many views does your deep neural network use for prediction?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;Allen-Zhu&#21644;Li&#65288;2023&#65289;&#24341;&#20837;&#20102;&#22810;&#35270;&#22270;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#38598;&#25104;&#25110;&#33976;&#39311;&#27169;&#22411;&#65292;&#24182;&#26410;&#35752;&#35770;&#29992;&#20110;&#29305;&#23450;&#36755;&#20837;&#39044;&#27979;&#30340;&#22810;&#35270;&#22270;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#30495;&#23454;&#22270;&#20687;&#12290;MSVs&#26159;&#36755;&#20837;&#20013;&#30340;&#19968;&#32452;&#26368;&#23567;&#19988;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#20445;&#30041;&#20102;&#27169;&#22411;&#23545;&#35813;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#27169;&#22411;&#65288;&#21253;&#25324;&#21367;&#31215;&#21644;&#36716;&#25442;&#27169;&#22411;&#65289;&#30340;MSV&#25968;&#37327;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#20851;&#31995;&#65292;&#36825;&#34920;&#26126;&#22810;&#35270;&#22270;&#30340;&#35282;&#24230;&#23545;&#20110;&#29702;&#35299;&#65288;&#38750;&#38598;&#25104;&#25110;&#38750;&#33976;&#39311;&#65289;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu &amp; Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00306</link><description>&lt;p&gt;
&#21033;&#29992;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#31532;&#19968;&#27493;&#24377;&#24615;&#25514;&#26045; &#8212;&#8212; &#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25216;&#26415;&#22312;&#25552;&#39640;&#20379;&#24212;&#38142;&#24377;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;4.0&#21644;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#25512;&#33616;&#31995;&#32479; (RS) &#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#25552;&#21319;&#20379;&#24212;&#38142;&#24377;&#24615;&#30340;&#24037;&#20855;&#34987;&#24573;&#35270;&#65292;&#20294;&#20174;&#24212;&#21464;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;RS &#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#20840;&#26032;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#39564;&#35777;&#20102;&#27010;&#24565;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#25514;&#26045;&#22312;&#31532;&#19968;&#38454;&#27573;&#24471;&#21040;&#23454;&#26045;&#65292;&#24182;&#24110;&#21161;&#20379;&#24212;&#38142;&#21442;&#19982;&#32773;&#22312;&#20379;&#24212;&#38142;&#20013;&#26029;&#20043;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00306v1 Announce Type: cross  Abstract: Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of Recommender systems (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent recommender system techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#65292;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#36807;&#31243;&#24182;&#25972;&#21512;&#19987;&#23478;&#25512;&#29702;&#65292;&#25552;&#39640;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18405</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18405
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#65292;&#33021;&#22815;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#36807;&#31243;&#24182;&#25972;&#21512;&#19987;&#23478;&#25512;&#29702;&#65292;&#25552;&#39640;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#30456;&#20851;&#21028;&#20915;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20934;&#30830;&#21028;&#26029;&#20004;&#20010;&#27861;&#24459;&#26696;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38656;&#35201;&#38405;&#35835;&#20887;&#38271;&#30340;&#25991;&#26412;&#24182;&#20855;&#22791;&#39640;&#27700;&#24179;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#20197;&#25552;&#21462;&#27861;&#24459;&#20107;&#23454;&#24182;&#20316;&#20986;&#21496;&#27861;&#21028;&#26029;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;LLM&#65288;Large Language Models&#65289;&#36827;&#34892;&#30456;&#20851;&#24615;&#21028;&#26029;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#19968;&#33324;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20013;&#21487;&#38752;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#36718;&#24037;&#20316;&#27969;&#31243;&#65292;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#30340;&#30456;&#20851;&#21028;&#26029;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#31243;&#23558;&#27880;&#37322;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#27880;&#37322;&#32773;&#25152;&#20351;&#29992;&#30340;&#36807;&#31243;&#65292;&#24182;&#20351;&#19987;&#23478;&#25512;&#29702;&#33021;&#22815;&#28789;&#27963;&#22320;&#25972;&#21512;&#20197;&#22686;&#24378;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18405v1 Announce Type: new  Abstract: Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16014</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#35268;&#27169;&#19978;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Flexible Machine Learning Models for Scientific Computing at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16014
&lt;/p&gt;
&lt;p&gt;
OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1 Announce Type: cross  Abstract: Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering ap
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;</title><link>https://arxiv.org/abs/2310.18940</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#36827;&#34892;&#25112;&#30053;&#23545;&#25112;
&lt;/p&gt;
&lt;p&gt;
Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18940
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#30340;&#20195;&#29702;&#22312;&#21508;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#32431;LLM&#20195;&#29702;&#24448;&#24448;&#34920;&#29616;&#20986;&#22266;&#26377;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#26469;&#28304;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#21319;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#29436;&#20154;&#26432;&#20316;&#20026;&#20855;&#26377;&#22810;&#26679;&#27807;&#36890;&#21644;&#25112;&#30053;&#28216;&#25103;&#29609;&#27861;&#30340;&#25361;&#25112;&#27979;&#35797;&#24179;&#21488;&#12290;&#20026;&#20102;&#20943;&#36731;&#35821;&#35328;&#34892;&#20026;&#20013;&#30340;&#22266;&#26377;&#20559;&#35265;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;LLM&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#24182;&#29983;&#25104;&#22810;&#26679;&#34892;&#20026;&#20505;&#36873;&#38598;&#12290;&#28982;&#21518;&#65292;&#32463;&#36807;&#35757;&#32451;&#20197;&#20248;&#21270;&#20915;&#31574;&#33021;&#21147;&#30340;RL&#31574;&#30053;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18940v3 Announce Type: replace  Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidat
&lt;/p&gt;</description></item></channel></rss>