<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00944</link><description>&lt;p&gt;
NCoder -- &#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NCoder -- A Quantum Field Theory approach to encoding data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00944
&lt;/p&gt;
&lt;p&gt;
NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#65288;QFT&#65289;&#30340;&#21487;&#35299;&#37322; AI &#26041;&#27861;&#65292;&#31216;&#20026; NCoder&#12290;NCoder &#26159;&#19968;&#20010;&#20462;&#25913;&#36807;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#28508;&#22312;&#23618;&#34987;&#35268;&#23450;&#20026;&#19968;&#20010; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#12290;&#23558;&#22270;&#20687;&#35270;&#20026;&#26684;&#28857;&#22330;&#35770;&#30340;&#25277;&#26679;&#65292;&#36825;&#20010;&#26550;&#26500;&#27169;&#25311;&#20102;&#20351;&#29992;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26469;&#25200;&#21160;&#22320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#20219;&#21153;&#12290;&#25110;&#32773;&#65292;NCoder &#21487;&#20197;&#34987;&#35270;&#20026;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#29992;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#27719;&#24635;&#32479;&#35745;&#37327;&#65288;&#36825;&#37324;&#26159; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#65289;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#26469;&#29983;&#25104;&#26679;&#26412;&#22806;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;NCoder &#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NCoder &#22312;&#25968;&#25454;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.04588</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20840;&#29699;&#24037;&#20316;&#31354;&#38388;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#38646;&#23556;&#20987;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#22810;&#31181;&#24863;&#23448;&#24863;&#30693;&#19990;&#30028;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#32508;&#21512;&#22320;&#34920;&#36798;&#21608;&#22260;&#29615;&#22659;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#27867;&#21270;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#8220;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#8221;&#30340;&#35748;&#30693;&#31185;&#23398;&#27010;&#24565;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#21450;&#22312;&#26426;&#22120;&#20154;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#23558;&#20854;&#24212;&#29992;&#20110;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04588v1 Announce Type: new  Abstract: Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired multimodal re
&lt;/p&gt;</description></item><item><title>MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10969</link><description>&lt;p&gt;
MacroSwarm: &#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32452;&#21512;&#26694;&#26550;&#29992;&#20110;&#32676;&#20307;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
MacroSwarm: A Field-based Compositional Framework for Swarm Programming. (arXiv:2401.10969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10969
&lt;/p&gt;
&lt;p&gt;
MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#34892;&#20026;&#24037;&#31243;&#26159;&#19968;&#39033;&#26088;&#22312;&#30740;&#31350;&#21327;&#35843;&#31616;&#21333;&#26234;&#33021;&#20307;&#22242;&#20307;&#20869;&#35745;&#31639;&#21644;&#34892;&#21160;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#22914;&#22270;&#26696;&#24418;&#25104;&#12289;&#38598;&#20307;&#31227;&#21160;&#12289;&#32858;&#31867;&#21644;&#20998;&#24067;&#24335;&#24863;&#30693;&#12290;&#23613;&#31649;&#22312;&#32676;&#20307;&#65288;&#26080;&#20154;&#26426;&#12289;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#65289;&#20998;&#26512;&#21644;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#36890;&#29992;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#23450;&#20041;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#20026;&#20102;&#23545;&#27492;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;MacroSwarm&#65292;&#20197;&#21487;&#37325;&#29992;&#19988;&#23436;&#20840;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;&#23884;&#20837;&#38598;&#20307;&#35745;&#31639;&#21644;&#21327;&#35843;&#12290;&#22522;&#20110;&#38598;&#25104;&#35745;&#31639;&#30340;&#23439;&#32534;&#31243;&#33539;&#24335;&#65292;MacroSwarm&#25552;&#20986;&#20102;&#23558;&#27599;&#20010;&#32676;&#20307;&#34892;&#20026;&#22359;&#34920;&#31034;&#20026;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#30340;&#32431;&#20989;&#25968;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01310</link><description>&lt;p&gt;
EPIC: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#30340;&#22270;&#24418;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01310
&lt;/p&gt;
&lt;p&gt;
EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#22270;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#32463;&#24120;&#38480;&#21046;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EPIC&#65288;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#32534;&#36753;&#36317;&#31163;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#32467;&#26500;&#26377;&#25152;&#21464;&#21270;&#30340;&#26032;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24102;&#26631;&#31614;&#30340;&#22270;&#26469;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#22312;&#21407;&#22987;&#22270;&#23545;&#20043;&#38388;&#21019;&#24314;&#20102;&#22270;&#32534;&#36753;&#36335;&#24452;&#12290;&#36890;&#36807;&#20174;&#22270;&#32534;&#36753;&#36335;&#24452;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#20016;&#23500;&#20102;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
&lt;/p&gt;</description></item></channel></rss>