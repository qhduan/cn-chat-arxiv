<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04577</link><description>&lt;p&gt;
Wiki-TabNER:&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25512;&#36827;&#34920;&#26684;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#32593;&#32476;&#34920;&#26684;&#21253;&#21547;&#22823;&#37327;&#23453;&#36149;&#30693;&#35782;&#65292;&#28608;&#21457;&#20102;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#35299;&#37322;&#65288;TI&#65289;&#20219;&#21153;&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;TI&#20219;&#21153;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#35813;&#25968;&#25454;&#38598;&#36807;&#20110;&#31616;&#21270;&#65292;&#21487;&#33021;&#38477;&#20302;&#20854;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26410;&#20934;&#30830;&#20195;&#34920;&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22806;&#35266;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27880;&#37322;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#20171;&#32461;&#26032;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26032;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#19968;&#26032;&#30340;TI&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#25552;&#31034;LLMs&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#20102;&#38543;&#26426;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 Announce Type: new  Abstract: Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random
&lt;/p&gt;</description></item><item><title>AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09830</link><description>&lt;p&gt;
AutoPlanBench: &#20174;PDDL&#33258;&#21160;&#29983;&#25104;LLM&#35268;&#21010;&#22120;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09830
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#36923;&#36753;-&#27010;&#29575;&#27169;&#22411;&#65289;&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPlanBench&#65292;&#19968;&#31181;&#23558;PDDL&#20013;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#20182;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
&lt;/p&gt;</description></item></channel></rss>