<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00530</link><description>&lt;p&gt;
&#23558;&#22351;&#33529;&#26524;&#19982;&#22909;&#27224;&#23376;&#36827;&#34892;&#27604;&#36739;&#65306;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20559;&#22909;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#36890;&#36807;&#27604;&#36739;&#22312;&#22266;&#23450;&#19978;&#19979;&#25991;&#20013;&#26465;&#20214;&#29983;&#25104;&#30340;&#22810;&#20010;&#29983;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29983;&#25104;&#25918;&#32622;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#36825;&#20165;&#21033;&#29992;&#20102;&#25104;&#23545;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26465;&#20214;&#25490;&#21517;&#36890;&#24120;&#26080;&#27861;&#25429;&#33719;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#21644;&#22810;&#32500;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20559;&#22909;&#33719;&#21462;&#30340;&#20256;&#32479;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#25351;&#20196;-&#21709;&#24212;&#23545;&#19978;&#32852;&#21512;&#24341;&#21457;&#20559;&#22909;&#30340;&#26032;&#36724;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#20559;&#22909;&#20248;&#21270;&#26159;&#38024;&#23545;&#26465;&#20214;&#25490;&#21517;&#21327;&#35758;&#65288;&#20363;&#22914;&#65292;DPO&#65289;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#33719;&#21462;&#21327;&#35758;&#24341;&#20837;&#20102;DOVE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20559;&#22909;&#20248;&#21270;&#30446;&#26631;&#65292;&#36890;&#36807;&#25552;&#21319;&#25152;&#36873;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#26469;&#38477;&#20302;&#25152;&#25298;&#32477;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18205</link><description>&lt;p&gt;
Lemur: &#20351;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18205
&lt;/p&gt;
&lt;p&gt;
Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#20135;&#29983;&#30340;&#26085;&#24535;&#23545;&#30417;&#35270;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#26085;&#24535;&#20998;&#26512;&#26377;&#21161;&#20110;&#26816;&#27979;&#12289;&#25253;&#35686;&#21644;&#35786;&#26029;&#31995;&#32479;&#25925;&#38556;&#12290;&#26085;&#24535;&#35299;&#26512;&#26159;&#26085;&#24535;&#20998;&#26512;&#33258;&#21160;&#21270;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23427;&#28041;&#21450;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#27169;&#26495;&#12290;&#29616;&#26377;&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#30001;&#20110;&#20381;&#36182;&#20110;&#20154;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#32780;&#26080;&#27861;&#35782;&#21035;&#27491;&#30830;&#30340;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#32479;&#35745;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#26085;&#24535;&#28040;&#24687;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65288;Lemur&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#21160;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29109;&#21551;&#21457;&#30340;&#26032;&#22411;&#25277;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20856;&#22411;&#26085;&#24535;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#26085;&#24535;&#27169;&#26495;&#30340;&#21512;&#24182;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24605;&#32500;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18205v1 Announce Type: cross  Abstract: Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17853</link><description>&lt;p&gt;
&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65306;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#36895;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#31995;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#39640;&#32500;&#31163;&#25955;&#21270;&#22330;&#19978;&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#35758;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20351;&#29992;&#26356;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; - &#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#20013;&#65292;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23558;&#31995;&#32479;&#30340;&#20840;&#38454;&#34920;&#31034;&#25237;&#24433;&#21040;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#65292;&#25509;&#30528;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#27169;&#22411;&#26469;&#39044;&#27979;&#36825;&#20010;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#36825;&#31181;&#38477;&#38454;&#36807;&#31243;&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#20276;&#38543;&#32454;&#31890;&#24230;&#31163;&#25955;&#21270;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#31616;&#21270;&#20102;&#26102;&#38388;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#20197;&#21450;&#20960;&#31181;&#20854;&#20182;&#27969;&#34892;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#30456;&#21644;&#22810;&#30456;&#27969;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17853v1 Announce Type: cross  Abstract: Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows a
&lt;/p&gt;</description></item><item><title>NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.13809</link><description>&lt;p&gt;
NeuralDiffuser&#65306;&#20855;&#26377;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#30340;&#21487;&#25511;fMRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13809
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#20026;&#22823;&#33041;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#26816;&#32034;&#12290;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#37325;&#24314;&#32454;&#33410;&#30340;&#36830;&#36143;&#23545;&#40784;&#65288;&#22914;&#32467;&#26500;&#12289;&#32972;&#26223;&#12289;&#32441;&#29702;&#12289;&#39068;&#33394;&#31561;&#65289;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;LDM&#20063;&#20250;&#29983;&#25104;&#19981;&#21516;&#30340;&#22270;&#20687;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22522;&#20110;LDM&#30340;&#31070;&#32463;&#31185;&#23398;&#35270;&#35282;&#65292;&#21363;&#22522;&#20110;&#26469;&#33258;&#28023;&#37327;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36827;&#34892;&#33258;&#19978;&#32780;&#19979;&#30340;&#21019;&#24314;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#32454;&#33410;&#39537;&#21160;&#30340;&#33258;&#19979;&#32780;&#19978;&#24863;&#30693;&#65292;&#23548;&#33268;&#32454;&#33410;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralDiffuser&#65292;&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#20197;&#28176;&#21464;&#24418;&#24335;&#25552;&#20379;&#32454;&#33410;&#32447;&#32034;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#37325;&#22797;&#37325;&#24314;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13809v1 Announce Type: cross  Abstract: Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08640</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#39044;&#27979;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Forecasting high-impact research topics via machine learning on evolving knowledge graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26500;&#25104;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#23427;&#36843;&#20351;&#30740;&#31350;&#32773;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#26356;&#29421;&#31364;&#30340;&#23376;&#39046;&#22495;&#19978;&#65292;&#20351;&#24471;&#21457;&#29616;&#20854;&#20182;&#39046;&#22495;&#30340;&#26032;&#39062;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#21512;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21150;&#27861;&#39044;&#27979;&#31185;&#23398;&#35770;&#25991;&#26410;&#26469;&#30340;&#24341;&#29992;&#27425;&#25968;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#21040;&#30740;&#31350;&#23436;&#25104;&#24182;&#19988;&#35770;&#25991;&#20889;&#25104;&#21518;&#25165;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26679;&#23601;&#38169;&#36807;&#20102;&#24819;&#27861;&#26500;&#24605;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#27979;&#20174;&#26410;&#34987;&#30740;&#31350;&#32773;&#21457;&#24067;&#30340;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#12290;&#23427;&#32467;&#21512;&#20102;&#20174;&#35770;&#25991;&#20869;&#23481;&#20013;&#21019;&#24314;&#30340;&#35821;&#20041;&#32593;&#32476;&#21644;&#20174;&#21382;&#21490;&#24341;&#29992;&#20013;&#21019;&#24314;&#30340;&#24433;&#21709;&#32593;&#32476;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#28436;&#21270;&#32593;&#32476;&#30340;&#21160;&#24577;&#24773;&#20917;&#65292;&#20174;&#32780;&#39044;&#27979;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#39044;&#26399;&#36825;&#31181;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#32773;&#21457;&#29616;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06941</link><description>&lt;p&gt;
DEFormer: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#21644;&#26263;&#35270;&#35273;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#32454;&#33410;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#39640;&#32423;&#35270;&#35273;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;RGB&#39046;&#22495;&#24456;&#38590;&#24674;&#22797;&#26263;&#21306;&#22495;&#30340;&#20002;&#22833;&#32454;&#33410;&#12290;&#26412;&#25991;&#23558;&#39057;&#29575;&#20316;&#20026;&#32593;&#32476;&#30340;&#26032;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#29992;&#20110;&#39057;&#29575;&#22686;&#24378;&#65292;&#21253;&#25324;DCT&#22788;&#29702;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#12290;CFE&#35745;&#31639;&#27599;&#20010;&#36890;&#36947;&#30340;&#26354;&#29575;&#20197;&#34920;&#31034;&#19981;&#21516;&#39057;&#29575;&#24102;&#30340;&#32454;&#33410;&#20016;&#23500;&#24230;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#39057;&#29575;&#29305;&#24449;&#21010;&#20998;&#20026;&#26356;&#20016;&#23500;&#32441;&#29702;&#30340;&#39057;&#29575;&#24102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;RGB&#39046;&#22495;&#21644;&#39057;&#29575;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;DEFormer&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;DEFormer&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05764</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270; (ECG) &#26159;&#19968;&#31181;&#24191;&#27867;&#21487;&#29992;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#24555;&#36895;&#21644;&#32463;&#27982;&#39640;&#25928;&#22320;&#35780;&#20272;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#35786;&#26029;&#20013;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#26114;&#36149;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391; (CMR) &#25104;&#20687;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#26816;&#26597;&#12290;&#34429;&#28982; CMR &#25104;&#20687;&#21487;&#20197;&#25552;&#20379;&#35814;&#32454;&#30340;&#24515;&#33039;&#35299;&#21078;&#21487;&#35270;&#21270;&#65292;&#20294;&#30001;&#20110;&#38271;&#26102;&#38388;&#25195;&#25551;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#65292;&#23427;&#24182;&#19981;&#24191;&#27867;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#26041;&#27861;&#65292;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#23631;&#34109;&#25968;&#25454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;40044&#21517;UK Biobank&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#25105;&#20204;&#39044;&#27979;&#20102;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#65292;&#24182;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#30830;&#23450;&#20102;&#19981;&#21516;&#30340;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05412</link><description>&lt;p&gt;
&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#23398;&#20064;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#31561;&#26041;&#24335;&#34987;&#24212;&#29992;&#21040;&#34920;&#29616;&#33391;&#22909;&#21644;&#34920;&#29616;&#24046;&#30340;&#34892;&#21160;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#31574;&#30053;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#29992;&#20110;&#23558;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#32622;&#20110;&#26356;&#39057;&#32321;&#30340;&#35775;&#38382;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#33021;&#22815;&#24341;&#36215;&#34892;&#20026;&#31574;&#30053;&#30340;&#25913;&#21892;&#65292;&#24403;&#31574;&#30053;&#32422;&#26463;&#21040;&#36825;&#20010;&#25913;&#36827;&#30340;&#31574;&#30053;&#19978;&#26102;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#21487;&#33021;&#24471;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#23454;&#29992;&#31574;&#30053;&#26469;&#33719;&#24471;&#22522;&#20110;&#25311;&#21512;&#20540;&#32593;&#32476;&#30340;&#20248;&#20808;&#26435;&#37325;&#65288;OPER-A&#65289;&#25110;&#32773;u
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2301.01828</link><description>&lt;p&gt;
&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30495;&#23454;&#21518;&#39564;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#26080;&#27861;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#35823;&#24046;&#21644;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#20063;&#20250;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#65292;&#20197;&#38450;&#27490;&#36807;&#21435;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#27979;&#35797;&#26159;&#21542;&#26377;&#35775;&#38382;&#30495;&#23454;&#21518;&#39564;&#30340;&#20445;&#35777;&#21487;&#20197;&#38450;&#27490;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#26679;&#26412;&#25311;&#21512;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#23558;&#21518;&#39564;&#20256;&#25773;&#20026;&#26032;&#20219;&#21153;&#30340;&#20808;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#35777;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25191;&#34892;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20998;&#26512;&#31034;&#20363;&#65292;&#24182;&#24378;&#35843;&#20102;&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#38382;&#39064;&#65292;&#21363;&#20351;&#36827;&#34892;&#20102;&#20934;&#30830;&#30340;&#25512;&#26029;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20219;&#21153;&#25968;&#25454;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.06652</link><description>&lt;p&gt;
&#39640;&#32500;&#31354;&#38388;&#20013;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Differentiable Inductive Logic Programming in High-Dimensional Space. (arXiv:2208.06652v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#21487;&#24494;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#65292;&#20197;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21495;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#21512;&#25104;&#22823;&#22411;&#36923;&#36753;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#20013;&#38388;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20869;&#28085;&#35859;&#35789;&#26434;&#20081;&#22320;&#21344;&#25454;&#20551;&#35774;&#31354;&#38388;&#36890;&#24120;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#26799;&#24230;&#19979;&#38477;&#25552;&#20379;&#20102;&#22312;&#36825;&#20123;&#39640;&#32500;&#31354;&#38388;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31070;&#32463;&#31526;&#21495;ILP&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#25193;&#23637;{\delta}ILP&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#30340;&#24402;&#32435;&#21512;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#39640;&#32500;&#26799;&#24230;&#19979;&#38477;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#35859;&#35789;&#21457;&#26126;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21463;&#30410;&#20110;&#21487;&#24494;&#24402;&#32435;&#21512;&#25104;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#36229;&#20986;&#29616;&#26377;&#31070;&#32463;&#31526;&#21495;ILP&#31995;&#32479;&#33021;&#21147;&#30340;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19981;&#25351;&#23450;&#35299;&#20915;&#26041;&#26696;&#30340;&#31934;&#30830;&#32467;&#26500;&#30340;&#35821;&#35328;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing large logic programs through symbolic Inductive Logic Programming (ILP) typically requires intermediate definitions. However, cluttering the hypothesis space with intensional predicates typically degrades performance. In contrast, gradient descent provides an efficient way to find solutions within such high- dimensional spaces. Neuro-symbolic ILP approaches have not fully exploited this so far. We propose extending the {\delta}ILP approach to inductive synthesis with large-scale predicate invention, thus allowing us to exploit the efficacy of high-dimensional gradient descent. We show that large-scale predicate invention benefits differentiable inductive synthesis through gradient descent and allows one to learn solutions for tasks beyond the capabilities of existing neuro-symbolic ILP systems. Furthermore, we achieve these results without specifying the precise structure of the solution within the language bias.
&lt;/p&gt;</description></item></channel></rss>