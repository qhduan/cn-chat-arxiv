<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#23450;&#20041;&#20197;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00046
&lt;/p&gt;
&lt;p&gt;
SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LLMs&#20197;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#20294;&#23454;&#38469;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#36739;&#24046;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#35843;&#25972;LLMs&#20197;&#36866;&#24212;&#26032;&#22330;&#26223;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#24403;&#21069;&#20195;&#30721;&#29983;&#25104;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21363;Sample-Efficient adaptation with Error-Driven learning for code generation&#12290;SEED&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#21033;&#29992;&#38169;&#35823;&#20462;&#35746;&#26469;&#20811;&#26381;&#33258;&#36523;&#32570;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEED&#28041;&#21450;&#35782;&#21035;LLMs&#29983;&#25104;&#30340;&#38169;&#35823;&#20195;&#30721;&#65292;&#20351;&#29992;Self-revise&#36827;&#34892;&#20195;&#30721;&#20462;&#35746;&#65292;&#20248;&#21270;&#27169;&#22411;&#24182;&#36845;&#20195;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item></channel></rss>