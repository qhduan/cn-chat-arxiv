<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07376</link><description>&lt;p&gt;
NavCoT: &#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#25552;&#21319;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#20316;&#20026;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#20215;&#20540;&#30340;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#36523;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#31359;&#36234;&#22797;&#26434;&#30340;3D&#29615;&#22659;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;VLN&#20013;&#25552;&#39640;&#23548;&#33322;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#31163;&#32447;&#26041;&#24335;&#19979;&#30340;&#20351;&#29992;&#36890;&#24120;&#22312;VLN&#20219;&#21153;&#21644;LLM&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#36973;&#21463;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23548;&#33322;&#24605;&#32500;&#38142;(NavCoT)&#30340;&#26032;&#22411;&#31574;&#30053;&#65292;&#25105;&#20204;&#36890;&#36807;&#23436;&#25104;&#39046;&#22495;&#20869;&#39640;&#25928;&#21442;&#25968;&#35757;&#32451;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#39046;&#22495;&#24046;&#36317;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;LLM&#34987;&#25552;&#31034;&#36890;&#36807;&#20316;&#20026;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#23548;&#33322;&#24605;&#32500;&#38142;&#65306;1)&#26681;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
&lt;/p&gt;</description></item><item><title>GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.08855</link><description>&lt;p&gt;
GhostWriter:&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20889;&#20316;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08855
&lt;/p&gt;
&lt;p&gt;
GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#20889;&#20316;&#36741;&#21161;&#26041;&#38754;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#19988;&#20855;&#26377;&#26080;&#22788;&#19981;&#22312;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#24615;&#21270;&#21644;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#65292;LLM&#39537;&#21160;&#30340;&#20889;&#20316;&#31995;&#32479;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#24863;&#21040;&#27822;&#20007;&#65292;&#24403;&#29992;&#25143;&#32570;&#20047;&#25552;&#31034;&#24037;&#31243;&#32463;&#39564;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#21152;&#21095;&#12290;&#25105;&#20204;&#35748;&#20026;&#35774;&#35745;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20043;&#19968;&#65292;&#24182;&#24341;&#20837;GhostWriter&#65292;&#36825;&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#30340;&#20195;&#29702;&#21644;&#20010;&#24615;&#21270;&#26469;&#36827;&#34892;&#20889;&#20316;&#12290;GhostWriter&#21033;&#29992;LLMs&#22312;&#29992;&#25143;&#32534;&#20889;&#30340;&#36807;&#31243;&#20013;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#25152;&#26399;&#26395;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#21516;&#26102;&#20801;&#35768;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#36827;&#34892;&#26174;&#24335;&#25945;&#23398;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;18&#21517;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20889;&#20316;&#20219;&#21153;&#20013;&#20351;&#29992;GhostWriter&#65292;&#35266;&#23519;&#21040;&#23427;&#24110;&#21161;&#29992;&#25143;&#32534;&#20889;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#22810;&#31181;&#26041;&#24335;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#26469;&#22686;&#24378;&#29992;&#25143;&#30340;&#33021;&#21147;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08855v1 Announce Type: cross Abstract: Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. From this study, we present insights re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.06196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06196
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#33258;2022&#24180;11&#26376;ChatGPT&#21457;&#24067;&#20197;&#26469;&#12290;LLMs&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#21313;&#20159;&#21442;&#25968;&#26469;&#33719;&#24471;&#24191;&#27867;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#31526;&#21512;&#32553;&#25918;&#23450;&#24459;&#30340;&#39044;&#27979;&#12290;LLMs&#30340;&#30740;&#31350;&#39046;&#22495;&#23613;&#31649;&#38750;&#24120;&#26032;&#65292;&#20294;&#22312;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#26368;&#33879;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;&#19977;&#20010;&#27969;&#34892;&#30340;LLM&#31995;&#21015;&#65288;GPT&#12289;LLaMA&#12289;PaLM&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#36129;&#29486;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;LLM&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#20934;&#22791;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#65292;&#23457;&#26597;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#27969;&#34892;LLM&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
&lt;/p&gt;</description></item><item><title>&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00248</link><description>&lt;p&gt;
&#23558;&#8220;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#8221;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00248
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#20854;&#20998;&#21106;&#33945;&#29256;&#32570;&#20047;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;SAM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#23454;&#29616;&#39640;&#24230;&#31934;&#30830;&#30340;&#23545;&#35937;&#20998;&#21106;&#65288;&#21363;&#31216;&#20026;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;DIS&#65289;&#25265;&#26377;&#24456;&#39640;&#26399;&#26395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIS-SAM&#65292;&#23558;SAM&#25512;&#36827;&#33267;DIS&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#31934;&#30830;&#32454;&#33410;&#12290;DIS-SAM&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#39640;&#24230;&#20934;&#30830;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;SAM&#30340;&#21487;&#20419;&#36827;&#35774;&#35745;&#12290;DIS-SAM&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;SAM&#19982;&#19987;&#38376;&#29992;&#20110;DIS&#30340;&#20462;&#25913;&#21518;&#30340;IS-Net&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;DIS-SAM&#30456;&#27604;SAM&#21644;HQ-SA&#34920;&#29616;&#20986;&#26174;&#30528;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Notation3&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#37096;&#20998;Notation3&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Notation3&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07332</link><description>&lt;p&gt;
Notation3&#20316;&#20026;&#19968;&#31181;&#23384;&#22312;&#35268;&#21017;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Notation3 as an Existential Rule Language. (arXiv:2308.07332v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Notation3&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#37096;&#20998;Notation3&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Notation3&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Notation3&#36923;&#36753;&#65288;\nthree&#65289;&#26159;RDF&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#29992;&#25143;&#32534;&#20889;&#24341;&#20837;&#26032;&#30340;&#31354;&#30333;&#33410;&#28857;&#21040;RDF&#22270;&#20013;&#30340;&#35268;&#21017;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#26412;&#20307;&#26144;&#23556;&#65289;&#20381;&#36182;&#20110;&#27492;&#21151;&#33021;&#65292;&#22240;&#20026;&#31354;&#30333;&#33410;&#28857;&#22312;Web&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#30452;&#25509;&#20351;&#29992;&#25110;&#20316;&#20026;&#36741;&#21161;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#28085;&#30422;&#35813;&#36923;&#36753;&#38750;&#24120;&#37325;&#35201;&#21151;&#33021;&#30340;&#24555;&#36895;\nthree&#25512;&#29702;&#22120;&#30340;&#25968;&#37327;&#30456;&#23545;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;VLog&#25110;Nemo&#20043;&#31867;&#30340;&#24341;&#25806;&#19981;&#30452;&#25509;&#25903;&#25345;&#35821;&#20041;Web&#35268;&#21017;&#26684;&#24335;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#20026;&#38750;&#24120;&#30456;&#20284;&#30340;&#26500;&#36896;&#65288;&#23384;&#22312;&#35268;&#21017;&#65289;&#24320;&#21457;&#21644;&#20248;&#21270;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31354;&#30333;&#33410;&#28857;&#30340;\nthree&#35268;&#21017;&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;\nthree&#23376;&#38598;&#65292;&#24182;&#23450;&#20041;&#20102;&#36825;&#26679;&#19968;&#20010;&#26144;&#23556;&#65292;&#20445;&#25345;&#20102;\nthree&#20844;&#24335;&#30340;&#31561;&#20215;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;\nthree&#25512;&#29702;&#21487;&#20197;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#36716;&#25442;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26144;&#23556;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Notation3 Logic (\nthree) is an extension of RDF that allows the user to write rules introducing new blank nodes to RDF graphs. Many applications (e.g., ontology mapping) rely on this feature as blank nodes -- used directly or in auxiliary constructs -- are omnipresent on the Web. However, the number of fast \nthree reasoners covering this very important feature of the logic is rather limited. On the other hand, there are engines like VLog or Nemo which do not directly support Semantic Web rule formats but which are developed and optimized for very similar constructs: existential rules. In this paper, we investigate the relation between \nthree rules with blank nodes in their heads and existential rules. We identify a subset of \nthree which can be mapped directly to existential rules and define such a mapping preserving the equivalence of \nthree formulae. In order to also illustrate that in some cases \nthree reasoning could benefit from our translation, we then employ this mapping i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2303.12032</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;DLMs&#65289;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#30001;&#20110;&#21151;&#33021;&#21644;&#20851;&#31995;&#27010;&#24565;&#30340;&#28151;&#28102;&#65292;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#36825;&#24847;&#21619;&#30528;&#21547;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#34429;&#28982;DLM&#20197;&#20851;&#31995;&#24847;&#20041;&#19978;&#30340;&#34920;&#24449;&#20854;&#30446;&#26631;&#65292;&#20294;&#26368;&#22909;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#24182;&#24341;&#23548;&#21746;&#23398;&#20851;&#27880;DLM&#34920;&#24449;&#30340;&#29702;&#24819;&#21270;&#24615;&#36136;&#21450;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
&lt;/p&gt;</description></item></channel></rss>