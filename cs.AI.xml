<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.07887</link><description>&lt;p&gt;
&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65306;&#22312;&#26032;&#20852;&#30340;&#27133;&#34920;&#31034;&#20013;&#25509;&#22320;&#23545;&#35937;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#26041;&#27861;&#22312;&#23558;&#21407;&#22987;&#24863;&#30693;&#26080;&#30417;&#30563;&#20998;&#35299;&#20026;&#20016;&#23500;&#30340;&#31867;&#20284;&#29289;&#20307;&#30340;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#25509;&#22320;&#21040;&#23398;&#21040;&#30340;&#25277;&#35937;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#29702;&#35299;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#23427;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#12290;NSI&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31867;&#20284;XML&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#27861;&#35268;&#21017;&#23558;&#22330;&#26223;&#30340;&#29289;&#20307;&#35821;&#20041;&#32452;&#32455;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#21407;&#35821;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#21452;&#23618;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#31243;&#24207;&#21407;&#35821;&#25509;&#22320;&#21040;&#27133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;NSI&#31243;&#24207;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#40784;&#27169;&#22411;&#25512;&#26029;&#30340;&#23494;&#38598;&#20851;&#32852;&#20174;&#27133;&#29983;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#12290;&#22312;&#21452;&#27169;&#24335;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07887v1 Announce Type: cross  Abstract: Object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. However, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. We present the Neural Slot Interpreter (NSI) that learns to ground and generate object semantics via slot representations. At the core of NSI is an XML-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. Then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. Finally, we formulate the NSI program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. Experiments on bi-modal retrie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17410</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#29992;&#20110;&#22122;&#22768;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20026;MRI&#37325;&#24314;&#20013;&#30340;&#22270;&#20687;&#22495;&#25554;&#20540;&#24320;&#21457;&#22810;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#20256;&#25773;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#20540;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#22312;&#20613;&#37324;&#21494;&#22495;&#65288;&#20063;&#31216;&#20026;k&#31354;&#38388;&#65289;&#20013;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#19982;&#28608;&#27963;&#25513;&#27169;&#30340;&#36880;&#20803;&#32032;&#20056;&#27861;&#12290;&#36825;&#31181;&#25805;&#20316;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#36716;&#25442;&#20026;&#21367;&#31215;&#12290;&#22312;k&#31354;&#38388;&#32593;&#32476;&#35757;&#32451;&#21518;&#65292;&#36825;&#31181;&#26041;&#27861;&#20026;&#30456;&#23545;&#20110;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#30340;&#23548;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20195;&#25968;&#34920;&#36798;&#24335;&#65292;&#36825;&#20123;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#20316;&#20026;&#22270;&#20687;&#31354;&#38388;&#20013;&#32593;&#32476;&#30340;&#36755;&#20837;&#24352;&#37327;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#29992;&#20110;&#25551;&#36848;&#22122;&#22768;&#29305;&#24615;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17410v1 Announce Type: cross  Abstract: Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.14362</link><description>&lt;p&gt;
&#25171;&#23383;&#30103;&#27861;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#24037;&#20855;&#65292;&#20294;&#26377;&#35777;&#25454;&#34920;&#26126;&#36890;&#29992;&#22411;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#20063;&#23384;&#22312;&#19968;&#23450;&#39118;&#38505;&#65292;&#22914;&#26524;&#35774;&#35745;&#19981;&#36127;&#36131;&#20219;&#21487;&#33021;&#20250;&#21361;&#21450;&#29992;&#25143;&#30340;&#31119;&#31049;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#30495;&#23454;&#32463;&#21382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#32972;&#26223;&#30340;21&#20010;&#20010;&#20154;&#36827;&#34892;&#35775;&#35848;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#20182;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#22635;&#34917;&#26085;&#24120;&#25252;&#29702;&#30340;&#31354;&#30333;&#65292;&#24182;&#22312;&#23547;&#27714;&#26469;&#33258;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25903;&#25345;&#26102;&#22914;&#20309;&#23548;&#33322;&#30456;&#20851;&#30340;&#25991;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#25991;&#29486;&#20013;&#26377;&#25928;&#25903;&#25345;&#30340;&#27010;&#24565;&#65292;&#24182;&#24341;&#20837;&#20102;AI&#19982;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#30340;&#27835;&#30103;&#20215;&#20540;&#35266;&#23545;&#20854;&#36827;&#34892;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22914;&#20309;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica
&lt;/p&gt;</description></item></channel></rss>