<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01292</link><description>&lt;p&gt;
&#36808;&#21521;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#35777;&#25454;&#25903;&#25345;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#30340;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20043;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#21628;&#21505;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;&#35780;&#20215;&#22411;AI&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#36827;&#34892;&#20551;&#35774;&#39537;&#21160;&#30340;XAI&#65292;&#35813;&#26694;&#26550;&#20026;&#20154;&#20204;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#65292;&#32780;&#19981;&#19968;&#23450;&#32473;&#20986;&#20915;&#31574;&#36741;&#21161;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#65288;WoE&#65289;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;XAI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#32473;&#23450;&#30340;&#20551;&#35774;&#29983;&#25104;&#27491;&#38754;&#21644;&#36127;&#38754;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#19982;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#21644;&#20165;AI&#35299;&#37322;&#22522;&#32447;&#30456;&#27604;&#20943;&#23569;&#20102;&#20381;&#36182;&#31243;&#24230;&#65292;&#20294;&#30456;&#23545;&#20110;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#65292;&#22312;&#20381;&#36182;&#31243;&#24230;&#19979;&#38477;&#26041;&#38754;&#30053;&#24494;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#19982;&#32773;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#26102;&#19982;&#20004;&#20010;&#22522;&#32447;&#30340;&#26041;&#24335;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.
&lt;/p&gt;</description></item></channel></rss>