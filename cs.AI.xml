<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2312.16903</link><description>&lt;p&gt;
&#21035;&#20877;&#20986;&#29616;&#23574;&#23792;&#20102;&#65306;&#31283;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Spike No More: Stabilizing the Pre-training of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.16903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23574;&#23792;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25214;&#20986;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#32463;&#24120;&#20986;&#29616;&#25439;&#22833;&#23574;&#23792;&#12290;&#36825;&#20123;&#23574;&#23792;&#20250;&#38477;&#20302;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#24212;&#35813;&#36991;&#20813;&#36825;&#31181;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;&#20026;&#20102;&#30740;&#31350;&#25439;&#22833;&#23574;&#23792;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#37096;&#23618;&#30340;&#26799;&#24230;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26799;&#24230;&#29190;&#28856;&#30340;&#20004;&#20010;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#38450;&#26799;&#24230;&#29190;&#28856;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#21021;&#22987;&#21270;&#26041;&#27861;&#21644;&#23545;&#23884;&#20837;&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#26469;&#28385;&#36275;&#35201;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#23574;&#23792;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. To investigate the cause of loss spikes, we focus on gradients of internal layers. Through theoretical analyses, we reveal two causes of the exploding gradients, and provide requirements to prevent the explosion. In addition, we propose a method to satisfy the requirements by combining the initialization method and a simple modification to embeddings. We conduct various experiments to verify our theoretical analyses empirically. Experimental results indicate that the combination is effective in preventing spikes during pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item></channel></rss>