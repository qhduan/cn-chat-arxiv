<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AlphaMapleSAT&#26159;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#25512;&#29702;&#39537;&#21160;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#26469;&#39640;&#25928;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13770</link><description>&lt;p&gt;
AlphaMapleSAT&#65306;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems. (arXiv:2401.13770v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13770
&lt;/p&gt;
&lt;p&gt;
AlphaMapleSAT&#26159;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#25512;&#29702;&#39537;&#21160;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#26469;&#39640;&#25928;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AlphaMapleSAT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Monte Carlo Tree Search (MCTS)&#30340;Cube-and-Conquer (CnC) SAT&#27714;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#23613;&#31649;CnC&#27714;&#35299;&#22120;&#22312;&#35299;&#20915;&#21508;&#31181;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22810;&#24180;&#26469;&#65292;CnC&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#21457;&#23637;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24456;&#38590;&#25552;&#20986;&#26082;&#20302;&#25104;&#26412;&#21448;&#33021;&#26377;&#25928;&#22320;&#23558;&#36755;&#20837;&#20844;&#24335;&#20998;&#21106;&#20026;&#23376;&#20844;&#24335;&#30340;&#26032;&#22411;&#20998;&#21106;&#25216;&#26415;&#65292;&#20174;&#32780;&#20351;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#26368;&#23567;&#21270;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;CnC&#27714;&#35299;&#22120;&#65288;&#22914;March&#65289;&#20351;&#29992;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#36890;&#36807;&#32422;&#26463;&#25628;&#32034;&#26368;&#20248;&#20998;&#21106;&#21464;&#37327;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#39537;&#21160;&#30340;MCTS&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#65292;&#36890;&#36807;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#23547;&#25214;&#26377;&#25928;&#30340;&#20998;&#21106;&#65292;&#21516;&#26102;&#20351;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23545;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS) based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving challenging combinatorial problems. Despite the tremendous success of CnC solvers in solving a variety of hard combinatorial problems, the lookahead cubing techniques at the heart of CnC have not evolved much for many years. Part of the reason is the sheer difficulty of coming up with new cubing techniques that are both low-cost and effective in partitioning input formulas into sub-formulas, such that the overall runtime is minimized.  Lookahead cubing techniques used by current state-of-the-art CnC solvers, such as March, keep their cubing costs low by constraining the search for the optimal splitting variables. By contrast, our key innovation is a deductively-driven MCTS-based lookahead cubing technique, that performs a deeper heuristic search to find effective cubes, while keeping the cubing cost low. We perform an extensive compari
&lt;/p&gt;</description></item><item><title>DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10061</link><description>&lt;p&gt;
DiffusionGPT: &#22522;&#20110;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10061
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20026;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#23548;&#33268;&#20102;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#20849;&#20139;&#39640;&#36136;&#37327;&#27169;&#22411;&#30340;&#24191;&#27867;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#24448;&#24448;&#26080;&#27861;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#65292;&#25110;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#32479;&#19968;&#23581;&#35797;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#27491;&#20132;&#26041;&#38754;&#65306;i&#65289;&#22312;&#36755;&#20837;&#38454;&#27573;&#35299;&#26512;&#22810;&#26679;&#30340;&#25552;&#31034;&#65307;ii&#65289;&#28608;&#27963;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#36755;&#20986;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionGPT&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;DiffusionGPT&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#20026;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;Thought&#26641;&#12290;&#24403;&#25552;&#20379;&#36755;&#20837;&#26102;&#65292;LLM&#35299;&#26512;&#25552;&#31034;&#24182;&#21033;&#29992;Thought&#26641;&#26469;&#25351;&#23548;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25918;&#26494;&#36755;&#20837;&#32422;&#26463;&#24182;&#30830;&#20445;&#24322;&#24120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02317</link><description>&lt;p&gt;
GNN2R: &#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#29702;&#30001;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02317
&lt;/p&gt;
&lt;p&gt;
GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#21482;&#25552;&#20379;&#26368;&#32456;&#30340;&#30830;&#23450;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#35299;&#37322;&#65292;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#21644;&#26597;&#30475;&#30340;KG&#23454;&#20307;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65288;GNN2R&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNN2R&#33021;&#22815;&#36890;&#36807;&#20165;&#26377;&#30340;&#38382;&#39064;-&#26368;&#32456;&#31572;&#26696;&#23545;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#32972;&#21518;&#30340;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#65292;&#19988;&#20165;&#38656;&#35201;&#36890;&#36807;&#24369;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;GNN2R&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item></channel></rss>