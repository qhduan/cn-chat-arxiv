<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20256;&#32479;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#22312;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21644;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02914</link><description>&lt;p&gt;
DynST&#65306;&#36164;&#28304;&#21463;&#38480;&#26102;&#31354;&#39044;&#27979;&#30340;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02914
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#22312;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21644;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#26381;&#21153;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#23613;&#31649;&#20026;&#38754;&#21521;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29699;&#31185;&#23398;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36335;&#24452;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#22320;&#29699;&#31995;&#32479;&#25968;&#25454;&#65292;&#20294;&#36825;&#20063;&#32473;&#23427;&#20204;&#30340;&#24037;&#19994;&#32423;&#37096;&#32626;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#22320;&#29702;&#21644;&#31038;&#20250;&#22240;&#32032;&#65292;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#25910;&#38598;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20351;&#24471;&#23454;&#29616;&#20840;&#38754;&#35206;&#30422;&#21644;&#32479;&#19968;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38556;&#30861;&#65292;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#37096;&#32626;&#26041;&#27861;&#21033;&#29992;&#29305;&#23450;&#31639;&#27861;&#35774;&#35745;&#21644;&#37096;&#32626;&#20256;&#24863;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#20256;&#24863;&#22120;&#30340;&#28608;&#27963;&#26102;&#38388;&#65292;&#20197;&#20248;&#21270;&#23545;&#27599;&#20010;&#23376;&#21306;&#22495;&#30340;&#26816;&#27979;&#36807;&#31243;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#21644;&#22320;&#29702;&#29305;&#24449;&#21046;&#23450;&#28608;&#27963;&#31574;&#30053;&#65292;&#36825;&#20123;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#27169;&#22411;&#26082;&#19981;&#31616;&#21333;&#20063;&#19981;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02914v1 Announce Type: new  Abstract: The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.02797</link><description>&lt;p&gt;
PeFoMed&#65306;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#36827;&#21270;&#25193;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24212;&#23545;&#36229;&#36234;&#32431;&#25991;&#26412;&#24212;&#29992;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#20102;&#20808;&#21069;&#32534;&#30721;&#22312;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;MLLMs&#36866;&#24212;&#20026;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Med-VQA&#65289;&#20219;&#21153;&#30340;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Med-VQA&#24212;&#29992;&#29305;&#21035;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#20026;&#20102;&#20934;&#30830;&#34913;&#37327;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#19978;&#36798;&#21040;&#20102;81.9&#65285;&#65292;&#19988;&#20854;&#30456;&#23545;&#20110;GPT-4v&#27169;&#22411;&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item></channel></rss>