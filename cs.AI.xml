<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09034</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#26041;Sigmoid TanH (SST)&#28608;&#27963;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#25552;&#39640;&#39034;&#24207;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#21069;&#39304;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65292;&#20294;&#26159;&#39034;&#24207;&#27169;&#22411;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20173;&#28982;&#20381;&#36182;&#20110;Sigmoid&#21644;TanH&#28608;&#27963;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#24120;&#24120;&#22312;&#35757;&#32451;&#22312;&#23567;&#39034;&#24207;&#25968;&#25454;&#38598;&#19978;&#26102;&#38590;&#20197;&#24314;&#27169;&#31232;&#30095;&#27169;&#24335;&#20197;&#26377;&#25928;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#21035;&#38024;&#23545;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#22686;&#24378;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#12290;SST&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#26469;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#38543;&#30528;&#20449;&#21495;&#38543;&#26102;&#38388;&#20256;&#25773;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;SST&#30340;LSTM&#21644;GRU&#27169;&#22411;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09034v1 Announce Type: cross Abstract: Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.04687</link><description>&lt;p&gt;
&#25913;&#36827;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Attacks on Latent Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545; Latent Diffusion Model (LDM)&#65292;&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#38450;&#27490; LDM &#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#24694;&#24847;&#24494;&#35843;&#30340;&#20445;&#25252;&#25163;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25915;&#20987;&#20250;&#23545; LDM &#39044;&#27979;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#35780;&#20998;&#20989;&#25968;&#28155;&#21152;&#39069;&#22806;&#30340;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#36890;&#36807;&#19968;&#20010;&#20559;&#24046;&#38477;&#20302;&#35823;&#24046;&#65292;&#20174;&#32780;&#36973;&#21463;&#25915;&#20987;&#24182;&#20351;&#29992;&#20559;&#24046;&#39044;&#27979;&#35780;&#20998;&#20989;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19968;&#33268;&#24471;&#20998;&#20989;&#25968;&#38169;&#35823;&#36827;&#34892;&#25915;&#20987;&#65288;ACE&#65289;&#26469;&#25913;&#36827; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#12290;ACE &#32479;&#19968;&#20102;&#28155;&#21152;&#21040;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#39069;&#22806;&#35823;&#24046;&#30340;&#27169;&#24335;&#12290;&#36825;&#20419;&#20351;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#19982;&#23545;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#39044;&#27979;&#30340;&#20559;&#24046;&#23398;&#20064;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04687v3 Announce Type: replace-cross  Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.   Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04566</link><description>&lt;p&gt;
Knolling Bot: &#20174;&#25972;&#27905;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#35937;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#65306;arXiv:2310.04566v2  &#20844;&#21578;&#31867;&#22411;&#65306;replace-cross  &#25688;&#35201;&#65306;&#35299;&#20915;&#23478;&#24237;&#31354;&#38388;&#20013;&#25955;&#20081;&#29289;&#21697;&#30340;&#25972;&#29702;&#25361;&#25112;&#21463;&#21040;&#25972;&#27905;&#24615;&#30340;&#22810;&#26679;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#22797;&#26434;&#24615;&#24433;&#21709;&#12290;&#27491;&#22914;&#20154;&#31867;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20801;&#35768;&#21516;&#19968;&#29702;&#24565;&#30340;&#22810;&#31181;&#34920;&#36798;&#19968;&#26679;&#65292;&#23478;&#24237;&#25972;&#27905;&#20559;&#22909;&#21644;&#32452;&#32455;&#27169;&#24335;&#21464;&#21270;&#24191;&#27867;&#65292;&#22240;&#27492;&#39044;&#35774;&#29289;&#20307;&#20301;&#32622;&#23558;&#38480;&#21046;&#23545;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#27905;&#24067;&#23616;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#31867;&#20284;&#20110;&#20351;&#29992;&#20250;&#35805;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21518;&#32493;&#29289;&#20307;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#8220;&#25972;&#29702;&#8221;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#26800;&#33218;&#21644;RGB&#30456;&#26426;&#22312;&#26700;&#23376;&#19978;&#32452;&#32455;&#19981;&#21516;&#22823;&#23567;&#21644;&#25968;&#37327;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09051</link><description>&lt;p&gt;
&#22797;&#26434;&#38382;&#31572;&#21644;&#35821;&#35328;&#27169;&#22411;&#28151;&#21512;&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#26631;&#20934;&#38382;&#39064;&#19978;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20294;&#22312;&#35299;&#20915;&#26356;&#20855;&#20307;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65288;&#22914;&#22312;&#19981;&#21516;&#25991;&#21270;&#20013;&#20010;&#20154;&#33258;&#30001;&#27010;&#24565;&#30340;&#21464;&#21270;&#22914;&#20309;&#65311;&#20160;&#20040;&#26159;&#20026;&#20943;&#23569;&#27668;&#20505;&#21464;&#21270;&#32780;&#23454;&#29616;&#30340;&#26368;&#20339;&#21457;&#30005;&#26041;&#27861;&#32452;&#21512;&#65311;&#65289;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#26550;&#26500;&#12289;&#30693;&#35782;&#12289;&#25216;&#33021;&#12289;&#26041;&#27861;&#12289;&#25935;&#24863;&#25968;&#25454;&#20445;&#25252;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#31867;&#23457;&#25209;&#21644;&#22810;&#21151;&#33021;&#21453;&#39304;&#12290;&#26368;&#36817;&#30340;&#39033;&#30446;&#22914;ChatGPT&#21644;GALACTICA&#20801;&#35768;&#38750;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;LLM&#22312;&#22797;&#26434;QA&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#21516;&#31561;&#24378;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#35780;&#20272;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;LLM&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;AI/ML&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#36825;&#20123;CQA&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and an
&lt;/p&gt;</description></item></channel></rss>