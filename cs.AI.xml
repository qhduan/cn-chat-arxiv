<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.15751</link><description>&lt;p&gt;
&#31232;&#30095;MeZO&#65306;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#36890;&#24120;&#20250;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#32780;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#21033;&#29992;&#23384;&#20648;&#22120;&#30340;&#38646;&#38454;&#65288;MeZO&#65289;&#20248;&#21270;&#22120;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#20869;&#23384;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#24448;&#24448;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19982;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;MeZO&#20173;&#28982;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#21040;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;MeZO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#23558;ZO&#24212;&#29992;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#36873;&#25321;&#26041;&#26696;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
&lt;/p&gt;</description></item><item><title>TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.09807</link><description>&lt;p&gt;
TKN&#65306;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#20851;&#38190;&#28857;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction. (arXiv:2303.09807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09807
&lt;/p&gt;
&lt;p&gt;
TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#29992;&#36884;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36807;&#20110;&#24378;&#35843;&#20934;&#30830;&#24615;&#65292;&#24573;&#35270;&#20102;&#30001;&#20110;&#36807;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#32780;&#23548;&#33268;&#30340;&#36739;&#24930;&#30340;&#39044;&#27979;&#36895;&#24230;&#20197;&#21450;&#36807;&#22810;&#30340;&#20887;&#20313;&#20449;&#24687;&#23398;&#20064;&#21644;GPU&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TKN&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;TKN&#26159;&#25105;&#20204;&#30446;&#21069;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#20445;&#25345;&#20854;&#20182;&#24615;&#33021;&#12290;&#22312;KTH&#21644;Human Action 3D&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;TKN&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video prediction is a complex time-series forecasting task with great potential in many use cases. However, conventional methods overemphasize accuracy while ignoring the slow prediction speed caused by complicated model structures that learn too much redundant information with excessive GPU memory consumption. Furthermore, conventional methods mostly predict frames sequentially (frame-by-frame) and thus are hard to accelerate. Consequently, valuable use cases such as real-time danger prediction and warning cannot achieve fast enough inference speed to be applicable in reality. Therefore, we propose a transformer-based keypoint prediction neural network (TKN), an unsupervised learning method that boost the prediction process via constrained information extraction and parallel prediction scheme. TKN is the first real-time video prediction solution to our best knowledge, while significantly reducing computation costs and maintaining other performance. Extensive experiments on KTH and Hum
&lt;/p&gt;</description></item></channel></rss>