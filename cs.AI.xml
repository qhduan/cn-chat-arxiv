<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17213</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;
&lt;/p&gt;
&lt;p&gt;
VCD: Knowledge Base Guided Visual Commonsense Discovery in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#24120;&#35782;&#21253;&#21547;&#26377;&#20851;&#23545;&#35937;&#23646;&#24615;&#12289;&#20851;&#31995;&#21644;&#34892;&#20026;&#30340;&#30693;&#35782;&#12290;&#21457;&#29616;&#35270;&#35273;&#24120;&#35782;&#21487;&#20197;&#25552;&#20379;&#23545;&#22270;&#20687;&#30340;&#26356;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#29702;&#35299;&#65292;&#24182;&#22686;&#24378;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#30740;&#31350;&#20013;&#25152;&#23450;&#20041;&#30340;&#35270;&#35273;&#24120;&#35782;&#26159;&#31895;&#31890;&#24230;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#24211;ConceptNet&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#31995;&#32479;&#22320;&#23450;&#20041;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#25152;&#21253;&#21547;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#24120;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;Visual Genome&#21644;ConceptNet&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;VCDD&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17213v1 Announce Type: cross  Abstract: Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore pro
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09769</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning Using a Single Forward Pass
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09769
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#21333;&#27425;&#20256;&#36882;&#23884;&#20837;&#23398;&#20064;&#31639;&#27861;&#65288;SPELA&#65289;&#12290; SPELA&#26159;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35774;&#22791;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#24212;&#29992;&#30340;&#39318;&#36873;&#20505;&#36873;&#20154;&#12290; &#21516;&#26102;&#65292;SPELA&#21487;&#20197;&#26368;&#20339;&#22320;&#28385;&#36275;&#23545;&#30740;&#31350;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#21644;&#24418;&#25104;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290; SPELA&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#22914;&#23884;&#20837;&#21521;&#37327;&#24418;&#24335;&#30340;&#31070;&#32463;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#38656;&#35201;&#26435;&#37325;&#20256;&#36755;&#65292;&#19981;&#38145;&#23450;&#26435;&#37325;&#26356;&#26032;&#65292;&#23436;&#20840;&#23616;&#37096;&#36203;&#27604;&#23433;&#23398;&#20064;&#65292;&#19981;&#23384;&#20648;&#28608;&#27963;&#30340;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#21644;&#27599;&#20010;&#26679;&#26412;&#30340;&#21333;&#27425;&#26435;&#37325;&#26356;&#26032;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;SPELA&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#26377;&#22122;&#38899;&#30340;&#24067;&#23572;&#36816;&#31639;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25191;&#34892;&#38750;&#32447;&#24615;&#20998;&#31867;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SPELA&#22312;MNIST&#65292;KMNIST&#21644;Fashion MNIST&#19978;&#30340;&#39640;&#24615;&#33021;&#34920;&#29616;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SPELA&#22312;MNIST&#65292;KMNIST&#21644;Fashion MNIST&#19978;&#30340;&#23569;&#26679;&#26412;&#21644;1&#20010;&#26102;&#26399;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09769v1 Announce Type: new  Abstract: We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual representation learning and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST. Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on MNIST, KMNIST, and Fashio
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09448</link><description>&lt;p&gt;
&#26222;&#36890;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09448
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#25552;&#21319;&#36816;&#21160;&#38556;&#30861;&#20010;&#20307;&#30340;BCI&#24212;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#26159;&#35299;&#35835;&#21644;&#35299;&#30721;&#21508;&#31181;&#25235;&#25569;&#21160;&#20316;&#65292;&#22914;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#21738;&#31181;EEG&#25216;&#26415;&#22312;&#22788;&#29702;&#21644;&#32763;&#35793;&#19982;&#25235;&#25569;&#30456;&#20851;&#30340;&#33041;&#30005;&#20449;&#21495;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;&#30740;&#31350;&#28041;&#21450;&#23545;&#21313;&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25569;&#25345;&#36816;&#21160;&#65306;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#65292;&#26080;&#36816;&#21160;&#26465;&#20214;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#35299;&#30721;&#25235;&#25569;&#21160;&#20316;&#26041;&#38754;&#23545;EEG&#21644;&#19977;&#26497;EEG&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#35813;&#27604;&#36739;&#28085;&#30422;&#20102;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#12289;&#36890;&#36807;&#21151;&#33021;&#36830;&#25509;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#28041;&#21450;&#20174;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09448v1 Announce Type: cross  Abstract: This study aims to enhance BCI applications for individuals with motor impairments by comparing the effectiveness of tripolar EEG (tEEG) with conventional EEG. The focus is on interpreting and decoding various grasping movements, such as power grasp and precision grasp. The goal is to determine which EEG technology is more effective in processing and translating grasp related neural signals. The approach involved experimenting on ten healthy participants who performed two distinct grasp movements: power grasp and precision grasp, with a no movement condition serving as the baseline. Our research presents a thorough comparison between EEG and tEEG in decoding grasping movements. This comparison spans several key parameters, including signal to noise ratio (SNR), spatial resolution via functional connectivity, ERPs, and wavelet time frequency analysis. Additionally, our study involved extracting and analyzing statistical features from th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07140</link><description>&lt;p&gt;
&#25991;&#23383;&#25551;&#36848;&#20013;&#30340;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07140
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22270;&#25512;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#26174;&#33879;&#24433;&#21709;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;42.22&#65285;&#25552;&#39640;&#21040;70&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19981;&#38543;&#22270;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;AI&#22522;&#20110;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35843;&#26597;&#21644;&#20351;&#29992;&#36965;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;GPT-4&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#24182;&#22312;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#21644;&#20010;&#24615;&#21270;&#19982;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08960</link><description>&lt;p&gt;
&#20174;&#29992;&#25143;&#35843;&#26597;&#21040;&#36965;&#27979;&#39537;&#21160;&#20195;&#29702;&#65306;&#25506;&#32034;&#20010;&#24615;&#21270;&#30340;&#29983;&#20135;&#21147;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
From User Surveys to Telemetry-Driven Agents: Exploring the Potential of Personalized Productivity Solutions. (arXiv:2401.08960v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;AI&#22522;&#20110;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35843;&#26597;&#21644;&#20351;&#29992;&#36965;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;GPT-4&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#24182;&#22312;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#21644;&#20010;&#24615;&#21270;&#19982;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20102;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#20135;&#21147;&#20195;&#29702;&#30340;&#20559;&#22909;&#65292;&#24182;&#24320;&#21457;&#20986;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;363&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25506;&#32034;&#20102;&#29983;&#20135;&#21147;&#12289;&#27807;&#36890;&#39118;&#26684;&#12289;&#20195;&#29702;&#26041;&#27861;&#12289;&#20010;&#24615;&#29305;&#24449;&#12289;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;&#20511;&#21161;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;Viva Insights&#25910;&#38598;&#30340;&#36965;&#27979;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#29983;&#20135;&#21147;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;GPT-4&#25552;&#20379;&#23450;&#21046;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;40&#21517;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#20202;&#34920;&#26495;&#21644;&#21465;&#36848;&#31561;&#26367;&#20195;&#30340;&#29983;&#20135;&#21147;&#36741;&#21161;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#12289;&#36866;&#24212;&#24615;&#20197;&#21450;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#22312;AI&#36741;&#21161;&#29983;&#20135;&#21147;&#24037;&#20855;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20511;&#37492;&#25105;&#20204;&#30740;&#31350;&#20013;&#25552;&#28860;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#21551;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive, user-centric approach to understand preferences in AI-based productivity agents and develop personalized solutions tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on the insights distilled from our study, we believe that our work can enable and guide futur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2401.04003</link><description>&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#22312;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#19979;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2401.04003v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#26426;&#22120;&#20154;&#35268;&#21010;&#19982;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#20027;&#35201;&#26159;&#22522;&#20110;&#38024;&#23545;&#20010;&#20307;&#25110;&#32676;&#20307;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#20844;&#24335;&#12290;&#20294;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;LTL&#20844;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#24471;&#20887;&#38271;&#65292;&#20351;&#35299;&#37322;&#21644;&#35268;&#33539;&#29983;&#25104;&#21464;&#24471;&#22797;&#26434;&#65292;&#21516;&#26102;&#36824;&#23545;&#35268;&#21010;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#36896;&#25104;&#21387;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;&#20855;&#26377;&#35821;&#27861;&#21644;&#35821;&#20041;&#38656;&#27714;&#30340;LTL&#35268;&#33539;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#27604;&#25153;&#24179;&#35268;&#33539;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;&#25628;&#32034;&#31354;&#38388;&#30001;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#36817;&#20284;&#34920;&#31034;&#65292;&#27599;&#20010;&#23376;&#31354;&#38388;&#23545;&#24212;&#19968;&#20010;LTL&#35268;&#33539;&#12290;&#25628;&#32034;&#20027;&#35201;&#21463;&#38480;&#20110;&#21333;&#20010;&#23376;&#31354;&#38388;&#65292;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research into robotic planning with temporal logic specifications, notably Linear Temporal Logic (LTL), was largely based on singular formulas for individual or groups of robots. But with increasing task complexity, LTL formulas unavoidably grow lengthy, complicating interpretation and specification generation, and straining the computational capacities of the planners. By leveraging the intrinsic structure of tasks, we introduced a hierarchical structure to LTL specifications with requirements on syntax and semantics, and proved that they are more expressive than their flat counterparts. Second, we employ a search-based approach to synthesize plans for a multi-robot system, accomplishing simultaneous task allocation and planning. The search space is approximated by loosely interconnected sub-spaces, with each sub-space corresponding to one LTL specification. The search is predominantly confined to a single sub-space, transitioning to another sub-space under certain conditions, de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#27915;&#27969;&#23454;&#29616;&#39640;&#29983;&#38271;&#21306;&#22495;&#30340;&#25506;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.01916</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#36827;&#34892;&#21160;&#21147;&#32534;&#31243;&#30340;&#26080;&#25928;&#31995;&#32479;&#19978;&#30340;&#33258;&#20027;&#20892;&#22330;&#19978;&#30340;&#28023;&#34299;&#29983;&#38271;&#30340;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximizing Seaweed Growth on Autonomous Farms: A Dynamic Programming Approach for Underactuated Systems Navigating on Uncertain Ocean Currents. (arXiv:2307.01916v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01916
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#30340;&#28023;&#27915;&#27915;&#27969;&#20013;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#27915;&#27969;&#23454;&#29616;&#39640;&#29983;&#38271;&#21306;&#22495;&#30340;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34299;&#29983;&#29289;&#37327;&#22312;&#27668;&#20505;&#20943;&#32531;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#33258;&#20027;&#24320;&#25918;&#24335;&#28023;&#27915;&#20892;&#22330;&#26469;&#20805;&#20998;&#21033;&#29992;&#12290;&#36825;&#20123;&#20892;&#22330;&#36890;&#24120;&#20855;&#26377;&#20302;&#25512;&#36827;&#21147;&#65292;&#24182;&#21463;&#21040;&#28023;&#27915;&#27915;&#27969;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#19968;&#20010;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#32447;&#24615;&#26102;&#21464;&#30340;&#28023;&#27915;&#27915;&#27969;&#26469;&#36798;&#21040;&#39640;&#29983;&#38271;&#21306;&#22495;&#65292;&#20174;&#32780;&#22312;&#20960;&#20010;&#26376;&#20869;&#26368;&#22823;&#21270;&#28023;&#34299;&#29983;&#38271;&#12290;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#26080;&#25928;&#24615;&#20351;&#24471;&#21363;&#20351;&#30693;&#36947;&#27915;&#27969;&#24773;&#20917;&#65292;&#36825;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21482;&#26377;&#30701;&#26399;&#19981;&#23436;&#21892;&#30340;&#39044;&#27979;&#19988;&#19981;&#30830;&#23450;&#24615;&#36880;&#28176;&#22686;&#22823;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#30495;&#23454;&#27915;&#27969;&#24773;&#20917;&#26102;&#26377;&#25928;&#22320;&#27714;&#35299;&#26368;&#20248;&#29983;&#38271;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#25193;&#23637;&#65292;&#21363;&#22312;&#29616;&#23454;&#20013;&#21482;&#30693;&#36947;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65306;&#65288;1&#65289;&#25105;&#20204;&#26041;&#27861;&#24471;&#21040;&#30340;&#20540;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#21453;&#39304;&#31574;&#30053;&#65292;&#20197;&#33719;&#24471;&#25152;&#26377;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#26368;&#20339;&#29983;&#38271;&#25511;&#21046;&#65292;&#23454;&#29616;&#38381;&#29615;&#25511;&#21046;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seaweed biomass offers significant potential for climate mitigation, but large-scale, autonomous open-ocean farms are required to fully exploit it. Such farms typically have low propulsion and are heavily influenced by ocean currents. We want to design a controller that maximizes seaweed growth over months by taking advantage of the non-linear time-varying ocean currents for reaching high-growth regions. The complex dynamics and underactuation make this challenging even when the currents are known. This is even harder when only short-term imperfect forecasts with increasing uncertainty are available. We propose a dynamic programming-based method to efficiently solve for the optimal growth value function when true currents are known. We additionally present three extensions when as in reality only forecasts are known: (1) our methods resulting value function can be used as feedback policy to obtain the growth-optimal control for all states and times, allowing closed-loop control equival
&lt;/p&gt;</description></item></channel></rss>