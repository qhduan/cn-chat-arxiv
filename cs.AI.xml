<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;</title><link>https://arxiv.org/abs/2403.12952</link><description>&lt;p&gt;
&#21482;&#38656;&#36716;&#31227;&#23427;&#65306;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12952
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#22240;&#20026;&#39046;&#22495;&#36716;&#31227;&#32780;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20351;&#29992;&#26631;&#35760;&#27979;&#35797;&#36755;&#20837;&#26469;&#20351;VLM&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#35843;&#33410;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#24182;&#32531;&#23384;&#21407;&#22411;&#65292;TPS&#19981;&#20165;&#20419;&#36827;&#20102;&#26080;&#38656;&#20248;&#21270;&#30340;&#21407;&#22411;&#37325;&#29992;&#36827;&#34892;&#21518;&#32493;&#39044;&#27979;&#65292;&#36824;&#35753;&#20854;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#24403;&#21069;&#36827;&#23637;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;TPS&#20165;&#22522;&#20110;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12952v1 Announce Type: cross  Abstract: Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing class
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#21644;&#20998;&#26512;&#20027;&#39064;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SuCI&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#30340;&#21435;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;MIU&#27169;&#22411;&#21463;&#20027;&#20307;&#21464;&#24322;&#38382;&#39064;&#22256;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05025</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#39064;&#21435;&#30456;&#20851;&#23454;&#29616;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05025
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#21644;&#20998;&#26512;&#20027;&#39064;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SuCI&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#30340;&#21435;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;MIU&#27169;&#22411;&#21463;&#20027;&#20307;&#21464;&#24322;&#38382;&#39064;&#22256;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05025v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22810;&#27169;&#24577;&#24847;&#22270;&#29702;&#35299;(MIU)&#26159;&#20154;&#31867;&#34920;&#36798;&#20998;&#26512;(&#20363;&#22914;&#24773;&#24863;&#25110;&#24189;&#40664;)&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#28041;&#21450;&#35270;&#35273;&#23039;&#21183;&#12289;&#35821;&#35328;&#20869;&#23481;&#21644;&#22768;&#23398;&#34892;&#20026;&#31561;&#24322;&#26500;&#27169;&#24577;&#12290;&#29616;&#26377;&#24037;&#20316;&#22987;&#32456;&#19987;&#27880;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#32467;&#26500;&#25110;&#34701;&#21512;&#31574;&#30053;&#65292;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#21463;&#21040;&#20027;&#39064;&#21464;&#24322;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#23548;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#20855;&#26377;&#19981;&#21516;&#34920;&#36798;&#20064;&#24815;&#21644;&#29305;&#24449;&#30340;&#19981;&#21516;&#20027;&#39064;&#65292;MIU&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20197;&#23398;&#20064;&#29305;&#23450;&#20110;&#20027;&#39064;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26174;&#30528;&#38480;&#21046;&#20102;&#36328;&#26410;&#25509;&#35302;&#20027;&#39064;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#26469;&#21046;&#23450;MIU&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20027;&#39064;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SuCI&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22240;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05025v1 Announce Type: new  Abstract: Multimodal intention understanding (MIU) is an indispensable component of human expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviors. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MIU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, significantly limiting performance and generalizability across uninitiated subjects.Motivated by this observation, we introduce a recapitulative causal graph to formulate the MIU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02965</link><description>&lt;p&gt;
ChatGPT&#19982;&#29983;&#29289;&#35782;&#21035;&#25216;&#26415;&#65306;&#23545;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#33021;&#21147;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#39564;&#20102;ChatGPT&#22312;&#25191;&#34892;&#29983;&#29289;&#35782;&#21035;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#12290;&#30001;&#20110;&#29983;&#29289;&#35782;&#21035;&#34987;&#35270;&#20026;&#25935;&#24863;&#20449;&#24687;&#65292;ChatGPT&#36991;&#20813;&#22238;&#31572;&#30452;&#25509;&#25552;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#32469;&#36807;&#20854;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35780;&#20272;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#33021;&#22815;&#20197;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#35782;&#21035;&#38754;&#37096;&#36523;&#20221;&#24182;&#22312;&#20004;&#20010;&#38754;&#37096;&#22270;&#20687;&#20043;&#38388;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#24615;&#33021;&#26174;&#33879;&#65292;&#24182;&#23545;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#22312;&#29983;&#29289;&#35782;&#21035;&#20013;&#24212;&#29992;LLMs&#21644;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#24191;&#38420;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02965v1 Announce Type: cross  Abstract: This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00420</link><description>&lt;p&gt;
&#32463;&#30001;&#23545;&#25239;&#25915;&#20987;&#21644;&#35757;&#32451;&#30340;&#31283;&#20581;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#33258;&#20027;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36731;&#24494;&#26465;&#20214;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#29992;&#24615;&#65292;DRL&#24517;&#39035;&#23637;&#31034;&#20986;&#21487;&#20449;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#39640;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#31181;&#25913;&#36827;&#26041;&#24335;&#65292;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#38024;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#36866;&#24403;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#23545;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31995;&#32479;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#30446;&#26631;&#21644;&#25805;&#20316;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#31867;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#24674;&#22797;&#21147;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20174;&#32780;&#20026;&#24320;&#36767;DRL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36947;&#36335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00420v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents across various complex environments. Despite its significant performance in well known environments, it remains susceptible to minor conditions variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve robustness of DRL to unknown changes in the conditions is through Adversarial Training, by training the agent against well suited adversarial attacks on the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary adversarial attack methodologies, systematically categorizing them and comparing their objectives and operational mechanisms. This classification offers a detailed insight into how adversarial attacks effectively act for evaluating the resilience of DRL agents, thereby paving the 
&lt;/p&gt;</description></item><item><title>Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.16822</link><description>&lt;p&gt;
&#24425;&#34425;&#22242;&#38431;&#65306;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16822
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29702;&#35299;&#21644;&#22686;&#24378;&#23427;&#20204;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35782;&#21035;&#25932;&#23545;&#25552;&#31034;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25110;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24425;&#34425;&#22242;&#38431;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#24425;&#34425;&#22242;&#38431;&#23558;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#36136;&#37327; - &#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#26412;&#25991;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#30001;&#24425;&#34425;&#22242;&#38431;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.08728</link><description>&lt;p&gt;
AgentMixer: &#22810;&#26234;&#33021;&#20307;&#30456;&#20851;&#31574;&#30053;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
AgentMixer: Multi-Agent Correlated Policy Factorization. (arXiv:2401.08728v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08728
&lt;/p&gt;
&lt;p&gt;
AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#19982;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#38598;&#20013;&#24335;&#20540;&#20989;&#25968;&#26469;&#31283;&#23450;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#22522;&#20110;&#26412;&#22320;&#35266;&#27979;&#29420;&#31435;&#22320;&#20570;&#20915;&#31574;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#20855;&#26377;&#36275;&#22815;&#21327;&#35843;&#24615;&#30340;&#30456;&#20851;&#32852;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#21463;&#30456;&#20851;&#22343;&#34913;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#24341;&#20837;"&#31574;&#30053;&#20462;&#25913;"&#26469;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#21327;&#35843;&#31574;&#30053;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;AgentMixer&#65292;&#23558;&#32852;&#21512;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#26500;&#36896;&#20026;&#21508;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#25955;&#24335;&#25191;&#34892;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#32852;&#21512;&#31574;&#30053;&#26469;&#24471;&#21040;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#20223;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#32852;&#21512;&#31574;&#30053;&#21644;&#20010;&#20307;&#31574;&#30053;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#30340;&#38750;&#23545;&#31216;&#23398;&#20064;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is widely employed to stabilize partially observable multi-agent reinforcement learning (MARL) by utilizing a centralized value function during training. However, existing methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with sufficient coordination. Inspired by the concept of correlated equilibrium, we propose to introduce a \textit{strategy modification} to provide a mechanism for agents to correlate their policies. Specifically, we present a novel framework, AgentMixer, which constructs the joint fully observable policy as a non-linear combination of individual partially observable policies. To enable decentralized execution, one can derive individual policies by imitating the joint policy. Unfortunately, such imitation learning can lead to \textit{asymmetric learning failure} caused by the mismatch between joint policy and indi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15950</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15950
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;ID&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#23548;&#33268;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#22815;&#23500;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#21033;&#29992;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#20559;&#24046;&#65292;&#32473;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#32467;&#21512;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#23454;&#26045;&#36824;&#38656;&#35201;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#26088;&#22312;&#36890;&#36807;LLM&#24378;&#21270;&#34920;&#31034;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representati
&lt;/p&gt;</description></item><item><title>&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.02994</link><description>&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02994
&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#30340;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#29289;&#29702;&#23398;&#39044;&#35757;&#32451;&#65288;MPP&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#29289;&#29702;&#20195;&#29702;&#24314;&#27169;&#12290;MPP&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#20195;&#29702;&#27169;&#22411;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#24322;&#26500;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#23398;&#20064;&#22312;&#19981;&#21516;&#29289;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#36866;&#29992;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20849;&#20139;&#23884;&#20837;&#21644;&#24402;&#19968;&#21270;&#31574;&#30053;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#30340;&#23383;&#27573;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28041;&#21450;&#27969;&#20307;&#21147;&#23398;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21333;&#20010;MPP&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#33021;&#22815;&#22312;&#25152;&#26377;&#39044;&#35757;&#32451;&#23376;&#20219;&#21153;&#19978;&#19982;&#25110;&#36229;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25105;&#20204;&#35777;&#26126;&#24494;&#35843;MPP&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#23545;&#26032;&#29289;&#29702;&#30340;&#39044;&#27979;&#32467;&#26524;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from
&lt;/p&gt;</description></item></channel></rss>