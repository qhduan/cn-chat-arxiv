<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.09580</link><description>&lt;p&gt;
&#31639;&#27861;&#21477;&#27861;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Algorithmic syntactic causal identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09580
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#20013;&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20855;&#65292;&#20801;&#35768;&#20174;&#29702;&#35770;&#19978;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#20998;&#24067;&#25512;&#23548;&#24178;&#39044;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#35782;&#21035;&#24418;&#24335;&#65292;&#22914;&#20351;&#29992;d&#20998;&#31163;&#21644;do-&#28436;&#31639;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;CBN&#19978;&#21033;&#29992;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25968;&#23398;&#35821;&#35328;&#34920;&#36798;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#65292;&#27010;&#29575;&#35770;&#21644;&#22240;&#27492;&#30446;&#21069;&#30340;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#19981;&#36866;&#29992;&#65292;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#25968;&#25454;&#27969;&#31243;&#24207;&#65288;&#20363;&#22914;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65289;&#12289;&#20998;&#24067;&#24335;&#31995;&#32479;&#21644;&#22823;&#22810;&#25968;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#20844;&#29702;&#22522;&#30784;&#26469;&#28040;&#38500;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26367;&#20195;&#20844;&#29702;&#21270;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#33719;&#24471;&#19968;&#20010;&#26126;&#30830;&#19988;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.05652</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is different between these datasets?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.15384</link><description>&lt;p&gt;
&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Homeostatic motion planning with innate physics knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15384
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#20197;&#38381;&#29615;&#26041;&#24335;&#19982;&#21608;&#22260;&#29615;&#22659;&#36827;&#34892;&#20114;&#21160;&#65292;&#20854;&#20013;&#24863;&#23448;&#36755;&#20837;&#20915;&#23450;&#34892;&#20026;&#30340;&#21551;&#21160;&#21644;&#32456;&#27490;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#21160;&#29289;&#20063;&#33021;&#21046;&#23450;&#24182;&#25191;&#34892;&#22797;&#26434;&#35745;&#21010;&#65292;&#20294;&#32431;&#38381;&#29615;&#36755;&#20837;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#23578;&#26410;&#22797;&#21046;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23450;&#20041;&#19968;&#32452;&#31163;&#25955;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#65292;&#31216;&#20026;&#8220;&#20219;&#21153;&#8221;&#65292;&#27599;&#20010;&#20219;&#21153;&#20195;&#34920;&#19968;&#20010;&#38381;&#29615;&#34892;&#20026;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#36890;&#36807;&#35813;&#27169;&#22359;&#21487;&#20197;&#27169;&#25311;&#38543;&#26102;&#38388;&#25191;&#34892;&#20219;&#21153;&#24207;&#21015;&#24182;&#23558;&#32467;&#26524;&#23384;&#20648;&#22312;&#29615;&#22659;&#27169;&#22411;&#20013;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#38142;&#25509;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#36827;&#34892;&#21046;&#23450;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24050;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#23454;&#26045;&#65292;&#24182;&#22312;&#20004;&#31181;&#22330;&#26223;&#19979;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15384v1 Announce Type: cross  Abstract: Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called "tasks", each representing a closed-loop behaviour. We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.01968</link><description>&lt;p&gt;
&#23545;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#35843;&#26597;&#65306;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20852;&#20027;&#39064;&#30340;&#20852;&#36215;&#65292;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#23601;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#33258;&#20027;&#20195;&#29702;&#20013;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#20351;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#23548;&#33322;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#22788;&#29702;&#21160;&#24577;&#24773;&#20917;&#26102;&#65292;&#19978;&#19979;&#25991;&#24847;&#35782;&#25104;&#20026;&#24378;&#21270;&#22810;agent&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;agent&#31995;&#32479;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#27010;&#36848;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#19982;&#22810;agent&#31995;&#32479;&#38598;&#25104;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20419;&#36827;&#36825;&#20123;&#31995;&#32479;&#20043;&#38388;&#38598;&#25104;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810; agent &#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#26469;&#24314;&#27169;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;agent&#31995;&#32479;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/1909.03820</link><description>&lt;p&gt;
&#29992;&#35745;&#25968;&#31526;&#21495;&#30340;&#19968;&#38454;&#36923;&#36753;&#23450;&#20041;&#30340;&#27010;&#24565;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Concepts Definable in First-Order Logic with Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Grohe&#21644;Tur\'an&#24341;&#20837;&#30340;&#36923;&#36753;&#26694;&#26550;&#19979;&#30340;&#20851;&#31995;&#32972;&#26223;&#32467;&#26500;&#19978;&#30340;&#24067;&#23572;&#20998;&#31867;&#38382;&#39064;&#12290;&#20247;&#25152;&#21608;&#30693;(Grohe&#21644;Ritzert, LICS 2017)&#65292;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20854;&#20013;&#32467;&#26500;&#30340;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26159;&#20197;&#32467;&#26500;&#30340;&#22823;&#23567;&#20026;&#21333;&#20301;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#30001;Kuske&#21644;Schweikardt(LICS 2017)&#24341;&#20837;&#30340;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;FOCN&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#25512;&#24191;&#21508;&#31181;&#35745;&#25968;&#36923;&#36753;&#30340;&#34920;&#29616;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#31867;&#19978;&#23450;&#20041;&#30340;FOCN&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#19968;&#33268;&#22320;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20197;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#26041;&#38754;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26080;&#35270;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.12846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22914;&#20309;&#35299;&#37322;&#19994;&#21153;&#27969;&#31243;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well can large language models explain business processes?. (arXiv:2401.12846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#22312;&#26410;&#26469;&#30340;AI&#36741;&#21161;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#31995;&#32479;&#65288;ABPMSs&#65289;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#21151;&#33021;&#28085;&#30422;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#21151;&#33021;&#26159;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#65292;&#23427;&#28041;&#21450;&#29983;&#25104;&#22312;&#32771;&#34385;&#25152;&#35299;&#37322;&#26465;&#20214;&#20986;&#29616;&#30340;&#27969;&#31243;&#19978;&#19979;&#25991;&#30340;&#21069;&#25552;&#19979;&#26082;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#21448;&#21487;&#20154;&#31867;&#35299;&#35835;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#21457;&#29992;&#20110;&#29983;&#25104;SAX&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#12290;SAX4BPM&#22871;&#20214;&#21253;&#25324;&#19968;&#32452;&#26381;&#21153;&#21644;&#19968;&#20010;&#20013;&#22830;&#30693;&#35782;&#24211;&#12290;&#36825;&#20123;&#26381;&#21153;&#30340;&#21151;&#33021;&#26159;&#33719;&#21462;&#26500;&#25104;SAX&#35299;&#37322;&#30340;&#21508;&#31181;&#30693;&#35782;&#35201;&#32032;&#12290;&#20854;&#20013;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22240;&#26524;&#36807;&#31243;&#25191;&#34892;&#35270;&#22270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#19982;LLM&#38598;&#25104;&#65292;&#20197;&#21033;&#29992;&#20854;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;SAX&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for
&lt;/p&gt;</description></item><item><title>netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.17025</link><description>&lt;p&gt;
netFound: &#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17025
&lt;/p&gt;
&lt;p&gt;
netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20256;&#32479;&#24037;&#20316;&#27969;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#20294;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38459;&#30861;&#20102;&#29305;&#24449;&#36873;&#25321;&#65292;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20851;&#38190;&#20851;&#31995;&#21644;&#26377;&#25928;&#27867;&#21270;&#12290;&#21463;&#21040;GPT-4&#21644;Vision Transformers&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;netFound&#65292;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#32593;&#32476;&#25968;&#25454;&#21253;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;netFound&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#38544;&#34255;&#30340;&#32593;&#32476;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#24212;&#29992;&#36923;&#36753;&#12289;&#36890;&#20449;&#21327;&#35758;&#21644;&#32593;&#32476;&#26465;&#20214;&#12290;&#26377;&#20102;&#36825;&#20010;&#39044;&#35757;&#32451;&#22522;&#30784;&#65292;&#21363;&#20351;&#22788;&#29702;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#23545;netFound&#36827;&#34892;&#24494;&#35843;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;netFound&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.  With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound'
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.08167</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21442;&#25968;&#24179;&#31227;&#35268;&#21017;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26102;&#65292;&#38656;&#35201;&#23545;&#32593;&#32476;&#30340;&#21333;&#20010;&#21487;&#35843;&#21442;&#25968;&#35745;&#31639;&#20004;&#27425;&#20195;&#20215;&#20989;&#25968;&#12290;&#24403;&#21442;&#25968;&#24635;&#25968;&#36739;&#39640;&#26102;&#65292;&#38656;&#35201;&#35843;&#25972;&#21644;&#36816;&#34892;&#22810;&#27425;&#29992;&#20110;&#35745;&#31639;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#19968;&#20010;&#30005;&#36335;&#35745;&#31639;&#25152;&#26377;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32463;&#20856;&#23492;&#23384;&#22120;&#12290;&#25105;&#20204;&#36824;&#22312;&#30495;&#23454;&#37327;&#23376;&#30828;&#20214;&#21644;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30005;&#36335;&#32534;&#35793;&#26102;&#38388;&#26126;&#26174;&#32553;&#30701;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
&lt;/p&gt;</description></item></channel></rss>