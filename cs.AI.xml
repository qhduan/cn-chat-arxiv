<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01840</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30001;&#27880;&#37322;&#26631;&#31614;&#36827;&#34892;&#20154;-&#29289;&#20114;&#21160;&#26816;&#27979;&#30340;FreeA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FreeA: Human-object Interaction Detection using Free Annotation Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;-&#29289;&#20114;&#21160;&#65288;HOI&#65289;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#21171;&#21160;&#21147;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#38656;&#35201;&#20840;&#38754;&#27880;&#37322;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;CLIP&#30340;&#36866;&#24212;&#24615;&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#26080;&#38656;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FreeA&#23558;&#20154;-&#29289;&#23545;&#30340;&#22270;&#20687;&#29305;&#24449;&#19982;HOI&#25991;&#26412;&#27169;&#26495;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25513;&#27169;&#26041;&#27861;&#26469;&#25233;&#21046;&#19981;&#22826;&#21487;&#33021;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;FreeA&#21033;&#29992;&#20102;&#25552;&#20986;&#30340;&#20132;&#20114;&#30456;&#20851;&#24615;&#21305;&#37197;&#26041;&#27861;&#26469;&#22686;&#24378;&#19982;&#25351;&#23450;&#21160;&#20316;&#30456;&#20851;&#30340;&#21160;&#20316;&#30340;&#21487;&#33021;&#24615;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#29983;&#25104;&#30340;HOI&#26631;&#31614;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreeA&#22312;&#24369;&#30417;&#30563;HOI&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HICO-DET&#19978;&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mAP&#65289;&#25552;&#39640;&#20102;+8.58&#65292;&#22312;V-COCO&#19978;&#25552;&#39640;&#20102;+1.23&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01840v1 Announce Type: cross  Abstract: Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09447</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#23567;&#27874;&#20998;&#26512;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#23545;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#25104;&#21151;&#21306;&#20998;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#25235;&#25569;&#31867;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23567;&#27874;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#35299;&#30721;&#65292;&#20026;&#28789;&#24039;&#30340;&#31070;&#32463;&#20551;&#32930;&#24320;&#21457;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#24212;&#29992;&#26469;&#21306;&#20998;&#25163;&#37096;&#25235;&#25569;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36816;&#21160;&#38556;&#30861;&#24739;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;BCI&#24179;&#21488;&#21644;&#23567;&#27874;&#20449;&#21495;&#22788;&#29702;&#65292;&#21306;&#20998;&#20004;&#31181;&#22797;&#26434;&#30340;&#33258;&#28982;&#21147;&#37327;&#21644;&#31934;&#30830;&#25235;&#25569;&#31867;&#22411;&#20197;&#21450;&#19968;&#31181;&#20013;&#31435;&#26465;&#20214;&#20316;&#20026;&#26080;&#36816;&#21160;&#26465;&#20214;&#12290;&#23567;&#27874;&#20998;&#26512;&#28041;&#21450;&#20174;&#23567;&#27874;&#33021;&#37327;&#31995;&#25968;&#29983;&#25104;&#26102;&#38388;&#39057;&#29575;&#21644;&#25299;&#25169;&#22270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26032;&#22411;&#23567;&#27874;&#29305;&#24449;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#65306;&#22810;&#31867;&#21035;&#20026;85.16%&#65292;&#26080;&#36816;&#21160; vs &#21147;&#37327;&#20026;95.37%&#65292;&#26080;&#36816;&#21160; vs &#31934;&#30830;&#20026;95.40%&#65292;&#21147;&#37327; vs &#31934;&#30830;&#20026;88.07%&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#29305;&#24449;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#25235;&#25569;&#21306;&#20998;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09447v1 Announce Type: cross  Abstract: This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature impo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17570</link><description>&lt;p&gt;
&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#12289;&#39640;&#25928;&#12289;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#24050;&#37096;&#32626;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#30456;&#27604;&#20043;&#21069;&#20381;&#36182;&#20110;&#22266;&#23450;&#26679;&#26412;&#37327;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#24207;&#36143;&#30340;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#36319;&#36394;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20063;&#20801;&#35768;&#25968;&#25454;&#36890;&#36807;&#27010;&#29575;&#31574;&#30053;&#36827;&#34892;&#25910;&#38598;&#65292;&#32780;&#19981;&#26159;&#20174;&#20154;&#21475;&#20013;&#22343;&#21248;&#37319;&#26679;&#12290;&#36825;&#20351;&#24471;&#23457;&#35745;&#21487;&#20197;&#22312;&#20026;&#20854;&#20182;&#30446;&#30340;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#12290;&#27492;&#22806;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#38543;&#26102;&#38388;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#23376;&#20154;&#32676;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22240;&#27169;&#22411;&#21464;&#26356;&#25110;&#22522;&#30784;&#20154;&#32676;&#21464;&#26356;&#23548;&#33268;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110; anytime-valid &#25512;&#26029;&#21644;&#21338;&#24328;&#32479;&#35745;&#23398;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;"&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#27979;&#35797;"&#26694;&#26550;&#12290;&#36825;&#20123;&#32852;&#31995;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#24555;&#36895;&#21644;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
&lt;/p&gt;</description></item></channel></rss>