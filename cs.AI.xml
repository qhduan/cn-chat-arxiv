<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10726</link><description>&lt;p&gt;
&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Planning Action Models from State Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;&#35268;&#21010;&#34892;&#21160;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#20851;&#27880;&#22312;&#21442;&#25968;&#26410;&#25552;&#20379;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20174;&#29366;&#24577;&#36712;&#36857;&#20013;&#23398;&#20064;STRIPS&#39046;&#22495;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#20174;&#35201;&#23398;&#20064;&#30340;&#34892;&#21160;&#30340;&#21517;&#31216;&#21644;&#21442;&#25968;&#24320;&#22987;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#21807;&#19968;&#20219;&#21153;&#26159;&#25512;&#26029;&#32473;&#23450;&#34892;&#21160;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#26102;&#26410;&#25552;&#20379;&#23398;&#20064;&#34892;&#21160;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#36319;&#36394;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#22312;&#19968;&#20010;&#32423;&#21035;(L1)&#20013;&#65292;&#36712;&#36857;&#20013;&#30340;&#29366;&#24577;&#34987;&#26631;&#35760;&#20026;&#34892;&#21160;&#21517;&#31216;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#34892;&#21160;&#30340;&#25968;&#37327;&#21644;&#21517;&#31216;&#65292;&#20294;&#25105;&#20204;&#20173;&#38656;&#35201;&#24324;&#28165;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#12290;&#22312;&#21478;&#19968;&#20010;&#32423;&#21035;(L2)&#20013;&#65292;&#29366;&#24577;&#36824;&#39069;&#22806;&#26631;&#35760;&#26377;&#26500;&#25104;&#30456;&#24212;&#22522;&#20110;&#23545;&#35937;&#30340;&#34892;&#21160;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20173;&#28982;&#38656;&#35201;&#25512;&#26029;&#23398;&#20064;&#34892;&#21160;&#20013;&#21442;&#25968;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10726v1 Announce Type: new  Abstract: Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06287</link><description>&lt;p&gt;
AI&#65292;&#19982;&#20154;&#30456;&#36935;&#65306;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#39640;&#39118;&#38505;&#20219;&#21153;&#21644;&#20915;&#31574;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#20154;&#31867;&#29616;&#22312;&#19981;&#26029;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#27599;&#22825;&#36827;&#34892;&#27169;&#22411;&#30340;&#22521;&#35757;&#21644;&#20351;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#32771;&#34385;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#20294;&#20854;&#20998;&#31867;&#31232;&#30095;&#19988;&#30446;&#26631;&#21508;&#24322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00389</link><description>&lt;p&gt;
&#20851;&#20110;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;$O(\frac{\sqrt{d}}{T^{1/4}})$&#25910;&#25947;&#36895;&#24230;&#21644;&#23545;&#32500;&#24230;&#30340;&#25913;&#36827;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20854;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;$\ell_1$&#33539;&#25968;&#24314;&#31435;&#20102;&#25910;&#25947;&#29575;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#26080;&#38656;&#20551;&#35774;&#26799;&#24230;&#26377;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#20248;&#21270;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;$T$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#30001;&#20110;&#23545;&#20110;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#65292;$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#31867;&#27604;&#20026;SGD&#30340;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$&#65292;&#27979;&#24230;&#20026;$\ell_1$&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;</title><link>http://arxiv.org/abs/2304.06348</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#30340;&#21453;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#65288;&#36890;&#36807;&#26576;&#20123;&#31867;&#22411;&#30340;&#23485;&#24230;&#37327;&#26469;&#34913;&#37327;&#65292;&#21253;&#25324;&#26641;&#23485;&#21644;&#22242;&#23485;&#31561;&#65289;&#65292;&#20026;&#24191;&#27867;&#30340;&#36923;&#36753;&#34164;&#21547;&#38382;&#39064;&#65288;&#31616;&#31216;&#26597;&#35810;&#65289;&#30340;&#21487;&#20915;&#23450;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23637;&#29616;&#20986;&#23485;&#24230;&#26377;&#38480;&#26377;&#38480;&#36890;&#29992;&#27169;&#22411;&#38598;&#30340;&#36923;&#36753;&#65292;&#20445;&#35777;&#20102;&#21508;&#31181;&#21516;&#24577;&#23553;&#38381;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#24378;&#22823;&#30340;&#23485;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Blumensath&#30340;&#20998;&#21106;&#23485;&#24230;&#65292;&#35813;&#37327;&#21253;&#21547;&#20102;&#21508;&#31181;&#36890;&#24120;&#32771;&#34385;&#30340;&#23485;&#24230;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#30340;&#35745;&#31639;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#38024;&#23545;&#26222;&#36941;&#23637;&#29616;&#23384;&#22312;&#24615;&#35268;&#21017;&#20026;&#19968;&#20010;&#23637;&#31034;&#26696;&#20363;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#26377;&#38480;&#20998;&#21106;&#23485;&#24230;&#35268;&#21017;&#38598;&#21253;&#21547;&#20854;&#20182;&#24050;&#30693;&#30340;&#25277;&#35937;&#21487;&#20915;&#23450;&#31867;&#65292;&#20294;&#20511;&#21161;&#29616;&#26377;&#30340;&#20998;&#23618;&#21644;&#21463;&#25511;&#35268;&#21017;&#38598;&#27010;&#24565;&#65292;&#20063;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#20363;&#22914;&#27491;&#21017;&#65292;&#36830;&#25509;&#21644;&#24067;&#23572;&#36830;&#25509;&#26597;&#35810;&#12290;&#25105;&#20204;&#20197;&#23384;&#22312;&#35268;&#21017;&#30340;&#24418;&#24335;&#20026;&#37325;&#28857;&#65292;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#39640;&#32423;&#30693;&#35782;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item></channel></rss>