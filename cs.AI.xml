<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.01438</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25163;&#35821;&#30340;&#29983;&#25104;&#19982;&#26816;&#27979;-&#35821;&#35328;&#21644;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01438
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#39046;&#22495;&#20013;&#19968;&#20010;&#36880;&#28176;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36229;&#36234;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#65292;&#20197;&#21450;&#36825;&#23545;&#31038;&#20250;&#26159;&#21542;&#26377;&#30410;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#20013;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#21516;&#26102;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#65288;DHoH&#65289;&#31038;&#21306;&#25191;&#34892;&#25163;&#35821;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#23545;&#29983;&#25104;&#30340;&#35270;&#39057;&#36827;&#34892;&#23457;&#26680;&#12290;&#37492;&#20110;&#25163;&#35821;&#30340;&#22797;&#26434;&#24615;&#12289;&#25163;&#35821;&#19987;&#23478;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#20581;&#24247;&#21644;&#25945;&#32946;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#36825;&#31181;&#20570;&#27861;&#23588;&#20026;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#21253;&#25324;&#26500;&#24314;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35780;&#20272;&#20854;&#25216;&#26415;&#21644;&#35270;&#35273;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#12290;&#20351;&#29992;1200&#22810;&#20010;&#35270;&#39057;&#65292;&#27169;&#22411;&#21253;&#21547;&#20197;&#21069;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#65292;&#20511;&#21161;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#30340;&#24110;&#21161;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01438v1 Announce Type: cross  Abstract: A question in the realm of deepfakes is slowly emerging pertaining to whether we can go beyond facial deepfakes and whether it would be beneficial to society. Therefore, this research presents a positive application of deepfake technology in upper body generation, while performing sign-language for the Deaf and Hard of Hearing (DHoH) community. The resulting videos are later vetted with a sign language expert. This is particularly helpful, given the intricate nature of sign language, a scarcity of sign language experts, and potential benefits for health and education. The objectives of this work encompass constructing a reliable deepfake dataset, evaluating its technical and visual credibility through computer vision and natural language processing models, and assessing the plausibility of the generated content. With over 1200 videos, featuring both previously seen and unseen individuals for the generation model, using the help of a si
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17706</link><description>&lt;p&gt;
&#22686;&#24378;&#30701;&#25991;&#26412;&#24314;&#27169;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20027;&#39064;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17706
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#26500;&#24314;&#38024;&#23545;&#31616;&#30701;&#25991;&#26412;&#65288;&#22914;&#25512;&#25991;&#21644;&#26032;&#38395;&#26631;&#39064;&#65289;&#30340;&#20027;&#39064;&#27169;&#22411;&#23545;&#25429;&#25417;&#31038;&#20250;&#21160;&#24577;&#30340;&#36805;&#36895;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#34920;&#36798;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#31616;&#27905;&#24615;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24182;&#38750;&#30452;&#25509;&#21442;&#19982;&#20027;&#39064;&#30340;&#21021;&#27493;&#24314;&#27169;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#25913;&#36827;&#20027;&#39064;&#22312;&#34987;&#25366;&#25496;&#21518;&#30340;&#38454;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25351;&#23548;LLMs&#28040;&#38500;&#32473;&#23450;&#20027;&#39064;&#20013;&#30340;&#31163;&#39064;&#35789;&#65292;&#30830;&#20445;&#20165;&#20445;&#30041;&#19982;&#35821;&#22659;&#30456;&#20851;&#30340;&#35789;&#27719;&#25110;&#29992;&#26356;&#31526;&#21512;&#35821;&#20041;&#30340;&#35789;&#27719;&#26367;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33324;&#30340;&#23457;&#26597;&#21644;&#25913;&#36827;&#20027;&#39064;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#21508;&#31181;&#20027;&#39064;&#29983;&#25104;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17706v1 Announce Type: cross  Abstract: Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed "Topic Refinement". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by vario
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15245</link><description>&lt;p&gt;
&#35270;&#39057;&#30340;&#22686;&#24378;&#25512;&#29702;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasoning-Enhanced Object-Centric Learning for Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15245
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#29289;&#20307;&#34920;&#31034;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#35270;&#39057;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#26377;&#25928;&#25512;&#29702;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;STATM&#65289;&#30340;&#26032;&#22411;&#25512;&#29702;&#27169;&#22359;&#12290;&#35760;&#24518;&#32531;&#20914;&#21306;&#20027;&#35201;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#19978;&#28216;&#27169;&#22359;&#30340;&#27133;&#20301;&#20449;&#24687;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#36890;&#36807;&#27133;&#20301;&#20026;&#22522;&#30784;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13682</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13682
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#36890;&#36807;&#20174;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#24433;&#21709;&#26469;&#23454;&#29616;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#21435;&#38500;&#30340;&#36807;&#31243;&#35299;&#20915;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#12289;&#25935;&#24863;&#24615;&#12289;&#29256;&#26435;&#38480;&#21046;&#21644;&#36807;&#26102;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#30693;&#35782;&#21435;&#38500;&#26377;&#21161;&#20110;&#20943;&#36731;&#26377;&#23475;&#32467;&#26524;&#30340;&#39118;&#38505;&#65292;&#38450;&#33539;&#20559;&#35265;&#12289;&#35823;&#23548;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35774;&#35745;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#30740;&#31350;MU&#26381;&#21153;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#38598;&#25104;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25552;&#20132;&#35831;&#27714;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.09606</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21327;&#20316;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09606
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#21508;&#31181;NLP&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#35843;&#26597;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;LLMs&#30340;&#22240;&#26524;&#35270;&#35282;&#65292;&#22312;&#20197;&#19979;&#39046;&#22495;&#23637;&#24320;&#65306;&#29702;&#35299;&#21644;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#20026;LLMs&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#21516;&#26102;&#65292;LLMs&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21453;&#36807;&#26469;&#21487;&#20197;&#36890;&#36807;&#24110;&#21161;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26469;&#20419;&#36827;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19982;LLMs&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#38598;&#20307;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#27604;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Contrastive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08211
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26041;&#27861;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27604;&#25552;&#31034;&#65288;CP&#65289;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;LLMs&#25552;&#20379;&#31572;&#26696;&#20043;&#21069;&#28155;&#21152;"&#35753;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#21644;&#19968;&#20010;&#38169;&#35823;&#31572;&#26696;"&#26469;&#28436;&#31034;LLMs&#26159;&#20307;&#38754;&#30340;&#23545;&#27604;&#25512;&#29702;&#32773;&#12290;&#23545;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38646;&#36801;&#31227;&#23545;&#27604;&#25552;&#31034;&#25552;&#21319;&#20102;&#22312;&#19968;&#31995;&#21015;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;&#37327;&#36801;&#31227;&#31034;&#20363;&#65292;&#27604;&#22914;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;35.9%&#21040;88.8%&#20197;&#21450;AQUA-RAT&#20174;41.3%&#21040;62.2%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22823;&#22810;&#25968;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#32988;&#36807;&#38646;&#36801;&#31227;CoT&#21644;&#23569;&#37327;&#36801;&#31227;CoT&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#32541;&#25972;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#25110;&#32773;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.07887</link><description>&lt;p&gt;
&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65306;&#22312;&#26032;&#20852;&#30340;&#27133;&#34920;&#31034;&#20013;&#25509;&#22320;&#23545;&#35937;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#26041;&#27861;&#22312;&#23558;&#21407;&#22987;&#24863;&#30693;&#26080;&#30417;&#30563;&#20998;&#35299;&#20026;&#20016;&#23500;&#30340;&#31867;&#20284;&#29289;&#20307;&#30340;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#25509;&#22320;&#21040;&#23398;&#21040;&#30340;&#25277;&#35937;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#29702;&#35299;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#23427;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#12290;NSI&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31867;&#20284;XML&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#27861;&#35268;&#21017;&#23558;&#22330;&#26223;&#30340;&#29289;&#20307;&#35821;&#20041;&#32452;&#32455;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#21407;&#35821;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#21452;&#23618;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#31243;&#24207;&#21407;&#35821;&#25509;&#22320;&#21040;&#27133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;NSI&#31243;&#24207;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#40784;&#27169;&#22411;&#25512;&#26029;&#30340;&#23494;&#38598;&#20851;&#32852;&#20174;&#27133;&#29983;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#12290;&#22312;&#21452;&#27169;&#24335;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07887v1 Announce Type: cross  Abstract: Object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. However, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. We present the Neural Slot Interpreter (NSI) that learns to ground and generate object semantics via slot representations. At the core of NSI is an XML-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. Then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. Finally, we formulate the NSI program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. Experiments on bi-modal retrie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11068</link><description>&lt;p&gt;
&#26550;&#36215;&#22240;&#26524;&#21457;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#25972;&#21512;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#30528;&#20004;&#20010;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#23427;&#20204;&#36215;&#28304;&#19981;&#21516;&#65292;CD&#20391;&#37325;&#20110;&#20174;&#25968;&#25454;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;LLMs&#21017;&#20391;&#37325;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#20026;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;CD&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#21644;&#27604;&#36739;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#21508;&#31181;CD&#20219;&#21153;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#20197;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19981;&#23436;&#32654;&#19987;&#23478;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#24403;&#21069;&#23454;&#36341;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11068v1 Announce Type: cross  Abstract: Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01737</link><description>&lt;p&gt;
&#20026;&#31038;&#20132;&#24863;&#30693;&#30340;&#35848;&#21028;&#23545;&#35805;&#24320;&#21457;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25198;&#28436;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20004;&#21517;&#35848;&#21028;&#32773;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#35848;&#21028;&#12290;&#31532;&#19977;&#20010;LLM&#20805;&#24403;&#20462;&#27491;&#20195;&#29702;&#65292;&#37325;&#26032;&#32534;&#20889;&#36829;&#21453;&#35268;&#33539;&#30340;&#35805;&#35821;&#20197;&#25913;&#21892;&#35848;&#21028;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#19981;&#23384;&#22312;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20135;&#21697;&#38144;&#21806;&#12289;&#25151;&#20215;&#21644;&#34218;&#36164;&#35848;&#21028;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02073</link><description>&lt;p&gt;
&#19968;&#21058;&#30149;&#27602;&#65311;&#20351;&#29992;Fakepedia&#23450;&#20301;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20174;&#20854;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#26032;&#39062;&#20449;&#24687;&#20013;&#33719;&#24471;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24773;&#20917;&#19979;&#65292;&#25903;&#25745;&#36825;&#31181;&#19978;&#19979;&#25991;&#22522;&#30784;&#30340;&#26426;&#21046;&#65292;LLMs&#22312;&#22238;&#24518;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#20559;&#22909;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#19982;&#26368;&#26032;&#20449;&#24687;&#20016;&#23500;&#65292;&#24076;&#26395;&#22522;&#30784;&#21487;&#20197;&#32416;&#27491;&#36807;&#26102;&#25110;&#26377;&#22122;&#22768;&#30340;&#23384;&#20648;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Fakepedia&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#30784;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#27169;&#22411;&#20869;&#37096;&#21442;&#25968;&#30693;&#35782;&#20914;&#31361;&#30340;&#21453;&#20107;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Fakepedia&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36974;&#34109;&#20998;&#32452;&#22240;&#26524;&#36861;&#36394;&#65288;MGCT&#65289;&#65292;&#23545;&#22238;&#31572;Fakepedia&#26597;&#35810;&#26102;&#30340;LLM&#32452;&#20214;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#37492;&#21035;&#20986;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#37319;&#29992;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#38382;&#39064;&#30340;&#27714;&#35299;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2306.02611</link><description>&lt;p&gt;
&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#21487;&#20197;&#34987;&#35777;&#26126;&#26159;&#26377;&#24110;&#21161;&#30340;
&lt;/p&gt;
&lt;p&gt;
Stochastic Population Update Can Provably Be Helpful in Multi-Objective Evolutionary Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#37319;&#29992;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#38382;&#39064;&#30340;&#27714;&#35299;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#22240;&#20854;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#29305;&#24615;&#65292;&#24050;&#34987;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#31181;&#32676;&#26356;&#26032;&#26159;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#24120;&#20197;&#36138;&#23146;&#12289;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19979;&#19968;&#20195;&#31181;&#32676;&#26159;&#36890;&#36807;&#20174;&#24403;&#21069;&#31181;&#32676;&#21644;&#26032;&#29983;&#25104;&#30340;&#35299;&#20013;&#36873;&#25321;&#26368;&#20248;&#35299;&#24418;&#25104;&#30340;&#65288;&#26080;&#35770;&#20351;&#29992;&#30340;&#36873;&#25321;&#26631;&#20934;&#26159;Pareto&#25903;&#37197;&#12289;&#25317;&#25380;&#24230;&#36824;&#26159;&#25351;&#26631;&#31561;&#65289;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#23545;&#20110;MOEAs&#30340;&#25628;&#32034;&#26159;&#26377;&#30410;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#30830;&#23450;&#24615;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#26367;&#25442;&#20026;&#38543;&#26426;&#26426;&#21046;&#65292;&#21487;&#20197;&#25351;&#25968;&#32423;&#20943;&#23569;&#20004;&#20010;&#24050;&#32463;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;MOEAs&#65288;SMS-EMOA&#21644;NSGA-II&#65289;&#22312;&#35299;&#20915;&#20004;&#20010;&#21452;&#30446;&#26631;&#38382;&#39064;&#65288;OneJumpZeroJump&#21644;&#21452;&#30446;&#26631;RealRoyalRoad&#65289;&#19978;&#30340;&#39044;&#35745;&#36816;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have been widely and successfully applied to solve multi-objective optimization problems, due to their nature of population-based search. Population update, a key component in multi-objective EAs (MOEAs), is usually performed in a greedy, deterministic manner. That is, the next-generation population is formed by selecting the best solutions from the current population and newly-generated solutions (irrespective of the selection criteria used such as Pareto dominance, crowdedness and indicators). In this paper, we question this practice. We analytically present that stochastic population update can be beneficial for the search of MOEAs. Specifically, we prove that the expected running time of two well-established MOEAs, SMS-EMOA and NSGA-II, for solving two bi-objective problems, OneJumpZeroJump and bi-objective RealRoyalRoad, can be exponentially decreased if replacing its deterministic population update mechanism by a stochastic one. Empirical studies als
&lt;/p&gt;</description></item><item><title>ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.16744</link><description>&lt;p&gt;
ShaRP&#65306;&#29992;Shapley&#20540;&#35299;&#37322;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ShaRP: Explaining Rankings with Shapley Values. (arXiv:2401.16744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16744
&lt;/p&gt;
&lt;p&gt;
ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25307;&#32856;&#12289;&#22823;&#23398;&#25307;&#29983;&#21644;&#36151;&#27454;&#31561;&#37325;&#35201;&#39046;&#22495;&#30340;&#31639;&#27861;&#20915;&#31574;&#24120;&#24120;&#26159;&#22522;&#20110;&#25490;&#21517;&#30340;&#12290;&#30001;&#20110;&#36825;&#20123;&#20915;&#31574;&#23545;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#20154;&#32676;&#30340;&#24433;&#21709;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#23427;&#20204;&#65306;&#20102;&#35299;&#20915;&#31574;&#26159;&#21542;&#36981;&#23432;&#27861;&#24459;&#65292;&#24110;&#21161;&#20010;&#20154;&#25552;&#39640;&#20182;&#20204;&#30340;&#25490;&#21517;&#65292;&#24182;&#35774;&#35745;&#26356;&#22909;&#30340;&#25490;&#21517;&#31243;&#24207;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ShaRP&#65288;Shapley for Rankings and Preferences&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#29305;&#24449;&#23545;&#25490;&#21517;&#32467;&#26524;&#19981;&#21516;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20351;&#29992;ShaRP&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#31639;&#27861;&#25490;&#21517;&#22120;&#20351;&#29992;&#30340;&#35780;&#20998;&#20989;&#25968;&#26159;&#24050;&#30693;&#30340;&#19988;&#26159;&#32447;&#24615;&#30340;&#65292;&#27599;&#20010;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#12290;&#36129;&#29486;&#21462;&#20915;&#20110;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#24494;&#22937;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;ShaRP&#22522;&#20110;&#37327;&#21270;&#36755;&#20837;&#24433;&#21709;&#26694;&#26550;&#65292;&#24182;&#21487;&#20197;&#35745;&#31639;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Because of the impact these decisions have on individuals, organizations, and population groups, there is a need to understand them: to know whether the decisions are abiding by the law, to help individuals improve their rankings, and to design better ranking procedures.  In this paper, we present ShaRP (Shapley for Rankings and Preferences), a framework that explains the contributions of features to different aspects of a ranked outcome, and is based on Shapley values. Using ShaRP, we show that even when the scoring function used by an algorithmic ranker is known and linear, the weight of each feature does not correspond to its Shapley value contribution. The contributions instead depend on the feature distributions, and on the subtle local interactions between the scoring features. ShaRP builds on the Quantitative Input Influence framework, and can compute the contri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.08957</link><description>&lt;p&gt;
SWBT&#65306;&#20855;&#26377;&#19981;&#23436;&#32654;&#28436;&#31034;&#30340;&#30456;&#20284;&#24615;&#21152;&#26435;&#34892;&#20026;&#36716;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
SWBT: Similarity Weighted Behavior Transformer with the Imperfect Demonstration for Robotic Manipulation. (arXiv:2401.08957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#20165;&#20351;&#29992;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#24182;&#24573;&#30053;&#19981;&#23436;&#32654;&#30340;&#28436;&#31034;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#20174;&#22312;&#32447;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Similarity Weighted Behavior Transformer&#65288;SWBT&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;SWBT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26131;&#33719;&#21462;&#30340;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#22914;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#20449;&#24687;&#26174;&#33879;&#22686;&#24378;&#20102;&#32593;&#32476;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#12290;&#22312;ManiSkill2 bench&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL), aiming to learn optimal control policies from expert demonstrations, has been an effective method for robot manipulation tasks. However, previous IL methods either only use expensive expert demonstrations and omit imperfect demonstrations or rely on interacting with the environment and learning from online experiences. In the context of robotic manipulation, we aim to conquer the above two challenges and propose a novel framework named Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from both expert and imperfect demonstrations without interaction with environments. We reveal that the easy-to-get imperfect demonstrations, such as forward and inverse dynamics, significantly enhance the network by learning fruitful information. To the best of our knowledge, we are the first to attempt to integrate imperfect demonstrations into the offline imitation learning setting for robot manipulation tasks. Extensive experiments on the ManiSkill2 bench
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.05333</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#30140;&#30171;&#35780;&#20272;&#65306;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#29289;&#32852;&#32593;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#20020;&#24202;&#35843;&#26597;&#65289;&#19982;&#21487;&#25193;&#23637;&#30340;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#30140;&#30171;&#29366;&#24577;&#30340;&#36523;&#20307;&#12289;&#34892;&#20026;&#21644;&#24515;&#29702;&#31038;&#20132;&#25351;&#26631;&#30340;&#21457;&#29616;&#12290;&#23613;&#31649;&#20197;&#25216;&#26415;&#36827;&#27493;&#25913;&#21464;&#21307;&#30103;&#31995;&#32479;&#30340;&#28909;&#24773;&#21644;&#25215;&#35834;&#65292;&#20294;&#20020;&#24202;&#30140;&#30171;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21463;&#21040;&#20102;&#38382;&#39064;&#26412;&#36523;&#30340;&#22810;&#26679;&#24615;&#21644;&#20010;&#24615;&#21270;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#20854;&#20182;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20559;&#35265;&#65292;&#24182;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#65288;&#22914;&#22522;&#20110;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#65292;&#36825;&#24341;&#36215;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#20154;&#24037;&#26234;&#33021;&#36866;&#24212;&#24615;&#30340;&#24576;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#26088;&#22312;&#32771;&#34385;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#65292;&#24182;&#20844;&#24179;&#39044;&#27979;&#24739;&#32773;&#30340;&#30140;&#30171;&#29366;&#24577;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#36866;&#24212;&#29616;&#23454;&#29615;&#22659;&#20013;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;&#22312;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#65292;&#35813;&#20195;&#29702;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.12654</link><description>&lt;p&gt;
&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#32771;&#34385;&#21019;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World. (arXiv:2306.12654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#36866;&#24212;&#29616;&#23454;&#29615;&#22659;&#20013;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;&#22312;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#65292;&#35813;&#20195;&#29702;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#34892;&#21160;&#26102;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#26410;&#30693;&#30340;&#21019;&#26032;&#24178;&#25200;&#20854;&#35745;&#21010;&#25191;&#34892;&#12290;&#21019;&#26032;&#26159;&#19968;&#31181;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#21487;&#20197;&#25913;&#21464;&#29615;&#22659;&#30340;&#26680;&#24515;&#29305;&#24449;&#12289;&#32452;&#25104;&#21644;&#21160;&#24577;&#12290;&#21019;&#26032;&#22312;&#20219;&#20309;&#36275;&#22815;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#30340;&#20219;&#20309;&#26102;&#38388;&#37117;&#21487;&#33021;&#21457;&#29983;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#20107;&#20808;&#36890;&#30693;&#25110;&#35299;&#37322;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21019;&#26032;&#23545;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#19990;&#30028;&#30340;&#20869;&#22312;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#29702;&#35299;&#20854;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#25191;&#34892;&#20854;&#35745;&#21010;&#12290;&#21019;&#26032;&#36890;&#24120;&#20351;&#24471;&#20182;&#20204;&#30340;&#20869;&#37096;&#27169;&#22411;&#19981;&#20934;&#30830;&#24182;&#19988;&#29983;&#25104;&#30340;&#35745;&#21010;&#19981;&#20877;&#36866;&#29992;&#12290;&#21019;&#26032;&#29305;&#21035;&#26222;&#36941;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22312;&#20854;&#20013;&#39046;&#22495;&#29305;&#23450;&#29978;&#33267;&#26159;&#39044;&#27979;&#30340;&#21019;&#26032;&#29305;&#23450;&#26041;&#27861;&#34987;&#29992;&#20110;&#20943;&#36731;&#21019;&#26032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#26816;&#27979;&#65292;&#25551;&#32472;&#21644;&#36866;&#24212;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#20854;&#25216;&#33021;&#36716;&#31227;&#21040;&#39640;&#20445;&#30495;&#30340;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#22312;&#28041;&#21450;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#32780;&#19988;&#27604;&#29616;&#26377;&#30340;&#22522;&#32447;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents acting in real-world environments often need to reason with unknown novelties interfering with their plan execution. Novelty is an unexpected phenomenon that can alter the core characteristics, composition, and dynamics of the environment. Novelty can occur at any time in any sufficiently complex environment without any prior notice or explanation. Previous studies show that novelty has catastrophic impact on agent performance. Intelligent agents reason with an internal model of the world to understand the intricacies of their environment and to successfully execute their plans. The introduction of novelty into the environment usually renders their internal model inaccurate and the generated plans no longer applicable. Novelty is particularly prevalent in the real world where domain-specific and even predicted novelty-specific approaches are used to mitigate the novelty's impact. In this work, we demonstrate that a domain-independent AI agent designed to detect, chara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09147</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Learning of Multivariate Time Series with Temporal Irregularity. (arXiv:2306.09147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#30340;&#22810;&#20803;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#34920;&#29616;&#20986;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#26102;&#38388;&#38388;&#38548;&#21644;&#32452;&#20214;&#38169;&#20301;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#22343;&#21248;&#30340;&#38388;&#36317;&#21644;&#24322;&#27493;&#26159;&#25968;&#25454;&#20869;&#29983;&#29305;&#24449;&#32780;&#19981;&#26159;&#19981;&#36275;&#35266;&#23519;&#30340;&#32467;&#26524;&#65292;&#21017;&#36825;&#20123;&#19981;&#35268;&#21017;&#24615;&#30340;&#20449;&#24687;&#20869;&#23481;&#22312;&#34920;&#24449;&#22810;&#20803;&#20381;&#36182;&#32467;&#26500;&#26102;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#35201;&#20040;&#26131;&#21463;&#21040;&#25554;&#34917;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#20551;&#35774;&#24378;&#21152;&#20110;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#32452;&#20214;&#21551;&#29992;&#21807;&#19968;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#20415;&#21040;&#36798;&#26102;&#38388;&#21487;&#20197;&#25351;&#23548;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#21738;&#20010;&#38544;&#34255;&#29366;&#24577;&#24212;&#35813;&#26356;&#26032;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#35266;&#27979;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#20102;&#36328;&#32452;&#20214;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#21644;&#26465;&#20214;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden state
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05351</link><description>&lt;p&gt;
GPT-NAS: &#20197;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model. (arXiv:2305.05351v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05351
&lt;/p&gt;
&lt;p&gt;
GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;&#19968;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;NAS&#26041;&#27861;&#20013;&#24456;&#23569;&#20986;&#29616;&#36825;&#31867;&#25104;&#26524;&#65292;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#25628;&#32034;&#31354;&#38388;&#22826;&#22823;&#20102;&#65292;&#23548;&#33268;NAS&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;GPT-NAS&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#12290;&#22312;GPT-NAS&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26500;&#24314;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;GPT-NAS&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#20986;&#21512;&#29702;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GPT-NAS&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has emerged as one of the effective methods to design the optimal neural network architecture automatically. Although neural architectures have achieved human-level performances in several tasks, few of them are obtained from the NAS method. The main reason is the huge search space of neural architectures, making NAS algorithms inefficient. This work presents a novel architecture search algorithm, called GPT-NAS, that optimizes neural architectures by Generative Pre-Trained (GPT) model. In GPT-NAS, we assume that a generative model pre-trained on a large-scale corpus could learn the fundamental law of building neural architectures. Therefore, GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonable architecture components given the basic one. Such an approach can largely reduce the search space by introducing prior knowledge in the search process. Extensive experimental results show that our GPT-NAS method significantly outperforms
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07143</link><description>&lt;p&gt;
&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Longitudinal Car-Following Model. (arXiv:2304.07143v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36319;&#36710;&#27169;&#22411;&#26159;&#20132;&#36890;&#20223;&#30495;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#20869;&#32622;&#20110;&#35768;&#22810;&#37197;&#22791;ADAS&#30340;&#27773;&#36710;&#20013;&#12290;&#23545;&#36710;&#36319;&#36710;&#34892;&#20026;&#30340;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#30001;&#22522;&#26412;&#30340;&#36710;&#36742;&#20132;&#20114;&#36807;&#31243;&#24341;&#36215;&#30340;&#19981;&#21516;&#23439;&#35266;&#29616;&#35937;&#30340;&#26681;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21508;&#31181;&#36710;&#36319;&#36710;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12289;&#20114;&#34917;&#24615;&#21644;&#37325;&#21472;&#20043;&#22788;&#12290;&#35813;&#23457;&#26597;&#23558;&#22312;&#19981;&#21516;&#21407;&#21017;&#20013;&#27010;&#24565;&#21270;&#30340;&#36710;&#36319;&#36710;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and s
&lt;/p&gt;</description></item><item><title>&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11183</link><description>&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11183
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#30340;&#30446;&#30340;&#26159;&#20174;&#19968;&#32452;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#25968;&#25454;&#30693;&#35782;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#33021;&#20803;&#23398;&#20064;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#8212;&#8212;PURER&#65292;&#20854;&#20013;&#21253;&#21547;&#65306;&#65288;1&#65289;&#25968;&#25454;&#26080;&#20851;&#30340;&#20803;&#35757;&#32451;&#26399;&#38388;&#30340;&#33410;&#30446;&#35838;&#31243;&#21453;&#36716;&#65288;ECI&#65289;&#65307;&#65288;2&#65289;&#20803;&#27979;&#35797;&#26399;&#38388;&#20869;&#37096;&#24490;&#29615;&#21518;&#30340;&#21453;&#28436;&#26657;&#20934;&#65288;ICFIL&#65289;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECI&#26469;&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICFIL&#26469;&#26657;&#20934;&#21453;&#28436;&#26799;&#24230;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;&#21453;&#28436;&#30340;&#20248;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PURER&#21487;&#20197;&#26377;&#25928;&#22320;&#20803;&#23398;&#20064;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#22495;&#29978;&#33267;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01201</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#25110;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#19982;&#23548;&#33268;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#24335;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#36890;&#36807;&#34892;&#20026;&#21453;&#24212;&#25512;&#26029;&#20986;&#30340;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#23545;&#40784;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#19982;&#20154;&#31867;&#34892;&#20026;&#21453;&#24212;&#30340;&#23545;&#40784;&#22522;&#26412;&#19978;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#21017;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#25910;&#38598;&#30340;&#19977;&#20010;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#20154;&#31867;&#27010;&#24565;...
&lt;/p&gt;
&lt;p&gt;
Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
&lt;/p&gt;</description></item></channel></rss>