<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17428</link><description>&lt;p&gt;
&#36890;&#36807;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17428
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#37492;&#20110;&#31934;&#31070;&#31185;&#35775;&#35848;&#26159;&#19987;&#19994;&#38754;&#35797;&#32773;&#19982;&#34987;&#38754;&#35797;&#32773;&#20043;&#38388;&#30446;&#26631;&#23548;&#21521;&#21644;&#32467;&#26500;&#21270;&#23545;&#35805;&#65292;&#36825;&#26159;LLMs&#21487;&#20197;&#25552;&#20379;&#23454;&#36136;&#20215;&#20540;&#30340;&#26368;&#26410;&#34987;&#24320;&#21457;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#21019;&#20260;&#32463;&#21382;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#30340;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;LLMs&#29992;&#20110;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#29992;&#36884;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#26159;&#21542;&#33021;&#22815;&#65288;1&#65289;&#21010;&#20998;&#34920;&#31034;&#31934;&#31070;&#30151;&#29366;&#30340;&#23545;&#35805;&#37096;&#20998;&#24182;&#21629;&#21517;&#30151;&#29366;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#35775;&#35848;&#23545;&#35805;&#35760;&#24405;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#12290;&#36825;&#37324;&#65292;&#35775;&#35848;&#25968;&#25454;&#30001;&#31934;&#31070;&#20581;&#24247;&#19987;&#23478;&#36827;&#34892;&#26631;&#35760;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#25552;&#31034;&#30340;LLMs&#22312;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
&lt;/p&gt;</description></item><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10231</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#25512;&#23548;&#26032;&#30340;&#20107;&#23454;&#65292;&#38142;&#25509;&#39044;&#27979;&#22120;&#20174;&#22270;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25910;&#38598;&#23616;&#37096;&#35777;&#25454;&#20197;&#25214;&#21040;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#21033;&#29992;&#25972;&#20010;KG&#36827;&#34892;&#39044;&#27979;&#32780;&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;KG&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#35268;&#25277;&#26679;&#26041;&#27861;&#35299;&#20915;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#12290; &#35774;&#35745;&#21407;&#21017;&#26159;&#65292;&#39044;&#27979;&#36807;&#31243;&#19981;&#30452;&#25509;&#20316;&#29992;&#20110;&#25972;&#20010;KG&#65292;&#32780;&#26159;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#65288;i&#65289;&#26681;&#25454;&#26597;&#35810;&#20165;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#21644;&#65288;ii&#65289;&#22312;&#36825;&#20010;&#21333;&#19968;&#30340;&#12289;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10231v1 Announce Type: cross  Abstract: To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17417</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31359;&#22681;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Through-Wall Imaging based on WiFi Channel State Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#22312;&#31359;&#22681;&#22330;&#26223;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#21033;&#29992;WiFi&#30340;&#20248;&#21183;&#65292;&#22914;&#25104;&#26412;&#25928;&#30410;&#65292;&#20809;&#29031;&#19981;&#21464;&#24615;&#21644;&#31359;&#22681;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#29615;&#22659;&#30340;&#21487;&#35270;&#21270;&#30417;&#27979;&#65292;&#36234;&#36807;&#25151;&#38388;&#36793;&#30028;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23427;&#36890;&#36807;&#35299;&#38145;&#25191;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#27963;&#21160;&#35782;&#21035;&#65289;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;WiFi CSI&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20174;WiFi CSI&#21040;&#22270;&#20687;&#30340;&#36328;&#27169;&#24577;&#36716;&#25442;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36866;&#24212;&#25105;&#20204;&#38382;&#39064;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26550;&#26500;&#37197;&#32622;&#30340;&#21076;&#38500;&#30740;&#31350;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#23450;&#37327;/&#23450;&#24615;&#35780;&#20272;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.15196</link><description>&lt;p&gt;
&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#27491;&#21017;&#21270;&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#25506;&#32034;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#65292;&#36825;&#20123;&#31639;&#27861;&#65288;&#22914;&#36719;Q&#23398;&#20064;&#65289;&#30340;&#25910;&#25947;&#24615;&#36136;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21333;&#29615;&#36335;&#31639;&#27861;&#65292;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#35823;&#24046;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#23610;&#24230;&#19978;&#36816;&#34892;&#65306;&#19968;&#20010;&#36739;&#24930;&#30340;&#23610;&#24230;&#29992;&#20110;&#26356;&#26032;&#29366;&#24577;&#21160;&#20316;&#20540;&#30340;&#30446;&#26631;&#32593;&#32476;&#65292;&#19968;&#20010;&#36739;&#24555;&#30340;&#23610;&#24230;&#29992;&#20110;&#22312;&#22522;&#21521;&#37327;&#31354;&#38388;&#20013;&#36924;&#36817;&#36125;&#23572;&#26364;&#22791;&#20221;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#23384;&#22312;&#19979;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#20110;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#34893;&#29983;&#31574;&#30053;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.10036</link><description>&lt;p&gt;
LOCALINTEL&#65306;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#32593;&#32476;&#30693;&#35782;&#29983;&#25104;&#32452;&#32455;&#23041;&#32961;&#24773;&#25253;
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10036
&lt;/p&gt;
&lt;p&gt;
LOCALINTEL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#33258;&#21160;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;SoC&#65289;&#20998;&#26512;&#24072;&#20174;&#20844;&#24320;&#35775;&#38382;&#30340;&#20840;&#29699;&#23041;&#32961;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#23041;&#32961;&#25253;&#21578;&#65292;&#24182;&#25163;&#21160;&#33258;&#23450;&#20041;&#20197;&#36866;&#24212;&#29305;&#23450;&#32452;&#32455;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#20998;&#26512;&#24072;&#36824;&#20381;&#36182;&#20110;&#20869;&#37096;&#23384;&#20648;&#24211;&#65292;&#20316;&#20026;&#32452;&#32455;&#30340;&#31169;&#26377;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21487;&#20449;&#30340;&#32593;&#32476;&#24773;&#25253;&#12289;&#20851;&#38190;&#25805;&#20316;&#32454;&#33410;&#21644;&#30456;&#20851;&#32452;&#32455;&#20449;&#24687;&#37117;&#23384;&#20648;&#22312;&#36825;&#20123;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#12290;&#20998;&#26512;&#24072;&#21033;&#29992;&#36825;&#20123;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20174;&#20107;&#19968;&#39033;&#32321;&#37325;&#30340;&#20219;&#21153;&#65292;&#25163;&#21160;&#21019;&#24314;&#32452;&#32455;&#29420;&#29305;&#30340;&#23041;&#32961;&#21709;&#24212;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#39640;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30693;&#35782;&#28304;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#22788;&#29702;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#33258;&#21160;&#21270;&#29983;&#25104;&#32452;&#32455;&#29305;&#23450;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCALINTEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20840;&#29699;&#21644;&#26412;&#22320;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#29983;&#25104;&#32452;&#32455;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security Operations Center (SoC) analysts gather threat reports from openly accessible global threat databases and customize them manually to suit a particular organization's needs. These analysts also depend on internal repositories, which act as private local knowledge database for an organization. Credible cyber intelligence, critical operational details, and relevant organizational information are all stored in these local knowledge databases. Analysts undertake a labor intensive task utilizing these global and local knowledge databases to manually create organization's unique threat response and mitigation strategies. Recently, Large Language Models (LLMs) have shown the capability to efficiently process large diverse knowledge sources. We leverage this ability to process global and local knowledge databases to automate the generation of organization-specific threat intelligence.  In this work, we present LOCALINTEL, a novel automated knowledge contextualization system that, upon 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01030</link><description>&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26088;&#22312;&#21028;&#26029;&#21477;&#23376;&#20013;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#25152;&#20256;&#36798;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#24773;&#24863;&#26497;&#24615;&#19981;&#20165;&#30001;&#23616;&#37096;&#19978;&#19979;&#25991;&#20915;&#23450;&#65292;&#36824;&#19982;&#36828;&#31163;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#30340;&#35789;&#27719;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#36275;&#22815;&#22320;&#21306;&#20998;&#24212;&#35813;&#26356;&#20851;&#27880;&#21738;&#20123;&#35789;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#27491;&#22312;&#36827;&#20837;&#22522;&#20110;&#26041;&#21521;&#30340;&#24773;&#24863;&#20998;&#31867;&#20197;&#32534;&#30721;&#21477;&#27861;&#20381;&#36182;&#26641;&#20449;&#24687;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#27861;&#20381;&#36182;&#26641;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#23558;&#20381;&#36182;&#20851;&#31995;&#26631;&#31614;&#20449;&#24687;&#26377;&#25928;&#22320;&#25972;&#21512;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#12290;&#39640;&#26031;&#25513;&#30721;&#23618;&#20542;&#21521;&#20110;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21608;&#22260;&#26041;&#38754;&#26415;&#35821;&#30340;&#24863;&#21463;&#37326;&#65292;&#20197;&#20351;&#20854;&#19981;&#37325;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.16472</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#21322;&#29615;&#28335;&#28304;
&lt;/p&gt;
&lt;p&gt;
Semiring Provenance for Lightweight Description Logics. (arXiv:2310.16472v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25551;&#36848;&#36923;&#36753;&#20013;&#20351;&#29992;&#21322;&#29615;&#28335;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#28335;&#28304;&#35821;&#20041;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#24615;&#65292;&#24182;&#23545;why&#28335;&#28304;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#29615;&#28335;&#28304;&#8212;&#8212;&#19968;&#31181;&#26368;&#21021;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#29615;&#22659;&#20013;&#23450;&#20041;&#30340;&#25104;&#21151;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#36923;&#36753;&#12290;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#65292;&#26412;&#20307;&#20844;&#29702;&#34987;&#29992;&#20132;&#25442;&#21322;&#29615;&#30340;&#20803;&#32032;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#19988;&#36825;&#20123;&#27880;&#37322;&#26681;&#25454;&#23427;&#20204;&#30340;&#25512;&#23548;&#26041;&#24335;&#20256;&#25773;&#21040;&#26412;&#20307;&#30340;&#32467;&#26524;&#20013;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#28335;&#28304;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#20960;&#31181;&#36731;&#37327;&#32423;&#25551;&#36848;&#36923;&#36753;&#30340;&#35821;&#35328;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#20026;&#24102;&#26377;&#29305;&#23450;&#31867;&#22411;&#27880;&#37322;&#65288;&#22914;&#27169;&#31946;&#24230;&#65289;&#30340;&#26412;&#20307;&#23450;&#20041;&#30340;&#20854;&#20182;&#35821;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#20123;&#23545;&#21322;&#29615;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#20041;&#28385;&#36275;&#19968;&#20123;&#26399;&#26395;&#30340;&#29305;&#24615;&#65288;&#22914;&#25193;&#23637;&#20102;&#25968;&#25454;&#24211;&#20013;&#23450;&#20041;&#30340;&#21322;&#29615;&#28335;&#28304;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;&#33879;&#21517;&#30340;why&#28335;&#28304;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#35745;&#31639;&#27599;&#20010;&#21152;&#27861;&#24130;&#31561;&#21644;&#20056;&#27861;&#24130;&#31561;&#30340;&#20132;&#25442;&#21322;&#29615;&#30340;&#21322;&#29615;&#28335;&#28304;&#65292;&#24182;&#30740;&#31350;&#20102;&#19982;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate semiring provenance--a successful framework originally defined in the relational database setting--for description logics. In this context, the ontology axioms are annotated with elements of a commutative semiring and these annotations are propagated to the ontology consequences in a way that reflects how they are derived. We define a provenance semantics for a language that encompasses several lightweight description logics and show its relationships with semantics that have been defined for ontologies annotated with a specific kind of annotation (such as fuzzy degrees). We show that under some restrictions on the semiring, the semantics satisfies desirable properties (such as extending the semiring provenance defined for databases). We then focus on the well-known why-provenance, which allows to compute the semiring provenance for every additively and multiplicatively idempotent commutative semiring, and for which we study the complexity of problems related to the prov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.08184</link><description>&lt;p&gt;
&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#65288;LFM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#23427;&#19987;&#27880;&#20110;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#12289;&#20462;&#25913;&#21644;&#35774;&#35745;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26435;&#37325;&#65288;&#22312;&#40657;&#21283;&#23376;&#29615;&#22659;&#20013;&#65289;&#65292;&#24182;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;LFM&#25216;&#26415;&#30340;&#30740;&#31350;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.02711</link><description>&lt;p&gt;
&#35299;&#20915;&#19981;&#23436;&#20840;&#23545;&#31216;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension. (arXiv:2309.02711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#29702;&#35299;&#25105;&#20204;&#30340;&#29615;&#22659;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20294;&#24448;&#24448;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#36807;&#20110;&#31616;&#21270;&#20102;&#29616;&#23454;&#12290;&#20154;&#31867;&#26159;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#65292;&#22806;&#35980;&#21644;&#35748;&#30693;&#20559;&#35265;&#65288;&#20363;&#22914;&#26377;&#19968;&#21482;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#25163;&#65289;&#37117;&#19981;&#23436;&#32654;&#22320;&#20559;&#31163;&#20102;&#23545;&#31216;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#22823;&#33041;&#24456;&#23481;&#26131;&#20811;&#26381;&#36825;&#20123;&#19981;&#23436;&#32654;&#24182;&#39640;&#25928;&#22320;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#30340;&#39537;&#21160;&#21160;&#26426;&#22312;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25429;&#25417;&#36825;&#31181;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;-&#19968;&#31181;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#19968;&#20010;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#19968;&#20010;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#25152;&#26377;&#29366;&#24577;&#20013;&#24378;&#21046;&#23454;&#26045;&#20849;&#21516;&#30340;&#23545;&#31216;&#20851;&#31995;&#65292;&#24182;&#36866;&#24212;&#20102;&#25152;&#23398;&#31574;&#30053;&#12290;&#23558;ASL&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#23545;&#31216;&#22686;&#24378;&#26041;&#27861;&#22312;&#19968;&#20010;&#28041;&#21450;&#22235;&#36275;&#34434;&#34433;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for mul
&lt;/p&gt;</description></item></channel></rss>