<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.16674</link><description>&lt;p&gt;
&#29702;&#35299;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Understanding the Functional Roles of Modelling Components in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16674
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#33041;&#31070;&#32463;&#22238;&#36335;&#21551;&#21457;&#65292;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#23454;&#29616;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#29983;&#29289;&#20445;&#30495;&#24230;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;SNNs&#30456;&#24403;&#22256;&#38590;&#65292;&#22240;&#20026;&#20854;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;&#20173;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#32463;&#20856;&#27169;&#22411;&#30340;&#20960;&#20010;&#21464;&#20307;&#65292;&#25105;&#20204;&#31995;&#32479;&#30740;&#31350;&#20102;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#36825;&#20123;&#20851;&#38190;&#24314;&#27169;&#32452;&#20214;&#22312;&#22522;&#20110;&#28431;&#31215;&#20998;&#25918;&#30005;&#65288;LIF&#65289;&#30340;SNNs&#20013;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#20123;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;SNNs&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#28388;&#27844;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#37325;&#32622;&#26426;&#21046;&#23545;&#20110;&#19981;&#38388;&#26029;&#30340;&#26102;&#38388;&#22788;&#29702;&#21644;&#35745;&#31639;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24490;&#29615;&#21017;&#20016;&#23500;&#20102;&#27169;&#22411;&#22797;&#26434;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#20294;&#20250;&#25439;&#23475;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16674v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degr
&lt;/p&gt;</description></item><item><title>MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.15585</link><description>&lt;p&gt;
MedPromptX&#65306;&#22522;&#20110;&#29616;&#23454;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15585
&lt;/p&gt;
&lt;p&gt;
MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#22270;&#20687;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#21644;&#24930;&#24615;&#24515;&#32954;&#30142;&#30149;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#30340;&#21162;&#21147;&#38754;&#20020;&#30528;&#22240;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19981;&#23436;&#25972;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;MedPromptX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;FP&#65289;&#21644;&#35270;&#35273;&#22522;&#30784;&#65288;VG&#65289;&#30456;&#32467;&#21512;&#65292;&#23558;&#22270;&#20687;&#19982;EHR&#25968;&#25454;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;MLLM&#34987;&#29992;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#25552;&#20379;&#23545;&#24739;&#32773;&#30149;&#21490;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#23569;&#26679;&#26412;&#25552;&#31034;&#20943;&#23569;&#20102;&#23545;MLLM&#30340;&#22823;&#37327;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36807;&#31243;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#21487;&#33021;&#36807;&#20110;&#32321;&#29712;&#65292;&#20294;&#23427;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#26469;&#21160;&#24577;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17217</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#28385;&#36275;&#32422;&#26463;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#65292;&#23427;&#21033;&#29992;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25351;&#23450;&#20195;&#29702;&#24212;&#35813;&#36981;&#24490;&#30340;&#22797;&#26434;&#26102;&#38388;&#35268;&#21017;&#65292;&#20197;&#21450;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#23545;DSRL&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SDT&#22312;&#23398;&#20064;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17217v1 Announce Type: cross  Abstract: Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is condi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13514</link><description>&lt;p&gt;
&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65306;&#20309;&#26102;&#26816;&#32034;&#12289;&#20309;&#26102;&#29983;&#25104;&#65311;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;-&#28982;&#21518;&#38405;&#35835;&#21644;&#29983;&#25104;-&#28982;&#21518;&#38405;&#35835;&#26159;&#22788;&#29702;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26410;&#30693;&#21644;&#24050;&#30693;&#38382;&#39064;&#30340;&#20004;&#31181;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21069;&#32773;&#26816;&#32034;&#24517;&#35201;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#21518;&#32773;&#21017;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#20869;&#37096;&#24050;&#30693;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#24456;&#23569;&#26377;&#20316;&#21697;&#32771;&#34385;&#21040;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#30001;&#20960;&#20010;&#24050;&#30693;&#25110;&#26410;&#30693;&#30340;&#23376;&#38382;&#39064;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#31616;&#21333;&#30340;&#20108;&#20803;&#20998;&#31867;&#65288;&#24050;&#30693;&#25110;&#26410;&#30693;&#65289;&#21464;&#24471;&#27425;&#20248;&#21644;&#20302;&#25928;&#65292;&#22240;&#20026;&#23427;&#20250;&#23545;&#27599;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#36807;&#24230;&#35843;&#29992;&#22806;&#37096;&#26816;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65288;Self-DC&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CuQA&#21644;FreshQA&#65289;&#19978;&#34920;&#26126;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13514v1 Announce Type: cross  Abstract: Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03667</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38388;&#25509;&#25512;&#29702;&#22120;&#65306;&#23545;&#33258;&#21160;&#25512;&#29702;&#30340;&#21453;&#35777;&#21644;&#30683;&#30462;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36981;&#24490;&#30452;&#25509;&#25512;&#29702;&#65288;DR&#65289;&#26694;&#26550;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#22240;&#27492;&#22312;&#35299;&#20915;&#24456;&#38590;&#36890;&#36807;DR&#35299;&#20915;&#30340;&#20247;&#22810;&#23454;&#38469;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38388;&#25509;&#25512;&#29702;&#65288;IR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#20107;&#23454;&#25512;&#29702;&#21644;&#25968;&#23398;&#35777;&#26126;&#31561;IR&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#35777;&#30340;&#36923;&#36753;&#31561;&#20215;&#24615;&#26469;&#22686;&#24378;LLMs&#30340;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#29702;&#35299;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#25552;&#31034;&#27169;&#26495;&#65292;&#35302;&#21457;LLMs&#36827;&#34892;&#22522;&#20110;&#30683;&#30462;&#35777;&#26126;&#30340;IR&#65292;&#20854;&#36923;&#36753;&#19978;&#31561;&#20215;&#20110;&#21407;&#22987;&#30340;DR&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;IR&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02803</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Distilling Medication Recommendation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#26681;&#25454;&#24739;&#32773;&#29305;&#23450;&#30340;&#20581;&#24247;&#38656;&#27714;&#26469;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#21307;&#23398;&#25968;&#25454;&#30340;&#32454;&#24494;&#35821;&#20041;&#65292;&#32780;&#21482;&#26159;&#36807;&#24230;&#20381;&#36182;&#26631;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#39318;&#27425;&#35775;&#38382;&#21307;&#38498;&#30340;&#24739;&#32773;&#30340;&#24773;&#20917;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20043;&#21069;&#30340;&#22788;&#26041;&#21382;&#21490;&#21487;&#20197;&#21442;&#32771;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#36755;&#20837;&#19981;&#21487;&#30693;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;LLMs&#25913;&#36827;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#65288;LEADER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20351;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14197</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#20869;&#23481;&#30340;&#25972;&#21512;&#24050;&#32463;&#23454;&#29616;&#20102;LLMs&#30340;&#26356;&#26032;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#27604;&#22914;&#24494;&#36719;Copilot&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#20063;&#35753;LLMs&#38754;&#20020;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22806;&#37096;&#20869;&#23481;&#20013;&#23884;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;ompromising LLM&#36755;&#20986;&#24182;&#23548;&#33268;&#21709;&#24212;&#20559;&#31163;&#29992;&#25143;&#26399;&#26395;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#20197;&#35780;&#20272;&#36825;&#31867;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#20998;&#26512;&#20102;&#35813;&#25915;&#20987;&#25104;&#21151;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21363;LLMs&#26080;&#27861;&#21306;&#20998;&#25351;&#20196;&#21644;&#22806;&#37096;&#20869;&#23481;&#20197;&#21450;&#32570;&#20047;&#24847;&#35782;&#19981;&#25191;&#34892;&#22806;&#37096;&#20869;&#23481;&#20869;&#30340;&#25351;&#20196;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#40657;&#30418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13444</link><description>&lt;p&gt;
&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65306;&#22522;&#20110;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption. (arXiv:2401.13444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#20302;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#20026;&#20013;&#24515;&#30340;&#39640;&#25928;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30693;&#35782;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#23427;&#20204;&#30340;&#30693;&#35782;&#38754;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#24403;&#38754;&#23545;&#19981;&#29087;&#24713;&#30340;&#26597;&#35810;&#26102;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23558;LLMs&#35270;&#20026;&#20027;&#35201;&#30340;&#20915;&#31574;&#32773;&#65292;&#23545;&#20854;&#33021;&#21147;&#25552;&#20986;&#20102;&#36739;&#39640;&#30340;&#35201;&#27714;&#12290;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#30340;LLMs&#26469;&#35828;&#65292;&#36825;&#26159;&#19981;&#22826;&#21512;&#36866;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#32447;&#32034;&#24341;&#23548;&#36335;&#24452;&#25506;&#32034;&#20026;&#26680;&#24515;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65288;CGPE&#65289;&#65292;&#23427;&#23558;&#30693;&#35782;&#24211;&#19982;LLMs&#39640;&#25928;&#22320;&#34701;&#21512;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#35201;&#27714;&#36739;&#20302;&#12290;&#21463;&#20154;&#31867;&#25163;&#21160;&#26816;&#32034;&#30693;&#35782;&#30340;&#26041;&#27861;&#21551;&#21457;&#65292;CGPE&#21033;&#29992;&#38382;&#39064;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#32447;&#32034;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#30693;&#35782;&#24211;&#20013;&#25152;&#38656;&#30340;&#30693;&#35782;&#36335;&#24452;&#12290;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CGPE&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#19988;&#24615;&#33021;&#36739;&#24046;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, large language models (LLMs) have showcased remarkable capabilities. However, updating their knowledge poses challenges, potentially leading to inaccuracies when confronted with unfamiliar queries. While integrating knowledge graphs with LLMs has been explored, existing approaches treat LLMs as primary decision-makers, imposing high demands on their capabilities. This is particularly unsuitable for LLMs with lower computational costs and relatively poorer performance. In this paper, we introduce a Clue-Guided Path Exploration framework (CGPE) that efficiently merges a knowledge base with an LLM, placing less stringent requirements on the model's capabilities. Inspired by the method humans use to manually retrieve knowledge, CGPE employs information from the question as clues to systematically explore the required knowledge path within the knowledge base. Experiments on open-source datasets reveal that CGPE outperforms previous methods and is highly applicable to LLMs w
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#26469;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#30456;&#20284;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#32570;&#38519;&#21644;&#24322;&#24120;&#24773;&#20917;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20116;&#20010;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2312.14856</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#21644;&#33258;&#21160;&#21270;&#27979;&#35797;&#38024;&#23545;&#20195;&#30721;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28065;&#27969;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code. (arXiv:2312.14856v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#26469;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#30456;&#20284;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#32570;&#38519;&#21644;&#24322;&#24120;&#24773;&#20917;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20116;&#20010;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Turbulence&#65292;&#31995;&#32479;&#35780;&#20272;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27491;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;Turbulence&#21253;&#21547;&#19968;&#32452;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#8220;&#38382;&#39064;&#27169;&#26495;&#8221;&#65292;&#27599;&#20010;&#27169;&#26495;&#37117;&#26159;&#19968;&#20010;&#32534;&#31243;&#38382;&#39064;&#65292;&#21442;&#25968;&#21270;&#20351;&#24471;&#21487;&#20197;&#20197;&#22810;&#31181;&#19981;&#21516;&#24418;&#24335;&#25552;&#38382;&#12290;&#27599;&#20010;&#38382;&#39064;&#27169;&#26495;&#37117;&#26377;&#19968;&#20010;&#30456;&#20851;&#30340;&#8220;&#27979;&#35797;&#39044;&#27979;&#22120;&#8221;&#65292;&#29992;&#26469;&#21028;&#26029;LLM&#36820;&#22238;&#30340;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#27491;&#30830;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#27169;&#26495;&#65292;&#21487;&#20197;&#21521;LLM&#25552;&#38382;&#19968;&#20010;&#38750;&#24120;&#30456;&#20284;&#30340;&#32534;&#31243;&#38382;&#39064;&#8220;&#37051;&#22495;&#8221;&#65292;&#24182;&#35780;&#20272;&#27599;&#20010;&#38382;&#39064;&#36820;&#22238;&#30340;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20801;&#35768;&#35782;&#21035;LLM&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24046;&#36317;&#65292;&#21253;&#25324;LLM&#22312;&#37051;&#22495;&#20013;&#35299;&#20915;&#8220;&#20960;&#20046;&#25152;&#26377;&#8221;&#38382;&#39064;&#20294;&#23545;&#29305;&#23450;&#21442;&#25968;&#23454;&#20363;&#21270;&#22833;&#36133;&#30340;&#8220;&#24322;&#24120;&#8221;&#12290;&#25105;&#20204;&#38024;&#23545;OpenAI&#12289;Co&#31561;&#20116;&#20010;LLM&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language $\textit{question templates}$, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated $\textit{test oracle}$ that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including $\textit{anomalies}$ where the LLM correctly solves $\textit{almost all}$ questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00287</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27880;&#20837;&#65306;&#35780;&#20272;&#21644;&#25512;&#36827;&#20020;&#24202;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38656;&#35201;&#33021;&#22815;&#24212;&#23545;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#21644;&#20020;&#24202;&#32972;&#26223;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30452;&#25509;&#37096;&#32626;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20020;&#24202;NLP&#20219;&#21153;&#30340;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;ClinGen&#65292;&#23427;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#36825;&#20010;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28041;&#21450;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;LLM&#25552;&#31034;&#12290;&#20020;&#24202;&#20027;&#39064;&#21644;&#20889;&#20316;&#39118;&#26684;&#37117;&#26469;&#33258;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#20197;&#24341;&#23548;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;7&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#21644;16&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;ClinGen&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#20351;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#26174;&#33879;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.15772</link><description>&lt;p&gt;
&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#30340;&#22240;&#26524;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Understanding of Why Users Share Hate Speech on Social Media. (arXiv:2310.15772v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#23041;&#32961;&#21040;&#20010;&#20154;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#23548;&#33268;&#29616;&#23454;&#20013;&#30340;&#26292;&#21147;&#20107;&#20214;&#12290;&#20167;&#24680;&#35328;&#35770;&#20256;&#25773;&#32972;&#21518;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#26159;&#36716;&#21457;&#65292;&#20294;&#26159;&#20154;&#20204;&#24456;&#23569;&#20102;&#35299;&#20026;&#20160;&#20040;&#29992;&#25143;&#20250;&#36716;&#21457;&#20167;&#24680;&#35328;&#35770;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#22240;&#26524;&#20998;&#26512;&#30340;&#29992;&#25143;&#23646;&#24615;&#26694;&#26550;&#65292;&#30740;&#31350;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#31867;&#25968;&#25454;&#24456;&#21487;&#33021;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#33030;&#24369;&#24615;&#23384;&#22312;&#28151;&#28102;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27493;&#22240;&#26524;&#26694;&#26550;&#65306;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#36870;&#21521;&#20542;&#21521;&#35780;&#20998;&#26469;&#28040;&#38500;&#35266;&#23519;&#24615;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#28040;&#38500;&#20559;&#24046;&#30340;&#20542;&#21521;&#35780;&#20998;&#26469;&#27169;&#25311;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#28508;&#22312;&#33030;&#24369;&#24615;&#20316;&#20026;&#28508;&#22312;&#23884;&#20837;&#12290;&#65288;3&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#29992;&#25143;&#23646;&#24615;&#23545;&#29992;&#25143;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#27010;&#29575;&#30340;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. An important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. In this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. However, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. We develop a novel, three-step causal framework: (1) We debias the observational social media data by applying inverse propensity scoring. (2) We use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) We model the causal effects of user attributes on users' probability of sharing h
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.09051</link><description>&lt;p&gt;
GenDOM&#65306;&#20855;&#26377;&#21442;&#25968;&#24863;&#30693;&#30340;&#27867;&#21270;&#24615;&#19968;&#27425;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
GenDOM: Generalizable One-shot Deformable Object Manipulation with Parameter-Aware Policy. (arXiv:2309.09051v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09051
&lt;/p&gt;
&lt;p&gt;
GenDOM&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#25805;&#20316;&#31574;&#30053;&#21644;&#22810;&#26679;&#21270;&#27169;&#25311;&#35757;&#32451;&#65292;&#20351;&#25805;&#20316;&#31574;&#30053;&#33021;&#22815;&#20165;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#30340;&#28857;&#20113;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#26032;&#29289;&#20307;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#36816;&#21160;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#21464;&#24418;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#24448;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#26041;&#27861;&#65288;&#22914;&#32499;&#23376;&#21644;&#24067;&#26009;&#65289;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#35757;&#32451;&#27599;&#20010;&#29289;&#20307;&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GenDOM&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#25805;&#20316;&#31574;&#30053;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25805;&#20316;&#31574;&#30053;&#19982;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#21487;&#21464;&#24418;&#29289;&#20307;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#29289;&#20307;&#21442;&#25968;&#35843;&#25972;&#21160;&#20316;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#29289;&#20307;&#65292;GenDOM&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#21644;&#27169;&#25311;&#20043;&#38388;&#28857;&#20113;&#30340;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#29289;&#20307;&#21442;&#25968;&#65292;&#32780;&#21482;&#38656;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in deformable object manipulation, such as rope and cloth, often required hundreds of real-world demonstrations to train a manipulation policy for each object, which hinders their applications in our ever-changing world. To address this issue, we introduce GenDOM, a framework that allows the manipulation policy to handle different deformable objects with only a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable object parameters and training it with a diverse range of simulated deformable objects so that the policy can adjust actions based on different object parameters. At the time of inference, given a new object, GenDOM can estimate the deformable object parameters with only a single real-world demonstration by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations in a differentiable phys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#25439;&#22833;&#20989;&#25968;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#38590;&#20813;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#36825;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#36866;&#24212;&#36825;&#20123;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#21482;&#26377;&#20351;&#29992;&#19981;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#40065;&#26834;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21019;&#24314;&#22122;&#22768;&#40065;&#26834;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25968;&#37327;&#20247;&#22810;&#65292;&#23427;&#20204;&#36890;&#24120;&#20276;&#38543;&#30528;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#21487;&#33021;&#23398;&#20064;&#36895;&#24230;&#27604;&#24191;&#27867;&#20351;&#29992;&#20294;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#35201;&#24930;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#32771;&#34385;&#21644;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#25439;&#22833;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#24102;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#65292;&#21363;&#30053;&#24494;&#22686;&#21152;&#19982;&#27491;&#30830;&#26631;&#31614;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#25216;&#26415;&#22312;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item></channel></rss>