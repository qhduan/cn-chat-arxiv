<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16760</link><description>&lt;p&gt;
&#21644;&#25243;&#30828;&#24065;&#19968;&#26679;&#22909;&#65306;&#20154;&#31867;&#23545;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#23186;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20351;&#29992;&#23427;&#30340;&#38556;&#30861;&#19981;&#26029;&#38477;&#20302;&#65292;&#36825;&#39033;&#25216;&#26415;&#36234;&#26469;&#36234;&#34987;&#24694;&#24847;&#21033;&#29992;&#65292;&#20174;&#37329;&#34701;&#27450;&#35784;&#21040;&#38750;&#33258;&#24895;&#33394;&#24773;&#12290;&#20170;&#22825;&#65292;&#23545;&#25239;&#34987;&#21512;&#25104;&#23186;&#20307;&#35823;&#23548;&#30340;&#20027;&#35201;&#38450;&#24481;&#20381;&#36182;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#22312;&#35270;&#35273;&#21644;&#21548;&#35273;&#19978;&#21306;&#20998;&#30495;&#20551;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#38469;&#19978;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#26377;&#22810;&#33030;&#24369;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1276&#21517;&#21442;&#19982;&#32773;&#30340;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20154;&#20204;&#22312;&#21306;&#20998;&#21512;&#25104;&#22270;&#20687;&#12289;&#20165;&#38899;&#39057;&#12289;&#20165;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#12290;&#20026;&#20102;&#21453;&#26144;&#20154;&#20204;&#22312;&#37326;&#22806;&#21487;&#33021;&#36935;&#21040;&#21512;&#25104;&#23186;&#20307;&#30340;&#24773;&#20917;&#65292;&#27979;&#35797;&#26465;&#20214;&#21644;&#21050;&#28608;&#27169;&#25311;&#20102;&#20856;&#22411;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#32780;&#35843;&#26597;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#21512;&#25104;&#23186;&#20307;&#22343;&#26469;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16760v1 Announce Type: cross  Abstract: As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from 
&lt;/p&gt;</description></item><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;</title><link>https://arxiv.org/abs/2403.15297</link><description>&lt;p&gt;
&#29992;&#20110;&#29702;&#24615;&#25512;&#29702;&#30340;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sphere Neural-Networks for Rational Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#25104;&#21151;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#20854;&#31867;&#20154;&#38382;&#39064;&#22238;&#31572;&#30340;&#33021;&#21147;&#20197;&#21450;&#19981;&#26029;&#25552;&#21319;&#30340;&#25512;&#29702;&#24615;&#33021;&#37117;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#21542;&#20250;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22312;&#23450;&#24615;&#19978;&#25193;&#23637;&#20197;&#36229;&#36234;&#32479;&#35745;&#33539;&#24335;&#24182;&#23454;&#29616;&#39640;&#32423;&#35748;&#30693;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#30340;&#26041;&#24335;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31616;&#30340;&#23450;&#24615;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#29992;&#20110;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#21644;&#26816;&#26597;&#36827;&#34892;&#31867;&#20154;&#25512;&#29702;&#65292;&#24182;&#20026;&#19977;&#27573;&#35770;&#25512;&#29702;&#24320;&#21457;&#20102;SphNN&#65292;&#36825;&#26159;&#20154;&#31867;&#29702;&#24615;&#30340;&#32553;&#24433;&#12290;SphNN&#19981;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#37051;&#22495;&#31354;&#38388;&#20851;&#31995;&#30340;&#31070;&#32463;&#31526;&#21495;&#36716;&#25442;&#26144;&#23556;&#26469;&#25351;&#23548;&#20174;&#24403;&#21069;&#29699;&#24418;&#37197;&#32622;&#21521;&#30446;&#26631;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15297v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.14692</link><description>&lt;p&gt;
AI &#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#65306;GenAI &#25903;&#25345;&#35780;&#20272;&#30340;&#35797;&#28857;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#24555;&#36895;&#37319;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#24341;&#21457;&#20102;&#23545;&#23398;&#26415;&#35802;&#20449;&#12289;&#35780;&#20272;&#23454;&#36341;&#21644;&#23398;&#29983;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#31105;&#27490;&#25110;&#38459;&#27490;GenAI&#24037;&#20855;&#24050;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#65292;&#24809;&#32602;&#24615;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#33521;&#22269;&#36234;&#21335;&#22823;&#23398;&#65288;BUV&#65289;&#36827;&#34892;&#30340;&#35797;&#28857;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#26045;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;GenAI&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#12290;AIAS&#30001;&#20116;&#20010;&#32423;&#21035;&#32452;&#25104;&#65292;&#20174;&#8220;&#26080;AI&#8221;&#21040;&#8220;&#23436;&#20840;AI&#8221;&#65292;&#20351;&#25945;&#32946;&#24037;&#20316;&#32773;&#33021;&#22815;&#35774;&#35745;&#20391;&#37325;&#20110;&#38656;&#35201;&#20154;&#31867;&#36755;&#20837;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#35780;&#20272;&#12290;&#22312;&#23454;&#26045;AIAS&#21518;&#65292;&#35797;&#28857;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#26174;&#30528;&#20943;&#23569;&#65292;&#23398;&#29983;&#25104;&#32489;&#25552;&#39640;&#20102;5.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14692v1 Announce Type: cross  Abstract: The rapid adoption of Generative Artificial Intelligence (GenAI) technologies in higher education has raised concerns about academic integrity, assessment practices, and student learning. Banning or blocking GenAI tools has proven ineffective, and punitive approaches ignore the potential benefits of these technologies. This paper presents the findings of a pilot study conducted at British University Vietnam (BUV) exploring the implementation of the Artificial Intelligence Assessment Scale (AIAS), a flexible framework for incorporating GenAI into educational assessments. The AIAS consists of five levels, ranging from 'No AI' to 'Full AI', enabling educators to design assessments that focus on areas requiring human input and critical thinking.   Following the implementation of the AIAS, the pilot study results indicate a significant reduction in academic misconduct cases related to GenAI, a 5.9% increase in student attainment across the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.11000</link><description>&lt;p&gt;
ASGEA&#65306;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#35782;&#21035;&#20195;&#34920;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#26041;&#27861;&#22312;EA&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#38754;&#20020;&#30528;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23884;&#20837;&#36317;&#31163;&#65292;&#24182;&#24573;&#35270;&#20102;&#19968;&#23545;&#23545;&#40784;&#23454;&#20307;&#32972;&#21518;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Align-Subgraph&#23454;&#20307;&#23545;&#40784;&#65288;ASGEA&#65289;&#26694;&#26550;&#26469;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;ASGEA&#20351;&#29992;&#38170;&#38142;&#25509;&#20316;&#20026;&#26725;&#26753;&#26469;&#26500;&#24314;Align-Subgraphs&#65292;&#24182;&#27839;&#30528;&#36328;&#30693;&#35782;&#22270;&#30340;&#36335;&#24452;&#20256;&#25773;&#65292;&#36825;&#20351;&#20854;&#21306;&#21035;&#20110;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#30693;&#35782;&#22270;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33410;&#28857;&#32423;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#38170;&#28857;&#26469;&#22686;&#24378;Align-Subgraph&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.05399</link><description>&lt;p&gt;
CURE: &#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#27169;&#25311;&#36741;&#21161;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CURE: Simulation-Augmented Auto-Tuning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#30001;&#22810;&#20010;&#23376;&#31995;&#32479;&#32452;&#25104;&#65292;&#20363;&#22914;&#23450;&#20301;&#21644;&#23548;&#33322;&#65292;&#27599;&#20010;&#23376;&#31995;&#32479;&#21448;&#21253;&#21547;&#35768;&#22810;&#21487;&#37197;&#32622;&#30340;&#32452;&#20214;&#65288;&#20363;&#22914;&#36873;&#25321;&#19981;&#21516;&#30340;&#35268;&#21010;&#31639;&#27861;&#65289;&#12290;&#19968;&#26086;&#36873;&#25321;&#20102;&#26576;&#20010;&#31639;&#27861;&#65292;&#23601;&#38656;&#35201;&#35774;&#32622;&#30456;&#20851;&#30340;&#37197;&#32622;&#36873;&#39033;&#20197;&#36798;&#21040;&#36866;&#24403;&#30340;&#20540;&#12290;&#31995;&#32479;&#22534;&#26632;&#20013;&#30340;&#37197;&#32622;&#36873;&#39033;&#20250;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#22312;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#26426;&#22120;&#20154;&#20013;&#25214;&#21040;&#26368;&#20339;&#37197;&#32622;&#26469;&#23454;&#29616;&#26399;&#26395;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36719;&#20214;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#37197;&#32622;&#36873;&#39033;&#20132;&#20114;&#23548;&#33268;&#20102;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#37197;&#32622;&#31354;&#38388;&#12290;&#24615;&#33021;&#36801;&#31227;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#20063;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#25968;&#25454;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#24050;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#21487;&#37197;&#32622;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20248;&#21270;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24212;&#29992;&#20173;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimiz
&lt;/p&gt;</description></item><item><title>D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06150</link><description>&lt;p&gt;
D-STGCNT:&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#23494;&#38598;&#26102;&#31354;&#22270;&#21367;&#31215;GRU&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06150
&lt;/p&gt;
&lt;p&gt;
D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35780;&#20272;&#26080;&#20020;&#24202;&#30417;&#30563;&#24773;&#20917;&#19979;&#24739;&#32773;&#36827;&#34892;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#30340;&#25361;&#25112;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#20379;&#36136;&#37327;&#35780;&#20998;&#20197;&#30830;&#20445;&#27491;&#30830;&#25191;&#34892;&#21644;&#33719;&#24471;&#26399;&#26395;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;Dense Spatio-Temporal Graph Conv-GRU Network with Transformer&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;STGCN&#21644;transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#27599;&#20010;&#24247;&#22797;&#38203;&#28860;&#20013;&#36215;&#20027;&#35201;&#20316;&#29992;&#30340;&#20851;&#33410;&#12290;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#29992;&#20110;&#24555;&#36895;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#24182;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#12290;transformer&#32534;&#30721;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20391;&#37325;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#20351;&#20854;&#22312;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item></channel></rss>