<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.12264</link><description>&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12264
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12289;&#36951;&#24536;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#20449;&#20219;&#20854;&#39044;&#27979;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#19968;&#33324;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#23545;&#31934;&#35843;LLMs&#36827;&#34892;&#22522;&#20110;&#21518;&#39564;&#36924;&#36817;&#30340;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Mistral-7b&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#20102;&#19977;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#23545;&#19981;&#21516;&#30446;&#26631;&#39046;&#22495;&#30340;&#24863;&#30693;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#25928;&#33021;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#25105;&#20204;&#23545;&#37027;&#20123;&#23545;&#20110;&#32473;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#29109;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#25552;&#20986;&#20102;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#24037;&#31243;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.04835</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28436;&#21270;&#65306;&#19968;&#39033;&#35843;&#30740;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
On the Evolution of Knowledge Graphs: A Survey and Perspective. (arXiv:2310.04835v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#65292;&#20171;&#32461;&#20102;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#24037;&#31243;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26159;&#22810;&#26679;&#21270;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#26234;&#33021;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#28436;&#21270;&#65288;&#38745;&#24577;KGs&#65292;&#21160;&#24577;KGs&#65292;&#26102;&#24577;KGs&#21644;&#20107;&#20214;KGs&#65289;&#21644;&#30693;&#35782;&#25552;&#21462;&#21644;&#25512;&#29702;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;KGs&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#36130;&#21153;&#20998;&#26512;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#30693;&#35782;&#24037;&#31243;&#26410;&#26469;&#26041;&#21521;&#30340;&#23637;&#26395;&#65292;&#21253;&#25324;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21147;&#37327;&#20197;&#21450;&#30693;&#35782;&#25552;&#21462;&#12289;&#25512;&#29702;&#21644;&#34920;&#31034;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07174</link><description>&lt;p&gt;
HurriCast&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#29992;&#20110;&#39123;&#39118;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39123;&#39118;&#30001;&#20110;&#20854;&#28798;&#23475;&#24615;&#24433;&#21709;&#32780;&#22312;&#32654;&#22269;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#24456;&#37325;&#35201;&#65292;&#20445;&#38505;&#19994;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#20851;&#38190;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#39123;&#39118;&#27169;&#24335;&#65292;&#24182;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#23558;ARIMA&#27169;&#22411;&#21644;K-MEANS&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#28508;&#22312;&#26410;&#26469;&#36335;&#24452;&#21644;&#24378;&#24230;&#30340;&#35814;&#32454;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#32780;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item></channel></rss>