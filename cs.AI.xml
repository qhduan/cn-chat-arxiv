<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17421</link><description>&lt;p&gt;
MA4DIV&#65306;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17421
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;MA4DIV&#26041;&#27861;&#65292;&#23558;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#65292;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20197;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#65288;SRD&#65289;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#25152;&#36873;&#25991;&#26723;&#28085;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#21516;&#23376;&#20027;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#8220;&#36138;&#23146;&#36873;&#25321;&#8221;&#33539;&#24335;&#65292;&#21363;&#19968;&#27425;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#39640;&#22810;&#26679;&#24615;&#20998;&#25968;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#25928;&#29575;&#20302;&#19979;&#65292;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#26088;&#22312;&#36817;&#20284;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#20294;&#32467;&#26524;&#20173;&#28982;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;MA4DIV&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#25991;&#26723;&#37117;&#26159;&#19968;&#20010;&#26234;&#33021;&#20307;&#65292;&#25628;&#32034;&#32467;&#26524;&#22810;&#26679;&#21270;&#34987;&#24314;&#27169;&#20026;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20248;&#21270;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#22914;$\alpha$-NDCG&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Collage Prompting&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#25490;&#21015;&#39034;&#24207;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11468</link><description>&lt;p&gt;
Collage Prompting: &#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11468
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Collage Prompting&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;GPT-4V&#21512;&#20316;&#30340;&#32463;&#27982;&#21487;&#34892;&#30340;&#35270;&#35273;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#22270;&#20687;&#25490;&#21015;&#39034;&#24207;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#37319;&#29992;&#35270;&#35273;&#25552;&#31034;&#65292;GPT-4V&#21487;&#20197;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24230;&#12290;&#23613;&#31649;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;GPT-4V&#30340;&#25512;&#26029;&#30456;&#20851;&#30340;&#36130;&#21153;&#25104;&#26412;&#26500;&#25104;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;Collage Prompting&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#27982;&#23454;&#24800;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#22270;&#20687;&#36830;&#25509;&#25104;&#21333;&#20010;&#35270;&#35273;&#36755;&#20837;&#12290;&#20511;&#21161;&#25340;&#36148;&#25552;&#31034;&#65292;GPT-4V&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#24133;&#22270;&#20687;&#19978;&#25191;&#34892;&#22270;&#20687;&#35782;&#21035;&#12290;&#22522;&#20110;GPT-4V&#30340;&#22270;&#20687;&#35782;&#21035;&#20934;&#30830;&#24615;&#19982;&#25340;&#36148;&#25552;&#31034;&#20013;&#22270;&#20687;&#39034;&#24207;&#26126;&#26174;&#21464;&#21270;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#23398;&#20064;&#20248;&#21270;&#22270;&#20687;&#23433;&#25490;&#20197;&#33719;&#24471;&#26368;&#22823;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#39044;&#27979;&#22120;&#26469;&#25351;&#31034;&#27599;&#20010;&#25340;&#36148;&#25552;&#31034;&#30340;&#20934;&#30830;&#24615;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#23548;&#33322;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11468v1 Announce Type: cross  Abstract: Recent advancements in generative AI have suggested that by taking visual prompt, GPT-4V can demonstrate significant proficiency in image recognition task. Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents a substantial barrier for its wide use. To address this challenge, our work introduces Collage Prompting, a budget-friendly prompting approach that concatenates multiple images into a single visual input. With collage prompt, GPT-4V is able to perform image recognition on several images simultaneously. Based on the observation that the accuracy of GPT-4V's image recognition varies significantly with the order of images within the collage prompt, our method further learns to optimize the arrangement of images for maximum recognition accuracy. A graph predictor is trained to indicate the accuracy of each collage prompt, then we propose an optimization method to navigate the search space
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11204</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20998;&#21306;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Partitioned Neural Network Training via Synthetic Intermediate Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040;&#19981;&#21516;GPU&#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26222;&#21450;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290; GPU &#20869;&#23384;&#32422;&#26463;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#36825;&#20123;&#24222;&#22823;&#27169;&#22411;&#30340;&#19968;&#20010;&#26126;&#26174;&#29942;&#39048;&#12290;&#29616;&#26377;&#31574;&#30053;&#65292;&#21253;&#25324;&#25968;&#25454;&#24182;&#34892;&#12289;&#27169;&#22411;&#24182;&#34892;&#12289;&#27969;&#27700;&#32447;&#24182;&#34892;&#21644;&#23436;&#20840;&#20998;&#29255;&#25968;&#25454;&#24182;&#34892;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#12290; &#29305;&#21035;&#26159;&#27169;&#22411;&#24182;&#34892;&#20801;&#35768;&#23558;&#25972;&#20010;&#27169;&#22411;&#20998;&#24067;&#22312;&#22810;&#20010; GPU &#19978;&#65292;&#20294;&#38543;&#21518;&#30340;&#36825;&#20123;&#20998;&#21306;&#20043;&#38388;&#30340;&#25968;&#25454;&#36890;&#20449;&#20943;&#24930;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#22312;&#27599;&#20010; GPU &#19978;&#23384;&#20648;&#36741;&#21161;&#21442;&#25968;&#25152;&#38656;&#30340;&#22823;&#37327;&#20869;&#23384;&#24320;&#38144;&#22686;&#21152;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290; &#26412;&#30740;&#31350;&#20027;&#24352;&#19981;&#20351;&#29992;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26159;&#23558;&#27169;&#22411;&#20998;&#21306;&#21040; GPU &#19978;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#20013;&#38388;&#26631;&#31614;&#26469;&#35757;&#32451;&#21508;&#20010;&#37096;&#20998;&#12290; &#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#36825;&#20123;&#26631;&#31614;&#20943;&#32531;&#20102;&#35757;&#32451;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11204v1 Announce Type: cross  Abstract: The proliferation of extensive neural network architectures, particularly deep learning models, presents a challenge in terms of resource-intensive training. GPU memory constraints have become a notable bottleneck in training such sizable models. Existing strategies, including data parallelism, model parallelism, pipeline parallelism, and fully sharded data parallelism, offer partial solutions. Model parallelism, in particular, enables the distribution of the entire model across multiple GPUs, yet the ensuing data communication between these partitions slows down training. Additionally, the substantial memory overhead required to store auxiliary parameters on each GPU compounds computational demands. Instead of using the entire model for training, this study advocates partitioning the model across GPUs and generating synthetic intermediate labels to train individual segments. These labels, produced through a random process, mitigate me
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.08222</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#40065;&#26834;&#20915;&#31574;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Robust Decision Aggregation with Adversarial Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08222
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#12290;&#30495;&#23454;&#19987;&#23478;&#23558;&#20250;&#22914;&#23454;&#25253;&#21578;&#20182;&#20204;&#30340;&#31169;&#20154;&#20449;&#21495;&#65292;&#24182;&#33719;&#24471;&#36866;&#24403;&#30340;&#28608;&#21169;&#65292;&#32780;&#23545;&#25239;&#24615;&#19987;&#23478;&#21487;&#20197;&#20219;&#24847;&#25253;&#21578;&#12290;&#20915;&#31574;&#32773;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32858;&#21512;&#22120;&#65292;&#26681;&#25454;&#19987;&#23478;&#30340;&#25253;&#21578;&#26469;&#39044;&#27979;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20915;&#31574;&#32773;&#19981;&#20102;&#35299;&#20855;&#20307;&#30340;&#20449;&#24687;&#32467;&#26500;&#65292;&#21363;&#20449;&#21495;&#12289;&#29366;&#24577;&#20197;&#21450;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#31574;&#30053;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#22312;&#26368;&#22351;&#20449;&#24687;&#32467;&#26500;&#19979;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26368;&#20248;&#32858;&#21512;&#22120;&#12290;&#36951;&#25022;&#34987;&#23450;&#20041;&#20026;&#32858;&#21512;&#22120;&#21644;&#19968;&#20010;&#22522;&#20934;&#20043;&#38388;&#30340;&#26399;&#26395;&#25439;&#22833;&#24046;&#65292;&#35813;&#22522;&#20934;&#26681;&#25454;&#32852;&#21512;&#20998;&#24067;&#21644;&#30495;&#23454;&#19987;&#23478;&#30340;&#25253;&#21578;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08222v1 Announce Type: cross  Abstract: We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is opt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00144</link><description>&lt;p&gt;
EBBS: &#19968;&#20010;&#20855;&#26377;&#21452;&#23618;&#26463;&#25628;&#32034;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#38646;&#32763;&#35793;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#29992;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38646;&#32763;&#35793;&#30340;&#33021;&#21147;&#23601;&#20250;&#20986;&#29616;&#65307;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22312;&#26410;&#35265;&#36807;&#30340;&#26041;&#21521;&#36827;&#34892;&#32763;&#35793;&#12290;&#21478;&#22806;&#65292;&#38646;&#32763;&#35793;&#20063;&#21487;&#20197;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#37117;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EBBS&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38598;&#25104;&#32452;&#20214;&#22312;&#19979;&#23618;&#36880;&#27493;&#25506;&#32034;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#19978;&#23618;&#30340;&#8220;&#36719;&#25237;&#31080;&#8221;&#26426;&#21046;&#36827;&#34892;&#21516;&#27493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EBBS&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38598;&#25104;&#30340;&#30693;&#35782;&#20256;&#22238;&#21040;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2401.15296</link><description>&lt;p&gt;
&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#39592;&#26550;&#36827;&#34892;&#20154;&#21592;&#20877;&#35782;&#21035;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#27169;&#24335;&#35782;&#21035;&#31038;&#21306;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#39592;&#26550;&#24314;&#27169;&#21644;&#29305;&#24449;&#23398;&#20064;&#20013;&#31361;&#20986;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#30340;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;SRID&#65289;&#26041;&#27861;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#36825;&#20123;&#30740;&#31350;&#21450;&#20854;&#25361;&#25112;&#36827;&#34892;&#32508;&#21512;&#24635;&#32467;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;SRID&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;SRID&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;SRID&#30740;&#31350;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24635;&#32467;&#20102;&#24120;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24120;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#35780;&#20215;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;SRID&#27169;&#22411;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00094</link><description>&lt;p&gt;
&#34920;&#36798;&#24314;&#27169;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19981;&#36275;&#65306;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#26159;&#20808;&#23558;&#31163;&#32447;&#36712;&#36857;&#25311;&#21512;&#21040;&#19968;&#20010;&#24207;&#21015;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#36890;&#36807;&#35813;&#27169;&#22411;&#25552;&#31034;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#26222;&#36941;&#35748;&#20026;&#34920;&#36798;&#24615;&#26356;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#21487;&#22788;&#29702;&#24615;&#65292;&#21363;&#31934;&#30830;&#32780;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#21516;&#26679;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#24102;&#26469;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#39640;&#24230;&#38750;&#24179;&#20961;&#30340;&#26465;&#20214;/&#32422;&#26463;&#29983;&#25104;&#65292;&#20197;&#24341;&#20986;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#20173;&#28982;&#21487;&#20197;&#36817;&#20284;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31895;&#31961;&#30340;&#20272;&#35745;&#26174;&#33879;&#21066;&#24369;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#65288;TPM&#65289;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12053</link><description>&lt;p&gt;
&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Feedback Propagation. (arXiv:2308.12053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#36129;&#29486;&#29420;&#31435;&#20998;&#37197;&#22870;&#21169;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26159;&#26397;&#21521;&#20272;&#35745;&#30340;&#25439;&#22833;&#26368;&#23567;&#20540;&#26356;&#26032;&#21442;&#25968;&#12290;LFP&#22312;&#27169;&#22411;&#20013;&#20256;&#25773;&#22870;&#21169;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#35745;&#31639;&#12290;&#23427;&#22686;&#24378;&#25509;&#25910;&#21040;&#27491;&#21453;&#39304;&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#38477;&#20302;&#25509;&#25910;&#21040;&#36127;&#21453;&#39304;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#35777;&#26126;&#20102;LFP&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LFP&#20811;&#26381;&#20102;&#26799;&#24230;&#26041;&#27861;&#30340;&#26576;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#23545;&#26377;&#24847;&#20041;&#30340;&#23548;&#25968;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;LFP&#22914;&#20309;&#35299;&#20915;&#26799;&#24230;&#26041;&#27861;&#30456;&#20851;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13981</link><description>&lt;p&gt;
&#20445;&#25345;&#30693;&#35782;&#19981;&#21464;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#30830;&#20445;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#37117;&#19987;&#27880;&#20110;&#39564;&#35777;&#37197;&#23545;&#21305;&#37197;&#30340;&#27491;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21516;&#19968;&#30693;&#35782;&#21547;&#20041;&#30340;&#21477;&#27861;&#21644;&#34920;&#36798;&#20998;&#24067;&#20250;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#37117;&#26159;&#19968;&#20010;&#30693;&#35782;&#19981;&#21464;&#30340;&#22242;&#20307;&#65292;&#30001;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#32467;&#26500;&#19981;&#21516;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#38416;&#36848;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#24403;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#22987;&#32456;&#20934;&#30830;&#26102;&#65292;&#34987;&#21028;&#23450;&#20026;&#40065;&#26834;&#24615;&#24378;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#21457;&#34920;&#30340;&#20960;&#31181;&#20856;&#22411;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
&lt;/p&gt;</description></item></channel></rss>