<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01551</link><description>&lt;p&gt;
&#20855;&#26377;&#25511;&#21046;&#29702;&#35770;&#23433;&#20840;&#20445;&#35777;&#30340;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01551
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#19979;&#35299;&#20915;&#22797;&#26434;&#30340;&#21512;&#20316;&#20219;&#21153;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#26465;&#20214;&#19979;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#26234;&#33021;&#20307;&#20301;&#32622;&#65292;&#20197;&#20445;&#25345;&#23433;&#20840;&#26465;&#20214;&#32780;&#19981;&#24433;&#21709;&#20219;&#21153;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#30456;&#27604;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19982;&#38646;&#23433;&#20840;&#36829;&#35268;&#30456;&#27604;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#23433;&#20840;&#25511;&#21046;&#19982;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#19981;&#20165;&#22686;&#24378;&#20102;&#23433;&#20840;&#21512;&#35268;&#24615;&#65292;&#36824;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01551v1 Announce Type: cross  Abstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability. This work introduces a hybrid approach that integrates Multi-Agent Reinforcement Learning with control-theoretic methods to ensure safe and efficient distributed strategies. Our contributions include a novel setpoint update algorithm that dynamically adjusts agents' positions to preserve safety conditions without compromising the mission's objectives. Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations. Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16442</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37197;&#23545;&#27425;&#27169;&#27169;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22823;&#20110;&#20869;&#23384;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#21462;&#20915;&#20110;&#23376;&#38598;&#36873;&#25321;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#32452;&#37325;&#35201;&#21644;&#20195;&#34920;&#24615;&#30340;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#20272;&#35745;&#36817;&#20284;&#20445;&#35777;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01727</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#23545;&#37327;&#23376;&#36807;&#31243;&#36827;&#34892;&#28789;&#27963;&#30340;&#35823;&#24046;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#35745;&#31639;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37327;&#23376;&#35823;&#24046;&#32531;&#35299;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#23545;&#26080;&#22122;&#22768;&#32479;&#35745;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#36825;&#26159;&#23454;&#29616;&#23454;&#38469;&#37327;&#23376;&#36827;&#23637;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#35823;&#24046;&#32531;&#35299;&#65288;DAEM&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29305;&#23450;&#22122;&#22768;&#31867;&#22411;&#21644;&#27979;&#37327;&#35774;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#20165;&#26681;&#25454;&#30446;&#26631;&#37327;&#23376;&#36807;&#31243;&#30340;&#22122;&#22768;&#27979;&#37327;&#32467;&#26524;&#20272;&#35745;&#26080;&#22122;&#22768;&#32479;&#35745;&#20540;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#23454;&#38469;&#23454;&#26045;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#32531;&#35299;&#21508;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#65288;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#65289;&#26041;&#38754;&#19982;&#20808;&#21069;&#30340;&#35823;&#24046;&#32531;&#35299;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#26469;&#32531;&#35299;&#22810;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#26469;&#23637;&#31034;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, includ
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2012.12689</link><description>&lt;p&gt;
&#20803;&#32032;&#36234;&#31528;&#65292;&#25972;&#20307;&#36234;&#32874;&#26126;&#12290;&#25110;&#32773;&#65292;&#21487;&#33021;&#24182;&#38750;&#22914;&#27492;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?. (arXiv:2012.12689v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.12689
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20010;&#20307;&#26234;&#33021;&#26159;&#21542;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#21450;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#22312;Lotka-Volterra&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20010;&#20307;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026;&#65292;&#26377;&#21033;&#20110;&#19982;&#20854;&#20182;&#31181;&#32676;&#20849;&#23384;&#65292;&#20294;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23558;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#33041;&#20013;&#30340;&#31070;&#32463;&#20803;&#19982;&#31038;&#20250;&#20013;&#30340;&#20154;&#31867;&#20043;&#38388;&#30340;&#21033;&#32500;&#22374;&#31867;&#27604;&#65292;&#38382;&#33258;&#24049;&#26159;&#21542;&#20010;&#20307;&#26234;&#33021;&#23545;&#20110;&#38598;&#20307;&#26234;&#33021;&#30340;&#20135;&#29983;&#26159;&#24517;&#35201;&#30340;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24590;&#26679;&#30340;&#20010;&#20307;&#26234;&#33021;&#26377;&#21033;&#20110;&#26356;&#22823;&#30340;&#38598;&#20307;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36830;&#25509;&#20027;&#20041;&#35748;&#30693;&#31185;&#23398;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#24314;&#27169;&#12289;&#32676;&#20307;&#24515;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#29289;&#29702;&#23398;&#30340;&#19981;&#21516;&#27934;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#27934;&#35265;&#24212;&#29992;&#20110;Lotka-Volterra&#27169;&#22411;&#20013;&#23548;&#33268;&#25504;&#39135;&#32773;&#21644;&#29454;&#29289;&#35201;&#20040;&#20849;&#23384;&#35201;&#20040;&#20840;&#29699;&#28781;&#32477;&#30340;&#26234;&#33021;&#31867;&#22411;&#21644;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20010;&#20010;&#20307;&#34892;&#20026; - &#23588;&#20854;&#26159;&#25504;&#39135;&#32773;&#30340;&#34892;&#20026; - &#26377;&#21033;&#20110;&#20849;&#23384;&#65292;&#26368;&#32456;&#22312;&#19968;&#20010;&#24179;&#34913;&#28857;&#21608;&#22260;&#20135;&#29983;&#38663;&#33633;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#22914;&#26524;&#29454;&#29289;&#21644;&#25504;&#39135;&#32773;&#37117;&#36275;&#22815;&#26234;&#33021;&#20197;&#25512;&#26029;&#24444;&#27492;&#30340;&#34892;&#20026;&#65292;&#20849;&#23384;&#23601;&#20250;&#20276;&#38543;&#30528;&#20004;&#20010;&#31181;&#32676;&#30340;&#26080;&#38480;&#22686;&#38271;&#12290;&#30001;&#20110;Lotka-Volterra&#27169;&#22411;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a Leviathan analogy between neurons in a brain and human beings in society, asking ourselves whether individual intelligence is necessary for collective intelligence to emerge and, most importantly, what sort of individual intelligence is conducive of greater collective intelligence. We first review disparate insights from connectionist cognitive science, agent-based modeling, group psychology, economics and physics. Subsequently, we apply these insights to the sort and degrees of intelligence that in the Lotka-Volterra model lead to either co-existence or global extinction of predators and preys.  We find several individual behaviors -- particularly of predators -- that are conducive to co-existence, eventually with oscillations around an equilibrium. However, we also find that if both preys and predators are sufficiently intelligent to extrapolate one other's behavior, co-existence comes along with indefinite growth of both populations. Since the Lotka-Volterra model is al
&lt;/p&gt;</description></item></channel></rss>