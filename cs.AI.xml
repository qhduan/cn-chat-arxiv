<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.10056</link><description>&lt;p&gt;
&#19981;&#35201;&#21322;&#24515;&#21322;&#24847;&#65306;&#25429;&#25417;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10056
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#39537;&#20351;&#23427;&#20204;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#31526;&#21512;&#20154;&#31867;&#30446;&#26631;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#65288;CIT&#65289;&#36807;&#31243;&#21487;&#33021;&#20250;&#24102;&#26469;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#38382;&#39064;&#65292;&#23548;&#33268;&#20808;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36864;&#21270;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#27169;&#22411;&#25110;&#37325;&#25918;&#25968;&#25454;&#26469;&#32531;&#35299;CF&#38382;&#39064;&#65292;&#20294;&#36825;&#21487;&#33021;&#21482;&#35760;&#20303;&#25351;&#20196;&#30340;&#34920;&#38754;&#27169;&#24335;&#24182;&#22312;&#30041;&#23384;&#20219;&#21153;&#19978;&#24863;&#21040;&#22256;&#24785;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#65288;KPIG&#65289;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25513;&#30422;&#37096;&#20998;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#24182;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#19982;&#27491;&#30830;&#21709;&#24212;&#30456;&#20851;&#30340;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#65292;&#24182;&#20943;&#36731;&#23545;&#25351;&#23548;&#20013;&#36890;&#29992;&#25551;&#36848;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;P&#20998;&#21644;V&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 Announce Type: cross  Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score,
&lt;/p&gt;</description></item><item><title>RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.09948</link><description>&lt;p&gt;
RadCLIP: &#36890;&#36807;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09948
&lt;/p&gt;
&lt;p&gt;
RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#25918;&#23556;&#23398;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#21464;&#38761;&#26102;&#20195;&#12290;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#24050;&#34987;&#37319;&#29992;&#26469;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#23545;2D&#21644;3D&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#35299;&#35835;&#65292;&#24102;&#26469;&#20102;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#36890;&#29992;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21307;&#23398;&#25104;&#20687;&#25152;&#38656;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RadCLIP&#65306;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26469;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;RadCLIP&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#19987;&#20026;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#65292;&#20351;&#29992;&#20102;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RadCLIP&#33021;&#26377;&#25928;&#22320;&#23545;&#40784;&#25918;&#23556;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 Announce Type: cross  Abstract: The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36719;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.01508</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#30340;&#36719;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Reasoning on Uncertain Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36719;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#30693;&#35782;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#39033;&#30740;&#31350;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#30340;&#30740;&#31350;&#12290;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#30693;&#35782;&#19978;&#30340;&#36719;&#26597;&#35810;&#35774;&#32622;&#65292;&#21463;&#36719;&#32422;&#26463;&#32534;&#31243;&#30340;&#24314;&#31435;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#21069;&#21521;&#25512;&#29702;&#21448;&#20855;&#26377;&#21518;&#21521;&#26657;&#20934;&#65292;&#29992;&#20110;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#36719;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01508v1 Announce Type: new  Abstract: The study of machine learning-based logical query-answering enables reasoning with large-scale and incomplete knowledge graphs. This paper further advances this line of research by considering the uncertainty in the knowledge. The uncertain nature of knowledge is widely observed in the real world, but \textit{does not} align seamlessly with the first-order logic underpinning existing studies. To bridge this gap, we study the setting of soft queries on uncertain knowledge, which is motivated by the establishment of soft constraint programming. We further propose an ML-based approach with both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain knowledge graphs. Theoretical discussions present that our methods share the same complexity as state-of-the-art inference algorithms for first-order queries. Empirical results justify the superior performance of our approach against previous M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04059</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multivariate Time Series Imputation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#23384;&#22312;&#30340;&#32570;&#22833;&#20540;&#23548;&#33268;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#37096;&#20998;&#35266;&#27979;&#65292;&#30772;&#22351;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#23436;&#25972;&#24615;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#22312;&#25552;&#39640;&#25439;&#22351;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24378;&#35843;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#26469;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#37197;&#32622;&#65292;&#21253;&#25324;&#23450;&#26399;&#32500;&#25252;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#35770;&#25991;&#21015;&#34920;&#65292;&#21487;&#20197;&#22312;&#20197;&#19979;&#20301;&#32622;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be foun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.05502</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#65306;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#22810;&#20010;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#24418;&#25104;&#20132;&#21449;&#30340;&#32452;&#12290;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30830;&#20445;&#20174;&#27599;&#20010;&#32452;&#20013;&#36873;&#25321;&#26368;&#23569;&#25968;&#37327;&#30340;&#32858;&#31867;&#20013;&#24515;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#65292;&#21487;&#20197;&#26159;$k$-&#20013;&#20301;&#25968;&#65292;$k$-&#22343;&#20540;&#25110;$k$-&#20379;&#24212;&#21830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#65292;$1+\frac{8}{e}$&#21644;$3$&#65292;&#29992;&#20110;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20013;&#20301;&#25968;&#65292;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#22343;&#20540;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20379;&#24212;&#21830;&#12290;&#36825;&#20123;&#36817;&#20284;&#27604;&#22312;&#20551;&#35774;Gap-ETH&#21644;FPT $\neq$ W[2]&#30340;&#24773;&#20917;&#19979;&#26159;&#32039;&#30830;&#30340;&#12290;&#23545;&#20110;&#20844;&#24179;$k$-&#20013;&#20301;&#25968;&#21644;&#20844;&#24179;$k$-&#22343;&#20540;&#30340;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#21644;$1+\frac{8}{e}$&#12290;&#23545;&#20110;&#20855;&#26377;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#30340;&#20844;&#24179;$k$-&#20379;&#24212;&#21830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#65292;&#22240;&#23376;&#20026;$3$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.04095</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#31169;&#26377;&#25968;&#25454;&#20445;&#25345;&#26412;&#22320;&#65292;&#20801;&#35768;&#23433;&#20840;&#35745;&#31639;&#21644;&#26412;&#22320;&#27169;&#22411;&#26799;&#24230;&#19982;&#31532;&#19977;&#26041;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21361;&#21450;&#38544;&#31169;&#24182;&#24674;&#22797;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#35270;&#35282;&#12290;&#36825;&#20123;&#29702;&#35770;&#24037;&#20316;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#21482;&#26377;&#38145;&#23450;&#30340;&#26799;&#24230;&#34987;&#20256;&#36755;&#21040;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#25152;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#23494;&#38053;&#38145;&#27169;&#22359;&#21487;&#20197;&#30830;&#20445;&#65292;&#27809;&#26377;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#31169;&#26377;&#20449;&#24687;&#65306;a) &#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
&lt;/p&gt;</description></item></channel></rss>