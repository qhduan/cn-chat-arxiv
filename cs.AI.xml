<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;</title><link>https://arxiv.org/abs/2403.10700</link><description>&lt;p&gt;
&#27880;&#24847;&#38169;&#35823;&#65281;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25351;&#20196;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation in Continuous Environments (VLN-CE) &#26159;&#19968;&#39033;&#30452;&#35266;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#39564;&#26234;&#33021;&#20219;&#21153;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#20302;&#32423;&#21160;&#20316;&#12289;&#36981;&#24490;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#21040;&#30446;&#26631;&#30446;&#26631;&#12290;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340; VLN-CE &#26041;&#27861;&#37117;&#20551;&#35774;&#35821;&#35328;&#25351;&#20196;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#31867;&#32473;&#20986;&#30340;&#25351;&#20196;&#21487;&#33021;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#35760;&#24518;&#25110;&#28151;&#28102;&#32780;&#21253;&#21547;&#31354;&#38388;&#29615;&#22659;&#25551;&#36848;&#20013;&#30340;&#38169;&#35823;&#12290;&#24403;&#21069; VLN-CE &#22522;&#20934;&#27809;&#26377;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471; VLN-CE &#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#26469;&#33258;&#20154;&#31867;&#29992;&#25143;&#30340;&#38169;&#35823;&#25351;&#20196;&#26102;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20837;&#21508;&#31181;&#31867;&#22411;&#25351;&#20196;&#38169;&#35823;&#32771;&#34385;&#28508;&#22312;&#20154;&#31867;&#21407;&#22240;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20026;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; noticeable...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16705</link><description>&lt;p&gt;
SelectIT: &#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#25105;&#21453;&#24605;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16705
&lt;/p&gt;
&lt;p&gt;
SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#65288;IT&#65289;&#23545;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#20154;&#31867;&#20013;&#24515;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24120;&#35265;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#65292;&#36825;&#22686;&#21152;&#20102;&#25104;&#26412;&#24182;&#38480;&#21046;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SelectIT&#65292;&#23427;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IT&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#32650;&#39548;&#65288;Selective Alpaca&#65289;&#65292;&#36890;&#36807;&#23558;SelectIT&#24212;&#29992;&#20110;Alpaca-GPT4&#25968;&#25454;&#38598;&#32780;&#21019;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#32650;&#39548;&#36827;&#34892;IT&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;SelectIT&#30340;&#31283;&#20581;&#24615;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.07437</link><description>&lt;p&gt;
&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Tax Design in Nonatomic Congestion Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#26368;&#22823;&#21270;&#25928;&#29575;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#29609;&#23478;&#20043;&#38388;&#30340;&#33258;&#21033;&#34892;&#20026;&#21487;&#33021;&#20250;&#30772;&#22351;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#31246;&#21153;&#26426;&#21046;&#26159;&#32531;&#35299;&#27492;&#38382;&#39064;&#24182;&#24341;&#23548;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#21462;&#20102;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#35813;&#26368;&#20248;&#31246;&#25910;&#21487;&#20197;&#36890;&#36807;&#24179;&#34913;&#21453;&#39304;&#26469;&#26368;&#23567;&#21270;&#31038;&#20250;&#25104;&#26412;&#65292;&#21363;&#31246;&#21153;&#35774;&#35745;&#32773;&#21482;&#33021;&#35266;&#23519;&#21040;&#24378;&#21046;&#31246;&#25910;&#19979;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#65292;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#65292;&#29616;&#26377;&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#26469;&#36817;&#20284;&#26368;&#20248;&#31246;&#25910;&#65307;&#65288;2&#65289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#26469;&#20445;&#35777;&#24378;&#20984;&#28508;&#21147;&#20989;&#25968;&#65307;&#65288;3&#65289;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#26469;&#25214;&#21040;&#8220;&#36793;&#30028;&#8221;&#31246;&#25910;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#31246;&#25910;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\bet
&lt;/p&gt;
&lt;p&gt;
We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\epsilon$-optimal tax with $O(\bet
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#27491;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.09537</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#22411;&#35780;&#20272;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65306;&#22312;&#20449;&#24687;&#25193;&#25955;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A performance characteristic curve for model evaluation: the application in information diffusion prediction. (arXiv:2309.09537v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#27491;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#20449;&#24687;&#25193;&#25955;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#28040;&#24687;&#30340;&#25509;&#25910;&#32773;&#65292;&#22312;&#24066;&#22330;&#33829;&#38144;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#19981;&#21516;&#30340;&#39044;&#27979;&#27169;&#22411;&#37117;&#22768;&#31216;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24615;&#33021;&#35780;&#20272;&#30340;&#36890;&#29992;&#26694;&#26550;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#65292;&#35813;&#26354;&#32447;&#25429;&#33719;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#25193;&#25955;&#25968;&#25454;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#28982;&#21518;&#30830;&#23450;&#20102;&#38543;&#26426;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#32553;&#25918;&#27169;&#24335;&#12290;&#19981;&#21516;&#24207;&#21015;&#38271;&#24230;&#12289;&#31995;&#32479;&#22823;&#23567;&#21644;&#38543;&#26426;&#24615;&#19979;&#30340;&#25968;&#25454;&#28857;&#37117;&#21512;&#24182;&#25104;&#19968;&#26465;&#26354;&#32447;&#65292;&#25429;&#25417;&#20102;&#27169;&#22411;&#22312;&#38754;&#23545;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#20316;&#20986;&#27491;&#30830;&#39044;&#27979;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36825;&#26465;&#26354;&#32447;&#20855;&#26377;&#35780;&#20272;&#27169;&#22411;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information diffusion prediction on social networks aims to predict future recipients of a message, with practical applications in marketing and social media. While different prediction models all claim to perform well, general frameworks for performance evaluation remain limited. Here, we aim to identify a performance characteristic curve for a model, which captures its performance on tasks of different complexity. We propose a metric based on information entropy to quantify the randomness in diffusion data, then identify a scaling pattern between the randomness and the prediction accuracy of the model. Data points in the patterns by different sequence lengths, system sizes, and randomness all collapse into a single curve, capturing a model's inherent capability of making correct predictions against increased uncertainty. Given that this curve has such important properties that it can be used to evaluate the model, we define it as the performance characteristic curve of the model.
&lt;/p&gt;</description></item><item><title>NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.04640</link><description>&lt;p&gt;
NeuroBench&#65306;&#36890;&#36807;&#21512;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#22522;&#20934;&#27979;&#35797;&#25512;&#36827;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking. (arXiv:2304.04640v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04640
&lt;/p&gt;
&lt;p&gt;
NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#39046;&#22495;&#22312;&#36981;&#24490;&#20223;&#29983;&#23398;&#21407;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#25512;&#36827;&#35745;&#31639;&#25928;&#29575;&#21644;&#33021;&#21147;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#24418;&#24577;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#25216;&#26415;&#22810;&#26679;&#24615;&#23548;&#33268;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#31070;&#32463;&#24418;&#24577;&#26041;&#27861;&#19982;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21155;&#21183;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#39033;&#30446;&#8212;&#8212;NeuroBench&#65292;&#23558;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#32858;&#38598;&#36215;&#26469;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#23450;&#20041;&#22522;&#20934;&#27979;&#35797;&#12290;NeuroBench&#30340;&#30446;&#26631;&#26159;&#25104;&#20026;&#31038;&#21306;&#24320;&#21457;&#30340;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20934;&#27979;&#35797;&#31070;&#32463;&#24418;&#24577;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;NeuroBench&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NeuroBench&#23558;&#26159;&#23450;&#20041;&#33021;&#22815;&#32479;&#19968;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30446;&#26631;&#30340;&#26631;&#20934;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic comput
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item></channel></rss>