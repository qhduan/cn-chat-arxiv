<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17224</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Gradient-based Explanations in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#12290;&#36825;&#20123;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#21442;&#19982;&#27169;&#22411;&#35843;&#35797;&#12289;&#24615;&#33021;&#20248;&#21270;&#65292;&#24182;&#33719;&#24471;&#23545;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#30340;&#27934;&#35265;&#12290;&#37492;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#34913;&#37327;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27969;&#31243;&#20026;CIFAR-10&#12289;FER+&#21644;California Housing&#25968;&#25454;&#38598;&#29983;&#25104;&#35299;&#37322;&#20998;&#24067;&#12290;&#36890;&#36807;&#35745;&#31639;&#36825;&#20123;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20302;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#20462;&#25913;&#30340;&#20687;&#32032;&#25554;&#20837;/&#21024;&#38500;&#24230;&#37327;&#26469;&#35780;&#20215;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
&lt;/p&gt;</description></item><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03026</link><description>&lt;p&gt;
LanguageMPC&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#29702;&#35299;&#39640;&#32423;&#20449;&#24687;&#12289;&#25512;&#24191;&#32597;&#35265;&#20107;&#20214;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#38656;&#35201;&#20154;&#31867;&#24120;&#35782;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35748;&#30693;&#36335;&#24452;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#25512;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#23558;LLM&#20915;&#31574;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLM&#20915;&#31574;&#36890;&#36807;&#24341;&#23548;&#21442;&#25968;&#30697;&#38453;&#36866;&#24212;&#19982;&#20302;&#32423;&#25511;&#21046;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21333;&#36710;&#20219;&#21153;&#20013;&#22987;&#32456;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#22788;&#29702;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#29978;&#33267;&#22810;&#36710;&#21327;&#35843;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;LLMs&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#20316;&#20026;&#26377;&#25928;&#20915;&#31574;&#32773;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.06774</link><description>&lt;p&gt;
&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss. (arXiv:2309.06774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21270;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#30005;&#23376;&#24037;&#31243;&#12289;&#25968;&#23398;&#12289;&#21307;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#29289;&#29702;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#33719;&#24471;&#32463;&#39564;&#25104;&#21151;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22522;&#26412;&#38590;&#20197;&#25226;&#25569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#26681;&#26412;&#38382;&#39064;&#24182;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#32972;&#21518;&#30340;&#22885;&#31192;&#65292;&#24050;&#32463;&#22312;&#24314;&#31435;&#32479;&#19968;&#29702;&#35770;&#30340;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#21019;&#26032;&#12290;&#36825;&#20123;&#21019;&#26032;&#21253;&#25324;&#20248;&#21270;&#12289;&#27867;&#21270;&#21644;&#36817;&#20284;&#31561;&#22522;&#30784;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#20010;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#35299;&#20915;&#27169;&#24335;&#20998;&#31867;&#38382;&#39064;&#26102;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;&#36825;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;Hinge Loss&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#27979;&#35797;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.09746</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning with Experience Replay. (arXiv:2306.09746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#65292;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#12290;&#22312;&#32463;&#39564;&#26041;&#38754;&#65292;&#32463;&#39564;&#22238;&#25918;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#25928;&#24212;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#39033;&#30340;&#31616;&#21333;&#20998;&#35299;&#65292;&#24182;&#20026;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#22343;&#36845;&#20195;&#21644;&#26368;&#32456;&#36845;&#20195;&#24773;&#20917;&#19979;&#65292;&#24120;&#25968;&#27493;&#38271;&#24341;&#36215;&#30340;&#35823;&#24046;&#26415;&#35821;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#22823;&#23567;&#21644;&#20174;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#25277;&#26679;&#30340;&#23567;&#25209;&#37327;&#26469;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.04723</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#21644;&#26080;&#27861;&#35299;&#37322;&#65292;&#36825;&#20351;&#24471;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#38590;&#20197;&#29702;&#35299;&#25110;&#24178;&#39044;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26049;&#36793;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#24433;&#21709;&#39044;&#27979;&#22120;&#26159;&#23398;&#20064;&#22870;&#21169;&#26469;&#28304;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#24674;&#22797;&#26377;&#20851;&#31574;&#30053;&#22914;&#20309;&#21453;&#26144;&#29615;&#22659;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
&lt;/p&gt;</description></item></channel></rss>