<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;p&gt;
&#25105;&#30340;&#25968;&#25454;&#22312;&#20320;&#30340;AI&#27169;&#22411;&#20013;&#21527;&#65311;&#36890;&#36807;&#24212;&#29992;&#20110;&#20154;&#33080;&#22270;&#20687;&#30340;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Is my Data in your AI Model? Membership Inference Test with Application to Face Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09225
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;&#65288;MINT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#32463;&#39564;&#24615;&#35780;&#20272;&#29305;&#23450;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;MINT&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#32463;&#36807;&#23457;&#35745;&#30340;&#27169;&#22411;&#26292;&#38706;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#19981;&#21516;&#28608;&#27963;&#27169;&#24335;&#12290;&#31532;&#19968;&#20010;&#26550;&#26500;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;MINT&#26550;&#26500;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#33080;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#20849;&#21253;&#21547;&#36229;&#36807;2200&#19975;&#24352;&#20154;&#33080;&#22270;&#20687;&#12290;&#26681;&#25454;&#21487;&#29992;&#30340;AI&#27169;&#22411;&#27979;&#35797;&#30340;&#19978;&#19979;&#25991;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#22330;&#26223;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09225v1 Announce Type: cross Abstract: This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are a
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04345</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning as Computationally Constrained Reinforcement Learning. (arXiv:2307.04345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#26469;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#33021;&#22815;&#22312;&#28459;&#38271;&#30340;&#29983;&#21629;&#21608;&#26399;&#20869;&#39640;&#25928;&#31215;&#32047;&#30693;&#35782;&#24182;&#21457;&#23637;&#36234;&#26469;&#36234;&#22797;&#26434;&#25216;&#33021;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#12290;&#36830;&#32493;&#23398;&#20064;&#36825;&#19968;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#27010;&#24565;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#19968;&#22871;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
&lt;/p&gt;</description></item><item><title>StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05770</link><description>&lt;p&gt;
StyleNAT&#65306;&#32473;&#27599;&#20010;&#22836;&#37096;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
StyleNAT: Giving Each Head a New Perspective. (arXiv:2211.05770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05770
&lt;/p&gt;
&lt;p&gt;
StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#19968;&#30452;&#26159;&#19968;&#20010;&#26082;&#26399;&#26395;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#21516;&#26679;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#21363;&#20351;&#26159;&#25130;&#28982;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26377;&#24456;&#23569;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;StyleNAT&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27880;&#24847;&#21147;&#22836;&#37096;&#21010;&#20998;&#20026;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#22836;&#37096;&#33021;&#22815;&#20851;&#27880;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#32467;&#21512;&#36825;&#20123;&#20449;&#24687;&#65292;&#24182;&#20197;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#25163;&#22836;&#30340;&#25968;&#25454;&#12290;StyleNAT&#22312;FFHQ-256&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;SOTA FID&#24471;&#20998;2.046 &#65292;&#20987;&#36133;&#20102;&#20197;&#21367;&#31215;&#27169;&#22411;&#65288;&#22914;StyleGAN-XL&#65289;&#21644;transformer&#27169;&#22411;&#65288;&#22914;HIT&#65289;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult. Often researchers attempt to create a "one size fits all" generator, where there are few differences in the parameter space for drastically different datasets. Herein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. At the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA). With different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand. StyleNAT attains a new SOTA FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT 
&lt;/p&gt;</description></item></channel></rss>