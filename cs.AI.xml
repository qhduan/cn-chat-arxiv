<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.17217</link><description>&lt;p&gt;
DiffusionAct&#65306;&#21487;&#25511;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19968;&#27425;&#24615;&#20154;&#33080;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17217
&lt;/p&gt;
&lt;p&gt;
DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39537;&#21160;&#30340;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#26088;&#22312;&#21512;&#25104;&#33021;&#25104;&#21151;&#20445;&#30041;&#28304;&#33080;&#30340;&#36523;&#20221;&#21644;&#22806;&#35266;&#65292;&#21516;&#26102;&#36716;&#31227;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36924;&#30495;&#38754;&#37096;&#22270;&#20687;&#12290;&#29616;&#26377;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#35201;&#20040;&#23384;&#22312;&#22833;&#30495;&#21644;&#35270;&#35273;&#20266;&#24433;&#65292;&#35201;&#20040;&#37325;&#26500;&#36136;&#37327;&#36739;&#24046;&#65292;&#21363;&#32972;&#26223;&#21644;&#19968;&#20123;&#37325;&#35201;&#30340;&#22806;&#35266;&#32454;&#33410;&#65288;&#22914;&#21457;&#22411;/&#39068;&#33394;&#12289;&#30524;&#38236;&#21644;&#37197;&#39280;&#65289;&#26410;&#34987;&#24544;&#23454;&#37325;&#24314;&#12290;&#26368;&#36817;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DiffusionAct&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29031;&#29255;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#26469;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#25511;&#21046;Diffusion&#33258;&#32534;&#30721;&#22120;&#65288;DiffAE&#65289;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#20197;&#20415;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23450;&#20041;&#20026;&#22836;&#37096;&#23039;&#21183;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17217v1 Announce Type: cross  Abstract: Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation an
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.08824</link><description>&lt;p&gt;
&#34913;&#37327;&#38750;&#20856;&#22411;&#24773;&#32490;&#23545;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65306;&#35745;&#31639;&#26041;&#27861;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Measuring Non-Typical Emotions for Mental Health: A Survey of Computational Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20856;&#22411;&#24773;&#32490;&#65288;&#22914;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#20998;&#26512;&#65289;&#30456;&#36739;&#20110;&#32463;&#24120;&#35752;&#35770;&#30340;&#24773;&#32490;&#65288;&#22914;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#24656;&#24807;&#21644;&#24868;&#24594;&#65289;&#26469;&#35828;&#65292;&#26356;&#20026;&#32597;&#35265;&#19988;&#22797;&#26434;&#12290;&#30001;&#20110;&#23545;&#24515;&#29702;&#20581;&#24247;&#21644;&#24184;&#31119;&#30340;&#24433;&#21709;&#65292;&#20154;&#20204;&#23545;&#36825;&#20123;&#38750;&#20856;&#22411;&#24773;&#32490;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#12290;&#21387;&#21147;&#21644;&#25233;&#37057;&#24433;&#21709;&#20102;&#26085;&#24120;&#20219;&#21153;&#30340;&#21442;&#19982;&#65292;&#31361;&#26174;&#20102;&#29702;&#35299;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#20110;&#35745;&#31639;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20856;&#22411;&#24773;&#32490;&#20998;&#26512;&#26041;&#27861;&#30340;&#26102;&#38388;&#34920;&#21644;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#36890;&#29992;&#27969;&#31243;&#21644;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08824v1 Announce Type: cross  Abstract: Analysis of non-typical emotions, such as stress, depression and engagement is less common and more complex compared to that of frequently discussed emotions like happiness, sadness, fear, and anger. The importance of these non-typical emotions has been increasingly recognized due to their implications on mental health and well-being. Stress and depression impact the engagement in daily tasks, highlighting the need to understand their interplay. This survey is the first to simultaneously explore computational methods for analyzing stress, depression, and engagement. We discuss the most commonly used datasets, input modalities, data processing techniques, and information fusion methods used for the computational analysis of stress, depression and engagement. A timeline and taxonomy of non-typical emotion analysis approaches along with their generic pipeline and categories are presented. Subsequently, we describe state-of-the-art computa
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15552</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Morphological Symmetries in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15552
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#21644;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;&#12290;&#36825;&#20123;&#26159;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#32463;&#24120;&#22312;&#21160;&#29289;&#29983;&#29289;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#20013;&#35266;&#23519;&#21040;&#65292;&#28304;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#22797;&#21046;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#20123;&#23545;&#31216;&#24615;&#22914;&#20309;&#24310;&#20280;&#21040;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#31354;&#38388;&#20197;&#21450;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#37096;&#24863;&#30693;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#31561;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#24418;&#24577;&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#20010;&#30456;&#20851;&#19988;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#21463;&#29289;&#29702;&#21551;&#31034;&#30340;&#20960;&#20309;&#20808;&#39564;&#65292;&#23545;&#26426;&#22120;&#20154;&#24314;&#27169;&#12289;&#25511;&#21046;&#12289;&#20272;&#35745;&#21644;&#35774;&#35745;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#21644;&#20998;&#26512;&#26041;&#27861;&#37117;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#24418;&#24577;&#23545;&#31216;&#24615;&#22914;&#20309;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15552v1 Announce Type: cross  Abstract: We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models 
&lt;/p&gt;</description></item><item><title>&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00248</link><description>&lt;p&gt;
&#23558;&#8220;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#8221;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00248
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#20854;&#20998;&#21106;&#33945;&#29256;&#32570;&#20047;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;SAM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#23454;&#29616;&#39640;&#24230;&#31934;&#30830;&#30340;&#23545;&#35937;&#20998;&#21106;&#65288;&#21363;&#31216;&#20026;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;DIS&#65289;&#25265;&#26377;&#24456;&#39640;&#26399;&#26395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIS-SAM&#65292;&#23558;SAM&#25512;&#36827;&#33267;DIS&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#31934;&#30830;&#32454;&#33410;&#12290;DIS-SAM&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#39640;&#24230;&#20934;&#30830;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;SAM&#30340;&#21487;&#20419;&#36827;&#35774;&#35745;&#12290;DIS-SAM&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;SAM&#19982;&#19987;&#38376;&#29992;&#20110;DIS&#30340;&#20462;&#25913;&#21518;&#30340;IS-Net&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;DIS-SAM&#30456;&#27604;SAM&#21644;HQ-SA&#34920;&#29616;&#20986;&#26174;&#30528;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18593</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#24314;&#27169;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#25104;&#20026;&#22522;&#20110;&#23494;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#23588;&#20854;&#26159;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#22791;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#36890;&#36807;&#31616;&#21270;DDPM&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24341;&#20986;&#21478;&#19968;&#31181;&#31216;&#20026;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65288;DTPM&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;DTPM&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#30340;&#25193;&#25955;&#26102;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27492;&#21518;&#39564;&#20998;&#24067;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;ADBenh&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.01892</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#21644;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#36890;&#24120;&#20195;&#34920;&#26080;&#20851;&#29305;&#24449;&#30340;&#28151;&#21512;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#22810;&#20041;&#24615;&#65292;&#20351;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#20854;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#29305;&#24449;&#23481;&#37327;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#29305;&#24449;&#23481;&#37327;&#26159;&#27599;&#20010;&#29305;&#24449;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21344;&#29992;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#29609;&#20855;&#27169;&#22411;&#20013;&#65292;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65288;&#19982;&#20854;&#23545;&#25439;&#22833;&#30340;&#24433;&#21709;&#25104;&#27604;&#20363;&#65289;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#65292;&#22810;&#20041;&#24615;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#22312;&#24471;&#21040;&#26368;&#20248;&#23481;&#37327;&#20998;&#37197;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20998;&#22359;&#21322;&#27491;&#20132;&#30340;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
&lt;/p&gt;</description></item></channel></rss>