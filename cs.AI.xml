<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2404.02314</link><description>&lt;p&gt;
&#20998;&#23376;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#21542;&#30495;&#30340;&#38656;&#35201;&#20803;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-training Really Necessary for Molecular Few-Shot Learning ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#32780;&#26368;&#36817;&#24555;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#22823;&#22810;&#28041;&#21450;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20026;&#30452;&#25509;&#30340;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36991;&#20813;&#20102;&#25105;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#36864;&#21270;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#40657;&#21283;&#23376;&#35774;&#32622;&#65292;&#24182;&#28040;&#38500;&#20102;&#29305;&#23450;&#24773;&#33410;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#31454;&#20105;&#26041;&#27861;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;&#22522;&#32447;&#22987;&#32456;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02314v1 Announce Type: cross  Abstract: Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07277</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to OOD Robustness in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#30830;&#20445;&#31639;&#27861;&#23545;&#22270;&#20687;&#39046;&#22495;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#27809;&#26377;&#27880;&#37322;&#30340;&#22270;&#20687;&#12290;&#22312;&#38754;&#20020;&#30495;&#23454;&#19990;&#30028;&#30340;&#22495;&#20043;&#22806;&#65288;OOD&#65289;&#24178;&#25200;&#21644;&#36974;&#25377;&#30340;OOD-CV&#22522;&#20934;&#25361;&#25112;&#30340;&#28608;&#21169;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#23454;&#29616;&#29289;&#20307;&#20998;&#31867;&#30340;OOD&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24050;&#34987;&#35777;&#26126;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#20294;&#22312;OOD&#25968;&#25454;&#27979;&#35797;&#26102;&#20005;&#37325;&#38477;&#32423;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;CompNets&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;CompNets&#21253;&#21547;&#30340;&#22312;von Mises-Fisher&#65288;vMF&#65289;&#26680;&#34920;&#31034;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#23450;&#20041;&#30340;&#29983;&#25104;&#22836;&#65292;&#36825;&#20123;&#26680;&#22823;&#33268;&#23545;&#24212;&#20110;&#23545;&#35937;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#26576;&#20123;vMF&#26680;&#26159;&#30456;&#20284;&#30340;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#19981;&#26159;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;transiti
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07277v1 Announce Type: cross  Abstract: An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transiti
&lt;/p&gt;</description></item><item><title>DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.10269</link><description>&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#65306;&#31038;&#20132;&#23186;&#20307;&#33258;&#25105;&#25253;&#21578;&#30340;&#23457;&#26680;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media. (arXiv:2312.10269v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10269
&lt;/p&gt;
&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;2023&#24180;9&#26376;&#24320;&#22987;&#65292;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;(DSA)&#35201;&#27714;&#22823;&#22411;&#22312;&#32447;&#24179;&#21488;&#21521;DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#25552;&#20132;&#20851;&#20110;&#20182;&#20204;&#22312;&#27431;&#30431;&#20869;&#37319;&#21462;&#30340;&#27599;&#20010;&#23457;&#26680;&#34892;&#21160;&#30340;&#35814;&#32454;&#25968;&#25454;&#12290;&#20174;&#19968;&#24320;&#22987;&#65292;&#36825;&#20010;&#38598;&#20013;&#24335;&#25968;&#25454;&#24211;&#23601;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#26159;&#29616;&#23454;&#19990;&#30028;&#22312;&#32447;&#23457;&#26680;&#25968;&#25454;&#30340;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#12289;&#21487;&#33021;&#26159;&#29420;&#29305;&#30340;&#23453;&#24211;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27431;&#30431;&#20843;&#20010;&#26368;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#25968;&#25454;&#24211;&#30340;&#21069;100&#22825;&#25552;&#20132;&#30340;&#25152;&#26377;3.53&#20159;&#26465;&#35760;&#24405;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#24179;&#21488;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#65306;&#23457;&#26680;&#34892;&#21160;&#30340;&#25968;&#37327;&#12289;&#20915;&#31574;&#20381;&#25454;&#12289;&#24212;&#29992;&#30340;&#38480;&#21046;&#31867;&#22411;&#12289;&#23457;&#26680;&#20869;&#23481;&#31867;&#22411;&#12289;&#23457;&#26680;&#34892;&#21160;&#30340;&#21450;&#26102;&#24615;&#21644;&#25552;&#20132;&#24773;&#20917;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#33258;&#21160;&#21270;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#19982;&#24179;&#21488;&#33258;&#24049;&#30340;&#36879;&#26126;&#25253;&#21578;&#36827;&#34892;&#20102;&#20869;&#23481;&#20132;&#21449;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#12290;(i)&#24179;&#21488;&#21482;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36981;&#24490;&#20102;&#23457;&#26680;&#34892;&#21160;&#30340;&#21746;&#23398;&#21644;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item><item><title>MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.18407</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18407
&lt;/p&gt;
&lt;p&gt;
MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22522;&#20110; AI &#30340;&#33647;&#29289;&#21457;&#29616;&#24615;&#33021;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#30340;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#21270;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#21487;&#20197;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#33021;&#21147;&#12290;&#32780;&#29616;&#26377;&#30340;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20174;&#25299;&#25169;&#21644;&#20960;&#20309;&#32534;&#30721;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#20272;&#35745; MI&#65292;&#22240;&#27492;&#20002;&#22833;&#20102;&#20998;&#23376;&#30340;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MoleculeSDE&#12290;MoleculeSDE&#21033;&#29992;&#32676;&#23545;&#31216;&#65288;&#22914; SE&#65288;3&#65289;-&#31561;&#21464;&#21644;&#21453;&#23556;-&#21453;&#23545;&#31216;&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104; 3D &#20960;&#20309;&#24418;&#29366;&#19982; 2D &#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#23427;&#19981;&#20165;&#33719;&#24471;&#26356;&#32039;&#30340;MI&#30028;&#38480;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
&lt;/p&gt;</description></item></channel></rss>