<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.10107</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65306;&#29702;&#35299;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#22909;&#22788;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10107
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#36755;&#20837;$X$&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#39044;&#27979;&#30340;&#26465;&#20214;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25968;&#25454;&#28857;&#30340;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#26469;&#33258;&#20110;&#19982;&#36755;&#20837;&#26412;&#36523;&#30456;&#21516;&#30340;&#22495;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#22909;&#22788;&#30340;&#26465;&#20214;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#21046;&#23450;&#20102;&#20004;&#20010;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#36731;&#26494;&#39564;&#35777;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#26041;&#27861;&#26377;&#26395;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#35265;&#35299;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26631;&#20934;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#26377;&#21033;&#21644;&#19981;&#21033;&#30340;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#27169;&#22411;&#38754;&#20020;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
&lt;/p&gt;</description></item></channel></rss>