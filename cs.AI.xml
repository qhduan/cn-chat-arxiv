<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13534</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24120;&#24120;&#21463;&#30410;&#20110;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20570;&#27861;&#24341;&#20837;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#20351;&#27169;&#22411;&#21464;&#24471;&#22797;&#26434;&#65292;&#23548;&#33268;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#12290;TCL&#26694;&#26550;&#36890;&#36807;&#36880;&#28176;&#24341;&#20837;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#30340;&#19981;&#21516;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#65288;CWS&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#39640;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;TCL&#21152;&#36895;&#20102;&#35757;&#32451;&#24182;&#32531;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13534v1 Announce Type: cross  Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.13018</link><description>&lt;p&gt;
&#21160;&#24577;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#29992;&#20110;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#30340;&#39640;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24615;&#33021;&#25439;&#22833;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23545;&#27599;&#31181;&#35821;&#35328;&#36816;&#34892;&#22810;&#36718;&#20462;&#21098;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#20197;&#20004;&#31181;&#22330;&#26223;&#39640;&#25928;&#22320;&#20462;&#21098;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#65288;&#31216;&#20026;&#21160;&#24577;ASR&#36335;&#24452;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#36866;&#24212;&#23376;&#32593;&#32476;&#65292;&#36991;&#20813;&#23545;&#22266;&#23450;&#30340;&#23376;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#36807;&#26089;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35828;&#26126;&#20102;&#21160;&#24577;ASR&#36335;&#24452;&#36890;&#36807;&#33258;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#21021;&#22987;&#21270;&#36827;&#34892;&#35843;&#25972;&#65292;&#20849;&#21516;&#21457;&#29616;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#19968;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65288;&#36335;&#24452;&#65289;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.05201</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;"&#29983;&#25104;"&#24037;&#20316;&#65306;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#32463;&#39564;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
"Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets. (arXiv:2308.05201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#23545;&#20854;&#23545;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#22635;&#34917;&#29616;&#26377;&#30340;&#23454;&#35777;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#25512;&#20986;&#35299;&#37322;&#20026;&#19968;&#31181;&#22806;&#29983;&#20914;&#20987;&#65292;&#24182;&#37319;&#29992;&#24046;&#24322;&#27861;&#26469;&#37327;&#21270;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#20013;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#24037;&#20316;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#19979;&#38477;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#36807;&#21435;&#20132;&#26131;&#37327;&#25110;&#36739;&#20302;&#30340;&#36136;&#37327;&#26631;&#20934;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#26381;&#21153;&#25552;&#20379;&#21830;&#37117;&#26222;&#36941;&#32463;&#21382;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#36716;&#22411;&#26399;&#38388;&#65292;&#33021;&#22815;&#36866;&#24212;&#26032;&#36827;&#23637;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#21487;&#20197;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#30410;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#30340;&#20986;&#29616;&#26377;&#21487;&#33021;&#26367;&#20195;&#20154;&#21147;&#21171;&#21160;
&lt;/p&gt;
&lt;p&gt;
With the advent of general-purpose Generative AI, the interest in discerning its impact on the labor market escalates. In an attempt to bridge the extant empirical void, we interpret the launch of ChatGPT as an exogenous shock, and implement a Difference-in-Differences (DID) approach to quantify its influence on text-related jobs and freelancers within an online labor marketplace. Our results reveal a significant decrease in transaction volume for gigs and freelancers directly exposed to ChatGPT. Additionally, this decline is particularly marked in units of relatively higher past transaction volume or lower quality standards. Yet, the negative effect is not universally experienced among service providers. Subsequent analyses illustrate that freelancers proficiently adapting to novel advancements and offering services that augment AI technologies can yield substantial benefits amidst this transformative period. Consequently, even though the advent of ChatGPT could conceivably substitute
&lt;/p&gt;</description></item></channel></rss>