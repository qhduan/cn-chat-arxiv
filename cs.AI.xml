<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item></channel></rss>