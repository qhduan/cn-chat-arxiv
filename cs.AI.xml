<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15449</link><description>&lt;p&gt;
&#24974;&#24680;&#28304;&#20110;&#26080;&#30693;&#65281;&#23545;&#25239;&#20250;&#35805;&#24615;&#20167;&#24680;&#35328;&#35770;&#20013;&#35828;&#26381;&#26041;&#24335;&#30340;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#25239;&#35328;&#35770;&#20351;&#29992;&#30340;&#22240;&#32032;&#26159;&#29702;&#35299;&#22312;&#32447;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#26680;&#24515;&#12290;&#21508;&#31181;&#30740;&#31350;&#35780;&#20272;&#23545;&#25239;&#35328;&#35770;&#20013;&#20351;&#29992;&#30340;&#24773;&#24863;&#22522;&#30784;&#22240;&#32032;&#65292;&#22914;&#24773;&#24863;&#20849;&#40483;&#12289;&#20882;&#29359;&#31243;&#24230;&#21644;&#25932;&#24847;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20250;&#35805;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#26412;&#30740;&#31350;&#23558;&#35828;&#26381;&#26041;&#24335;&#20998;&#35299;&#20026;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#22312;&#28041;&#21450;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#23447;&#25945;&#38382;&#39064;&#30340;&#20004;&#31181;&#23545;&#35805;&#20132;&#20114;&#31867;&#22411;&#20013;&#30340;&#20351;&#29992;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20154;&#31867;&#19982;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22238;&#22797;&#30340;&#31435;&#22330;&#19982;&#27599;&#31181;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#35828;&#26381;&#26041;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#20132;&#20114;&#30340;&#23545;&#25239;&#35328;&#35770;&#35828;&#26381;&#26041;&#24335;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322; -- &#23588;&#20854;&#26159;&#22312;&#35805;&#39064;&#23618;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15449v1 Announce Type: cross  Abstract: Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13843</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Vision Transformers for Thyroid Carcinoma Diagnosis: A review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21457;&#23637;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#24110;&#21161;&#21307;&#23398;&#19987;&#23478;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#27835;&#30103;&#19981;&#21487;&#27835;&#24840;&#30142;&#30149;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35782;&#21035;&#30002;&#29366;&#33146;&#30284;&#65288;TC&#65289;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#32467;&#21512;transformer&#35780;&#20272;TC&#39044;&#21518;&#65292;&#24182;&#30830;&#23450;&#20010;&#20307;&#30340;&#24694;&#24615;&#39118;&#38505;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#21508;&#31181;&#20851;&#20110;&#20197;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;transformer&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30446;&#26631;&#21644;&#20351;&#29992;&#30340;&#35745;&#31639;&#29615;&#22659;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20854;&#29305;&#24449;&#23457;&#26597;&#21644;&#23545;&#27604;&#20102;&#21487;&#29992;&#30340;TC&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;AI&#24037;&#20855;&#22312;&#36890;&#36807;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#26041;&#24335;&#21327;&#21161;&#35786;&#26029;&#21644;&#27835;&#30103;TC&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13843v1 Announce Type: cross  Abstract: The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AIbased approaches, especially those employing transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artifcial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;LLM4SBR&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#36866;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#19988;&#26377;&#25928;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13840</link><description>&lt;p&gt;
LLM4SBR: &#19968;&#20010;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;LLM4SBR&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#36866;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#19988;&#26377;&#25928;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;(SBR)&#21033;&#29992;&#26469;&#33258;&#21311;&#21517;&#29992;&#25143;&#30340;&#20250;&#35805;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#25512;&#33616;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#38750;&#24120;&#39640;&#25928;&#65292;&#20294;&#29306;&#29298;&#20102;&#21830;&#21697;&#30340;&#22266;&#26377;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#20250;&#35805;&#30340;&#30495;&#27491;&#24847;&#22270;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34028;&#21187;&#21457;&#23637;&#65292;&#20026;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#24102;&#26469;&#20102;&#19968;&#32447;&#24076;&#26395;&#12290;&#21463;LLMs&#24433;&#21709;&#65292;&#25506;&#35752;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;(RS)&#38598;&#25104;&#30340;&#30740;&#31350;&#22914;&#38632;&#21518;&#26149;&#31499;&#33324;&#28044;&#29616;&#12290;&#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#25104;&#26412;&#65292;&#20197;&#21450;&#20250;&#35805;&#25968;&#25454;&#30701;&#26242;&#19988;&#21311;&#21517;&#30340;&#29305;&#24615;&#65292;&#31532;&#19968;&#20010;&#36866;&#21512;&#24037;&#19994;&#37096;&#32626;&#30340;LLM&#25512;&#33616;&#26694;&#26550;&#22312;SBR&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13840v1 Announce Type: cross  Abstract: Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12280</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39592;&#26550;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Adaptive Skeleton Graph Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;70&#20159;+&#65289;&#65307;&#28982;&#32780;&#65292;LLM&#25512;&#26029;&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65292;&#20363;&#22914;&#8220;&#24605;&#24819;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#20998;&#35299;&#20026;&#21487;&#20197;&#24182;&#34892;&#35299;&#30721;&#30340;&#23376;&#38382;&#39064;&#26469;&#25913;&#21892;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#36973;&#21463;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#29983;&#25104;&#23376;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#35831;&#27714;&#39069;&#22806;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#20851;&#31995;&#21644;&#38590;&#24230;&#65292;&#20197;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#26292;&#38706;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20381;&#36182;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20449;&#24687;&#36716;&#21457;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#21516;&#26102;&#26292;&#38706;&#29420;&#31435;&#23376;&#38382;&#39064;&#35299;&#30721;&#30340;&#24182;&#34892;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.06038</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#25235;&#21462;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#26088;&#22312;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#24110;&#21161;&#29992;&#25143;&#25235;&#21462;&#29289;&#20307;&#30340;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#28789;&#24039;&#25235;&#21462;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#30528;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#31574;&#30053;&#38656;&#35201;&#36866;&#24212;&#19981;&#21516;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#30001;&#20004;&#20010;&#23376;&#27169;&#22359;&#32452;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#19968;&#31181;&#25163;-&#29289;&#20307;&#26465;&#20214;&#25235;&#21462;&#21407;&#35821;&#31216;&#20026;Grasping Gradient Field&#65288;GraspGF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#12290;GraspGF&#36890;&#36807;&#20272;&#35745;&#26469;&#33258;&#25104;&#21151;&#25235;&#21462;&#31034;&#20363;&#38598;&#30340;&#26799;&#24230;&#26469;&#23398;&#20064;&#8220;&#22914;&#20309;&#8221;&#25235;&#21462;&#65292;&#32780;&#27531;&#24046;&#31574;&#30053;&#26681;&#25454;&#36712;&#36857;&#21382;&#21490;&#30830;&#23450;&#8220;&#20309;&#26102;&#8221;&#21644;&#20197;&#20309;&#31181;&#36895;&#24230;&#25191;&#34892;&#25235;&#21462;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field~(GraspGF), and a history-conditional residual policy. GraspGF learns `how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines `when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item></channel></rss>