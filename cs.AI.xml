<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14268</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Misinformation in Scientific News Reporting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14268
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#32469;&#36807;&#29983;&#25104;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#27493;&#39588;&#65292;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#26126;&#30830;&#26631;&#35760;&#32034;&#36180;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20107;&#23454;&#32463;&#24120;&#34987;&#22312;&#27969;&#34892;&#23186;&#20307;&#20013;&#25805;&#32437;&#65292;&#24847;&#22270;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#21644;&#34892;&#21160;&#65292;&#27491;&#22914;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#25152;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20004;&#31181;&#23186;&#20307;&#31867;&#22411;&#30340;&#20889;&#20316;&#39118;&#26684;&#26377;&#30528;&#26126;&#26174;&#19981;&#21516;&#65292;&#24182;&#19988;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#26816;&#27979;&#31185;&#23398;&#25253;&#36947;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14268v1 Announce Type: cross  Abstract: Scientific facts are often spun in the popular press with the intent to influence public opinion and action, as was evidenced during the COVID-19 pandemic. Automatic detection of misinformation in the scientific domain is challenging because of the distinct styles of writing in these two media types and is still in its nascence. Most research on the validity of scientific reporting treats this problem as a claim verification challenge. In doing so, significant expert human effort is required to generate appropriate claims. Our solution bypasses this step and addresses a more real-world scenario where such explicit, labeled claims may not be available. The central research question of this paper is whether it is possible to use large language models (LLMs) to detect misinformation in scientific reporting. To this end, we first present a new labeled dataset SciNews, containing 2.4k scientific news stories drawn from trusted and untrustwo
&lt;/p&gt;</description></item></channel></rss>