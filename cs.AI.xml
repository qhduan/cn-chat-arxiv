<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08004</link><description>&lt;p&gt;
Pix2Pix-OnTheFly: &#21033;&#29992;LLMs&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26368;&#36817;&#32467;&#21512;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#39044;&#22791;&#24037;&#20316;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08004v1 Announce Type: cross  Abstract: The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;</title><link>https://arxiv.org/abs/2403.02167</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#35782;&#21035;&#35821;&#38899;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition from voice messages recorded in the wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02167
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#34920;&#28436;&#25110;&#24341;&#21457;&#30340;&#35821;&#38899;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Emotional Voice Messages&#65288;EMOVOME&#65289;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#22312;&#28040;&#24687;&#24212;&#29992;&#20013;&#30340;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#30001;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#26631;&#27880;&#32773;&#20197;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#24773;&#24863;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;eGeMAPS&#29305;&#24449;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#35762;&#35805;&#32773;&#26080;&#20851;&#30340;SER&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#21442;&#32771;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20998;&#26512;&#20102;&#26631;&#27880;&#32773;&#21644;&#24615;&#21035;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#30340;Unispeech-L&#27169;&#22411;&#21450;&#20854;&#19982;eGeMAPS&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#22312;3&#31867;valence&#21644;arousal&#39044;&#27979;&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;61.64%&#21644;55.57%&#30340;Unweighted Accuracy&#65288;UA&#65289;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;&#23545;&#20110;&#24773;&#24863;&#31867;&#21035;&#65292;&#33719;&#24471;&#20102;42.58%&#30340;UA&#12290;EMOVOME&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18908</link><description>&lt;p&gt;
&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Facility Location Games with Scaling Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18908
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#35268;&#27169;&#25928;&#24212;&#30340;&#35774;&#26045;&#36873;&#22336;&#28216;&#25103;&#65292;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#19979;&#20195;&#29702;&#21487;&#33021;&#19981;&#20877;&#21333;&#23792;&#20559;&#22909;&#30340;&#26465;&#20214;&#19982;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#20010;&#20154;&#25104;&#26412;&#20989;&#25968;&#31561;&#20110;&#20182;&#20204;&#36317;&#31163;&#35774;&#26045;&#30340;&#36317;&#31163;&#20056;&#20197;&#19968;&#20010;&#30001;&#35774;&#26045;&#20301;&#32622;&#30830;&#23450;&#30340;&#27604;&#20363;&#22240;&#23376;&#12290;&#38500;&#20102;&#19968;&#33324;&#31867;&#21035;&#30340;&#36830;&#32493;&#27604;&#20363;&#20989;&#25968;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#30340;&#27604;&#20363;&#20989;&#25968;&#30340;&#20998;&#27573;&#32447;&#24615;&#27604;&#20363;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#30340;&#30446;&#26631;&#65292;&#24182;&#25551;&#36848;&#20102;&#26368;&#20248;&#35299;&#30340;&#35745;&#31639;&#12290;&#28982;&#21518;&#25105;&#20204;&#36716;&#21521;&#36817;&#20284;&#26426;&#21046;&#35774;&#35745;&#35774;&#32622;&#65292;&#35266;&#23519;&#21040;&#20195;&#29702;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#20877;&#26159;&#21333;&#23792;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#30830;&#20445;&#20195;&#29702;&#20855;&#26377;&#21333;&#23792;&#20559;&#22909;&#30340;&#27604;&#20363;&#20989;&#25968;&#26465;&#20214;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#33021;&#22815;&#36890;&#36807;strategyproof&#21644;anonymous me&#36798;&#21040;&#30340;&#24635;&#25104;&#26412;&#21644;&#26368;&#22823;&#25104;&#26412;&#36817;&#20284;&#27604;&#29575;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18908v1 Announce Type: cross  Abstract: We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous me
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.04376</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for learning with real and surrogate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#25104;&#26412;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#33539;&#22260;&#20869;, &#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#30456;&#21453;&#22320;, &#21487;&#20197;&#23558;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#25110;&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;, &#20316;&#20026;&#26367;&#20195;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#23558;&#26367;&#20195;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;, &#24182;&#20351;&#29992;&#29702;&#35770;&#27169;&#22411;&#21644;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(i) &#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21407;&#22987;&#20998;&#24067;&#30340;&#27979;&#35797;&#35823;&#24046;&#65307;(ii) &#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#25928;&#30410;, &#20351;&#29992;&#26368;&#20248;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38750;&#24120;&#20851;&#38190;&#65307;(iii) &#22312;&#28151;&#21512;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#12290;&#36825;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11178</link><description>&lt;p&gt;
FocDepthFormer: &#20351;&#29992;LSTM&#30340;Transformer&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus. (arXiv:2310.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11178
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28966;&#28857;&#22534;&#26632;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#22534;&#26632;&#20013;&#30340;&#28966;&#28857;/&#31163;&#28966;&#32447;&#32034;&#25512;&#26029;&#28145;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#22270;&#20687;&#22534;&#26632;&#19978;&#24212;&#29992;&#20108;&#32500;&#25110;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20197;&#22312;&#22270;&#20687;&#21644;&#22534;&#26632;&#20043;&#38388;&#23398;&#20064;&#29305;&#24449;&#12290;&#30001;&#20110;CNN&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#22788;&#29702;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#19968;&#33268;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;&#22534;&#26632;&#19978;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;FocDepthFormer&#65292;&#20027;&#35201;&#30001;&#24102;&#26377;LSTM&#27169;&#22359;&#21644;CNN&#35299;&#30721;&#22120;&#30340;Transformer&#32452;&#25104;&#12290;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#36890;&#36807;&#38544;&#21547;&#38750;&#23616;&#37096;&#20132;&#21449;&#21442;&#32771;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;LSTM&#27169;&#22359;&#34987;&#23398;&#20064;&#29992;&#20110;&#23558;&#34920;&#31034;&#38598;&#25104;&#21040;&#20855;&#26377;&#20219;&#24847;&#22270;&#20687;&#30340;&#22534;&#26632;&#20013;&#12290;&#20026;&#20102;&#30452;&#25509;&#25429;&#33719;&#20302;&#32423;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level featur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;</title><link>http://arxiv.org/abs/2307.03937</link><description>&lt;p&gt;
&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks. (arXiv:2307.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HINs)&#26159;&#20855;&#26377;&#22810;&#31181;&#33410;&#28857;&#31867;&#22411;&#21644;&#36793;&#31867;&#22411;&#30340;&#20449;&#24687;&#32593;&#32476;&#12290;&#20803;&#36335;&#24452;&#30340;&#27010;&#24565;&#21363;&#19968;&#31995;&#21015;&#36830;&#25509;&#20004;&#20010;&#23454;&#20307;&#30340;&#23454;&#20307;&#31867;&#22411;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#24207;&#21015;&#34987;&#25552;&#20986;&#20026;&#25552;&#20379;&#23545;&#19981;&#21516;HIN&#20219;&#21153;&#30340;&#20803;&#32423;&#21487;&#35299;&#37322;&#35821;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#20803;&#36335;&#24452;&#20027;&#35201;&#29992;&#20110;&#27169;&#24335;&#31616;&#21333;&#30340;HINs&#65292;&#20363;&#22914;&#21482;&#26377;&#23569;&#37327;&#23454;&#20307;&#31867;&#22411;&#30340;&#25991;&#29486;&#32593;&#32476;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20803;&#36335;&#24452;&#36890;&#24120;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#26522;&#20030;&#12290;&#28982;&#32780;&#65292;&#20803;&#36335;&#24452;&#22312;&#27169;&#24335;&#22797;&#26434;&#30340;HINs(&#20363;&#22914;&#20855;&#26377;&#25968;&#30334;&#31181;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#30693;&#35782;&#24211;)&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30001;&#20803;&#36335;&#24452;&#26522;&#20030;&#24341;&#36215;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#35780;&#20272;&#20803;&#36335;&#24452;&#38656;&#35201;&#26522;&#20030;&#30456;&#20851;&#36335;&#24452;&#23454;&#20363;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#20803;&#36335;&#24452;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#24335;&#22797;&#26434;&#30340;HINs&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learning framework for schema-complex HINs. We represent m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30528;&#30524;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#20986;&#21457;&#25552;&#20986;&#20102;&#35299;&#20915;&#20559;&#35265;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06967</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#27491;AI&#30340;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Data quality dimensions for fair AI. (arXiv:2305.06967v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#30524;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#20986;&#21457;&#25552;&#20986;&#20102;&#35299;&#20915;&#20559;&#35265;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#38750;&#26412;&#36136;&#19978;&#20855;&#26377;&#20013;&#31435;&#24615;&#65292;&#22240;&#27492;&#20559;&#35265;&#20250;&#28183;&#36879;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#25216;&#26415;&#24037;&#20855;&#20013;&#12290;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20154;&#31867;&#26102;&#65292;AI&#31639;&#27861;&#20250;&#21453;&#26144;&#20986;&#28304;&#20110;&#38169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25216;&#26415;&#38169;&#35823;&#12290;&#30001;&#20110;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#21644;&#27495;&#35270;&#24615;&#30340;&#20998;&#31867;&#65292;&#24310;&#32493;&#20102;&#32467;&#26500;&#24615;&#31181;&#26063;&#20027;&#20041;&#21644;&#36793;&#32536;&#21270;&#29616;&#35937;&#65292;&#36825;&#20123;&#31995;&#32479;&#24182;&#26410;&#31995;&#32479;&#22320;&#38450;&#33539;&#20559;&#35265;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;AI&#31995;&#32479;&#20559;&#35265;&#38382;&#39064;&#65292;&#20197;&#20004;&#20010;&#36890;&#24120;&#36739;&#20026;&#22256;&#38590;&#30340;&#24773;&#22659;&#65292;&#21363;&#38750;&#20108;&#20803;&#20010;&#20307;&#30340;&#20998;&#31867;&#21644;&#36328;&#24615;&#21035;&#20010;&#20307;&#30340;&#20998;&#31867;&#20026;&#20363;&#65292;&#35828;&#26126;&#20102;&#20559;&#35265;&#32531;&#35299;&#24037;&#20855;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#30830;&#23450;&#35201;&#23454;&#26045;&#30340;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#30446;&#30340;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#24314;&#35758;&#22312;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#26041;&#38754;&#32771;&#34385;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems are not intrinsically neutral and biases trickle in any type of technological tool. In particular when dealing with people, AI algorithms reflect technical errors originating with mislabeled data. As they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. In this article we consider the problem of bias in AI systems from the point of view of Information Quality dimensions. We illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. The identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. Hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item></channel></rss>