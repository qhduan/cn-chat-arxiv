<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#27010;&#29575;&#21270;&#25551;&#36848;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08646</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#25512;&#29702;&#25277;&#35937;&#30340;&#32479;&#19968;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#27010;&#29575;&#21270;&#25551;&#36848;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#23545;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#22823;&#33041;&#21151;&#33021;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#29575;&#21270;&#35299;&#37322;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#25551;&#36848;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23545;&#23427;&#20204;&#36827;&#34892;&#25551;&#36848;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item></channel></rss>