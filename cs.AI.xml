<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.05168</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#35299;&#38145;&#22810;&#27169;&#24577;&#32479;&#19968;&#31163;&#25955;&#34920;&#31034;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#32479;&#19968;&#30721;&#26412;&#30340;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;DCID&#65289;&#27169;&#22411;&#22312;&#23454;&#29616;&#32454;&#31890;&#24230;&#34920;&#31034;&#21644;&#36328;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#21463;&#21040;&#23545;&#25152;&#26377;&#36890;&#36947;&#30340;&#22343;&#31561;&#23545;&#24453;&#20197;&#21450;&#24573;&#35270;&#27425;&#35201;&#20107;&#20214;&#20449;&#24687;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;&#26469;&#33258;&#26080;&#20851;&#36890;&#36947;&#30340;&#24178;&#25200;&#24182;&#22312;&#32454;&#31890;&#24230;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#65288;TOC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#36873;&#25321;&#37325;&#35201;&#36890;&#36947;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;H-DCID&#65289;&#26041;&#27861;&#23558;&#20449;&#24687;&#20998;&#31163;&#21644;&#23545;&#40784;&#25193;&#23637;&#21040;&#20004;&#20010;&#32423;&#21035;&#65292;&#25429;&#25417;&#26356;&#22810;&#36328;&#27169;&#24577;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05168v1 Announce Type: cross  Abstract: Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements a
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#22833;&#26799;&#24230;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06026</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20197;&#32531;&#35299;&#24179;&#26495;&#22369;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Quantum neural network with ensemble learning to mitigate barren plateaus and cost function concentration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#22833;&#26799;&#24230;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24555;&#36895;&#21457;&#23637;&#25215;&#35834;&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#39046;&#22495;&#20135;&#29983;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#21069;&#27839;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#22411;&#65292;&#20294;&#25345;&#32493;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#28040;&#22833;&#26799;&#24230;&#65288;VG&#65289;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#65288;CFC&#65289;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#25104;&#21151;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;VG&#21644;CFC&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#65292;&#25512;&#23815;&#21516;&#26102;&#37096;&#32626;&#22810;&#20010;&#28145;&#24230;&#20026;1&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#21333;&#19968;&#28145;&#24230;&#20026;L&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#23637;&#24320;&#65292;&#20026;&#20102;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21069;&#26223;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of quantum computers promises transformative impacts across diverse fields of science and technology. Quantum neural networks (QNNs), as a forefront application, hold substantial potential. Despite the multitude of proposed models in the literature, persistent challenges, notably the vanishing gradient (VG) and cost function concentration (CFC) problems, impede their widespread success. In this study, we introduce a novel approach to quantum neural network construction, specifically addressing the issues of VG and CFC. Our methodology employs ensemble learning, advocating for the simultaneous deployment of multiple quantum circuits with a depth equal to $1$, a departure from the conventional use of a single quantum circuit with depth $L$. We assess the efficacy of our proposed model through a comparative analysis with a conventionally constructed QNN. The evaluation unfolds in the context of a classification problem, yielding valuable insights into the potential a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00350</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Based Fuzzing Techniques: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36719;&#20214;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#26102;&#20195;&#65292;&#36719;&#20214;&#23433;&#20840;&#21644;&#28431;&#27934;&#20998;&#26512;&#23545;&#20110;&#36719;&#20214;&#24320;&#21457;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#65292;&#27169;&#31946;&#27979;&#35797;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#24182;&#19981;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#36719;&#20214;&#28431;&#27934;&#19981;&#26029;&#28436;&#21270;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#36235;&#21183;&#26159;&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#27169;&#31946;&#27979;&#35797;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#34701;&#21512;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#36890;&#36807;&#24635;&#32467;2024&#24180;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#36824;&#30740;&#31350;&#20102;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also invest
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.05502</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#65306;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#22810;&#20010;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#24418;&#25104;&#20132;&#21449;&#30340;&#32452;&#12290;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30830;&#20445;&#20174;&#27599;&#20010;&#32452;&#20013;&#36873;&#25321;&#26368;&#23569;&#25968;&#37327;&#30340;&#32858;&#31867;&#20013;&#24515;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#65292;&#21487;&#20197;&#26159;$k$-&#20013;&#20301;&#25968;&#65292;$k$-&#22343;&#20540;&#25110;$k$-&#20379;&#24212;&#21830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#65292;$1+\frac{8}{e}$&#21644;$3$&#65292;&#29992;&#20110;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20013;&#20301;&#25968;&#65292;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#22343;&#20540;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20379;&#24212;&#21830;&#12290;&#36825;&#20123;&#36817;&#20284;&#27604;&#22312;&#20551;&#35774;Gap-ETH&#21644;FPT $\neq$ W[2]&#30340;&#24773;&#20917;&#19979;&#26159;&#32039;&#30830;&#30340;&#12290;&#23545;&#20110;&#20844;&#24179;$k$-&#20013;&#20301;&#25968;&#21644;&#20844;&#24179;$k$-&#22343;&#20540;&#30340;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#21644;$1+\frac{8}{e}$&#12290;&#23545;&#20110;&#20855;&#26377;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#30340;&#20844;&#24179;$k$-&#20379;&#24212;&#21830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#65292;&#22240;&#23376;&#20026;$3$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13259</link><description>&lt;p&gt;
WikiMT++&#25968;&#25454;&#38598;&#21345;&#29255;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ Dataset Card. (arXiv:2309.13259v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13259
&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;WikiMusicText&#65288;WikiMT&#65289;&#30340;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#20026;&#20102;&#25193;&#23637;WikiMT&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#65288;&#19987;&#36753;&#12289;&#27468;&#35789;&#12289;&#35270;&#39057;&#65289;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65288;12&#20010;&#24773;&#24863;&#24418;&#23481;&#35789;&#65289;&#21644;&#24773;&#24863;4Q&#65288;Russell 4Q&#65289;&#65292;&#22686;&#24378;&#20102;&#20854;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#12289;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#12289;&#33258;&#21160;&#20316;&#26354;&#21644;&#24773;&#24863;&#20998;&#31867;&#31561;&#26041;&#38754;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;CLaMP&#26469;&#32416;&#27491;&#20174;WikiMT&#32487;&#25215;&#30340;&#23646;&#24615;&#65292;&#20197;&#20943;&#23569;&#21407;&#22987;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38169;&#35823;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT), featuring 1010 curated lead sheets in ABC notation. To expand application scenarios of WikiMT, we add both objective (album, lyrics, video) and subjective emotion (12 emotion adjectives) and emo\_4q (Russell 4Q) attributes, enhancing its usability for music information retrieval, conditional music generation, automatic composition, and emotion classification, etc. Additionally, CLaMP is implemented to correct the attributes inherited from WikiMT to reduce errors introduced during original data collection and enhance the accuracy and completeness of our dataset.
&lt;/p&gt;</description></item><item><title>PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12555</link><description>&lt;p&gt;
PlanFitting&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12555
&lt;/p&gt;
&lt;p&gt;
PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#23545;&#20110;&#30830;&#20445;&#36275;&#22815;&#30340;&#20307;&#32946;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20154;&#20204;&#30340;&#22797;&#26434;&#26085;&#31243;&#21644;&#32771;&#34385;&#22240;&#32032;&#20197;&#21450;&#35745;&#21010;&#30340;&#21019;&#24314;&#36890;&#24120;&#38656;&#35201;&#19982;&#19987;&#23478;&#30340;&#21453;&#22797;&#27807;&#36890;&#65292;&#36825;&#19968;&#36807;&#31243;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PlanFitting&#65292;&#23427;&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#36741;&#21161;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;PlanFitting&#20351;&#29992;&#25143;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21508;&#31181;&#32422;&#26463;&#21644;&#26597;&#35810;&#65292;&#20174;&#32780;&#20415;&#20110;&#21019;&#24314;&#21644;&#20248;&#21270;&#36866;&#21512;&#20854;&#29305;&#23450;&#24773;&#20917;&#30340;&#27599;&#21608;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#20445;&#25345;&#22522;&#26412;&#21407;&#21017;&#30340;&#25166;&#26681;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#65288;N=18&#65289;&#20351;&#29992;PlanFitting&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19987;&#23478;&#35268;&#21010;&#32773;&#65288;N=3&#65289;&#23545;&#36825;&#20123;&#35745;&#21010;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;PlanFitting&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;AI&#21161;&#25163;&#22312;&#21019;&#24314;&#35745;&#21010;&#26041;&#38754;&#30340;&#26410;&#26469;&#35774;&#35745;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#22871;&#32467;&#26500;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#23427;&#25552;&#20986;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#21644;LLM-assisted&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#65292;&#24110;&#21161;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#65292;&#24182;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12132</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A knowledge representation approach for construction contract knowledge modeling. (arXiv:2309.12132v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#22871;&#32467;&#26500;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#23427;&#25552;&#20986;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#21644;LLM-assisted&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#65292;&#24110;&#21161;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#65292;&#24182;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#24314;&#31569;&#21512;&#21516;&#31649;&#29702;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#20943;&#23569;&#20102;&#20154;&#20026;&#38169;&#35823;&#65292;&#24182;&#33410;&#30465;&#20102;&#22823;&#37327;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;LLMs&#21487;&#33021;&#20250;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#20294;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19987;&#23478;&#39537;&#21160;&#30340;&#21512;&#21516;&#30693;&#35782;&#21487;&#20197;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#34920;&#31034;&#65292;&#20197;&#32422;&#26463;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#23884;&#22871;&#32467;&#26500;&#26469;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#22797;&#26434;&#24615;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#23884;&#22871;&#30693;&#35782;&#34920;&#31034;&#26694;&#26550;&#12289;&#19968;&#20010;&#24314;&#31435;&#22312;&#35813;&#26694;&#26550;&#19978;&#30340;NCKG&#26412;&#20307;&#20197;&#21450;&#19968;&#31181;&#23454;&#29616;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;NCKG&#20013;&#22686;&#24378;&#22806;&#37096;&#30693;&#35782;&#30340;LLM&#36741;&#21161;&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;LLM&#21644;KG&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) presents an unprecedented opportunity to automate construction contract management, reducing human errors and saving significant time and costs. However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise. To address this issue, expert-driven contract knowledge can be represented in a structured manner to constrain the automatic contract management process. This paper introduces the Nested Contract Knowledge Graph (NCKG), a knowledge representation approach that captures the complexity of contract knowledge using a nested structure. It includes a nested knowledge representation framework, a NCKG ontology built on the framework, and an implementation method. Furthermore, we present the LLM-assisted contract review pipeline enhanced with external knowledge in NCKG. Our pipeline achieves a promising performance in contract risk reviewing, shedding light on the combination of LLM and KG towards m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10908</link><description>&lt;p&gt;
&#22810;&#21103;&#26412;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Multicopy Reinforcement Learning Agents. (arXiv:2309.10908v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#25110;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#21333;&#20010;&#26234;&#33021;&#20307;&#20219;&#21153;&#12290;&#22914;&#26524;&#29615;&#22659;&#22024;&#26434;&#65292;&#24182;&#19988;&#21333;&#20010;&#26234;&#33021;&#20307;&#21103;&#26412;&#26377;&#26102;&#26080;&#27861;&#23436;&#25104;&#20219;&#21153;&#65292;&#21017;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#21103;&#26412;&#38382;&#39064;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#22914;&#20309;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07992</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23500;&#21547;&#22270;&#29255;&#31561;&#35270;&#35273;&#25968;&#25454;&#19982;&#29289;&#21697;&#20851;&#32852;&#24230;&#22686;&#21152;&#65292;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65288;VARS&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;VARS&#26131;&#21463;&#21040;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21521;&#19982;&#36825;&#20123;&#29289;&#21697;&#20851;&#32852;&#30340;&#24178;&#20928;&#22270;&#20687;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#23545;VARS&#30340;&#25915;&#20987;&#20026;&#24191;&#27867;&#20351;&#29992;VARS&#30340;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#24102;&#26469;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22914;&#20309;&#20445;&#25252;VARS&#20813;&#21463;&#27492;&#31867;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;VARS&#35270;&#35273;&#25915;&#20987;&#30340;&#23433;&#20840;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;VARS&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;(1)&#36890;&#36807;&#22522;&#20110;&#20840;&#23616;&#35270;&#35273;&#20256;&#36755;&#30340;&#22270;&#20687;&#37325;&#26500;&#26469;&#38450;&#24481;&#20197;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;(2)&#20351;&#29992;&#22312;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#23545;VARS&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03340</link><description>&lt;p&gt;
&#23450;&#21521;&#28436;&#21270;&#21644;&#29983;&#24577;&#36827;&#21270;&#21160;&#21147;&#23398;&#30340;&#29983;&#29289;&#29289;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics. (arXiv:2305.03340v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#21160;&#21147;&#23398;&#20013;&#30340;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#21487;&#20197;&#22312;&#21338;&#24328;&#29702;&#35770;&#32972;&#26223;&#19979;&#20197;&#38543;&#26426;&#36712;&#36857;&#20998;&#26512;&#30340;&#26041;&#24335;&#26377;&#24847;&#20041;&#22320;&#26144;&#23556;&#21040;&#20998;&#26512;&#20013;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#23569;&#37327;&#19981;&#21516;&#30340;&#32676;&#20307;&#21644;/&#25110;&#20551;&#35774;&#21160;&#21147;&#23398;&#21457;&#29983;&#22312;&#20154;&#21475;&#35268;&#27169;&#22823;&#21040;&#36275;&#20197;&#20351;&#30830;&#23450;&#24615;&#36712;&#36857;&#25104;&#20026;&#29616;&#23454;&#30340;&#21306;&#22495;&#12290;&#34987;&#31216;&#20026;&#8220;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#8221;&#30340;&#29983;&#24577;&#22240;&#32032;&#30340;&#28155;&#21152;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#21160;&#21147;&#23398;&#65292;&#24182;&#23548;&#33268;&#35768;&#22810;&#38382;&#39064;&#38590;&#20197;&#22788;&#29702;&#25110;&#24403;&#21069;&#30340;&#29702;&#35770;&#26041;&#27861;&#38590;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#19968;&#31181;&#31867;&#20284;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#26041;&#27861;&#26159;&#23558;&#37325;&#28857;&#25918;&#22312;&#27169;&#22411;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#19978;&#65292;&#20381;&#25454;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21644;&#30456;&#37051;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#35821;&#35328;&#65292;&#32780;&#34987;&#31216;&#20026;&#8220;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#20598;&#24615;&#65292;&#23558;&#21516;&#26102;&#32771;&#34385;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#30340;&#22797;&#26434;&#24615;&#26144;&#23556;&#21040;&#19968;&#20010;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many major questions in the theory of evolutionary dynamics can in a meaningful sense be mapped to analyses of stochastic trajectories in game theoretic contexts. Often the approach is to analyze small numbers of distinct populations and/or to assume dynamics occur within a regime of population sizes large enough that deterministic trajectories are an excellent approximation of reality. The addition of ecological factors, termed "eco-evolutionary dynamics", further complicates the dynamics and results in many problems which are intractable or impractically messy for current theoretical methods. However, an analogous but underexplored approach is to analyze these systems with an eye primarily towards uncertainty in the models themselves. In the language of researchers in Reinforcement Learning and adjacent fields, a Partially Observable Markov Process. Here we introduce a duality which maps the complexity of accounting for both ecology and individual genotypic/phenotypic types onto a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item></channel></rss>