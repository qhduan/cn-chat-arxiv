<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#35270;&#35282;&#19979;&#36827;&#34892;&#21407;&#26408;&#30340;&#19977;&#32500;&#23618;&#26512;&#25104;&#20687;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.02820</link><description>&lt;p&gt;
&#38024;&#23545;&#26408;&#26448;&#24037;&#19994;&#20013;&#24212;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#23618;&#26512;&#25104;&#20687;&#30340;&#38271;&#29289;&#20307;&#30340;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Reconstruction for Sparse View Tomography of Long Objects Applied to Imaging in the Wood Industry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#65292;&#21487;&#20197;&#22312;&#31232;&#30095;&#35270;&#35282;&#19979;&#36827;&#34892;&#21407;&#26408;&#30340;&#19977;&#32500;&#23618;&#26512;&#25104;&#20687;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26408;&#26448;&#24037;&#19994;&#20013;&#65292;&#36890;&#36807;&#22312;&#31227;&#21160;&#20256;&#36865;&#24102;&#19978;&#20174;&#20960;&#20010;&#28304;&#20301;&#32622;&#36827;&#34892;&#31163;&#25955;X&#23556;&#32447;&#25195;&#25551;&#26469;&#23545;&#21407;&#26408;&#36827;&#34892;&#24120;&#35268;&#36136;&#37327;&#31579;&#26597;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#33719;&#24471;&#20108;&#32500;&#65288;2D&#65289;&#20999;&#29255;&#27979;&#37327;&#12290;&#27599;&#20010;2D&#20999;&#29255;&#21333;&#29420;&#19981;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#36827;&#34892;&#19977;&#32500;&#23618;&#26512;&#37325;&#24314;&#65292;&#22312;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#21407;&#26408;&#29983;&#29289;&#29305;&#24449;&#24471;&#20197;&#24456;&#22909;&#20445;&#30041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21407;&#22987;-&#23545;&#20598;&#31070;&#32463;&#32593;&#32476;&#30340;&#36845;&#20195;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39034;&#24207;&#25195;&#25551;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#31215;&#32047;&#20102;&#30456;&#37051;&#20999;&#29255;&#20043;&#38388;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#20165;&#22312;&#37325;&#24314;&#26399;&#38388;&#32771;&#34385;&#21333;&#20010;&#20999;&#29255;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20215;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#20351;&#29992;&#20116;&#20010;&#28304;&#20301;&#32622;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#30340;&#21407;&#26408;&#37325;&#24314;&#36275;&#22815;&#20934;&#30830;&#65292;&#20197;&#35782;&#21035;&#20687;&#33410;&#65288;&#20998;&#25903;&#65289;&#12289;&#24515;&#26448;&#31561;&#29983;&#29289;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02820v1 Announce Type: new  Abstract: In the wood industry, logs are commonly quality screened by discrete X-ray scans on a moving conveyor belt from a few source positions. Typically, two-dimensional (2D) slice-wise measurements are obtained by a sequential scanning geometry. Each 2D slice alone does not carry sufficient information for a three-dimensional tomographic reconstruction in which biological features of interest in the log are well preserved. In the present work, we propose a learned iterative reconstruction method based on the Learned Primal-Dual neural network, suited for sequential scanning geometries. Our method accumulates information between neighbouring slices, instead of only accounting for single slices during reconstruction. Our quantitative and qualitative evaluations with as few as five source positions show that our method yields reconstructions of logs that are sufficiently accurate to identify biological features like knots (branches), heartwood an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00751</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Algorithms for In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#26410;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#31934;&#30830;&#30340;&#21435;&#23398;&#20064;&#8212;&#8212;&#22312;&#27809;&#26377;&#20351;&#29992;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#27169;&#22411;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#22411;&#8212;&#8212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#20302;&#25928;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#36827;&#34892;&#20219;&#21153;&#36866;&#24212;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25928;&#31934;&#30830;&#21435;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21152;&#21040;LLM&#30340;&#25552;&#31034;&#21069;&#38754;&#65288;&#29992;&#20110;&#20219;&#21153;&#36866;&#24212;&#65289;&#65292;&#21517;&#20026;ERASE&#65292;&#23427;&#30340;&#21435;&#23398;&#20064;&#25805;&#20316;&#25104;&#26412;&#19982;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10300</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#19982;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#36890;&#24120;&#20551;&#23450;&#24050;&#26377;&#25928;&#29992;&#20989;&#25968;&#12289;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#25110;&#23581;&#35797;&#30830;&#23450;&#23436;&#25972;&#30340;Pareto&#21069;&#27839;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#32467;&#26524;&#24448;&#24448;&#22522;&#20110;&#38544;&#21547;&#21644;&#26174;&#24615;&#30340;&#19987;&#23478;&#30693;&#35782;&#65292;&#38590;&#20197;&#23450;&#20041;&#19968;&#20010;&#25928;&#29992;&#20989;&#25968;&#65292;&#32780;&#20114;&#21160;&#23398;&#20064;&#25110;&#21518;&#32493;&#21551;&#21457;&#24335;&#38656;&#35201;&#21453;&#22797;&#24182;&#19988;&#26114;&#36149;&#22320;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#65288;&#25104;&#23545;&#30340;&#65289;&#32467;&#26524;&#20559;&#22909;&#65292;&#32780;&#19988;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#32467;&#26524;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#21040;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
&lt;/p&gt;</description></item></channel></rss>