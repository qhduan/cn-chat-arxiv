<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00935</link><description>&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25919;&#31574;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Policy Expansion for Bridging Offline-to-Online Reinforcement Learning. (arXiv:2302.00935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#26159;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#30340;&#31574;&#30053;&#21021;&#22987;&#21270;&#22312;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#12290;&#22312;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20316;&#31574;&#30053;&#38598;&#20013;&#30340;&#19968;&#20010;&#20505;&#36873;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21478;&#19968;&#20010;&#31574;&#30053;&#26469;&#25193;&#23637;&#31574;&#30053;&#38598;&#65292;&#35813;&#31574;&#30053;&#23558;&#36127;&#36131;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#12290;&#20004;&#20010;&#31574;&#30053;&#23558;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#32452;&#21512;&#36215;&#26469;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20808;&#21069;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#23436;&#20840;&#22312;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#20013;&#24471;&#20197;&#20445;&#30041;&#65292;&#22240;&#27492;&#20943;&#36731;&#20102;&#28508;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#22312;&#32447;&#23398;&#20064;&#30340;&#21021;&#22987;&#38454;&#27573;&#30772;&#22351;&#31163;&#32447;&#31574;&#30053;&#30340;&#26377;&#29992;&#34892;&#20026;&#65292;&#21516;&#26102;&#20801;&#35768;&#31163;&#32447;&#31574;&#30053;&#22312;&#33258;&#36866;&#24212;&#26041;&#24335;&#19979;&#33258;&#28982;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
Pre-training with offline data and online fine-tuning using reinforcement learning is a promising strategy for learning control policies by leveraging the best of both worlds in terms of sample efficiency and performance. One natural approach is to initialize the policy for online learning with the one trained offline. In this work, we introduce a policy expansion scheme for this task. After learning the offline policy, we use it as one candidate policy in a policy set. We then expand the policy set with another policy which will be responsible for further learning. The two policies will be composed in an adaptive manner for interacting with the environment. With this approach, the policy previously learned offline is fully retained during online learning, thus mitigating the potential issues such as destroying the useful behaviors of the offline policy in the initial stage of online learning while allowing the offline policy participate in the exploration naturally in an adaptive mann
&lt;/p&gt;</description></item></channel></rss>