<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12093</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#23398;&#20064;&#65306;&#19968;&#31181;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#22330;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#22312;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#21644;&#31038;&#20250;&#31283;&#23450;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22522;&#20110;Stackelberg Mean Field Game&#65288;SMFG&#65289;&#27169;&#22411;&#65292;&#23558;&#26368;&#20248;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#38382;&#39064;&#24314;&#27169;&#65292;&#20854;&#20013;&#25919;&#24220;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#30340;&#39046;&#23548;&#32773;&#65292;&#22823;&#35268;&#27169;&#23478;&#24237;&#21160;&#24577;&#21709;&#24212;&#20026;&#36861;&#38543;&#32773;&#12290;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#25429;&#25417;&#20102;&#25919;&#24220;&#21644;&#22823;&#35268;&#27169;&#23478;&#24237;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#21160;&#24577;&#21338;&#24328;&#65292;&#24182;&#21487;&#20197;&#35299;&#37322;&#22320;&#35780;&#20272;&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#25928;&#26524;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;SMFG&#30340;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#65288;SMFRL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#20808;&#21069;&#30340;&#29615;&#22659;&#30693;&#35782;&#21644;&#36716;&#21464;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;SMFG&#26041;&#27861;&#22312;&#32463;&#27982;&#25919;&#31574;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12093v1 Announce Type: cross  Abstract: Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;</title><link>https://arxiv.org/abs/2403.08291</link><description>&lt;p&gt;
CleanAgent&#65306;&#22522;&#20110;LLM&#20195;&#29702;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
CleanAgent: Automating Data Standardization with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#26159;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#35832;&#22914;Pandas&#20043;&#31867;&#30340;&#24037;&#20855;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#23450;&#21046;&#20195;&#30721;&#20197;&#36866;&#24212;&#19981;&#21516;&#21015;&#31867;&#22411;&#30340;&#25163;&#21160;&#25805;&#20316;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#19994;&#31243;&#24230;&#30340;&#32534;&#31243;&#30693;&#35782;&#21644;&#25345;&#32493;&#20114;&#21160;&#20197;&#36827;&#34892;&#21450;&#26102;&#30340;&#23436;&#21892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#21015;&#31867;&#22411;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;Dataprep.Clean&#65292;&#20316;&#20026;Dataprep&#24211;&#30340;&#19968;&#20010;&#32452;&#20214;&#65292;&#36890;&#36807;&#19968;&#34892;&#20195;&#30721;&#23454;&#29616;&#29305;&#23450;&#21015;&#31867;&#22411;&#30340;&#26631;&#20934;&#21270;&#65292;&#26497;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;CleanAgen
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.05168</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#35299;&#38145;&#22810;&#27169;&#24577;&#32479;&#19968;&#31163;&#25955;&#34920;&#31034;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#32479;&#19968;&#30721;&#26412;&#30340;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;DCID&#65289;&#27169;&#22411;&#22312;&#23454;&#29616;&#32454;&#31890;&#24230;&#34920;&#31034;&#21644;&#36328;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#21463;&#21040;&#23545;&#25152;&#26377;&#36890;&#36947;&#30340;&#22343;&#31561;&#23545;&#24453;&#20197;&#21450;&#24573;&#35270;&#27425;&#35201;&#20107;&#20214;&#20449;&#24687;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;&#26469;&#33258;&#26080;&#20851;&#36890;&#36947;&#30340;&#24178;&#25200;&#24182;&#22312;&#32454;&#31890;&#24230;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#65288;TOC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#36873;&#25321;&#37325;&#35201;&#36890;&#36947;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;H-DCID&#65289;&#26041;&#27861;&#23558;&#20449;&#24687;&#20998;&#31163;&#21644;&#23545;&#40784;&#25193;&#23637;&#21040;&#20004;&#20010;&#32423;&#21035;&#65292;&#25429;&#25417;&#26356;&#22810;&#36328;&#27169;&#24577;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05168v1 Announce Type: cross  Abstract: Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements a
&lt;/p&gt;</description></item><item><title>SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17645</link><description>&lt;p&gt;
SongComposer&#65306;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#27468;&#35789;&#21644;&#26059;&#24459;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17645
&lt;/p&gt;
&lt;p&gt;
SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SongComposer&#65292;&#19968;&#20010;&#20026;&#27468;&#26354;&#21019;&#20316;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#22411;LLM&#12290;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#27468;&#26354;&#20013;&#30340;&#26059;&#24459;&#21644;&#27468;&#35789;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#20013;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;LLM&#23558;&#38899;&#20048;&#35270;&#20026;&#37327;&#21270;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#32780;&#36825;&#31181;&#38544;&#24335;&#32534;&#30721;&#23548;&#33268;&#20102;&#32534;&#30721;&#25928;&#29575;&#20302;&#19979;&#21644;&#28789;&#27963;&#24615;&#24046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#36825;&#26159;&#20154;&#31867;&#20026;&#38899;&#20048;&#35774;&#35745;&#30340;&#25104;&#29087;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;LLM&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#26126;&#30830;&#22320;&#21019;&#20316;&#27468;&#26354;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#32452;&#35774;&#35745;&#65292;&#29992;&#20110;&#26684;&#24335;&#21270;&#27468;&#35789;&#21644;&#26059;&#24459;&#20013;&#30340;&#19977;&#20010;&#38899;&#31526;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#20241;&#27490;&#26102;&#38388;&#65289;&#65292;&#20174;&#32780;&#20445;&#35777;LLM&#23545;&#38899;&#20048;&#31526;&#21495;&#30340;&#27491;&#30830;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#27468;&#35789;&#21644;&#26059;&#24459;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#40784;&#12290;&#20026;&#20102;&#21521;LLM&#28748;&#36755;&#22522;&#26412;&#30340;&#38899;&#20048;&#29702;&#35299;&#65292;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;SongCompose-PT&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27468;&#26354;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#27468;&#35789;&#12289;&#26059;&#24459;&#21644;&#25104;&#23545;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17645v1 Announce Type: cross  Abstract: We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired ly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item><item><title>PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.04435</link><description>&lt;p&gt;
PreGIP: &#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04435
&lt;/p&gt;
&lt;p&gt;
PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#30340;GNNs&#25104;&#20026;&#21512;&#27861;&#25317;&#26377;&#32773;&#30340;&#39640;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#25163;&#21487;&#33021;&#20250;&#38750;&#27861;&#22797;&#21046;&#21644;&#37096;&#32626;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#29992;&#20110;&#20854;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#22987;&#23581;&#35797;&#20026;IP&#20445;&#25252;&#28155;&#21152;GNN&#20998;&#31867;&#22120;&#30340;&#27700;&#21360;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#25165;&#33021;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;GNN&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;PreGIP&#65292;&#29992;&#20110;&#22312;&#20445;&#25345;&#23884;&#20837;&#31354;&#38388;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#32473;GNN&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#28155;&#21152;&#27700;&#21360;&#20197;&#36827;&#34892;IP&#20445;&#25252;&#12290;PreGIP&#24341;&#20837;&#20102;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#12290;&#21516;&#26102;&#37319;&#29992;&#20102;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#25193;&#23637;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.06634</link><description>&lt;p&gt;
CCFC&#65306;&#26725;&#25509;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CCFC: Bridging Federated Clustering and Contrastive Learning. (arXiv:2401.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#26159;&#23545;&#20110;&#32852;&#37030;&#22330;&#26223;&#20013;&#38598;&#20013;&#32858;&#31867;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#21487;&#20197;&#35753;&#22810;&#20010;&#25968;&#25454;&#25345;&#26377;&#23458;&#25143;&#31471;&#22312;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#36827;&#34892;&#25968;&#25454;&#20998;&#32452;&#12290;&#22312;&#38598;&#20013;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#39537;&#21160;&#30340;&#32858;&#31867;&#22312;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#32858;&#31867;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#32467;&#21512;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#34920;&#31034;&#23450;&#21046;&#20102;&#19968;&#20010;&#32858;&#31867;&#23545;&#27604;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#20316;&#20026;&#25552;&#20986;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#31216;&#20026;&#32858;&#31867;&#23545;&#27604;&#32852;&#37030;&#32858;&#31867;&#65288;CCFC&#65289;&#12290;&#21463;&#30410;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#30340;&#32858;&#31867;&#24615;&#33021;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;&#36825;&#31181;&#25910;&#30410;&#23548;&#33268;NMI&#24471;&#20998;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#26368;&#39640;&#36798;&#21040;0.4155&#12290;&#27492;&#22806;&#65292;CCFC&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCF
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.14634</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25253;&#21578;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#65292;&#29616;&#22312;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#33258;&#21160;&#25253;&#21578;&#26469;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#36827;&#34892;&#21021;&#27493;&#38405;&#35835;&#12290;&#36825;&#21487;&#20197;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#23548;&#33268;&#29983;&#25104;&#25253;&#21578;&#20013;&#20986;&#29616;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19982;&#25551;&#36848;&#30495;&#23454;&#25110;&#28508;&#22312;&#34394;&#20551;&#21457;&#29616;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24320;&#21457;&#30340;&#26680;&#26597;&#32773;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#26680;&#26597;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20266;&#36896;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#26469;&#33258;&#36825;&#20123;&#25253;&#21578;&#30340;&#30495;&#20551;&#21477;&#23376;&#30340;&#25991;&#26412;&#32534;&#30721;&#19982;&#22270;&#20687;&#32534;&#30721;&#37197;&#23545;&#65292;&#23398;&#20064;&#26144;&#23556;&#21040;&#30495;/&#20551;&#26631;&#31614;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09276</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#36935;&#35265;&#35745;&#31639;&#21644;&#20989;&#25968;&#24335;&#32534;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25104;&#20026;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#35748;&#20026;&#22312;&#31526;&#21495;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#23398;&#26500;&#36896;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#12289;&#36923;&#36753;&#25512;&#29702;&#65288;&#22914;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#29978;&#33267;&#25191;&#34892;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#22826;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#20559;&#35265;&#30340;&#20803;&#32032;&#65292;&#20197;&#38480;&#21046;&#21487;&#33021;&#35201;&#25191;&#34892;&#30340;&#31243;&#24207;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#65292;&#32780;&#26159;&#37319;&#29992;&#955;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2203.14523</link><description>&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Translation Consistent Semi-supervised Segmentation for 3D Medical Images. (arXiv:2203.14523v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#65292;&#20294;&#20854;&#20381;&#36182;&#20110;&#28085;&#30422;&#22823;&#37327;&#20307;&#32032;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#21155;&#21183;&#65292;&#22240;&#20026;&#33719;&#21462;&#36825;&#31181;&#27880;&#37322;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#25104;&#21151;&#30340;SSL&#26041;&#27861;&#22522;&#20110;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#25200;&#21160;&#35270;&#22270;&#33719;&#24471;&#30340;&#27169;&#22411;&#21709;&#24212;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#25200;&#21160;&#36890;&#24120;&#20250;&#20445;&#25345;&#35270;&#22270;&#20043;&#38388;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#30456;&#24403;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#20174;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#20174;&#20998;&#21106;&#23545;&#35937;&#26412;&#36523;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32763;&#35793;&#19968;&#33268;&#21327;&#21516;&#35757;&#32451;&#65288;TraCoCo&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19968;&#33268;&#24615;&#23398;&#20064;SSL&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25913;&#21464;&#19981;&#21516;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D medical image segmentation methods have been successful, but their dependence on large amounts of voxel-level annotated data is a disadvantage that needs to be addressed given the high cost to obtain such annotation. Semi-supervised learning (SSL) solve this issue by training models with a large unlabelled and a small labelled dataset. The most successful SSL approaches are based on consistency learning that minimises the distance between model responses obtained from perturbed views of the unlabelled data. These perturbations usually keep the spatial input context between views fairly consistent, which may cause the model to learn segmentation patterns from the spatial input contexts instead of the segmented objects. In this paper, we introduce the Translation Consistent Co-training (TraCoCo) which is a consistency learning SSL method that perturbs the input data views by varying their spatial input context, allowing the model to learn segmentation patterns from visual objects. Fur
&lt;/p&gt;</description></item></channel></rss>