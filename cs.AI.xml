<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Grover&#21644;Deutsch-Josza&#31561;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#32452;&#31934;&#24515;&#26500;&#24314;&#30340;&#26465;&#20214;&#65292;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26159;&#21542;&#20855;&#26377;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18963</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Grover&#21644;Deutsch-Josza&#31561;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#32452;&#31934;&#24515;&#26500;&#24314;&#30340;&#26465;&#20214;&#65292;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26159;&#21542;&#20855;&#26377;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38382;&#39064;&#31867;&#21035;&#30340;&#25506;&#32034;&#26159;&#37327;&#23376;&#35745;&#31639;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;&#12290;&#19968;&#20010;&#22522;&#26412;&#19978;&#23436;&#20840;&#26410;&#34987;&#25506;&#35752;&#30340;&#20027;&#39064;&#26159;&#20351;&#29992;&#37327;&#23376;&#31639;&#27861;&#21644;&#35745;&#31639;&#26469;&#25506;&#32034;&#21644;&#35810;&#38382;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#21160;&#24577;&#12290;&#36825;&#26159;&#23558;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#20110;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21644;&#20223;&#30495;&#30340;&#23578;&#26410;&#25104;&#29087;&#30340;&#20027;&#39064;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#30340;&#19968;&#32452;&#26465;&#20214;&#26469;&#20351;&#29992;&#20004;&#20010;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;Grover&#21644;Deutsch-Josza&#65292;&#20197;&#20351;&#36755;&#20986;&#27979;&#37327;&#20855;&#26377;&#19968;&#31181;&#35299;&#37322;&#65292;&#20445;&#35777;&#25105;&#20204;&#33021;&#22815;&#25512;&#26029;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65288;&#36866;&#29992;&#20110;&#29983;&#29289;&#21644;&#20154;&#24037;&#32593;&#32476;&#65289;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#26159;&#21542;&#26377;&#21487;&#33021;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#12290;&#25110;&#32773;&#36825;&#20123;&#21160;&#24577;&#20445;&#35777;&#20250;&#20572;&#27490;&#65292;&#35201;&#20040;&#26159;&#36890;&#36807;'&#30315;&#30187;'&#21160;&#24577;&#65292;&#35201;&#20040;&#26159;&#38745;&#27490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18963v1 Announce Type: cross  Abstract: The exploration of new problem classes for quantum computation is an active area of research. An essentially completely unexplored topic is the use of quantum algorithms and computing to explore and ask questions \textit{about} the functional dynamics of neural networks. This is a component of the still-nascent topic of applying quantum computing to the modeling and simulations of biological and artificial neural networks. In this work, we show how a carefully constructed set of conditions can use two foundational quantum algorithms, Grover and Deutsch-Josza, in such a way that the output measurements admit an interpretation that guarantees we can infer if a simple representation of a neural network (which applies to both biological and artificial networks) after some period of time has the potential to continue sustaining dynamic activity. Or whether the dynamics are guaranteed to stop either through 'epileptic' dynamics or quiescence
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18846</link><description>&lt;p&gt;
&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#39640;&#25928;&#30340;&#21069;&#23548;&#26816;&#27979;&#31639;&#27861;&#20173;&#28982;&#26159;&#35299;&#20915;&#23454;&#38469;&#36890;&#20449;&#22330;&#26223;&#20013;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;(RA)&#20013;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#27169;&#22411;&#30340;&#26089;&#26399;&#21069;&#23548;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#25480;&#20104;&#24335;RA&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;(SVGD)&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#33719;&#24471;MLE&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25506;&#32034;Hadamard&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;Hadamard&#21464;&#25442;(MHT)&#65292;&#20351;&#29992;&#20108;&#38454;&#23548;&#25968;&#28388;&#27874;&#22120;&#23558;&#39640;&#39057;&#20998;&#31163;&#20986;&#37325;&#35201;&#37096;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#28040;&#38500;SVGD&#26816;&#27979;&#22120;&#20013;&#30340;&#22122;&#22768;&#24182;&#20943;&#36731;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;MHT&#30340;&#22359;MHT&#23618;&#65292;&#35813;&#23618;&#22522;&#20110;MHT&#12289;&#32553;&#25918;&#23618;&#12289;&#36719;&#38408;&#20540;&#23618;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
&lt;/p&gt;</description></item><item><title>S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17643</link><description>&lt;p&gt;
S+t-SNE - &#23558;&#38477;&#32500;&#24341;&#20837;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
S+t-SNE - Bringing dimensionality reduction to data streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17643
&lt;/p&gt;
&lt;p&gt;
S+t-SNE&#26159;t-SNE&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#22686;&#37327;&#26356;&#26032;&#21644;&#30450;&#30446;&#28418;&#31227;&#31649;&#29702;&#30340;&#29305;&#28857;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38477;&#32500;&#21644;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;S+t-SNE&#65292;&#36825;&#26159;t-SNE&#31639;&#27861;&#30340;&#19968;&#31181;&#25913;&#36827;&#65292;&#26088;&#22312;&#22788;&#29702;&#26080;&#38480;&#25968;&#25454;&#27969;&#12290;S+t-SNE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#36880;&#27493;&#26356;&#26032;t-SNE&#23884;&#20837;&#65292;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#22788;&#29702;&#27969;&#24335;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#27599;&#19968;&#27493;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#28857;&#65292;&#35813;&#31639;&#27861;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#21487;&#35270;&#21270;&#12290;&#37319;&#29992;&#30450;&#30446;&#26041;&#27861;&#36827;&#34892;&#28418;&#31227;&#31649;&#29702;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#65292;&#20419;&#36827;&#19981;&#26029;&#21487;&#35270;&#21270;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;S+t-SNE&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#25429;&#25417;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#23454;&#26102;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
&lt;/p&gt;</description></item><item><title>Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04652</link><description>&lt;p&gt;
Yi: &#30001; 01.AI &#25512;&#20986;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Yi: Open Foundation Models by 01.AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04652
&lt;/p&gt;
&lt;p&gt;
Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Yi&#27169;&#22411;&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20855;&#26377;&#24378;&#22823;&#22810;&#32500;&#33021;&#21147;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;6B&#21644;34B&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#32842;&#22825;&#27169;&#22411;&#12289;200K&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35832;&#22914;MMLU&#20043;&#31867;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#25105;&#20204;&#24494;&#35843;&#36807;&#30340;&#32842;&#22825;&#27169;&#22411;&#22312;AlpacaEval&#21644;Chatbot Arena&#31561;&#20027;&#35201;&#35780;&#20272;&#24179;&#21488;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20154;&#31867;&#20559;&#22909;&#29575;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#36229;&#32423;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;Yi&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#25968;&#25454;&#24037;&#31243;&#24037;&#20316;&#25152;&#24102;&#26469;&#30340;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#30340;&#25968;&#25454;&#21435;&#37325;&#21644;&#36136;&#37327;&#36807;&#28388;&#27969;&#27700;&#32447;&#26500;&#24314;&#20102;3100&#20159;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#26009;&#24211;&#30340;&#26631;&#35760;&#12290;&#23545;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23545;&#23567;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.08095</link><description>&lt;p&gt;
DurFlex-EVC: &#20855;&#26377;&#24182;&#34892;&#29983;&#25104;&#30340;&#25345;&#32493;&#28789;&#27963;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08095
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#65288;EVC&#65289;&#26088;&#22312;&#20462;&#25913;&#35828;&#35805;&#32773;&#22768;&#38899;&#30340;&#24773;&#32490;&#33394;&#24425;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#29420;&#29305;&#30340;&#22768;&#38899;&#29305;&#24449;&#12290;&#26368;&#36817;EVC&#30340;&#36827;&#23637;&#28041;&#21450;&#21516;&#26102;&#24314;&#27169;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#36716;&#25442;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#65292;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#36716;&#21521;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DurFlex-EVC&#65292;&#23427;&#38598;&#25104;&#20102;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#21333;&#20803;&#23545;&#40784;&#22120;&#12290;&#20256;&#32479;&#27169;&#22411;&#34429;&#28982;&#34701;&#20837;&#20102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34920;&#31034;&#65292;&#20294;&#21364;&#24573;&#35270;&#20102;&#36825;&#31181;&#21452;&#37325;&#24615;&#36136;&#65292;&#23548;&#33268;&#20102;&#21487;&#25511;&#24615;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#20197;&#23558;&#36825;&#20123;&#34920;&#31034;&#19982;&#19981;&#21516;&#24773;&#32490;&#36827;&#34892;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08095v2 Announce Type: replace-cross  Abstract: Emotional voice conversion (EVC) seeks to modify the emotional tone of a speaker's voice while preserving the original linguistic content and the speaker's unique vocal characteristics. Recent advancements in EVC have involved the simultaneous modeling of pitch and duration, utilizing the potential of sequence-to-sequence (seq2seq) models. To enhance reliability and efficiency in conversion, this study shifts focus towards parallel speech generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates a style autoencoder and unit aligner. Traditional models, while incorporating self-supervised learning (SSL) representations that contain both linguistic and paralinguistic information, have neglected this dual nature, leading to reduced controllability. Addressing this issue, we implement cross-attention to synchronize these representations with various emotions. Additionally, a style autoencoder is developed for t
&lt;/p&gt;</description></item><item><title>AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.</title><link>https://arxiv.org/abs/2310.12963</link><description>&lt;p&gt;
AutoMix: &#33258;&#21160;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoMix: Automatically Mixing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12963
&lt;/p&gt;
&lt;p&gt;
AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#20113;API&#25552;&#20379;&#21830;&#33719;&#24471;&#12290;&#34429;&#28982;&#36825;&#31181;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20294;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#36873;&#39033;&#20197;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMix&#65292;&#19968;&#31181;&#26681;&#25454;&#36739;&#23567;LM&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#27491;&#30830;&#24615;&#26469;&#31574;&#30053;&#24615;&#22320;&#23558;&#26597;&#35810;&#36335;&#30001;&#21040;&#26356;&#22823;LM&#30340;&#26041;&#27861;&#12290;AutoMix&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#23569;&#37327;&#26679;&#26412;&#30340;&#33258;&#25105;&#39564;&#35777;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#37492;&#20110;&#39564;&#35777;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#25105;&#20204;&#22312;AutoMix&#20013;&#20351;&#29992;&#20102;&#20803;&#39564;&#35777;&#22120;&#26469;&#25552;&#39640;&#36825;&#20123;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;LLAMA2-13B&#21644;GPT-4&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;AutoMix&#36229;&#36234;&#20102;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#65292;&#27599;&#21333;&#20301;&#25104;&#26412;&#30340;&#22686;&#37327;&#25928;&#30410;&#25552;&#39640;&#20102;&#26368;&#22810;86%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.c&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12963v3 Announce Type: replace  Abstract: Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86%. Our code and data are available at https://github.c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.00284</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Selective Uncertainty Propagation in Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#27573;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#65292;&#30446;&#26631;&#22312;&#20110;&#24212;&#23545;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20013;&#27599;&#19968;&#27493;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35780;&#20272;&#31163;&#24320;&#34892;&#20026;&#31574;&#30053;&#22312;&#31532;h&#27493;&#26102;&#30340;&#22788;&#29702;&#25928;&#26524;&#65292;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#36825;&#19968;&#27493;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#27599;&#19968;&#27493;&#31574;&#30053;&#37117;&#20250;&#24433;&#21709;&#19979;&#19968;&#29366;&#24577;&#30340;&#20998;&#24067;&#65292;&#30456;&#20851;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#19978;&#27604;&#38543;&#26426;&#24773;&#22659;&#25361;&#25112;&#19979;&#30340;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#26356;&#21152;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#38590;&#24230;&#20171;&#20110;&#36825;&#20004;&#31181;&#24773;&#22659;&#20043;&#38388;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#29992;&#20110;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#38590;&#24230;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
&lt;/p&gt;</description></item><item><title>&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03158</link><description>&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#65306;&#25512;&#36827;&#30701;&#25991;&#26412;&#20998;&#31867;&#30340;&#22235;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03158
&lt;/p&gt;
&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#20998;&#31867;&#65288;STC&#65289;&#23545;&#20110;&#22788;&#29702;&#21644;&#29702;&#35299;&#24403;&#20195;&#25968;&#23383;&#24179;&#21488;&#19978;&#27969;&#34892;&#30340;&#31616;&#27905;&#32780;&#37325;&#35201;&#30340;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#12290;STC&#22312;&#25235;&#20303;&#35821;&#20041;&#21644;&#21477;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24456;&#26126;&#26174;&#12290;&#23613;&#31649;&#22270;&#21367;&#31215;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#24212;&#29992;&#30693;&#35782;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#20102;&#23427;&#20204;&#22312;&#22522;&#30784;NLP&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36816;&#29992;CoT&#26469;&#30740;&#31350;LLMs&#22312;STC&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short Text Classification (STC) is crucial for processing and comprehending the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study sought to employ CoT to investigate the capabilities of LLMs in STC tasks. This study introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This framework primarily incorporates Syntactic and Semantic Enrichment CoT, effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02842</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#25195;&#25551;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26377;&#33021;&#21147;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#26159;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#25552;&#31034;&#25351;&#23548;&#35843;&#33410;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#20197;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25193;&#23637;&#25552;&#31034;&#35843;&#33410;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26159;&#19968;&#20010;&#24191;&#27867;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"&#28151;&#21512;&#25552;&#31034;"&#25110;MoPs&#65292;&#24182;&#32467;&#21512;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65306;&#21518;&#32773;&#30340;&#35774;&#35745;&#26159;&#26412;&#25991;&#30340;&#36129;&#29486;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;(&#21363;&#19968;&#32452;&#25552;&#31034;)&#12290;&#27492;&#22806;&#65292;MoPs&#22312;&#24212;&#29992;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26102;&#37117;&#19981;&#21463;&#24433;&#21709;&#8212;&#8212;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09138</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases. (arXiv:2306.09138v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#27010;&#29575;&#35821;&#20041;&#26469;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#20041;Web&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#31649;&#29702;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20854;&#20869;&#23481;&#32463;&#24120;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#21333;&#29420;&#32771;&#34385;&#25110;&#32508;&#21512;&#32771;&#34385;&#26102;&#21487;&#33021;&#21253;&#21547;&#30456;&#20114;&#30683;&#30462;&#30340;&#25551;&#36848;&#12290;&#20256;&#32479;&#30340;&#25512;&#29702;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#65292;&#38656;&#35201;&#36890;&#36807;&#35843;&#35797;&#30693;&#35782;&#24211;&#26469;&#28040;&#38500;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;DISPONTE&#30340;&#29616;&#26377;&#27010;&#29575;&#35821;&#20041;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#19968;&#33268;&#30340;&#30693;&#35782;&#24211;&#20013;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;TRILL&#21644;BUNDLE&#25512;&#29702;&#22120;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25552;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20462;&#22797;&#35821;&#20041;&#36827;&#34892;&#20102;&#27491;&#24335;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#22312;&#32771;&#34385;DL&#25512;&#29702;&#20219;&#21153;&#26102;&#26368;&#20026;&#25104;&#29087;&#30340;&#35821;&#20041;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity to manage inconsistency in Description Logics Knowledge Bases~(KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06121</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65306;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#36710;&#36742;&#20013;&#65292;&#32473;&#23450;&#21333;&#20010;&#25668;&#20687;&#26426;&#22270;&#20687;&#20272;&#35745;&#25668;&#20687;&#26426;&#23039;&#21183;&#26159;&#19968;&#39033;&#20256;&#32479;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#22330;&#26223;&#36827;&#34892;&#24037;&#31243;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#21644;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#12290;Transformer&#26550;&#26500;&#24050;&#32479;&#27835;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#21069;&#27839;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#20272;&#35745;6-DoF&#25668;&#20687;&#26426;&#30340;&#23039;&#21183;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;TSformer-VO&#27169;&#22411;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#20272;&#35745;&#36816;&#21160;&#65292;&#19982;&#20960;&#20309;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the camera pose given images of a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and it often relies on geometric approaches that require engineering effort for a specific scenario. Deep learning methods have shown to be generalizable after proper training and a considerable amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6-DoF camera's pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#23558;&#30149;&#29702;&#19987;&#23478;&#30340;&#35748;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#65292;&#20197;&#32531;&#35299;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Experts' cognition-driven safe noisy labels learning for precise segmentation of residual tumor in breast cancer. (arXiv:2304.07295v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#23558;&#30149;&#29702;&#19987;&#23478;&#30340;&#35748;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#65292;&#20197;&#32531;&#35299;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#36741;&#21161;&#21270;&#30103;&#21518;&#20934;&#30830;&#20998;&#21106;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244; (PSRTBC) &#26159;&#20083;&#33146;&#30284;&#27835;&#30103;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#22312;&#26032;&#36741;&#21161;&#21270;&#30103;&#21518;&#24120;&#24120;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#31181;&#22810;&#26679;&#30340;&#24418;&#24577;&#23398;&#25913;&#21464;&#65292;&#22240;&#27492;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064; (ECDSNLL) &#26041;&#27861;&#12290;ECDSNLL&#26159;&#22312;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#19979;&#26500;&#24314;&#30340;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#23433;&#20840;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#30149;&#29702;&#19987;&#23478;&#23545;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#35782;&#21035;&#30340;&#35748;&#30693;&#19982;&#25552;&#20379;&#30340;&#25968;&#25454;&#22522;&#30784;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;ECDSNLL&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise segmentation of residual tumor in breast cancer (PSRTBC) after neoadjuvant chemotherapy is a fundamental key technique in the treatment process of breast cancer. However, achieving PSRTBC is still a challenge, since the breast cancer tissue and tumor cells commonly have complex and varied morphological changes after neoadjuvant chemotherapy, which inevitably increases the difficulty to produce a predictive model that has good generalization with machine learning. To alleviate this situation, in this paper, we propose an experts' cognition-driven safe noisy labels learning (ECDSNLL) approach. In the concept of safe noisy labels learning, which is a typical type of safe weakly supervised learning, ECDSNLL is constructed by integrating the pathology experts' cognition about identifying residual tumor in breast cancer and the artificial intelligence experts' cognition about data modeling with provided data basis. We show the advantages of the proposed ECDSNLL approach and its promi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30693;&#35782;&#24182;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/1910.01539</link><description>&lt;p&gt;
&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#30340;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#32479;&#19968;&#34920;&#31034;&#65292;&#20851;&#31995;&#25968;&#25454;&#24211;&#31995;&#32479;&#21644;&#36890;&#29992;&#19982;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Method for the semantic indexing of concept hierarchies, uniform representation, use of relational database systems and generic and case-based reasoning. (arXiv:1910.01539v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.01539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30693;&#35782;&#24182;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#20854;&#22312;&#30693;&#35782;&#34920;&#31034;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35821;&#20041;&#32034;&#24341;&#30340;&#36215;&#28857;&#26159;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#20854;&#30446;&#26631;&#26159;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65288;&#27010;&#24565;&#65289;&#65292;&#36825;&#20123;&#33410;&#28857;&#25353;&#23618;&#27425;&#39034;&#24207;&#25490;&#21015;&#19988;&#35821;&#27861;&#21644;&#35821;&#20041;&#27491;&#30830;&#12290;&#20351;&#29992;&#32034;&#24341;&#31639;&#27861;&#65292;&#35745;&#31639;&#38190;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#38190;&#34920;&#31034;&#26415;&#35821;&#20851;&#31995;&#12290;&#25152;&#36848;&#32034;&#24341;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#24050;&#34987;&#35777;&#26126;&#12290;&#25551;&#36848;&#20102;&#23558;&#32463;&#20856;&#20851;&#31995;&#25968;&#25454;&#24211;&#29992;&#20110;&#23454;&#20363;&#23384;&#20648;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for semantic indexing and describes its application in the field of knowledge representation. Starting point of the semantic indexing is the knowledge represented by concept hierarchies. The goal is to assign keys to nodes (concepts) that are hierarchically ordered and syntactically and semantically correct. With the indexing algorithm, keys are computed such that concepts are partially unifiable with all more specific concepts and only semantically correct concepts are allowed to be added. The keys represent terminological relationships. Correctness and completeness of the underlying indexing algorithm are proven. The use of classical relational databases for the storage of instances is described. Because of the uniform representation, inference can be done using case-based reasoning and generic problem solving methods.
&lt;/p&gt;</description></item></channel></rss>