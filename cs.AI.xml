<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.12431</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#32422;&#26463;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Geometric Constraints in Deep Learning Frameworks: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stereophotogrammetry&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#22330;&#26223;&#29702;&#35299;&#25216;&#26415;&#12290;&#20854;&#36215;&#28304;&#21487;&#20197;&#36861;&#28335;&#21040;&#33267;&#23569;19&#19990;&#32426;&#65292;&#24403;&#26102;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#20351;&#29992;&#29031;&#29255;&#26469;&#27979;&#37327;&#19990;&#30028;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#33258;&#37027;&#26102;&#20197;&#26469;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#25104;&#21315;&#19978;&#19975;&#31181;&#26041;&#27861;&#12290;&#32463;&#20856;&#20960;&#20309;&#25216;&#26415;&#30340;Shape from Stereo&#24314;&#31435;&#22312;&#20351;&#29992;&#20960;&#20309;&#26469;&#23450;&#20041;&#22330;&#26223;&#21644;&#25668;&#20687;&#26426;&#20960;&#20309;&#30340;&#32422;&#26463;&#65292;&#28982;&#21518;&#35299;&#20915;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#12290;&#26356;&#36817;&#26399;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#32780;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20960;&#20309;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#29992;&#20110;&#28145;&#24230;&#20272;&#35745;&#25110;&#20854;&#20182;&#23494;&#20999;&#30456;&#20851;&#38382;&#39064;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#26222;&#36941;&#20960;&#20309;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12431v1 Announce Type: cross  Abstract: Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.11482</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation. (arXiv:2310.11482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25345;&#32493;&#23398;&#20064;&#23558;&#31867;&#21035;&#21010;&#20998;&#21040;&#26032;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#21152;&#24555;&#20102;&#22686;&#37327;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#22240;&#20026;&#39640;&#24230;&#21487;&#20256;&#36755;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#34920;&#31034;&#20351;&#24471;&#22312;&#35843;&#25972;&#19968;&#23567;&#32452;&#21442;&#25968;&#26102;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#21453;&#22797;&#24494;&#35843;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#65288;TTACIL&#65289;&#65292;&#23427;&#39318;&#20808;&#22312;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23618;&#24402;&#19968;&#21270;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instan
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06441</link><description>&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06441
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#21151;&#33021;&#37325;&#26500;&#30340;&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#21644;&#30456;&#20851;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;RCA&#22312;&#24490;&#29615;&#20381;&#36182;&#25968;&#25454;&#19978;&#36820;&#22238;&#21333;&#19968;&#27010;&#24565;&#26684;&#23478;&#26063;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30456;&#20851;&#30340;&#35821;&#22659;&#12290;&#23427;&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25551;&#36848;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#20851;&#20110;RCA&#30340;&#19968;&#20010;&#20196;&#20154;&#22256;&#24785;&#30340;&#35266;&#23519;&#26159;&#65292;&#23613;&#31649;&#25968;&#25454;&#23384;&#22312;&#24490;&#29615;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#36820;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65292;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;RCA&#30340;&#35821;&#20041;&#20197;&#25805;&#20316;&#26041;&#24335;&#25552;&#20379;&#65292;&#23545;&#27492;&#38382;&#39064;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#23450;&#20041;&#20026;&#23646;&#20110;&#21021;&#22987;&#35821;&#22659;&#30830;&#23450;&#30340;&#31354;&#38388;&#30340;&#27010;&#24565;&#26684;&#23478;&#26063;&#65288;&#33391;&#26500;&#65289;&#65292;&#19981;&#33021;&#25193;&#23637;&#26032;&#23646;&#24615;&#65288;&#39281;&#21644;&#65289;&#65292;&#24182;&#19988;&#20165;&#28041;&#21450;&#35813;&#23478;&#26063;&#30340;&#27010;&#24565;&#65288;&#33258;&#25903;&#25345;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#33391;&#26500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#20197;&#21450;&#35813;&#31354;&#38388;&#19978;&#30340;&#20004;&#20010;&#20989;&#25968;&#65288;&#19968;&#20010;&#25193;&#24352;&#20989;&#25968;&#21644;&#19968;&#20010;&#25910;&#32553;&#20989;&#25968;&#65289;&#65292;&#37319;&#29992;&#21151;&#33021;&#35270;&#22270;&#26469;&#25551;&#36848;RCA&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#8230;
&lt;/p&gt;
&lt;p&gt;
Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item></channel></rss>