<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02225</link><description>&lt;p&gt;
CHOSEN&#65306;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02225
&lt;/p&gt;
&lt;p&gt;
CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHOSEN&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#12289;&#24378;&#22823;&#19988;&#26377;&#25928;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#20855;&#26377;&#38024;&#23545;&#19981;&#21516;&#22810;&#35270;&#35282;&#37319;&#38598;&#31995;&#32479;&#65288;&#22914;&#30456;&#26426;&#30456;&#23545;&#23450;&#20301;&#21644;&#38236;&#22836;&#65289;&#30340;&#30452;&#35266;&#27867;&#21270;&#33021;&#21147;&#12290;&#32473;&#23450;&#21021;&#22987;&#28145;&#24230;&#20272;&#35745;&#65292;CHOSEN&#36845;&#20195;&#22320;&#37325;&#26032;&#37319;&#26679;&#24182;&#36873;&#25321;&#26368;&#20339;&#20551;&#35774;&#65292;&#24182;&#33258;&#21160;&#36866;&#24212;&#30001;&#37319;&#38598;&#31995;&#32479;&#30830;&#23450;&#30340;&#19981;&#21516;&#24230;&#37327;&#25110;&#22266;&#26377;&#23610;&#24230;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#22312;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#20013;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27491;&#36127;&#20551;&#35774;&#12290;&#23558;CHOSEN&#38598;&#25104;&#21040;&#31616;&#21333;&#30340;&#22522;&#32447;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#22312;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#19982;&#35768;&#22810;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#30456;&#27604;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02225v1 Announce Type: cross  Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;GenAI&#30340;GenAINet&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;6G&#26102;&#20195;&#38138;&#24179;&#20102;&#36890;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.16631</link><description>&lt;p&gt;
GenAINet&#65306;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16631
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;GenAI&#30340;GenAINet&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36755;&#21644;&#25512;&#29702;&#23454;&#29616;&#26080;&#32447;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;6G&#26102;&#20195;&#38138;&#24179;&#20102;&#36890;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16631v2 &#22768;&#26126;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#21644;&#36890;&#20449;&#32593;&#32476;&#34987;&#26399;&#26395;&#22312;6G&#20013;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;GenAI&#20195;&#29702;&#21487;&#33021;&#20250;&#37322;&#25918;&#38598;&#20307;&#26234;&#33021;&#30340;&#21147;&#37327;&#65292;&#24182;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26080;&#32447;&#32593;&#32476;&#35774;&#35745;&#20026;&#8220;&#25968;&#25454;&#31649;&#36947;&#8221;&#65292;&#24182;&#19981;&#36866;&#21512;&#23481;&#32435;&#21644;&#21033;&#29992;GenAI&#30340;&#21147;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenAINet&#26694;&#26550;&#65292;&#20854;&#20013;&#20998;&#24067;&#24335;GenAI&#20195;&#29702;&#20256;&#36798;&#30693;&#35782;&#65288;&#39640;&#32423;&#27010;&#24565;&#25110;&#25688;&#35201;&#65289;&#20197;&#23436;&#25104;&#20219;&#24847;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#32593;&#32476;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;GenAI&#33021;&#21147;&#65292;&#20197;&#31649;&#29702;&#32593;&#32476;&#21327;&#35758;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35821;&#20041;&#26412;&#22320;&#21270;&#30340;GenAINet&#26469;&#30740;&#31350;&#26377;&#25928;&#30340;&#36890;&#20449;&#21644;&#25512;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;GenAI&#20195;&#29702;&#20174;&#22810;&#27169;&#24577;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#27010;&#24565;&#65292;&#26500;&#24314;&#19968;&#20010;&#30693;&#35782;&#24211;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16631v2 Announce Type: replace  Abstract: Generative artificial intelligence (GenAI) and communication networks are expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a wireless network can potentially unleash the power of collective intelligence and pave the way for artificial general intelligence (AGI). However, current wireless networks are designed as a "data pipe" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (high-level concepts or abstracts) to accomplish arbitrary tasks. We first provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantic concepts from multi-modal raw data, build a knowledgebase represe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07002</link><description>&lt;p&gt;
&#23458;&#25143;&#31471;&#21327;&#20316;&#65306;&#20855;&#26377;&#20445;&#35777;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#25913;&#36827;&#30340;&#28789;&#27963;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#23427;&#24182;&#19981;&#26159;&#20813;&#36153;&#30340;&#12290;&#22122;&#22768;&#30340;&#28155;&#21152;&#20250;&#38543;&#26426;&#24178;&#25200;&#27169;&#22411;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#24178;&#25200;&#20250;&#38543;&#30528;&#36890;&#20449;&#36718;&#27425;&#30340;&#22686;&#21152;&#32780;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;FedCEO&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;"&#30456;&#20114;&#21327;&#20316;"&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#29992;&#25143;&#38544;&#31169;&#20043;&#38388;&#25214;&#21040;&#19968;&#31181;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#22534;&#21472;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20102;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#28789;&#27963;&#25130;&#26029;&#39640;&#39057;&#32452;&#20998;&#30340;&#33021;&#21147;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;FedCEO&#33021;&#22815;&#36890;&#36807;&#24179;&#28369;&#20840;&#23616;&#35821;&#20041;&#31354;&#38388;&#26469;&#26377;&#25928;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#21644;&#25345;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SOTA&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#36793;&#30028;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;</title><link>https://arxiv.org/abs/2311.02181</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Joint Problems in Learning Multiple Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02181
&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#26159;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36890;&#36807;&#20195;&#35874;&#20135;&#29289;&#27987;&#24230;&#33719;&#24471;&#30340;&#23450;&#37327;&#20010;&#24615;&#21270;&#20195;&#35874;&#27169;&#22411;&#21040;&#37327;&#23376;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#29366;&#24577;&#21028;&#21035;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21464;&#31181;&#65292;&#21363;&#32473;&#23450;&#19968;&#32452;&#36712;&#36857;&#21644;&#19968;&#20123;&#37096;&#20998;&#65292;&#25105;&#20204;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#27169;&#22411;&#65292;&#20197;&#20351;&#24471;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;EM&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#38468;&#19978;&#20102;&#26377;&#21069;&#26223;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#21450;&#20854;&#19982;&#24037;&#36164;&#27700;&#24179;&#21644;&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#26367;&#20195;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#23545;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.08776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models at Work in China's Labor Market. (arXiv:2308.08776v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#24182;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#21450;&#20854;&#19982;&#24037;&#36164;&#27700;&#24179;&#21644;&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#26367;&#20195;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#23545;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20013;&#22269;&#21171;&#21160;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;LLM&#20998;&#31867;&#65292;&#25353;&#29031;Eloundou&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#20998;&#26512;&#20102;&#32844;&#19994;&#23545;LLM&#33021;&#21147;&#30340;&#26292;&#38706;&#31243;&#24230;&#12290;&#28982;&#21518;&#23558;&#32844;&#19994;&#26292;&#38706;&#31243;&#24230;&#32858;&#21512;&#21040;&#34892;&#19994;&#27700;&#24179;&#19978;&#65292;&#24471;&#21040;&#34892;&#19994;&#26292;&#38706;&#24471;&#20998;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32844;&#19994;&#26292;&#38706;&#21644;&#24037;&#36164;&#27700;&#24179;/&#32463;&#39564;&#28322;&#20215;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#34920;&#26126;&#39640;&#34218;&#21644;&#32463;&#39564;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#30528;LLM&#39537;&#21160;&#36719;&#20214;&#30340;&#26356;&#22823;&#26367;&#20195;&#39118;&#38505;&#12290;&#34892;&#19994;&#26292;&#38706;&#24471;&#20998;&#19982;&#19987;&#23478;&#35780;&#20272;&#21644;&#32463;&#27982;&#30452;&#35273;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32771;&#34385;&#34892;&#19994;&#26292;&#38706;&#30340;&#32463;&#27982;&#22686;&#38271;&#27169;&#22411;&#65292;&#20197;&#37327;&#21270;AI&#37319;&#29992;&#24102;&#26469;&#30340;&#29983;&#20135;&#21147;&#21644;&#23601;&#19994;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#26412;&#30740;&#31350;&#20026;&#29702;&#35299;&#20013;&#22269;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#23545;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#20998;&#26512;&#22522;&#30784;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#32844;&#19994;&#27700;&#24179;&#30340;&#26292;&#38706;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following Eloundou et al. (2023)'s methodology. We then aggregate occupation exposure to the industry level to obtain industry exposure scores. The results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from LLM-powered software. The industry exposure scores align with expert assessments and economic intuitions. We also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from AI adoption. Overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable AI systems in China. Key innovations include the occupation-level exposure
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06375</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;Transformer&#65306;&#34920;&#31034;&#21644;&#35757;&#32451;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20197;&#22797;&#21046;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#36825;&#19968;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#65292;&#34920;&#26684;&#25968;&#25454;&#20855;&#26377;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#37329;&#34701;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#20540;&#30340;&#24322;&#36136;&#24615;&#65292;&#20854;&#20013;&#31867;&#21035;&#20803;&#32032;&#19982;&#25968;&#20540;&#39033;&#28151;&#21512;&#65292;&#20351;&#24471;&#36825;&#31181;&#36866;&#24212;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#26469;&#34920;&#31034;&#24322;&#26500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#25968;&#20540;&#29305;&#24449;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#19988;&#25972;&#20010;&#32593;&#32476;&#20351;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
&lt;/p&gt;</description></item></channel></rss>