<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.00108</link><description>&lt;p&gt;
&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#65281;&#22312;Share-and-Play&#22330;&#26223;&#19979;&#31359;&#36879;LLM&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00108
&lt;/p&gt;
&lt;p&gt;
LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#23545;&#20110;&#22686;&#24378;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#30830;&#20445;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;LoRA&#22240;&#20854;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#36731;&#26494;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#21457;&#24067;&#21644;&#37319;&#29992;&#36731;&#37327;&#30340;LoRA&#27169;&#22359;&#65292;&#20197;&#23450;&#21046;&#20854;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#20415;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#35774;&#32622;&#25171;&#24320;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#32972;&#38376;&#27880;&#20837;&#65292;&#24182;&#24191;&#27867;&#20998;&#21457;&#23545;&#25239;&#24615;LoRA&#32473;&#31038;&#21306;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#20849;&#20139;LoRA&#27169;&#22359;&#23384;&#22312;&#24040;&#22823;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#20013;&#21487;&#33021;&#20570;&#20986;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#21518;&#38376;&#27880;&#20837;LoRA&#27169;&#22359;&#24182;&#28145;&#20837;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00108v1 Announce Type: cross  Abstract: Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences. Among various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes. Despite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored. To fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study how to inject backdoor into the LoRA module and dive deep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08532</link><description>&lt;p&gt;
&#36890;&#36807;&#36827;&#21270;&#31639;&#27861;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#22823;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21162;&#21147;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;EvoPrompt&#65292;&#23427;&#20511;&#37492;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#20351;&#36827;&#21270;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#24182;&#19988;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#39640;&#25928;&#20248;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EvoPrompt&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#32452;&#25552;&#31034;&#20013;&#24320;&#22987;&#65292;&#24182;&#22522;&#20110;&#36827;&#21270;&#31639;&#23376;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#65292;&#26681;&#25454;&#24320;&#21457;&#38598;&#25913;&#36827;&#25552;&#31034;&#30340;&#31181;&#32676;&#12290;&#25105;&#20204;&#23545;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
&lt;/p&gt;</description></item></channel></rss>