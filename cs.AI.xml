<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Link&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#36890;&#29992;&#30693;&#35782;&#65292;&#25552;&#21462;&#20102;Knowledge-Link&#22270;&#20197;&#25429;&#33719;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.03645</link><description>&lt;p&gt;
K-Link&#65306;&#22522;&#20110;LLMs&#30340;&#30693;&#35782;&#38142;&#25509;&#22270;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03645
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-Link&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#36890;&#29992;&#30693;&#35782;&#65292;&#25552;&#21462;&#20102;Knowledge-Link&#22270;&#20197;&#25429;&#33719;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#37319;&#38598;&#24182;&#25353;&#26102;&#38388;&#39034;&#24207;&#32452;&#32455;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#28041;&#21450;&#20851;&#38190;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22914;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20174;MTS&#25968;&#25454;&#26500;&#24314;&#22270;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#36890;&#24120;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#20174;MTS&#20449;&#21495;&#26500;&#24314;&#22270;&#65292;&#36825;&#21487;&#33021;&#20250;&#30001;&#20110;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#32780;&#24341;&#20837;&#20559;&#24046;&#65292;&#24182;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;K-Link&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#32534;&#30721;&#24191;&#27867;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#20943;&#23569;&#20559;&#24046;&#12290;&#21033;&#29992;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19968;&#20010;Knowledge-Link&#22270;&#65292;&#25429;&#33719;&#20102;&#20256;&#24863;&#22120;&#30340;&#24191;&#27867;&#35821;&#20041;&#30693;&#35782;&#21644;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03645v1 Announce Type: new  Abstract: Sourced from various sensors and organized chronologically, Multivariate Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, Graph Neural Networks (GNNs) have emerged as powerful tools, yet their effectiveness is restricted by the quality of graph construction from MTS data. Typically, existing approaches construct graphs solely from MTS signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging Large Language Models (LLMs) to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in LLMs, such as physical principles, we extract a \textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-le
&lt;/p&gt;</description></item><item><title>PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14048</link><description>&lt;p&gt;
PolyNet&#65306;&#23398;&#20064;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14048
&lt;/p&gt;
&lt;p&gt;
PolyNet&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#20154;&#20026;&#35268;&#21017;&#23548;&#33268;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36805;&#36895;&#25509;&#36817;&#20154;&#31867;&#35774;&#35745;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32553;&#23567;&#24046;&#36317;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#24517;&#39035;&#39640;&#25928;&#22320;&#25506;&#32034;&#35299;&#31354;&#38388;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#26469;&#20154;&#20026;&#22686;&#21152;&#25506;&#32034;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#25439;&#23475;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#65292;&#24182;&#19988;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PolyNet&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20114;&#34917;&#35299;&#20915;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#20316;&#21697;&#19981;&#21516;&#65292;PolyNet&#20165;&#20351;&#29992;&#21333;&#20010;&#35299;&#30721;&#22120;&#65292;&#24182;&#19988;&#35757;&#32451;&#22270;&#24335;&#19981;&#36890;&#36807;&#20154;&#20026;&#35268;&#21017;&#24378;&#21046;&#23454;&#26045;&#22810;&#26679;&#21270;&#35299;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;PolyNet&#65292;&#24182;&#35266;&#23519;&#21040;&#38544;&#24335;&#22810;&#26679;&#24615;&#26426;&#21046;&#20801;&#35768;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14048v1 Announce Type: cross  Abstract: Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07640</link><description>&lt;p&gt;
&#21512;&#25104;&#23545;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#29255;&#25968;&#25454;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24773;&#24863;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#30340;&#21453;&#39304;&#65292;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#22810;&#27169;&#24577;&#36755;&#20837;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#29255;&#65289;&#30340;&#24773;&#24863;&#25511;&#21046;&#21453;&#39304;&#33021;&#22815;&#24357;&#34917;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#20855;&#26377;&#21516;&#29702;&#24515;&#12289;&#20934;&#30830;&#24615;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#26377;&#30528;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#25511;&#22810;&#27169;&#24577;&#21453;&#39304;&#21512;&#25104;&#65288;CMFeed&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25511;&#30340;&#21453;&#39304;&#21512;&#25104;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#25511;&#21046;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#23427;&#20351;&#29992;Transformer&#21644;Faster R-CNN&#32593;&#32476;&#25552;&#21462;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#21453;&#39304;&#12290;CMFeed&#25968;&#25454;&#38598;&#21253;&#21547;&#22270;&#29255;&#12289;&#25991;&#26412;&#12289;&#23545;&#24086;&#23376;&#30340;&#21453;&#24212;&#12289;&#24102;&#26377;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35780;&#35770;&#20197;&#21450;&#23545;&#35780;&#35770;&#30340;&#21453;&#24212;&#12290;&#23545;&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#21453;&#24212;&#34987;&#29992;&#26469;&#35757;&#32451;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#65288;&#31215;&#26497;&#25110;&#28040;&#26497;&#65289;&#24773;&#24863;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate sentiment-controlled feedback in response to multimodal inputs, comprising both text and images, addresses a critical gap in human-computer interaction by enabling systems to provide empathetic, accurate, and engaging responses. This capability has profound applications in healthcare, marketing, and education. To this end, we construct a large-scale Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a controllable feedback synthesis system. The proposed system includes an encoder, decoder, and controllability block for textual and visual inputs. It extracts textual and visual features using a transformer and Faster R-CNN networks and combines them to generate feedback. The CMFeed dataset encompasses images, text, reactions to the post, human comments with relevance scores, and reactions to the comments. The reactions to the post and comments are utilized to train the proposed model to produce feedback with a particular (positive or negative)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.03970</link><description>&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#65306;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tabular Data: Is Attention All You Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20248;&#21183;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24110;&#21161;&#30740;&#31350;&#21644;&#23454;&#36341;&#31038;&#21306;&#22312;&#26410;&#26469;&#30340;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#20013;&#20570;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2308.03688</link><description>&lt;p&gt;
AgentBench: &#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
AgentBench: Evaluating LLMs as Agents. (arXiv:2308.03688v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03688
&lt;/p&gt;
&lt;p&gt;
AgentBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#30340;&#22810;&#32500;&#24230;&#22522;&#20934;&#65292;&#21457;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#21830;&#19994;LLMs&#22312;&#20805;&#24403;&#20195;&#29702;&#20154;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#19978;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#21644;&#33258;&#20027;&#65292;&#38024;&#23545;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#38469;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#22312;&#20114;&#21160;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AgentBench&#65292;&#19968;&#20010;&#22810;&#32500;&#24230;&#28436;&#21464;&#30340;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#25324;8&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#65292;&#20197;&#35780;&#20272;LLM&#20316;&#20026;&#20195;&#29702;&#20154;&#22312;&#22810;&#36718;&#24320;&#25918;&#24335;&#29983;&#25104;&#35774;&#32622;&#20013;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;27&#20010;&#22522;&#20110;API&#21644;&#24320;&#28304;&#30340;LLM&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#39030;&#32423;&#21830;&#19994;LLM&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20195;&#29702;&#20154;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19982;&#24320;&#28304;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#29615;&#22659;&#21644;LLM&#20013;&#22833;&#36133;&#30340;&#20856;&#22411;&#21407;&#22240;&#65292;&#34920;&#26126;&#38271;&#26399;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#36981;&#24490;&#25351;&#31034;&#33021;&#21147;&#19981;&#20339;&#26159;&#24320;&#21457;&#21487;&#29992;LLM&#20195;&#29702;&#20154;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#21644;&#39640;&#36136;&#37327;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality 
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00665</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25674;&#20248;&#21270;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on amortized optimization. (arXiv:2202.00665v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#32463;&#24120;&#22312;&#21453;&#22797;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#20998;&#25674;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#20123;&#35774;&#32622;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#30456;&#20284;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21464;&#20998;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#27604;&#19981;&#20351;&#29992;&#20998;&#25674;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#27425;&#25945;&#31243;&#20171;&#32461;&#20102;&#36825;&#20123;&#36827;&#27493;&#32972;&#21518;&#30340;&#20998;&#25674;&#20248;&#21270;&#22522;&#30784;&#65292;&#24182;&#27010;&#36848;&#20102;&#23427;&#20204;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25945;&#31243;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/facebookresearch/amortized-optimization-tutorial&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.
&lt;/p&gt;</description></item></channel></rss>