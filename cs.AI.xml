<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00971</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#35780;&#20272;LLM&#39537;&#21160;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploring and Evaluating Hallucinations in LLM-Powered Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#35768;&#22810;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;LLMs&#21487;&#33021;&#20135;&#29983;&#19982;&#29992;&#25143;&#24847;&#22270;&#20559;&#31163;&#12289;&#34920;&#29616;&#20986;&#20869;&#37096;&#19981;&#19968;&#33268;&#25110;&#19982;&#20107;&#23454;&#30693;&#35782;&#19981;&#31526;&#30340;&#36755;&#20986;&#65292;&#20351;&#24471;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#37096;&#32626;LLMs&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#30340;&#24187;&#35273;&#65292;&#32570;&#20047;&#23545;&#20195;&#30721;&#29983;&#25104;&#29615;&#22659;&#20013;&#24187;&#35273;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#20102;&#20027;&#39064;&#20998;&#26512;&#65292;&#24635;&#32467;&#21644;&#24402;&#31867;&#20854;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20027;&#35201;&#24187;&#35273;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio
&lt;/p&gt;</description></item><item><title>&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.05131</link><description>&lt;p&gt;
Sora&#20316;&#20026;AGI&#19990;&#30028;&#27169;&#22411;&#65311;&#20851;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#23436;&#25972;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05131
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#21069;&#27839;&#65292;&#25972;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#25991;&#26412;&#24341;&#23548;&#32534;&#36753;&#30340;&#36827;&#23637;&#12290;&#26412;&#35843;&#26597;&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#65292;&#37325;&#28857;&#20851;&#27880;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21521;&#23574;&#31471;Sora&#27169;&#22411;&#36716;&#21464;&#30340;&#36807;&#31243;&#65292;&#31361;&#20986;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;&#21306;&#21035;&#20110;&#20197;&#24448;&#20316;&#21697;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#26694;&#26550;&#21644;&#28436;&#21270;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#21644;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#25191;&#34892;&#22810;&#23454;&#20307;&#22788;&#29702;&#12289;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23398;&#20064;&#12289;&#29702;&#35299;&#29289;&#29702;&#20114;&#21160;&#12289;&#24863;&#30693;&#29289;&#20307;&#32553;&#25918;&#21644;&#27604;&#20363;&#20197;&#21450;&#23545;&#25239;&#29289;&#20307;&#24187;&#35273;&#65292;&#36825;&#20063;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 Announce Type: new  Abstract: Text-to-video generation marks a significant frontier in the rapidly evolving domain of generative AI, integrating advancements in text-to-image synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional generative models to the cutting-edge Sora model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in generative models. Our c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04343</link><description>&lt;p&gt;
CoTBal: &#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#31639;&#27861;&#65288;CoTBal&#65289;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 &#20844;&#21578;&#31867;&#22411;: &#26032;   &#25688;&#35201;: &#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#26159;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#20851;&#38190;&#35757;&#32451;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#26080;&#24046;&#21035;&#28151;&#21512;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#30340;&#26222;&#36941;&#20570;&#27861;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#25351;&#20196;&#26684;&#24335;&#21644;&#30693;&#35782;&#39046;&#22495;&#19981;&#21516;&#32780;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#20219;&#21153;&#24179;&#34913;&#65288;CoTBal&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;LMMs&#30340;&#22810;&#20219;&#21153;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#25506;&#32034;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#20013;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#24179;&#34913;&#30340;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;:&#65288;1&#65289;&#20219;&#21153;&#38388;&#36129;&#29486;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#21487;&#33021;&#22686;&#24378;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#29616;&#35937;&#65292;&#24402;&#22240;&#20110;&#37325;&#21472;&#30340;&#30693;&#35782;&#39046;&#22495;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#20869;&#38590;&#24230;&#65292;&#25351;&#30340;&#26159;&#21333;&#20010;&#20219;&#21153;&#20869;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#36890;&#36807;&#29992;&#22522;&#20110;&#24615;&#33021;&#30340;&#26041;&#27861;&#37327;&#21270;&#36825;&#20004;&#20010;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04343v1 Announce Type: new  Abstract: Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07191</link><description>&lt;p&gt;
GSINA: &#36890;&#36807;&#22270;Sinkhorn Attention&#25913;&#36827;&#22270;&#19981;&#21464;&#23398;&#20064;&#20013;&#30340;&#23376;&#22270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19981;&#21464;&#23398;&#20064;(GIL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#19979;&#21457;&#29616;&#22270;&#25968;&#25454;&#19982;&#20854;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21464;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;GIL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20316;&#20026;&#35268;&#21017;&#21270;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#33719;&#21462;&#19981;&#21464;&#23376;&#22270;&#26041;&#38754;&#20063;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#30340;&#30456;&#24212;&#21407;&#21017;&#65306;1&#65289;&#31232;&#30095;&#24615;&#65292;&#20197;&#36807;&#28388;&#25481;&#21464;&#24322;&#29305;&#24449;&#65307;2&#65289;&#36719;&#24615;&#65292;&#20197;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35299;&#31354;&#38388;&#65307;&#21644;3&#65289;&#21487;&#24494;&#24615;&#65292;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#20026;&#20102;&#22312;&#19968;&#27425;&#25805;&#20316;&#20013;&#28385;&#36275;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;(OT)&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#31216;&#20026;&#22270;Sinkhorn Attention&#65288;G)
&lt;/p&gt;
&lt;p&gt;
Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13658</link><description>&lt;p&gt;
&#20851;&#20110;AI&#38382;&#36131;&#25919;&#31574;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#20316;&#20986;&#30340;&#22238;&#24212;&#12290;&#22312;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#20851;&#38190;&#21477;&#23376;&#26411;&#23614;&#65292;&#25552;&#20379;&#20102;&#35201;&#27714;&#35780;&#35770;&#30340;&#38382;&#39064;&#32534;&#21495;&#30340;&#19978;&#26631;&#12290;&#35813;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12150</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#22240;&#32032;&#30740;&#31350;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24120;&#24120;&#22312;&#21307;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#25104;&#21151;&#22320;&#24212;&#29992;&#65292;&#36890;&#24120;&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CNN&#20135;&#29983;&#30340;&#27169;&#22411;&#39640;&#24230;&#22797;&#26434;&#19988;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#26377;&#20851;&#20854;&#39044;&#27979;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#36825;&#20419;&#20351;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;MRI&#20998;&#31867;&#20219;&#21153;&#20013;&#23450;&#37327;&#35780;&#20272;&#35299;&#37322;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#36801;&#31227;&#23398;&#20064;&#23545;&#35299;&#37322;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#20110;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;CNN&#30340;&#27969;&#34892;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;CNN&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#33021;&#21147;&#20005;&#37325;&#20381;&#36182;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
&lt;/p&gt;</description></item></channel></rss>