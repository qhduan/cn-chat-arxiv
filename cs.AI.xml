<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01219</link><description>&lt;p&gt;
AI&#28023;&#27915;&#20013;&#30340;&#22934;&#24618;&#20043;&#27468;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#34920;&#31034;&#25285;&#24551;&#65306;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19982;&#29992;&#25143;&#36755;&#20837;&#19981;&#31526;&#12289;&#19982;&#20808;&#21069;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#30683;&#30462;&#25110;&#19982;&#24050;&#24314;&#31435;&#30340;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#29616;&#35937;&#23545;LLMs&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#20851;&#20110;&#24187;&#35273;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26088;&#22312;&#32531;&#35299;LLM&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12766</link><description>&lt;p&gt;
&#23558; Emergent In-Context Learning &#35299;&#37322;&#20026;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#32463;&#20856;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#36807;&#31243;&#30456;&#27604;&#65292;&#20026;&#20102;&#23558;LLMs&#29992;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#65292;&#21363;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#25110;&#26356;&#26032;&#29616;&#26377;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#38750;&#24120;&#26377;&#24847;&#24605;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#39044;&#35757;&#32451;LLMs&#22914;&#20309;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#24403;&#38754;&#20020;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;LLMs&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#65292;&#26469;&#30740;&#31350;&#20026;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#26009;&#24211;&#20043;&#21518;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26680;&#22238;&#24402; $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
&lt;/p&gt;</description></item></channel></rss>