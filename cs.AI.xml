<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02186</link><description>&lt;p&gt;
&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Evolution Guided Generative Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#24182;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GFlowNets&#20195;&#29702;&#26102;&#23637;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#26159;&#19968;&#31867;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#25353;&#29031;&#22870;&#21169;&#27604;&#20363;&#23545;&#32452;&#21512;&#23545;&#35937;&#36827;&#34892;&#37319;&#26679;&#12290;GFlowNets&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#22312;&#22788;&#29702;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#26102;&#26377;&#25928;&#35757;&#32451;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36827;&#21270;&#24341;&#23548;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;EGFN&#65289;&#65292;&#36825;&#26159;&#23545;GFlowNets&#35757;&#32451;&#30340;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#65288;EA&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20219;&#20309;GFlowNets&#35757;&#32451;&#30446;&#26631;&#30340;&#22522;&#30784;&#19978;&#24037;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;EA&#35757;&#32451;&#19968;&#32452;&#20195;&#29702;&#21442;&#25968;&#65292;&#23558;&#32467;&#26524;&#36712;&#36857;&#23384;&#20648;&#22312;&#20248;&#20808;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#20351;&#29992;&#23384;&#20648;&#30340;&#36712;&#36857;&#35757;&#32451;GFlowNets&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#29609;&#20855;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#36712;&#36857;&#21644;&#31232;&#30095;&#22870;&#21169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item></channel></rss>