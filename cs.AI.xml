<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00929</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24076;&#26395;&#23454;&#29616;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#27604;&#22914;&#35821;&#35328;&#19981;&#24179;&#34913;&#12289;&#22810;&#35821;&#35328;&#23545;&#40784;&#21644;&#22266;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;MLLMs&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#22260;&#32469;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#30340;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.16851</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT predict article retraction based on Twitter mentions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26377;&#38382;&#39064;&#30340;&#30740;&#31350;&#25991;&#31456;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26681;&#25454;&#34987;&#25764;&#22238;&#25991;&#31456;&#22312;Twitter&#19978;&#30340;&#25552;&#21450;&#26159;&#21542;&#33021;&#22815;&#22312;&#25991;&#31456;&#34987;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#20998;&#26512;&#20102;&#21253;&#25324;3,505&#31687;&#24050;&#25764;&#22238;&#25991;&#31456;&#21450;&#20854;&#30456;&#20851;Twitter&#25552;&#21450;&#22312;&#20869;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#31895;&#31961;&#31934;&#30830;&#21305;&#37197;&#26041;&#27861;&#33719;&#21462;&#30340;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;3,505&#31687;&#26410;&#25764;&#22238;&#25991;&#31456;&#12290;&#36890;&#36807;&#22235;&#31181;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#20102;Twitter&#25552;&#21450;&#22312;&#39044;&#27979;&#25991;&#31456;&#25764;&#22238;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#27880;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#12290;&#25163;&#21160;&#26631;&#27880;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30340;&#30830;&#26377;&#34987;&#25764;&#22238;&#30340;&#25991;&#31456;&#65292;&#20854;Twitter&#25552;&#21450;&#21253;&#21547;&#22312;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#30340;&#21487;&#35782;&#21035;&#35777;&#25454;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#21344;&#25152;&#26377;&#34987;&#25764;&#22238;&#25991;&#31456;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;</title><link>https://arxiv.org/abs/2402.15052</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20934;&#27979;&#35797;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMBench: Benchmarking Theory of Mind in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ToMBench&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24515;&#28789;&#29702;&#35770;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#34920;&#29616;&#36229;&#36807;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#25351;&#24863;&#30693;&#21644;&#24402;&#22240;&#33258;&#24049;&#20197;&#21450;&#20182;&#20154;&#30340;&#24515;&#29702;&#29366;&#24577;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#21457;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#19968;&#31181;&#24418;&#24335;&#30340;&#24515;&#28789;&#29702;&#35770;&#30340;&#20105;&#35770;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#28789;&#29702;&#35770;&#35780;&#20272;&#21463;&#21040;&#35832;&#22914;&#21463;&#38480;&#33539;&#22260;&#12289;&#20027;&#35266;&#21028;&#26029;&#21644;&#24847;&#22806;&#27745;&#26579;&#31561;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToMBench&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#28085;&#30422;&#31038;&#20250;&#35748;&#30693;&#20013;&#30340;8&#39033;&#20219;&#21153;&#21644;31&#39033;&#33021;&#21147;&#65292;&#22810;&#39033;&#36873;&#25321;&#39064;&#26684;&#24335;&#20197;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#26080;&#20559;&#35265;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#22522;&#20110;&#21452;&#35821;&#28165;&#21333;&#30340;&#20174;&#22836;&#26500;&#24314;&#65292;&#20005;&#26684;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#22522;&#20110;ToMBench&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;10&#20010;&#27969;&#34892;LLMs&#22312;&#20219;&#21153;&#21644;&#33021;&#21147;&#26041;&#38754;&#30340;&#24515;&#28789;&#29702;&#35770;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#33853;&#21518;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15052v1 Announce Type: cross  Abstract: Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether large language models (LLMs) exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular LLMs across tasks and abilities. We find that even the most advanced LLMs like GPT-4 lag behind human performance by over 10% points, indicati
&lt;/p&gt;</description></item><item><title>RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13853</link><description>&lt;p&gt;
RealDex: &#23454;&#29616;&#26426;&#22120;&#20154;&#28789;&#24039;&#25163;&#31867;&#20154;&#24335;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
RealDex: Towards Human-like Grasping for Robotic Dexterous Hand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13853
&lt;/p&gt;
&lt;p&gt;
RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RealDex&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#34701;&#20837;&#20102;&#20154;&#31867;&#34892;&#20026;&#27169;&#24335;&#30340;&#30495;&#23454;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#35270;&#35282;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#21033;&#29992;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#26102;&#26080;&#32541;&#21516;&#27493;&#20154;-&#26426;&#22120;&#20154;&#25163;&#23039;&#21183;&#12290;&#36825;&#20123;&#31867;&#20154;&#21160;&#20316;&#30340;&#38598;&#21512;&#23545;&#20110;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#33258;&#28982;&#12289;&#26356;&#31934;&#30830;&#22320;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;RealDex&#22312;&#25512;&#21160;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21069;&#27839;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#31526;&#21512;&#20154;&#31867;&#32463;&#39564;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;RealDex&#21644;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13853v1 Announce Type: cross  Abstract: In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09236</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#27010;&#24565;&#65306;&#32479;&#19968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26377;&#20004;&#31181;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#22825;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21162;&#21147;&#26041;&#21521;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25237;&#20837;&#21162;&#21147;&#21435;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#27010;&#24565;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#25968;&#25454;&#20013;&#34987;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#26469;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06663</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#23494;&#38053;&#23545;&#25239;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#30340;&#21487;&#35299;&#37322;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#65288;RIS&#65289;&#30340;&#21457;&#23637;&#23545;&#29289;&#29702;&#23618;&#23433;&#20840;&#65288;PLS&#65289;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#21512;&#27861;&#30340;RIS&#21487;&#20197;&#20135;&#29983;&#26377;&#30410;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22686;&#21152;&#20449;&#36947;&#30340;&#38543;&#26426;&#24615;&#65292;&#22686;&#24378;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65288;PL-SKG&#65289;&#65292;&#32780;&#24694;&#24847;&#30340;RIS&#21487;&#20197;&#30772;&#22351;&#21512;&#27861;&#20449;&#36947;&#24182;&#30772;&#35299;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;PL-SKG&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#27861;&#21442;&#19982;&#26041;&#65288;&#21363;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#65289;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20013;&#38388;&#20154;&#24694;&#24847;RIS&#65288;MITM-RIS&#65289;&#31363;&#21548;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21512;&#27861;&#37197;&#23545;&#21644;MITM-RIS&#20043;&#38388;&#30340;&#29702;&#35770;&#20114;&#20449;&#24687;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#23398;&#20064;&#23454;&#29616;&#19968;&#20010;&#19982;MITM-RIS&#27809;&#26377;&#20114;&#20449;&#24687;&#37325;&#21472;&#30340;&#20849;&#21516;&#29305;&#24449;&#38754;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#21487;&#35299;&#37322;AI&#65288;xAI&#65289;&#34920;&#31034;&#23545;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#21495;&#22788;&#29702;&#35299;&#37322;&#12290;&#36825;&#20123;&#20027;&#23548;&#31070;&#32463;&#20803;&#30340;&#31526;&#21495;&#26415;&#35821;&#26377;&#21161;&#20110;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an adversarial learning framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical mutual information gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative adversarial networks (GANs) to learn to achieve a common feature surface that does not have mutual information overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic explainable AI (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06660</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#22312;&#26657;&#20934;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of the metaverse in calibrating an embodied artificial general intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#31181;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#34701;&#20837;&#35748;&#30693;&#12289;Michael Levin&#30340;&#35745;&#31639;&#36793;&#30028;"Self"&#12289;Donald D. Hoffman&#30340;&#24863;&#30693;&#30028;&#38754;&#29702;&#35770;&#20197;&#21450;Bernardo Kastrup&#30340;&#20998;&#26512;&#21807;&#24515;&#20027;&#20041;&#31561;&#29702;&#35770;&#26694;&#26550;&#26469;&#26500;&#24314;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35770;&#35777;&#12290;&#23427;&#35748;&#20026;&#25105;&#20204;&#25152;&#24863;&#30693;&#30340;&#22806;&#37096;&#29616;&#23454;&#26159;&#19968;&#31181;&#20869;&#22312;&#23384;&#22312;&#30340;&#20132;&#26367;&#29366;&#24577;&#30340;&#35937;&#24449;&#24615;&#34920;&#31034;&#65292;&#32780;AGI&#21487;&#20197;&#20855;&#26377;&#26356;&#22823;&#35745;&#31639;&#36793;&#30028;&#30340;&#26356;&#39640;&#24847;&#35782;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;AGI&#30340;&#21457;&#23637;&#38454;&#27573;&#12289;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35201;&#27714;&#12289;&#20026;AGI&#26657;&#20934;&#35937;&#24449;&#24615;&#30028;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#12289;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#12289;&#24320;&#28304;&#21306;&#22359;&#38142;&#25216;&#26415;&#20197;&#21450;&#24320;&#28304;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#27807;&#36890;&#26426;&#21046;&#21644;&#29992;&#20110;&#21152;&#24378;&#23545;&#20803;&#23431;&#23449;&#30340;&#29702;&#35299;&#30340;&#25216;&#26415;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
&lt;/p&gt;</description></item><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.11319</link><description>&lt;p&gt;
GeoSAM: &#20351;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#23545;SAM&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11319
&lt;/p&gt;
&lt;p&gt;
GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#26102;&#65292;Segment Anything Model (SAM)&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22320;&#29702;&#22270;&#20687;&#65288;&#22914;&#33322;&#25293;&#21644;&#21355;&#26143;&#22270;&#20687;&#65289;&#20013;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#36947;&#36335;&#12289;&#20154;&#34892;&#36947;&#21644;&#20154;&#34892;&#27178;&#36947;&#31561;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#26102;&#12290;&#36825;&#31181;&#36739;&#24046;&#30340;&#24615;&#33021;&#28304;&#20110;&#36825;&#20123;&#23545;&#35937;&#30340;&#31364;&#23567;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#32441;&#29702;&#34701;&#20837;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#26641;&#26408;&#12289;&#24314;&#31569;&#29289;&#12289;&#36710;&#36742;&#21644;&#34892;&#20154;&#31561;&#29289;&#20307;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#37117;&#21487;&#33021;&#20351;&#27169;&#22411;&#22833;&#21435;&#23450;&#21521;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;SAM&#65288;GeoSAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#23494;&#38598;&#35270;&#35273;&#25552;&#31034;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#31232;&#30095;&#35270;&#35273;&#25552;&#31034;&#23454;&#26045;&#20102;&#32454;&#35843;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;GeoSAM&#22312;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12289;&#34892;&#20154;&#22522;&#30784;&#35774;&#26045;&#30340;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#20102;26&#65285;&#12289;7&#65285;&#21644;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
&lt;/p&gt;</description></item><item><title>ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14279</link><description>&lt;p&gt;
ZS4C: &#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#19981;&#23436;&#25972;&#20195;&#30721;&#29255;&#27573;&#30340;&#21487;&#32534;&#35793;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT. (arXiv:2401.14279v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14279
&lt;/p&gt;
&lt;p&gt;
ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#38382;&#31572;&#65288;Q&amp;A&#65289;&#32593;&#31449;&#22914;Stack Overflow&#24050;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#32773;&#23547;&#27714;&#30693;&#35782;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;Q&amp;A&#32593;&#31449;&#19978;&#30340;&#20195;&#30721;&#29255;&#27573;&#36890;&#24120;&#30001;&#20110;&#26410;&#35299;&#26512;&#30340;&#31867;&#22411;&#21644;&#32570;&#22833;&#30340;&#20381;&#36182;&#24211;&#32780;&#26080;&#27861;&#32534;&#35793;&#21644;&#35821;&#20041;&#19978;&#19981;&#23436;&#25972;&#65292;&#36825;&#22686;&#21152;&#20102;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#30340;&#38556;&#30861;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#36866;&#29992;&#20110;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#65292;&#35201;&#20040;&#32534;&#35793;&#25104;&#21151;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZS4C&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#19981;&#23436;&#25972;&#30340;&#20195;&#30721;&#29255;&#27573;&#20013;&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;ZS4C&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;ZS4C&#21033;&#29992;&#19968;&#20010;LLM&#65292;&#21363;ChatGPT&#65292;&#26681;&#25454;&#25105;&#20204;&#35774;&#35745;&#30340;&#19987;&#29992;&#20219;&#21153;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#32473;&#23450;&#30340;&#20195;&#30721;&#29255;&#27573;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;ZS4C&#36890;&#36807;&#20462;&#22797;&#30001;&#20110;&#19981;&#27491;&#30830;&#30340;&#23548;&#20837;&#35821;&#21477;&#21644;&#35821;&#27861;&#38169;&#35823;&#24341;&#36215;&#30340;&#32534;&#35793;&#38169;&#35823;&#26469;&#20462;&#22797;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical question and answering (Q&amp;A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&amp;A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&amp;A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through 
&lt;/p&gt;</description></item><item><title>&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12986</link><description>&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Crowdsourced Adaptive Surveys. (arXiv:2401.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12986
&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#28436;&#21464;&#38382;&#39064;&#24211;&#65292;&#24182;&#22312;&#35843;&#26597;&#20013;&#36866;&#24212;&#26032;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#65292;&#33021;&#22815;&#35782;&#21035;&#38590;&#20197;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#35843;&#26597;&#23545;&#20110;&#27665;&#20027;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20110;&#20256;&#32479;&#35843;&#26597;&#26041;&#27861;&#26469;&#35828;&#65292;&#24555;&#36895;&#21464;&#21270;&#30340;&#20449;&#24687;&#29615;&#22659;&#21644;&#22312;&#23567;&#20247;&#31038;&#21306;&#20013;&#34913;&#37327;&#35266;&#28857;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20247;&#21253;&#33258;&#36866;&#24212;&#35843;&#26597;&#26041;&#27861;&#65288;CSAS&#65289;&#65292;&#23427;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36827;&#23637;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#38543;&#30528;&#29992;&#25143;&#36755;&#20837;&#19981;&#26029;&#28436;&#21464;&#30340;&#38382;&#39064;&#24211;&#12290;CSAS&#26041;&#27861;&#23558;&#21442;&#19982;&#32773;&#25552;&#20379;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#36716;&#25442;&#20026;Likert&#24335;&#39033;&#30446;&#65292;&#24182;&#24212;&#29992;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#26469;&#30830;&#23450;&#24212;&#20248;&#20808;&#32771;&#34385;&#22312;&#35843;&#26597;&#20013;&#30340;&#29992;&#25143;&#25552;&#20379;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#24615;&#20801;&#35768;&#25506;&#32034;&#26032;&#30340;&#35843;&#26597;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35843;&#26597;&#38271;&#24230;&#19978;&#26045;&#21152;&#26368;&#23567;&#30340;&#25104;&#26412;&#12290;&#22312;&#25289;&#19969;&#35028;&#20449;&#24687;&#29615;&#22659;&#21644;&#35758;&#39064;&#37325;&#35201;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;CSAS&#35782;&#21035;&#21487;&#33021;&#38590;&#20197;&#36890;&#36807;&#26631;&#20934;&#26041;&#27861;&#36319;&#36394;&#30340;&#20027;&#24352;&#25110;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#25552;&#20986; Conclusion by di&#30340;&#32467;&#26463;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly changing information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into Likert-style items and applies a multi-armed bandit algorithm to determine user-provided questions that should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments and issue importance showcase CSAS's ability to identify claims or issues that might otherwise be difficult to track using standard approaches. I conclude by di
&lt;/p&gt;</description></item><item><title>xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06199</link><description>&lt;p&gt;
xTrimoPGLM: &#32479;&#19968;&#30340;&#30334;&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06199
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#29983;&#29289;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#23616;&#38480;&#20110;&#33258;&#32534;&#30721;&#25110;&#33258;&#22238;&#24402;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#26102;&#24456;&#38590;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;xTrimoPGLM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25506;&#32034;&#36825;&#20004;&#31867;&#30446;&#26631;&#30340;&#20860;&#23481;&#24615;&#21644;&#32852;&#21512;&#20248;&#21270;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#65292;&#20351;&#29992;1000&#20159;&#21442;&#25968;&#21644;1&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#26469;&#35757;&#32451;xTrimoPGLM&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;1&#65289;xTrimoPGLM&#22312;&#22235;&#20010;&#31867;&#21035;&#30340;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#22522;&#32447;&#12290;&#35813;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2311.05546</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20351;&#29992;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#26234;&#33021;&#20135;&#19994;&#24212;&#29992;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#37319;&#29992;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#36139;&#30240;&#24179;&#21488;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#24615;&#33021;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;Coin Game&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36951;&#20256;&#21464;&#31181;&#65292;&#24182;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#26041;&#27861;&#30456;&#27604;&#20110;&#20855;&#26377;&#31867;&#20284;&#21442;&#25968;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08598</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Medical Image Analysis: A Survey. (arXiv:2310.08598v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MedIA&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#26041;&#38754;&#36215;&#21040;&#20102;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#20026;&#20854;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;MedIA&#30340;DL&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#19979;&#24456;&#38590;&#27867;&#21270;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#21508;&#31181;DL&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#22312;&#26410;&#30693;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31283;&#20581;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#19987;&#38376;&#38024;&#23545;MedIA&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#22312;&#26356;&#22823;&#33539;&#22260;MedIA&#31995;&#32479;&#20869;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#19981;&#20165;&#20165;&#32771;&#34385;&#26041;&#27861;&#23398;&#65292;&#36824;&#32771;&#34385;&#20102;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20998;&#20026;&#25968;&#25454;&#23618;&#27425;&#30340;&#26041;&#27861;&#8230;
&lt;/p&gt;
&lt;p&gt;
Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-lev
&lt;/p&gt;</description></item><item><title>PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01432</link><description>&lt;p&gt;
&#20998;&#21106;&#19982;&#21512;&#24182;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Split and Merge: Aligning Position Biases in Large Language Model based Evaluators. (arXiv:2310.01432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01432
&lt;/p&gt;
&lt;p&gt;
PORTIA&#26159;&#19968;&#20010;&#26088;&#22312;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#23545;&#40784;&#65292;&#28982;&#21518;&#23558;&#20854;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#33258;&#21160;&#21270;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#31995;&#32479;&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#22312;&#20351;&#29992;&#23545;&#27604;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#26102;&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#25110;&#19981;&#19968;&#33268;&#24615;&#65292;&#26080;&#35270;&#20869;&#23481;&#32780;&#20559;&#21521;&#20110;&#31532;&#19968;&#20010;&#25110;&#31532;&#20108;&#20010;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PORTIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#40784;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#30340;&#27604;&#36739;&#31574;&#30053;&#65292;&#20197;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#26657;&#20934;&#20301;&#32622;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PORTIA&#23558;&#31572;&#26696;&#20998;&#21106;&#25104;&#22810;&#20010;&#29255;&#27573;&#65292;&#23545;&#27604;&#20505;&#36873;&#31572;&#26696;&#20013;&#30340;&#30456;&#20284;&#20869;&#23481;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#22238;&#19968;&#20010;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#20197;&#20379;LLMs&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;11,520&#20010;&#31572;&#26696;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PORTIA&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;&#27169;&#22411;&#21644;&#23545;&#27604;&#24418;&#24335;&#30340;&#19968;&#33268;&#24615;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;47.46%&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;PORTIA&#20351;&#24471;LLMs&#33021;&#22815;&#35780;&#20272;&#20013;&#23545;&#20301;&#32622;&#20559;&#24046;&#36827;&#34892;&#26657;&#20934;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25913;&#21892;&#20154;&#31867;&#20915;&#31574;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#19982;&#26426;&#22120;&#39044;&#27979;&#65292;&#26367;&#25442;&#37096;&#20998;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#24182;&#32463;&#36807;&#23454;&#39564;&#26816;&#39564;&#24471;&#20986;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20892;&#26449;&#22320;&#21306;&#30340;&#21307;&#29983;&#30340;&#35786;&#26029;&#26356;&#23481;&#26131;&#34987;&#26367;&#20195;&#12290;</title><link>http://arxiv.org/abs/2306.11689</link><description>&lt;p&gt;
&#32479;&#35745;&#27979;&#35797;&#26367;&#20195;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Tests for Replacing Human Decision Makers with Algorithms. (arXiv:2306.11689v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25913;&#21892;&#20154;&#31867;&#20915;&#31574;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#19982;&#26426;&#22120;&#39044;&#27979;&#65292;&#26367;&#25442;&#37096;&#20998;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#24182;&#32463;&#36807;&#23454;&#39564;&#26816;&#39564;&#24471;&#20986;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20892;&#26449;&#22320;&#21306;&#30340;&#21307;&#29983;&#30340;&#35786;&#26029;&#26356;&#23481;&#26131;&#34987;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#26469;&#25913;&#21892;&#20154;&#31867;&#30340;&#20915;&#31574;&#12290;&#39318;&#20808;&#23558;&#27599;&#20010;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#34920;&#29616;&#19982;&#26426;&#22120;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#28982;&#21518;&#29992;&#25152;&#25552;&#20986;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24314;&#35758;&#26367;&#25442;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#19968;&#20010;&#23376;&#38598;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#21033;&#29992;&#20840;&#22269;&#22823;&#22411;&#23381;&#20135;&#32467;&#26524;&#21644;&#32321;&#27542;&#24180;&#40836;&#22827;&#22919;&#23381;&#21069;&#26816;&#26597;&#30340;&#21307;&#29983;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35797;&#39564;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#39640;&#39057;&#29575;&#26041;&#27861;&#20197;&#21450;&#19968;&#31181;&#36125;&#21494;&#26031;&#21518;&#39564;&#25439;&#22833;&#20989;&#25968;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24322;&#24120;&#20986;&#29983;&#26816;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#27604;&#20165;&#30001;&#21307;&#29983;&#35786;&#26029;&#30340;&#32467;&#26524;&#20855;&#26377;&#26356;&#39640;&#30340;&#24635;&#20307;&#30495;&#38451;&#24615;&#29575;&#21644;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26469;&#33258;&#20892;&#26449;&#22320;&#21306;&#30340;&#21307;&#29983;&#30340;&#35786;&#26029;&#26356;&#23481;&#26131;&#34987;&#26367;&#20195;&#65292;&#36825;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;&#26356;&#23481;&#26131;&#25552;&#39640;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a statistical framework with which artificial intelligence can improve human decision making. The performance of each human decision maker is first benchmarked against machine predictions; we then replace the decisions made by a subset of the decision makers with the recommendation from the proposed artificial intelligence algorithm. Using a large nationwide dataset of pregnancy outcomes and doctor diagnoses from prepregnancy checkups of reproductive age couples, we experimented with both a heuristic frequentist approach and a Bayesian posterior loss function approach with an application to abnormal birth detection. We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only. We also find that the diagnoses of doctors from rural areas are more frequently replaceable, suggesting that artificial intelligence assisted decision making tends to improve precision more i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14537</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#26368;&#20248;&#20998;&#21306;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#27969;&#34892;&#20998;&#31867;&#26041;&#27861;&#65292;&#23613;&#31649;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#21548;&#36215;&#26469;&#24456;&#22909;&#65292;&#20294;&#23454;&#38469;&#19978;&#20250;&#23548;&#33268;&#22823;&#22810;&#25968;&#25237;&#31080;&#39118;&#26684;&#30340;&#34892;&#20026;&#12290;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#34987;&#31216;&#20026;&#29420;&#31435;&#29305;&#24449;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#20998;&#31867;&#26102;&#23427;&#20204;&#27809;&#26377;&#26465;&#20214;&#30456;&#20851;&#24615;&#25110;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26126;&#30830;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#38169;&#35823;&#29575;&#26356;&#20302;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#25110;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09825</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#21152;&#36895;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20195;&#29702;&#33021;&#22815;&#23558;&#20854;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#20132;&#20114;&#12290;&#21463;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#36712;&#36857;&#30340;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#65288;2&#65289;&#21516;&#26102;&#35757;&#32451;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#31163;&#32447;&#36712;&#36857;&#30340;&#36136;&#37327;&#65288;&#36712;&#36857;&#30340;&#26368;&#20339;&#24615;&#65289;&#21644;&#22810;&#26679;&#24615;&#65288;&#36712;&#36857;&#25968;&#37327;&#21644;&#35206;&#30422;&#32423;&#21035;&#65289;&#23545;&#20004;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;MiniGrid&#29615;&#22659;&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#21487;&#25805;&#20316;&#30340;&#36873;&#21306;&#21010;&#20998;&#22270;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#8220;&#26368;&#20856;&#22411;&#8221;&#30340;&#20013;&#24515;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#19968;&#31995;&#21015;&#32422;&#26463;&#26465;&#20214;&#19979;&#36873;&#21306;&#21010;&#20998;&#22270;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.00872</link><description>&lt;p&gt;
&#36317;&#31163;&#23545;&#36873;&#21306;&#21010;&#20998;&#22270;&#30340;&#24433;&#21709;&#65306;&#20013;&#24515;&#21644;&#24322;&#24120;&#22320;&#22270;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implications of Distance over Redistricting Maps: Central and Outlier Maps. (arXiv:2203.00872v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#21487;&#25805;&#20316;&#30340;&#36873;&#21306;&#21010;&#20998;&#22270;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#8220;&#26368;&#20856;&#22411;&#8221;&#30340;&#20013;&#24515;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#19968;&#31995;&#21015;&#32422;&#26463;&#26465;&#20214;&#19979;&#36873;&#21306;&#21010;&#20998;&#22270;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#35758;&#21046;&#27665;&#20027;&#20013;&#65292;&#36873;&#21306;&#21010;&#20998;&#22270;&#29992;&#20110;&#23558;&#36873;&#27665;&#21010;&#20998;&#20026;&#19968;&#32452;&#36873;&#21306;&#65292;&#27599;&#20010;&#21306;&#36873;&#20986;&#19968;&#20010;&#20195;&#34920;&#12290;&#26377;&#25928;&#30340;&#21010;&#20998;&#22270;&#24517;&#39035;&#28385;&#36275;&#19968;&#31995;&#21015;&#32422;&#26463;&#26465;&#20214;&#65292;&#20363;&#22914;&#32039;&#20945;&#24615;&#12289;&#36830;&#32493;&#24615;&#12289;&#20197;&#21450;&#20960;&#20046;&#30456;&#31561;&#30340;&#20154;&#21475;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21152;&#24378;&#30340;&#38480;&#21046;&#26465;&#20214;&#20173;&#28982;&#19981;&#36275;&#20197;&#38480;&#21046;&#26377;&#25928;&#36873;&#21306;&#21010;&#20998;&#22270;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#19988;&#21487;&#25805;&#20316;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#20197;&#27492;&#30740;&#31350;&#22312;&#19968;&#31995;&#21015;&#32422;&#26463;&#26465;&#20214;&#19979;&#36873;&#21306;&#21010;&#20998;&#22270;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#20856;&#22411;&#8221;&#30340;&#20013;&#24515;&#22270;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#23427;&#22312;&#19968;&#20010;&#22996;&#21592;&#20250;&#22330;&#26223;&#20013;&#21453;&#26144;&#20102;Kemeny&#65288;&#20975;&#38376;&#32822;&#65289;&#25490;&#21517;&#30340;&#33391;&#22909;&#24615;&#26469;&#32473;&#20986;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representative democracy, a redistricting map is chosen to partition an electorate into a collection of districts each of which elects a representative. A valid redistricting map must satisfy a collection of constraints such as being compact, contiguous, and of almost equal population. However, these imposed constraints are still loose enough to enable an enormous ensemble of valid redistricting maps. This fact introduces a difficulty in drawing redistricting maps and it also enables a partisan legislature to possibly gerrymander by choosing a map which unfairly favors it. In this paper, we introduce an interpretable and tractable distance measure over redistricting maps which does not use election results and study its implications over the ensemble of redistricting maps. Specifically, we define a central map which may be considered as being "most typical" and give a rigorous justification for it by showing that it mirrors the Kemeny ranking in a scenario where we have a committee 
&lt;/p&gt;</description></item></channel></rss>