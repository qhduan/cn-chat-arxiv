<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#37327;&#21270;&#27010;&#24565;&#31867;&#27604;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#65292;&#21516;&#26102;&#20351;&#29992;&#25509;&#32447;&#22270;&#25805;&#20316;&#23450;&#20041;&#20102;&#24230;&#37327;&#21644;&#22270;&#32534;&#36753;&#36317;&#31163;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01020</link><description>&lt;p&gt;
&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#37327;&#21270;&#27010;&#24565;&#30340;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
Quantifying analogy of concepts via ologs and wiring diagrams
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;ologs&#21644;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#37327;&#21270;&#27010;&#24565;&#31867;&#27604;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#65292;&#21516;&#26102;&#20351;&#29992;&#25509;&#32447;&#22270;&#25805;&#20316;&#23450;&#20041;&#20102;&#24230;&#37327;&#21644;&#22270;&#32534;&#36753;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Spivak&#21644;Kent&#21019;&#24314;&#30340;&#26412;&#20307;&#26085;&#24535;(ologs)&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;&#25509;&#32447;&#22270;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25509;&#32447;&#22270;&#26159;&#19968;&#20010;&#26377;&#38480;&#30340;&#26377;&#21521;&#26631;&#35760;&#22270;&#12290;&#26631;&#35760;&#23545;&#24212;&#20110;olog&#20013;&#30340;&#31867;&#22411;&#65307;&#23427;&#20204;&#20063;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#33258;&#20027;&#31995;&#32479;&#20013;&#20256;&#24863;&#22120;&#30340;&#35835;&#25968;&#12290;&#22240;&#27492;&#65292;&#25509;&#32447;&#22270;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#24418;&#25104;&#25277;&#35937;&#27010;&#24565;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#39592;&#26550;&#25509;&#32447;&#22270;&#30340;&#22270;&#24418;&#24418;&#25104;&#19968;&#20010;&#33539;&#30068;&#12290;&#36825;&#20351;&#24471;&#39592;&#26550;&#25509;&#32447;&#22270;&#21487;&#20197;&#20351;&#29992;&#22270;&#35770;&#21644;&#33539;&#30068;&#35770;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#21644;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#20165;&#36866;&#29992;&#20110;&#25509;&#32447;&#22270;&#30340;&#25805;&#20316;&#23558;&#20256;&#32479;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#23450;&#20041;&#25193;&#23637;&#21040;&#25509;&#32447;&#22270;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25152;&#26377;&#39592;&#26550;&#25509;&#32447;&#22270;&#38598;&#21512;&#19978;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#30001;&#25509;&#32447;&#22270;&#34920;&#31034;&#30340;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#36317;&#31163;&#30340;&#25193;&#23637;&#31034;&#20363;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20219;&#20309;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build on the theory of ontology logs (ologs) created by Spivak and Kent, and define a notion of wiring diagrams. In this article, a wiring diagram is a finite directed labelled graph. The labels correspond to types in an olog; they can also be interpreted as readings of sensors in an autonomous system. As such, wiring diagrams can be used as a framework for an autonomous system to form abstract concepts. We show that the graphs underlying skeleton wiring diagrams form a category. This allows skeleton wiring diagrams to be compared and manipulated using techniques from both graph theory and category theory. We also extend the usual definition of graph edit distance to the case of wiring diagrams by using operations only available to wiring diagrams, leading to a metric on the set of all skeleton wiring diagrams. In the end, we give an extended example on calculating the distance between two concepts represented by wiring diagrams, and explain how to apply our framework to any applica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.10747</link><description>&lt;p&gt;
&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36830;&#32493;&#19968;&#33268;&#29289;&#29702;&#20449;&#24687;&#38477;&#27700;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUPIN&#65292;&#21363;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25289;&#26684;&#26391;&#26085;&#21452;U-Net&#30340;&#29616;&#22312;&#39044;&#25253;&#65292;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#22806;&#25512;&#30340;&#39044;&#25253;&#26041;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#21487;&#24494;&#19988;GPU&#21152;&#36895;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#25289;&#26684;&#26391;&#26085;&#22352;&#26631;&#31995;&#36716;&#25442;&#65292;&#20197;&#20801;&#35768;&#23454;&#26102;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;LUPIN&#19982;&#24182;&#36229;&#36807;&#20102;&#25152;&#36873;&#25321;&#22522;&#20934;&#30340;&#24615;&#33021;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10747v1 Announce Type: cross  Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;</title><link>http://arxiv.org/abs/2310.02277</link><description>&lt;p&gt;
"&#22403;&#22334;DNA&#20551;&#35774;&#65306;&#36890;&#36807;&#31232;&#30095;&#24615;&#23545;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#36827;&#34892;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#20998;&#26512;"
&lt;/p&gt;
&lt;p&gt;
Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#24615;&#20998;&#26512;LLM&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20219;&#21153;&#20013;&#24515;&#35282;&#24230;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#23545;&#20110;&#26435;&#37325;&#20013;&#20887;&#20313;&#24615;&#30340;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#23545;"&#22403;&#22334;DNA"&#30340;&#27010;&#24565;&#38271;&#26399;&#20197;&#26469;&#19982;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#30340;&#38750;&#32534;&#30721;&#29255;&#27573;&#30456;&#20851;&#32852;&#65292;&#21344;&#20854;&#32452;&#25104;&#30340;&#22823;&#32422;98%&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#36825;&#20123;&#30475;&#20284;&#26080;&#21151;&#33021;&#30340;DNA&#24207;&#21015;&#22312;&#32454;&#32990;&#36807;&#31243;&#20013;&#36215;&#21040;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#19982;&#20154;&#31867;&#22522;&#22240;&#20013;&#35266;&#23519;&#21040;&#30340;&#20887;&#20313;&#24615;&#26377;&#30528;&#26174;&#33879;&#30340;&#30456;&#20284;&#24615;&#12290;&#20154;&#20204;&#35748;&#20026;&#65292;&#24222;&#22823;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21253;&#21547;&#20102;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#21453;&#35770;&#26469;&#25361;&#25112;&#36825;&#20010;&#20256;&#32479;&#35266;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#26469;&#29420;&#31435;&#32780;&#20934;&#30830;&#22320;&#37327;&#21270;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20302;&#24133;&#24230;&#26435;&#37325;&#30340;&#32454;&#24494;&#37325;&#35201;&#24615;&#65292;&#20174;&#19979;&#28216;&#20219;&#21153;&#20013;&#24515;&#30340;&#35282;&#24230;&#29702;&#35299;&#23427;&#20204;&#21253;&#21547;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25903;&#25345;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#30340;"&#22403;&#22334;DNA&#20551;&#35774;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item></channel></rss>