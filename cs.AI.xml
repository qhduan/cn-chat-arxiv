<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00271</link><description>&lt;p&gt;
TG-NAS&#65306;&#21033;&#29992;Transformer&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39640;&#25928;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00271
&lt;/p&gt;
&lt;p&gt;
TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#26159;&#19968;&#31181;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32791;&#26102;&#30340;&#35757;&#32451;&#25110;&#23494;&#38598;&#30340;&#37319;&#26679;&#21644;&#35780;&#20272;&#12290;&#38646;&#25104;&#26412;NAS&#26088;&#22312;&#20026;&#26550;&#26500;&#24615;&#33021;&#39044;&#27979;&#21019;&#24314;&#20813;&#35757;&#32451;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#19988;&#24120;&#24120;&#34987;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25110;&#28014;&#28857;&#36816;&#31639;&#27425;&#25968;&#31561;&#31616;&#21333;&#25351;&#26631;&#25152;&#36229;&#36234;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26080;&#27861;&#23558;&#27867;&#21270;&#21040;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#35265;&#26032;&#31867;&#22411;&#36816;&#31639;&#31526;&#19988;&#19981;&#24102;&#26377;&#40644;&#37329;&#20934;&#30830;&#24230;&#12290;&#19968;&#20010;&#26222;&#36941;&#26368;&#20248;&#30340;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TG-NAS&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#30340;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#25351;&#23548;&#30528;&#22312;&#20219;&#20309;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20869;&#36827;&#34892;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.18195</link><description>&lt;p&gt;
&#29992;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#32416;&#27491;LEGO&#32452;&#35013;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18195
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;3D&#35270;&#35273;&#20013;&#65292;&#33258;&#20027;&#32452;&#35013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30830;&#20445;&#32452;&#35013;&#27491;&#30830;&#24615;&#12290;&#20027;&#27969;&#26041;&#27861;&#22914;MEPNet&#30446;&#21069;&#19987;&#27880;&#20110;&#22522;&#20110;&#25163;&#21160;&#25552;&#20379;&#30340;&#22270;&#20687;&#36827;&#34892;&#32452;&#20214;&#32452;&#35013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#21462;&#24471;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#21516;&#19968;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25972;&#21512;&#33258;&#26657;&#27491;&#27169;&#22359;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#21463;&#27492;&#38382;&#39064;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#35782;&#21035;&#21644;&#32416;&#27491;&#32452;&#20214;&#32452;&#35013;&#38169;&#35823;&#12290;&#20026;&#25903;&#25345;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#21253;&#25324;&#29992;&#20110;&#32452;&#35013;&#27493;&#39588;&#21644;&#32452;&#35013;&#22833;&#36133;&#23454;&#20363;&#30340;&#25163;&#21160;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;SCANet&#23558;&#32452;&#35013;&#30340;&#37096;&#20214;&#35270;&#20026;&#26597;&#35810;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16732</link><description>&lt;p&gt;
&#22312;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Enabling Uncertainty Estimation in Iterative Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16732
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#36882;&#32593;&#32476;&#26550;&#26500;&#36716;&#21464;&#20026;&#36845;&#20195;&#32593;&#32476;&#26550;&#26500;&#65292;&#36845;&#20195;&#32593;&#32476;&#20351;&#29992;&#33258;&#36523;&#30340;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#21319;&#24615;&#33021;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#26550;&#26500;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#65306;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#19982;&#20854;&#25910;&#25947;&#20540;&#30340;&#20934;&#30830;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#29575;&#29992;&#20316;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#27604;&#35832;&#22914;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#36845;&#20195;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#23884;&#20837;&#21040;&#20004;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#26469;&#23637;&#31034;&#20854;&#23454;&#29992;&#20215;&#20540;&#65306;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#36947;&#36335;&#26816;&#27979;&#21644;&#20108;&#32500;&#21644;&#19977;&#32500;&#24418;&#29366;&#30340;&#31354;&#27668;&#21160;&#21147;&#29305;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16732v1 Announce Type: new  Abstract: Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#26597;&#30740;&#31350;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#23545;&#20154;&#20204;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#32780;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#21457;&#21051;&#26495;&#21360;&#35937;&#21644;&#20260;&#23475;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04420</link><description>&lt;p&gt;
&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#65306;&#38656;&#35201;&#20102;&#35299;&#35841;&#27491;&#22312;&#21463;&#21040;&#21738;&#20123;&#38169;&#35823;&#20197;&#21450;&#20197;&#20309;&#31181;&#26041;&#24335;&#21463;&#21040;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#26597;&#30740;&#31350;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#23545;&#20154;&#20204;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#32780;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#21457;&#21051;&#26495;&#21360;&#35937;&#21644;&#20260;&#23475;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#23427;&#20204;&#21487;&#33021;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#24456;&#23569;&#22522;&#20110;&#20154;&#31867;&#23545;&#20260;&#23475;&#30340;&#24515;&#29702;&#20307;&#39564;&#12290;&#20511;&#37492;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#65292;&#25105;&#20204;&#20197;&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20154;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#30340;&#21453;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35843;&#26597;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#37117;&#21453;&#26144;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#20063;&#27809;&#26377;&#21516;&#26679;&#30340;&#20260;&#23475;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#20351;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#24378;&#21270;&#12289;&#36829;&#21453;&#21644;&#20013;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#20294;&#23545;&#35748;&#30693;&#20449;&#24565;&#12289;&#24577;&#24230;&#25110;&#34892;&#20026;&#30340;&#25913;&#21464;&#24456;&#23567;&#12290;&#36825;&#31181;&#20307;&#39564;&#19978;&#30340;&#20260;&#23475;&#23545;&#22899;&#24615;&#24433;&#21709;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#23545;&#30007;&#24615;&#38451;&#21018;&#24615;&#30340;&#23041;&#32961;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conc
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>Roq&#26159;&#19968;&#20010;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.15210</link><description>&lt;p&gt;
Roq&#65306;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#40065;&#26834;&#26597;&#35810;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model. (arXiv:2401.15210v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15210
&lt;/p&gt;
&lt;p&gt;
Roq&#26159;&#19968;&#20010;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;(RDBMS)&#20013;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#25628;&#32034;&#39044;&#26399;&#23545;&#20110;&#32473;&#23450;&#26597;&#35810;&#26368;&#20248;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#23427;&#20204;&#20351;&#29992;&#21442;&#25968;&#20272;&#35745;&#65292;&#36890;&#24120;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20570;&#20986;&#30340;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#20272;&#35745;&#21644;&#20551;&#35774;&#26080;&#25928;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#36873;&#25321;&#22312;&#36816;&#34892;&#26102;&#26159;&#27425;&#20248;&#30340;&#25191;&#34892;&#35745;&#21010;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26597;&#35810;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26597;&#35810;&#20248;&#21270;&#22120;&#19981;&#36275;&#20197;&#25903;&#25345;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#26469;&#25552;&#39640;&#25968;&#25454;&#31995;&#32479;&#30340;&#25928;&#29575;&#24182;&#20943;&#23569;&#20854;&#32500;&#25252;&#24320;&#38144;&#30340;&#20852;&#36259;&#26085;&#30410;&#39640;&#28072;&#65292;&#22312;&#26597;&#35810;&#20248;&#21270;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#22522;&#20110;IBM Db2&#22810;&#24180;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Roq: &#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query optimizers in relational database management systems (RDBMSs) search for execution plans expected to be optimal for a given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select execution plans that are suboptimal at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not sufficiently support robust query optimization. Recent years have seen a surge of interest in using machine learning (ML) to improve efficiency of data systems and reduce their maintenance overheads, with promising results obtained in the area of query optimization in particular. In this paper, inspired by these advancements, and based on several years of experience of IBM Db2 in this journey, we propose Robust Optimization of Queries, (Roq), a holistic framework that enables robust query optimization based on a risk-aware learning approach. Roq 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#65292;&#33021;&#22815;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#24182;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.07745</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#21644;&#23454;&#36341;&#30340;&#36830;&#32493;POMDP&#35268;&#21010;&#20013;&#31616;&#21270;&#22797;&#26434;&#30340;&#35266;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#65292;&#33021;&#22815;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#24182;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#65288;&#22914;&#30456;&#26426;&#22270;&#20687;&#65289;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#35268;&#21010;&#38382;&#39064;&#26159;&#24517;&#38656;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;&#20316;&#20026;&#35266;&#27979;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30446;&#21069;&#22312;&#32447;&#37096;&#32626;&#26102;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35268;&#21010;&#20013;&#20351;&#29992;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#24418;&#24335;&#21270;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#22522;&#20110;&#31616;&#21270;&#27169;&#22411;&#30340;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25512;&#24191;&#26368;&#36817;&#30340;&#31890;&#23376;&#32622;&#20449;&#24230;MDP&#38598;&#20013;&#30028;&#38480;&#30340;&#32467;&#26524;&#65292;&#23427;&#23558;&#29702;&#35770;POMDP&#20540;&#19982;&#31616;&#21270;&#27169;&#22411;&#19979;&#30340;&#23454;&#38469;&#35268;&#21010;&#20540;&#36827;&#34892;&#20102;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#21487;&#20197;&#20998;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#37096;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
Solving partially observable Markov decision processes (POMDPs) with high dimensional and continuous observations, such as camera images, is required for many real life robotics and planning problems. Recent researches suggested machine learned probabilistic models as observation models, but their use is currently too computationally expensive for online deployment. We deal with the question of what would be the implication of using simplified observation models for planning, while retaining formal guarantees on the quality of the solution. Our main contribution is a novel probabilistic bound based on a statistical total variation distance of the simplified model. We show that it bounds the theoretical POMDP value w.r.t. original model, from the empirical planned value with the simplified model, by generalizing recent results of particle-belief MDP concentration bounds. Our calculations can be separated into offline and online parts, and we arrive at formal guarantees without having to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17048</link><description>&lt;p&gt;
&#20851;&#20110;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#36830;&#32493;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26159;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#19981;&#21463;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#23545;&#25239;&#26679;&#26412;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#36825;&#31181;&#25913;&#36827;&#26159;&#20197;&#33258;&#28982;&#26679;&#26412;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#20154;&#25552;&#20986;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#35266;&#28857;&#65292;&#20551;&#35774;&#30340;&#36830;&#32493;&#24615;&#19982;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36830;&#32493;&#20989;&#25968;&#19981;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#31283;&#20581;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#35856;&#27874;&#21644;ho&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14587</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20855;&#26377;&#30072;&#21464;&#29575;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20934;&#30830;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24726;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24212;&#35813;&#36890;&#36807;&#35757;&#32451;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#32780;&#19981;&#26159;&#30001;&#32593;&#32476;&#32422;&#26463;&#25152;&#20915;&#23450;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#30001;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#24182;&#20998;&#26512;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#25968;&#25454;&#21644;&#30072;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#20808;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
&lt;/p&gt;</description></item><item><title>&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#65292;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#65292;&#24182;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;</title><link>http://arxiv.org/abs/2309.12655</link><description>&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural revision is contingently-conditionalized revision. (arXiv:2309.12655v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12655
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#26159;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;&#20462;&#27491;&#26041;&#24335;&#65292;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#65292;&#24182;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#20462;&#27491;&#30475;&#36215;&#26469;&#24456;&#33258;&#28982;&#65306;&#23427;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#26469;&#34701;&#20837;&#26032;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#21453;&#20363;&#26174;&#31034;&#36825;&#26159;&#38169;&#35823;&#30340;&#12290;&#23427;&#38750;&#24120;&#20445;&#23432;&#65292;&#20174;&#19981;&#23436;&#20840;&#30456;&#20449;&#12290;&#23427;&#21482;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#30456;&#20449;&#12290;&#36825;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#27491;&#30830;&#30340;&#65292;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#26159;&#38169;&#35823;&#30340;&#12290;&#21738;&#31181;&#24773;&#20917;&#26159;&#21738;&#31181;&#24773;&#20917;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#38656;&#35201;&#23558;&#33258;&#28982;&#20462;&#27491;&#20174;&#34920;&#36798;&#26222;&#36941;&#30495;&#29702;&#30340;&#31616;&#21333;&#20844;&#24335;&#65288;&#26576;&#29289;&#25104;&#31435;&#65289;&#25193;&#23637;&#21040;&#34920;&#36798;&#26465;&#20214;&#30495;&#29702;&#30340;&#26465;&#20214;&#35821;&#21477;&#65288;&#26576;&#31181;&#24773;&#20917;&#19979;&#25104;&#31435;&#65289;&#12290;&#36825;&#31181;&#25193;&#23637;&#22522;&#20110;&#33258;&#28982;&#20462;&#27491;&#36981;&#24490;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#34987;&#30830;&#23450;&#20026;&#26368;&#23567;&#25913;&#21464;&#12289;&#28448;&#19981;&#20851;&#24515;&#21644;&#22825;&#30495;&#65306;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#20449;&#24565;&#65307;&#40664;&#35748;&#24773;&#20917;&#19979;&#23558;&#22330;&#26223;&#30340;&#21487;&#33021;&#24615;&#35270;&#20026;&#30456;&#31561;&#65307;&#23545;&#25152;&#26377;&#24773;&#20917;&#25345;&#26377;&#20449;&#24565;&#65292;&#30452;&#21040;&#26377;&#30683;&#30462;&#21457;&#29983;&#12290;&#25193;&#23637;&#34920;&#26126;&#33258;&#28982;&#20462;&#27491;&#23558;&#20462;&#27491;&#38480;&#21046;&#22312;&#24403;&#21069;&#26465;&#20214;&#19979;&#12290;&#19982;&#19981;&#21463;&#38480;&#21046;&#30340;&#20462;&#27491;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#30830;&#23450;&#24403;&#21069;&#26465;&#20214;&#12290;&#23427;&#19981;&#26159;&#24403;&#21069;&#34987;&#35748;&#20026;&#26159;&#30495;&#30340;&#65292;&#22914;&#26524;&#19982;&#26032;&#20449;&#24687;&#30456;&#30683;&#30462;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural revision seems so natural: it changes beliefs as little as possible to incorporate new information. Yet, some counterexamples show it wrong. It is so conservative that it never fully believes. It only believes in the current conditions. This is right in some cases and wrong in others. Which is which? The answer requires extending natural revision from simple formulae expressing universal truths (something holds) to conditionals expressing conditional truth (something holds in certain conditions). The extension is based on the basic principles natural revision follows, identified as minimal change, indifference and naivety: change beliefs as little as possible; equate the likeliness of scenarios by default; believe all until contradicted. The extension says that natural revision restricts changes to the current conditions. A comparison with an unrestricting revision shows what exactly the current conditions are. It is not what currently considered true if it contradicts the new 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.03195</link><description>&lt;p&gt;
&#20154;&#25165;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31454;&#20105;&#28608;&#28872;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#21830;&#19994;&#29615;&#22659;&#19979;&#65292;&#32452;&#32455;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20197;&#37327;&#21270;&#26041;&#24335;&#20570;&#20986;&#20154;&#25165;&#30456;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#12290;&#22823;&#35268;&#27169;&#20154;&#25165;&#21644;&#31649;&#29702;&#30456;&#20851;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;&#20225;&#19994;&#39046;&#23548;&#32773;&#25552;&#20379;&#20102;&#20174;&#25968;&#25454;&#31185;&#23398;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#30340;&#26080;&#19982;&#20262;&#27604;&#26426;&#20250;&#65292;&#36827;&#32780;&#20026;&#23454;&#26102;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20154;&#25165;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#22312;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#24182;&#28608;&#21457;&#20102;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#12289;&#20840;&#38754;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;</title><link>http://arxiv.org/abs/2305.02251</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65306;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. (arXiv:2305.02251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21644;&#31526;&#21495;&#22238;&#24402;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#21644;&#20195;&#29702;&#12290;&#20174;&#8220;&#23439;&#35266;&#8221;&#21644;&#19978;&#19979;&#25991;&#35282;&#24230;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#20063;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35282;&#33394;&#65292;&#24110;&#21161;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#65292;&#20174;Adam&#31995;&#32479;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#21040;&#24403;&#21069;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#30340;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#33258;&#20027;&#24615;&#65292;&#24182;&#20197;&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#20027;&#32423;&#21035;&#20026;&#31867;&#27604;&#12290;&#26368;&#22823;&#32423;&#21035;&#65292;&#31532;&#20116;&#32423;&#65292;&#23450;&#20041;&#20026;&#22312;&#29983;&#20135;&#31185;&#23398;&#30693;&#35782;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#23454;&#29616;&#36825;&#19968;&#28857;&#26159;&#36808;&#21521;&#35299;&#20915;Nobel Turing Grand Challenge&#30340;&#19968;&#27493;&#65306;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#31185;&#23398;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478; - &#33021;&#21147;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality sc
&lt;/p&gt;</description></item></channel></rss>