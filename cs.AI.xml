<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05839</link><description>&lt;p&gt;
&#38271;&#26399;&#24103;&#20107;&#20214;&#35270;&#35273;&#36319;&#36394;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#19982;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#20107;&#20214;/&#24103;&#20107;&#20214;&#30340;&#36319;&#36394;&#22120;&#22312;&#30701;&#26399;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36319;&#36394;&#28041;&#21450;&#38271;&#26399;&#36319;&#36394;&#65292;&#29616;&#26377;&#36319;&#36394;&#31639;&#27861;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;FELT&#12290;&#23427;&#21253;&#21547;742&#20010;&#35270;&#39057;&#21644;1,594,474&#20010;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#23545;&#65292;&#24182;&#24050;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24103;&#20107;&#20214;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#20379;&#26410;&#26469;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#30001;&#20110;&#25361;&#25112;&#22240;&#32032;&#30340;&#24433;&#21709;&#21644;&#31354;&#38388;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#32780;&#33258;&#28982;&#19981;&#23436;&#25972;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#20316;&#20026;&#32479;&#19968;&#39592;&#24178;&#65292;&#36890;&#36807;&#23558;&#29616;&#20195;Hopfield&#23618;&#24341;&#20837;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05839v1 Announce Type: cross  Abstract: Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to
&lt;/p&gt;</description></item><item><title>Hulk&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;</title><link>https://arxiv.org/abs/2312.01697</link><description>&lt;p&gt;
Hulk: &#19968;&#31181;&#38754;&#21521;&#20154;&#31867;&#20013;&#24515;&#20219;&#21153;&#30340;&#36890;&#29992;&#30693;&#35782;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hulk: A Universal Knowledge Translator for Human-Centric Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01697
&lt;/p&gt;
&lt;p&gt;
Hulk&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#24863;&#30693;&#20219;&#21153;&#65292;&#20363;&#22914;&#34892;&#20154;&#26816;&#27979;&#12289;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#23039;&#24577;&#20272;&#35745;&#65292;&#22312;&#35832;&#22914;&#20803;&#23431;&#23449;&#21644;&#20307;&#32946;&#20998;&#26512;&#31561;&#24191;&#27867;&#30340;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#26469;&#65292;&#20986;&#29616;&#20102;&#21457;&#23637;&#26088;&#22312;&#21463;&#30410;&#20110;&#24191;&#27867;&#20154;&#31867;&#20013;&#24515;&#24863;&#30693;&#20219;&#21153;&#30340;&#20154;&#31867;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#34429;&#28982;&#35768;&#22810;&#20154;&#31867;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#25506;&#32034;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#21450;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#30340;3D&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22810;&#19979;&#28216;&#20219;&#21153;&#21644;&#24773;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hulk&#65292;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#21508;&#31181;&#20219;&#21153;&#29305;&#23450;&#22836;&#37096;&#21387;&#32553;&#25104;&#20004;&#20010;&#36890;&#29992;&#22836;&#37096;&#65292;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#34920;&#31034;&#65292;&#22914;&#35821;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01697v4 Announce Type: replace-cross  Abstract: Human-centric perception tasks, e.g., pedestrian detection, skeleton-based action recognition, and pose estimation, have wide industrial applications, such as metaverse and sports analysis. There is a recent surge to develop human-centric foundation models that can benefit a broad range of human-centric perception tasks. While many human-centric foundation models have achieved success, they did not explore 3D and vision-language tasks for human-centric and required task-specific finetuning. These limitations restrict their application to more downstream tasks and situations. To tackle these problems, we present Hulk, the first multimodal human-centric generalist model, capable of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks without task-specific finetuning. The key to achieving this is condensing various task-specific heads into two general heads, one for discrete representations, e.g., languages, 
&lt;/p&gt;</description></item></channel></rss>