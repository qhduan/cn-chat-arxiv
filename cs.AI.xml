<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;p&gt;
&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23041;&#32961;&#12289;&#28431;&#27934;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26684;&#23616;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#22788;&#29702;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23427;&#20204;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#29992;&#24615;&#22806;&#65292;LLMs&#36824;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#32771;&#34385;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65292;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;LLMs&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20174;&#20116;&#20010;&#20027;&#39064;&#35282;&#24230;&#36827;&#34892;&#65306;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12289;LLMs&#35823;&#29992;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#21361;&#23475;&#12289;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#35782;&#21035;&#24403;&#21069;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.01343</link><description>&lt;p&gt;
IoTGeM: &#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#32593;&#32476;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#25915;&#20987;&#26816;&#27979;&#30740;&#31350;&#65292;&#25152;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24314;&#27169;&#29289;&#32852;&#32593;&#32593;&#32476;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#27169;&#22411;&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24120;&#35265;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#20986;&#25903;&#25745;&#25915;&#20987;&#20934;&#30830;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06671</link><description>&lt;p&gt;
&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#24179;&#34913;&#27861;&#21017;&#19982;&#31283;&#24577;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26159;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;SGD&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21644;&#36864;&#21270;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#19968;&#20010;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#30001;&#20110;&#31616;&#21333;&#25193;&#25955;&#36807;&#31243;&#21644;SGD&#21160;&#21147;&#23398;&#30340;&#24046;&#24322;&#22312;&#23545;&#31216;&#24615;&#23384;&#22312;&#26102;&#26368;&#37325;&#35201;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#26159;&#20102;&#35299;SGD&#24037;&#20316;&#26041;&#24335;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#24212;&#29992;&#20110;&#23548;&#20986;&#20855;&#26377;&#20219;&#24847;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#31283;&#24577;&#20998;&#24067;&#23637;&#29616;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#22914;&#30456;&#21464;&#12289;&#30772;&#22351;&#30340;&#36941;&#21382;&#24615;&#21644;&#27874;&#21160;&#21453;&#36716;&#12290;&#36825;&#20123;&#29616;&#35937;&#20165;&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#23384;&#22312;&#65292;&#34920;&#26126;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#26032;&#30340;&#21152;&#28145;&#35757;&#32451;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
&lt;/p&gt;</description></item></channel></rss>