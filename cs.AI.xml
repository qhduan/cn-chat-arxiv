<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#20197;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05260</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#29305;&#24449;&#36827;&#34892;&#23545;&#25239;&#24615;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#20197;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;&#27979;&#24207;&#25216;&#26415;&#30340;&#21457;&#23637;&#25512;&#21160;&#20102;&#22823;&#37327;&#21333;&#32454;&#32990;&#36716;&#24405;&#35889;&#30340;&#29983;&#25104;&#65292;&#20026;&#25506;&#32034;&#32959;&#30244;&#20013;&#32784;&#33647;&#32454;&#32990;&#20122;&#32676;&#25552;&#20379;&#20102;&#23453;&#36149;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#21333;&#32454;&#32990;&#27700;&#24179;&#30340;&#33647;&#29289;&#25935;&#24863;&#24615;&#25968;&#25454;&#20173;&#28982;&#31232;&#32570;&#65292;&#36843;&#20999;&#38656;&#35201;&#23545;&#20010;&#20307;&#32454;&#32990;&#30340;&#33647;&#29289;&#25935;&#24863;&#24615;&#36827;&#34892;&#35745;&#31639;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#39033;&#32039;&#36843;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#19982;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20135;&#29983;&#37325;&#35201;&#24615;&#24863;&#30693;&#21644;&#30456;&#20114;&#29420;&#31435;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#22312;&#28304;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#32500;&#24230;&#32423;&#21035;&#23884;&#20837;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05260v1 Announce Type: new  Abstract: The development of single-cell sequencing technology had promoted the generation of a large amount of single-cell transcriptional profiles, providing valuable opportunities to explore drug-resistant cell subpopulations in a tumor. However, the drug sensitivity data in single-cell level is still scarce to date, pressing an urgent and highly challenging task for computational prediction of the drug sensitivity to individual cells. This paper proposed scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug sensitivity. We used an autoencoder to extract domain-invariant features related to drug sensitivity from multiple source domains by exploiting adversarial domain adaptation. Especially, we introduced an adaptive weight generator to produce importance-aware and mutual independent weights, which could adaptively modulate the embedding of each sample in dimension-level for both source and target domains. Extensive exp
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.00546</link><description>&lt;p&gt;
AllSpark: &#19968;&#20010;&#20855;&#26377;&#21313;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#31354;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#39640;&#24230;&#24322;&#36136;&#24615;&#65292;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#32852;&#21512;&#35299;&#37322;&#19968;&#30452;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#32780;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#24179;&#34913;&#34920;&#29616;&#20986;&#36880;&#28176;&#38750;&#32447;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#20316;&#20026;&#21442;&#32771;&#26694;&#26550;&#65288;LaRF&#65289;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#32479;&#19968;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21462;&#24471;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21253;&#25324;1D&#65288;&#25991;&#26412;&#65292;&#20195;&#30721;&#65289;&#65292;2D&#65288;RGB&#65292;&#32418;&#22806;&#32447;&#65292;SAR&#65292;&#22810;&#20809;&#35889;&#65292;&#39640;&#20809;&#35889;&#65292;&#34920;&#26684;&#65292;&#22270;&#34920;&#65292;&#36712;&#36857;&#65292;&#26012;&#35282;&#25668;&#24433;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06167</link><description>&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predictable Artificial Intelligence. (arXiv:2310.06167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06167
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#24182;&#24378;&#35843;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#25552;&#39640;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#24605;&#24819;&#21644;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#25506;&#32034;&#22914;&#20309;&#39044;&#27979;&#29616;&#26377;&#21644;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#20851;&#38190;&#25351;&#26631;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23454;&#29616;&#21487;&#39044;&#27979;&#24615;&#23545;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#20449;&#20219;&#12289;&#36131;&#20219;&#12289;&#25511;&#21046;&#12289;&#21327;&#35843;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#24212;&#20248;&#20808;&#32771;&#34385;&#32780;&#38750;&#24615;&#33021;&#12290;&#23613;&#31649;&#19982;&#20854;&#20182;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#19982;&#21487;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#38382;&#39064;&#12289;&#20551;&#35774;&#21644;&#25361;&#25112;&#23578;&#26410;&#34987;&#28165;&#26970;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#36825;&#20123;&#38382;&#39064;&#65292;&#21628;&#21505;&#25214;&#21040;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#21487;&#39044;&#27979;&#24615;&#30340;&#36335;&#24452;&#65292;&#24182;&#27010;&#36848;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the fundamental ideas and challenges of Predictable AI, a nascent research area that explores the ways in which we can anticipate key indicators of present and future AI ecosystems. We argue that achieving predictability is crucial for fostering trust, liability, control, alignment and safety of AI ecosystems, and thus should be prioritised over performance. While distinctive from other areas of technical and non-technical AI research, the questions, hypotheses and challenges relevant to Predictable AI were yet to be clearly described. This paper aims to elucidate them, calls for identifying paths towards AI predictability and outlines the potential impact of this emergent field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04195</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#32531;&#35299;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#23569;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#65306;&#30001;&#29305;&#23450;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#35757;&#32451;&#32593;&#32476;&#65289;&#21512;&#25104;&#30340;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#20182;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#27979;&#35797;&#32593;&#32476;&#65289;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26550;&#26500;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20849;&#21516;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#33976;&#39311;&#25968;&#25454;&#28041;&#21450;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#23481;&#37327;&#26356;&#22823;&#30340;&#32593;&#32476;&#23545;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
&lt;/p&gt;</description></item></channel></rss>