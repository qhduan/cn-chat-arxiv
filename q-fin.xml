<rss version="2.0"><channel><title>Chat Arxiv q-fin</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-fin</description><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.06031</link><description>&lt;p&gt;
FinGPT&#65306;&#24320;&#28304;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06031
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#26159;&#37329;&#34701;LLMs&#65288;FinLLMs&#65289;&#30340;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;FinGPT&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FinGPT&#37319;&#29992;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#20182;&#20204;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#24378;&#35843;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#24314;&#31435;FinGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#20316;&#20026;&#29992;&#25143;&#30340;&#22522;&#30784;&#65292;&#22914;&#26426;&#22120;&#39038;&#38382;&#12289;&#31639;&#27861;&#20132;&#26131;&#21644;&#35770; &#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#36229;&#32423;&#22823;&#22269;&#20855;&#26377;&#24322;&#36136;&#20559;&#22909;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32852;&#30431;&#30340;&#24418;&#25104;&#26469;&#24433;&#21709;&#36229;&#32423;&#22823;&#22269;&#30340;&#31454;&#20105;&#65292;&#24182;&#25581;&#31034;&#20102;&#38750;&#36229;&#32423;&#22823;&#22269;&#23545;&#36229;&#32423;&#22823;&#22269;&#20855;&#26377;&#19968;&#23450;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.10206</link><description>&lt;p&gt;
&#38750;&#36229;&#32423;&#22823;&#22269;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Non-Superpowers. (arXiv:2209.10206v3 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10206
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#22312;&#38750;&#36229;&#32423;&#22823;&#22269;&#20855;&#26377;&#24322;&#36136;&#20559;&#22909;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32852;&#30431;&#30340;&#24418;&#25104;&#26469;&#24433;&#21709;&#36229;&#32423;&#22823;&#22269;&#30340;&#31454;&#20105;&#65292;&#24182;&#25581;&#31034;&#20102;&#38750;&#36229;&#32423;&#22823;&#22269;&#23545;&#36229;&#32423;&#22823;&#22269;&#20855;&#26377;&#19968;&#23450;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#38750;&#36229;&#32423;&#22823;&#22269;&#22312;&#20855;&#26377;&#24322;&#36136;&#20559;&#22909;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#22609;&#36896;&#36229;&#32423;&#22823;&#22269;&#22312;&#21183;&#21147;&#33539;&#22260;&#20869;&#30340;&#31454;&#20105;&#12290;&#20004;&#20010;&#36229;&#32423;&#22823;&#22269;&#36827;&#34892;&#25552;&#20379;&#20465;&#20048;&#37096;&#20844;&#20849;&#21697;&#30340;&#26031;&#22612;&#20811;&#23572;&#20271;&#26684;&#21338;&#24328;&#65292;&#32780;&#38750;&#36229;&#32423;&#22823;&#22269;&#21017;&#36890;&#36807;&#24418;&#25104;&#32852;&#30431;&#26469;&#21152;&#20837;&#20465;&#20048;&#37096;&#24182;&#32771;&#34385;&#22806;&#37096;&#24615;&#30340;&#23384;&#22312;&#12290;&#32852;&#30431;&#30340;&#24418;&#25104;&#21462;&#20915;&#20110;&#38750;&#36229;&#32423;&#22823;&#22269;&#30340;&#29305;&#24449;&#65292;&#24433;&#21709;&#30528;&#36229;&#32423;&#22823;&#22269;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#24433;&#21709;&#20465;&#20048;&#37096;&#30340;&#35268;&#27169;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35828;&#65292;&#38750;&#36229;&#32423;&#22823;&#22269;&#23545;&#36229;&#32423;&#22823;&#22269;&#25317;&#26377;&#19968;&#23450;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23376;&#21338;&#24328;&#23436;&#32654;&#32435;&#20160;&#22343;&#34913;&#65292;&#24182;&#27169;&#25311;&#20102;&#28216;&#25103;&#65292;&#20197;&#25551;&#32472;&#32654;&#22269;&#21644;&#20013;&#22269;&#22914;&#20309;&#26681;&#25454;&#20854;&#20182;&#22269;&#23478;&#26469;&#24418;&#25104;&#20182;&#20204;&#30340;&#20465;&#20048;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a game-theoretic model to investigate how non-superpowers with heterogenous preferences and endowments shape the superpower competition for a sphere of influence. Two superpowers play a Stackelberg game of providing club goods while non-superpowers form coalitions to join a club in the presence of externalities. The coalition formation, which depends on the characteristics of non-superpowers, influences the behavior of superpowers and thus the club size. In this sense, non-superpowers have a power over superpowers. We study the subgame perfect Nash equilibrium and simulate the game to characterize how the US and China form their clubs depending on other countries.
&lt;/p&gt;</description></item></channel></rss>