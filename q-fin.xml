<rss version="2.0"><channel><title>Chat Arxiv q-fin</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-fin</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37325;&#22797;&#30456;&#21516;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#26356;&#20542;&#21521;&#20110;&#36981;&#24490;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#22312;&#26368;&#21518;&#19968;&#32452;&#20915;&#31574;&#20013;&#26377;&#26356;&#22810;&#20010;&#20307;&#34920;&#29616;&#20026;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.16538</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#65288;&#39044;&#26399;&#65289;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize (Expected) Utility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37325;&#22797;&#30456;&#21516;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#26356;&#20542;&#21521;&#20110;&#36981;&#24490;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#22312;&#26368;&#21518;&#19968;&#32452;&#20915;&#31574;&#20013;&#26377;&#26356;&#22810;&#20010;&#20307;&#34920;&#29616;&#20026;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#23454;&#39564;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#22312;&#37325;&#22797;&#20174;&#30456;&#21516;&#33756;&#21333;&#20013;&#20570;&#20986;&#20915;&#31574;&#19988;&#27809;&#26377;&#25509;&#25910;&#20219;&#20309;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20250;&#34920;&#29616;&#20986;&#19982;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#39044;&#27979;&#26356;&#21152;&#25509;&#36817;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#39033;&#38750;&#24378;&#21046;&#36873;&#25321;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37329;&#38065;&#24425;&#31080;&#65292;&#24182;&#27599;&#20010;&#33756;&#21333;&#37325;&#22797;&#20116;&#27425;&#65292;&#26088;&#22312;&#20174;&#22810;&#20010;&#34892;&#20026;&#35282;&#24230;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#25105;&#20204;&#20174;&#33521;&#22269;&#21644;&#24503;&#22269;&#30340;308&#21517;&#21463;&#35797;&#32773;&#20013;&#30340;&#25968;&#25454;&#20013;&#65292;&#26174;&#33879;&#26356;&#22810;&#30340;&#20010;&#20307;&#22312;&#20182;&#20204;&#26368;&#21518;15&#20010;&#30456;&#21516;&#20915;&#31574;&#38382;&#39064;&#20013;&#26159;&#24207;&#25968;&#25928;&#29992;&#21644;&#26399;&#26395;&#25928;&#29992;&#30340;&#26368;&#22823;&#21270;&#32773;&#65292;&#32780;&#19981;&#26159;&#22312;&#31532;&#19968;&#20010;15&#20010;&#20013;&#12290;&#27492;&#22806;&#65292;&#22823;&#32422;&#22235;&#20998;&#20043;&#19968;&#21644;&#20116;&#20998;&#20043;&#19968;&#30340;&#25152;&#26377;&#21463;&#35797;&#32773;&#65292;&#22312;&#23454;&#39564;&#20013;&#37117;&#20197;&#36825;&#20123;&#27169;&#24335;&#20570;&#20915;&#31574;&#65292;&#20960;&#20046;&#19968;&#21322;&#26174;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#19981;&#21516;&#12290;&#22312;&#37027;&#20123;&#22987;&#32456;&#29702;&#24615;&#30340;&#20010;&#20307;&#19982;&#28385;&#36275;&#38543;&#26426;&#25928;&#29992;&#29702;&#35770;&#26680;&#24515;&#21407;&#21017;&#30340;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16538v1 Announce Type: new  Abstract: We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Fi
&lt;/p&gt;</description></item></channel></rss>