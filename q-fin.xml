<rss version="2.0"><channel><title>Chat Arxiv q-fin</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-fin</description><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#21253;&#25324;&#24066;&#22330;&#20914;&#20987;&#21644;&#21442;&#25968;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;PPO&#21644;A2C&#22312;&#22788;&#29702;&#22122;&#22768;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#31163;&#31574;&#30053;&#31639;&#27861;DDPG&#12289;TD3&#21644;SAC&#21017;&#25928;&#26524;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.07694</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation. (arXiv:2307.07694v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#21253;&#25324;&#24066;&#22330;&#20914;&#20987;&#21644;&#21442;&#25968;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;PPO&#21644;A2C&#22312;&#22788;&#29702;&#22122;&#22768;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#31163;&#31574;&#30053;&#31639;&#27861;DDPG&#12289;TD3&#21644;SAC&#21017;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#27169;&#25311;&#22120;&#20316;&#20026;&#35780;&#20272;&#20381;&#25454;&#12290;&#35813;&#27169;&#25311;&#22120;&#22522;&#20110;&#30456;&#20851;&#20960;&#20309;&#24067;&#26391;&#36816;&#21160;&#65288;GBM&#65289;&#19982;Bertsimas-Lo&#65288;BL&#65289;&#24066;&#22330;&#20914;&#20987;&#27169;&#22411;&#12290;&#20351;&#29992;&#20975;&#21033;&#20934;&#21017;&#65288;&#23545;&#25968;&#25928;&#29992;&#65289;&#20316;&#20026;&#30446;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#27809;&#26377;&#24066;&#22330;&#20914;&#20987;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#20998;&#26512;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21253;&#25324;&#24066;&#22330;&#20914;&#20987;&#26102;&#24615;&#33021;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#31574;&#30053;&#31639;&#27861;DDPG&#12289;TD3&#21644;SAC&#30001;&#20110;&#22122;&#22768;&#22870;&#21169;&#30340;&#23384;&#22312;&#26080;&#27861;&#23398;&#20064;&#21040;&#27491;&#30830;&#30340;Q&#20989;&#25968;&#65292;&#22240;&#27492;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;PPO&#21644;A2C&#65292;&#22312;&#24191;&#20041;&#20248;&#21183;&#20272;&#35745;&#65288;GAE&#65289;&#30340;&#20351;&#29992;&#19979;&#33021;&#22815;&#24212;&#23545;&#22122;&#22768;&#24182;&#24471;&#20986;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;PPO&#30340;&#21098;&#20999;&#21464;&#20307;&#22312;&#38450;&#27490;&#31574;&#30053;&#22312;&#25910;&#25947;&#21518;&#20559;&#31163;&#26368;&#20248;&#20540;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;GBM&#21442;&#25968;&#21457;&#29983;&#21046;&#24230;&#24615;&#21464;&#21270;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#12289;TD3&#21644;SAC&#31639;&#27861;&#20173;&#33021;&#20445;&#25345;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate benchmark deep reinforcement learning (DRL) algorithms on the task of portfolio optimisation under a simulator. The simulator is based on correlated geometric Brownian motion (GBM) with the Bertsimas-Lo (BL) market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact and use it as an upper bound to measure performance when including market impact. We found that the off-policy algorithms DDPG, TD3 and SAC were unable to learn the right Q function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation (GAE), were able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we found that PPO,
&lt;/p&gt;</description></item></channel></rss>