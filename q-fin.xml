<rss version="2.0"><channel><title>Chat Arxiv q-fin</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for q-fin</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item></channel></rss>