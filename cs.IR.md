# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation.](http://arxiv.org/abs/2401.13870) | 本论文提出了一种将大型语言模型融入推荐系统的新框架，通过互补增强和自适应聚合，充分发挥它们各自的优势，以提升推荐性能。 |

# 详细

[^1]: 将大型语言模型融入推荐系统的互补增强和自适应聚合

    Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation. (arXiv:2401.13870v1 [cs.IR])

    [http://arxiv.org/abs/2401.13870](http://arxiv.org/abs/2401.13870)

    本论文提出了一种将大型语言模型融入推荐系统的新框架，通过互补增强和自适应聚合，充分发挥它们各自的优势，以提升推荐性能。

    

    传统的推荐方法通过利用用户行为中的协同或连续信息取得了显著的进展。最近，由于其在理解和推理文本语义方面的能力，大型语言模型（LLMs）在各个领域中得到了重视，并在推荐系统中发现了其实用性。传统的推荐方法和LLMs各自具有各自的优势和局限性。传统方法擅长挖掘协同信息和建模连续行为，但在数据稀疏和长尾问题方面存在困难。而LLMs则擅长利用丰富的文本上下文，但在挖掘协同或连续信息方面面临挑战。尽管它们各自取得了成功，但在利用它们的联合潜力来提升推荐性能方面存在着显著差距。在本文中，我们介绍了一个通用的、与模型无关的框架，称为大型语言模型互补增强和自适应聚合。

    Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance.  In this paper, we introduce a general and model-agnostic framework known as \textbf{L}arge \textbf{la}nguage
    

