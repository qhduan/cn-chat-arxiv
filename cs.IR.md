# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Sequential Recommendations with LLMs](https://rss.arxiv.org/abs/2402.01339) | 本研究探索了如何使用LLMs来改进序列推荐问题，并设计了三种正交方法和它们的混合形式来利用LLMs的能力。通过在大量实验和不同配置上的探索，我们发现通过初始化最先进的序列推荐模型可以实现性能改进。 |

# 详细

[^1]: 使用LLMs改进序列推荐

    Improving Sequential Recommendations with LLMs

    [https://rss.arxiv.org/abs/2402.01339](https://rss.arxiv.org/abs/2402.01339)

    本研究探索了如何使用LLMs来改进序列推荐问题，并设计了三种正交方法和它们的混合形式来利用LLMs的能力。通过在大量实验和不同配置上的探索，我们发现通过初始化最先进的序列推荐模型可以实现性能改进。

    

    过去几年，序列推荐问题引起了相当多的研究关注，导致了许多推荐模型的出现。在这项工作中，我们探讨了如何利用现今在许多基于人工智能的应用中引入了颠覆性影响的大型语言模型（LLMs）来构建或改进序列推荐方法。具体而言，我们设计了三种正交方法和它们的混合形式，以不同的方式利用LLMs的能力。此外，我们通过关注组成技术方面的潜力，并对每个方法确定一系列可行的替代选择，来研究每个方法的潜力。我们在三个数据集上进行了大量实验，并探索了各种配置，包括不同的语言模型和基准推荐模型，以获得每个方法的性能的综合图片。在其他观察中，我们强调通过初始化最先进的序列推荐模型可以实现的性能改进。

    The sequential recommendation problem has attracted considerable research attention in the past few years, leading to the rise of numerous recommendation models. In this work, we explore how Large Language Models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we design three orthogonal approaches and hybrids of those to leverage the power of LLMs in different ways. In addition, we investigate the potential of each approach by focusing on its comprising technical aspects and determining an array of alternative choices for each one. We conduct extensive experiments on three datasets and explore a large variety of configurations, including different language models and baseline recommendation models, to obtain a comprehensive picture of the performance of each approach. Among other observations, we highlight that initializing state-of-the-art sequential recommendat
    

