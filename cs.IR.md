# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks](https://arxiv.org/abs/2401.17723) | LoRec是一个针对顺序推荐系统的大规模语言模型（LLM），可以检测并识别未知的篡改攻击，提高了系统的鲁棒性。 |

# 详细

[^1]: LoRec: 针对篡改攻击的大规模语言模型用于鲁棒顺序推荐

    LoRec: Large Language Model for Robust Sequential Recommendation against Poisoning Attacks

    [https://arxiv.org/abs/2401.17723](https://arxiv.org/abs/2401.17723)

    LoRec是一个针对顺序推荐系统的大规模语言模型（LLM），可以检测并识别未知的篡改攻击，提高了系统的鲁棒性。

    

    顺序推荐系统以其捕捉用户动态兴趣和物品间转换模式的能力脱颖而出。然而，顺序推荐系统的固有开放性使其容易受到篡改攻击，即通过向训练数据中注入欺诈性用户来操纵学习模式。传统的防御策略主要依赖于预定的假设或从特定已知攻击中提取的规则，限制了它们对未知攻击类型的适用性。为了解决以上问题，考虑到大规模语言模型（LLMs）所囊括的丰富开放世界知识，我们的研究首先关注LLMs在检测推荐系统中未知欺诈活动方面的能力，我们将该策略称为LLM4Dec。经验评估展示了LLMs在识别未知欺诈者方面的巨大能力，利用其丰富的开放世界知识。在此基础上，我们提出了

    Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item-to-item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudulent users are injected into the training data to manipulate learned patterns. Traditional defense strategies predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attack types. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), our research initially focuses on the capabilities of LLMs in the detection of unknown fraudulent activities within recommender systems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the substantial capability of LLMs in identifying unknown fraudsters, leveraging their expansive, open-world knowledge.   Building upon this, we propose 
    

