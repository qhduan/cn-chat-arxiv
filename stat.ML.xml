<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.16829</link><description>&lt;p&gt;
&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis of the derivative-free loss method for solving PDEs. (arXiv:2309.16829v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#65292;&#21516;&#26102;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#22312;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#19968;&#31867;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26080;&#23548;&#25968;&#25439;&#22833;&#26041;&#27861;&#37319;&#29992;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#65292;&#32467;&#21512;&#38543;&#26426;&#34892;&#36208;&#32773;&#21450;&#20854;&#23545;&#24212;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#36153;&#26364;-&#21345;&#20811;&#20844;&#24335;&#20013;&#19982;&#26102;&#38388;&#38388;&#38548;&#30456;&#20851;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#34892;&#36208;&#32773;&#22823;&#23567;&#23545;&#35745;&#31639;&#25928;&#29575;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#37319;&#26679;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35757;&#32451;&#25439;&#22833;&#20559;&#24046;&#19982;&#26102;&#38388;&#38388;&#38548;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#38388;&#26799;&#24230;&#25104;&#27491;&#27604;&#65292;&#19982;&#34892;&#36208;&#32773;&#22823;&#23567;&#25104;&#21453;&#27604;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#26102;&#38388;&#38388;&#38548;&#24517;&#39035;&#36275;&#22815;&#38271;&#25165;&#33021;&#35757;&#32451;&#32593;&#32476;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#35828;&#26126;&#65292;&#22312;&#26102;&#38388;&#38388;&#38548;&#30340;&#26368;&#20248;&#19979;&#30028;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#23613;&#21487;&#33021;&#23567;&#30340;&#34892;&#36208;&#32773;&#22823;&#23567;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#20998;&#26512;&#30340;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
&lt;/p&gt;</description></item></channel></rss>