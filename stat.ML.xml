<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07391</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#21487;&#22797;&#21046;&#24615;&#28176;&#36827;&#33258;&#30001;
&lt;/p&gt;
&lt;p&gt;
Replicability is Asymptotically Free in Multi-armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#38656;&#27714;&#30340;&#25512;&#21160;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30830;&#20445;&#31639;&#27861;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#21463;&#25968;&#25454;&#38598;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#31639;&#27861;&#25152;&#38656;&#30340;&#36951;&#25022;&#20540;&#27604;&#19981;&#21487;&#22797;&#21046;&#31639;&#27861;&#22810;$O(1/\rho^2)$&#20493;&#65292;&#20854;&#20013;$\rho$&#26159;&#38750;&#22797;&#21046;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32473;&#23450;&#30340;$\rho$&#19979;&#26102;&#38388;&#30028;$T$&#36275;&#22815;&#22823;&#26102;&#65292;&#27492;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#21069;&#25552;&#26159;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#25506;&#32034;&#21518;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#22312;&#20915;&#31574;&#20043;&#21069;&#22343;&#21248;&#36873;&#25321;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#38454;&#27573;&#32467;&#26463;&#26102;&#28120;&#27760;&#27425;&#20248;&#21160;&#20316;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#20915;&#31574;&#21046;&#23450;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06293</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting of Irregular Time Series via Conditional Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#22825;&#25991;&#23398;&#21644;&#27668;&#20505;&#23398;&#12290;&#30446;&#21069;&#35813;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20165;&#20272;&#35745;&#21333;&#20010;&#36890;&#36947;&#21644;&#21333;&#20010;&#26102;&#38388;&#28857;&#19978;&#35266;&#27979;&#20540;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#24418;&#29366;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;ProFITi&#65292;&#29992;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#22312;&#36807;&#21435;&#35266;&#27979;&#21644;&#26597;&#35810;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#19978;&#26465;&#20214;&#19979;&#26102;&#38388;&#24207;&#21015;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#20316;&#20026;&#27169;&#22411;&#32452;&#20214;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#19968;&#20010;&#21487;&#36870;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#25972;&#20010;&#23454;&#25968;&#32447;&#19978;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. State-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. In this work, we propose a novel model, ProFITi, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. The model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. As model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. We conduct extensive experiments on four datasets and demonstrate that the proposed model pro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38750;&#35268;&#21017;&#31354;&#38388;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.02600</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#35268;&#21017;&#31354;&#38388;&#25968;&#25454;&#30340;&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Bayes Estimators for Irregular Spatial Data using Graph Neural Networks. (arXiv:2310.02600v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02600
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38750;&#35268;&#21017;&#31354;&#38388;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#26159;&#19968;&#31181;&#20197;&#24555;&#36895;&#21644;&#20813;&#20284;&#28982;&#26041;&#24335;&#36924;&#36817;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#22312;&#31354;&#38388;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#30340;&#20351;&#29992;&#38750;&#24120;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#20272;&#35745;&#32463;&#24120;&#26159;&#35745;&#31639;&#19978;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#31354;&#38388;&#24212;&#29992;&#20013;&#30340;&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#20165;&#38480;&#20110;&#22312;&#35268;&#21017;&#30340;&#32593;&#26684;&#19978;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#30446;&#21069;&#36824;&#20381;&#36182;&#20110;&#39044;&#20808;&#35268;&#23450;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#36825;&#24847;&#21619;&#30528;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#38598;&#65307;&#36825;&#20351;&#23427;&#20204;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#23454;&#29992;&#65292;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#20174;&#20219;&#24847;&#31354;&#38388;&#20301;&#32622;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#38500;&#20102;&#23558;&#31070;&#32463;&#36125;&#21494;&#26031;&#20272;&#35745;&#25193;&#23637;&#21040;&#38750;&#35268;&#21017;&#31354;&#38388;&#25968;&#25454;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#36824;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#22240;&#20026;&#35813;&#20272;&#35745;&#22120;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#25490;&#21015;&#25110;&#25968;&#37327;&#30340;&#20301;&#32622;&#21644;&#29420;&#31435;&#30340;&#37325;&#22797;&#23454;&#39564;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. They are appealing to use with spatial models and data, where estimation is often a computational bottleneck. However, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be re-trained for new data sets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks to tackle the important problem of parameter estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, our architecture leads to substantial computational benefits, since the estimator can be used with any arrangement or number of locations and independent repli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.02422</link><description>&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#36935;&#19978;&#31070;&#32463;&#32593;&#32476;&#65306;Radon-Kolmogorov-Smirnov&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#65288;MMD&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#26368;&#22823;&#21270;&#20004;&#20010;&#20998;&#24067;$P$&#21644;$Q$&#20043;&#38388;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#25152;&#26377;&#22312;&#26576;&#20010;&#20989;&#25968;&#31354;&#38388;$\mathcal{F}$&#20013;&#30340;&#25968;&#25454;&#21464;&#25442;$f$&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#26368;&#36817;&#23558;&#25152;&#35859;&#30340;Radon&#26377;&#30028;&#21464;&#24046;&#20989;&#25968;&#65288;RBV&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#32852;&#31995;&#36215;&#26469;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65288;Parhi&#21644;Nowak, 2021, 2023&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;$\mathcal{F}$&#21462;&#20026;&#32473;&#23450;&#24179;&#28369;&#24230;&#39034;&#24207;$k \geq 0$&#19979;&#30340;RBV&#31354;&#38388;&#20013;&#30340;&#21333;&#20301;&#29699;&#30340;MMD&#12290;&#36825;&#20010;&#26816;&#39564;&#34987;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#30340;&#32463;&#20856;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#30340;&#19968;&#33324;&#21270;&#12290;&#23427;&#36824;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#65306;&#25105;&#20204;&#35777;&#26126;RKS&#26816;&#39564;&#20013;&#30340;&#35777;&#25454;&#20989;&#25968;$f$&#65292;&#21363;&#36798;&#21040;&#26368;&#22823;&#22343;&#24046;&#30340;&#20989;&#25968;&#65292;&#24635;&#26159;&#19968;&#20010;&#20108;&#27425;&#26679;&#26465;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20999;&#29255;&#21644;&#37197;&#20934;&#36807;&#31243;&#23450;&#20041;&#30340;&#27979;&#37327;&#36716;&#31227;&#21644;&#36924;&#36817;&#38382;&#39064;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#24182;&#23545;&#38543;&#26426;&#20999;&#29255;&#21644;&#37197;&#20934;&#26041;&#26696;&#25552;&#20379;&#20102;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2307.05705</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#20999;&#29255;&#21644;&#37197;&#20934;&#36827;&#34892;&#27979;&#37327;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measure transfer via stochastic slicing and matching. (arXiv:2307.05705v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20999;&#29255;&#21644;&#37197;&#20934;&#36807;&#31243;&#23450;&#20041;&#30340;&#27979;&#37327;&#36716;&#31227;&#21644;&#36924;&#36817;&#38382;&#39064;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#24182;&#23545;&#38543;&#26426;&#20999;&#29255;&#21644;&#37197;&#20934;&#26041;&#26696;&#25552;&#20379;&#20102;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20999;&#29255;&#21644;&#37197;&#20934;&#36807;&#31243;&#23450;&#20041;&#30340;&#27979;&#37327;&#36716;&#31227;&#21644;&#36924;&#36817;&#38382;&#39064;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#31867;&#20284;&#20110;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#36825;&#20123;&#26041;&#26696;&#21463;&#30410;&#20110;&#19968;&#32500;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#38381;&#24335;&#35299;&#30340;&#21487;&#29992;&#24615;&#21644;&#30456;&#20851;&#35745;&#31639;&#20248;&#21183;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#26696;&#24050;&#32463;&#22312;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20851;&#20110;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#30340;&#32467;&#26524;&#19981;&#22826;&#22810;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#38543;&#26426;&#20999;&#29255;&#21644;&#37197;&#20934;&#26041;&#26696;&#25552;&#20379;&#20102;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#30340;&#35777;&#26126;&#12290;&#35813;&#35777;&#26126;&#24314;&#31435;&#22312;&#23558;&#20854;&#35299;&#37322;&#20026;Wasserstein&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;&#30340;&#22522;&#30784;&#20043;&#19978;&#12290;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#36880;&#27493;&#22270;&#20687;&#21464;&#24418;&#30340;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies iterative schemes for measure transfer and approximation problems, which are defined through a slicing-and-matching procedure. Similar to the sliced Wasserstein distance, these schemes benefit from the availability of closed-form solutions for the one-dimensional optimal transport problem and the associated computational advantages. While such schemes have already been successfully utilized in data science applications, not too many results on their convergence are available. The main contribution of this paper is an almost sure convergence proof for stochastic slicing-and-matching schemes. The proof builds on an interpretation as a stochastic gradient descent scheme on the Wasserstein space. Numerical examples on step-wise image morphing are demonstrated as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item></channel></rss>