<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24212;&#29992;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#12290;&#29305;&#21035;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#31216;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2309.01796</link><description>&lt;p&gt;
&#29992;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Asymmetric matrix sensing by gradient descent with small random initialization. (arXiv:2309.01796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24212;&#29992;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#12290;&#29305;&#21035;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#31216;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#65292;&#21363;&#20174;&#23569;&#37327;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36807;&#21442;&#25968;&#21270;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#30340;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#65292;&#24403;&#20174;&#19968;&#20010;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30697;&#38453;&#24863;&#30693;&#65292;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#20984;&#38382;&#39064;&#30340;&#20856;&#22411;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22797;&#26434;&#29616;&#35937;&#35299;&#24320;&#24182;&#35814;&#32454;&#30740;&#31350;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20363;&#22914;&#38750;&#23545;&#31216;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21644;&#23545;&#31216;&#21322;&#27491;&#23450;&#30697;&#38453;&#24863;&#30693;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#36275;&#22815;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;&#26799;&#24230;&#19979;&#38477;&#23545;&#30697;&#38453;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
We study matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements. It can be formulated as an overparameterized regression problem, which can be solved by factorized gradient descent when starting from a small random initialization.  Linear neural networks, and in particular matrix sensing by factorized gradient descent, serve as prototypical models of non-convex problems in modern machine learning, where complex phenomena can be disentangled and studied in detail. Much research has been devoted to studying special cases of asymmetric matrix sensing, such as asymmetric matrix factorization and symmetric positive semi-definite matrix sensing.  Our key contribution is introducing a continuous differential equation that we call the $\textit{perturbed gradient flow}$. We prove that the perturbed gradient flow converges quickly to the true target matrix whenever the perturbation is sufficiently bounded. The dynamics of gradient descent for matr
&lt;/p&gt;</description></item></channel></rss>