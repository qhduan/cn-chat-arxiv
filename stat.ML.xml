<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03167</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;: &#26080;&#29615;&#31639;&#27861;&#26356;&#26032;&#21644;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#32423;&#20248;&#21270;&#65288;SBO&#65289;&#22312;&#22788;&#29702;&#23884;&#22871;&#32467;&#26500;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;SBO&#65292;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#19982;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#22686;&#24378;&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26114;&#36149;&#30340;&#20869;&#37096;&#24490;&#29615;&#26356;&#26032;&#21644;&#23545;&#32593;&#32476;&#25299;&#25169;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#23884;&#22871;&#21452;&#32423;&#31639;&#27861;&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#65288;D-SOBA&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;&#65292;&#39318;&#27425;&#28548;&#28165;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#32447;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00098</link><description>&lt;p&gt;
&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Differential Privacy for End-to-End Speech Recognition. (arXiv:2310.00098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#20165;&#38480;&#20110;&#21021;&#27493;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#33021;&#26412;&#36136;&#19978;&#20445;&#35777;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#38656;&#35201;&#24046;&#20998;&#38544;&#31169;&#26469;&#25552;&#20379;&#31283;&#20581;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#19981;&#28165;&#26970;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20026;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#65292;&#24182;&#24314;&#31435;&#31532;&#19968;&#20010;&#22522;&#32447;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;&#22823;&#22411;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#38754;&#65306;&#26550;&#26500;&#35774;&#35745;&#65292;&#31181;&#23376;&#27169;&#22411;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#39046;&#22495;&#36716;&#31227;&#65292;&#20197;&#21450;cohort&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21512;&#29702;&#30340;&#20013;&#22830;&#32858;&#21512;&#25968;&#37327;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20986;&#21363;&#20351;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#26469;&#33258;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#31181;&#23376;&#27169;&#22411;&#25110;&#26080;&#39044;&#20808;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#25509;&#36817;&#26368;&#20248;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent $\textit{large end-to-end transformer models}$: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a $\textit{practical}$ number of central aggregations we are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even with heterogeneous data, a seed model from another domain, or no pre-trai
&lt;/p&gt;</description></item></channel></rss>