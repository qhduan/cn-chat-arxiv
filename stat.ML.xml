<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16991</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16991
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#22312;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#30001;&#20197;&#23618;&#27425;&#21644;&#32452;&#21512;&#26041;&#24335;&#32452;&#32455;&#30340;&#29305;&#24449;&#32452;&#25104;&#30340;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#36825;&#20123;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#25429;&#25417;&#21040;&#36825;&#31181;&#28508;&#22312;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;$t$&#21518;&#20316;&#29992;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21463;&#21040;&#26576;&#20010;&#38408;&#20540;&#26102;&#38388;&#22788;&#30340;&#30456;&#21464;&#25511;&#21046;&#65292;&#27492;&#26102;&#37325;&#24314;&#39640;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#65289;&#30340;&#27010;&#29575;&#31361;&#28982;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20302;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#20855;&#20307;&#32454;&#33410;&#65289;&#30340;&#37325;&#24314;&#22312;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#20013;&#24179;&#31283;&#28436;&#21464;&#12290;&#36825;&#19968;&#32467;&#26524;&#26263;&#31034;&#65292;&#22312;&#36229;&#20986;&#36716;&#21464;&#26102;&#38388;&#30340;&#26102;&#21051;&#65292;&#31867;&#21035;&#24050;&#21464;&#21270;&#65292;&#20294;&#26159;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06535</link><description>&lt;p&gt;
Bandit Convex Optimisation&#65288;&#24378;&#30423;&#20984;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#30423;&#20984;&#20248;&#21270;&#26159;&#30740;&#31350;&#38646;&#38454;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35768;&#22810;&#24037;&#20855;&#65292;&#21253;&#25324;&#20999;&#24179;&#38754;&#26041;&#27861;&#12289;&#20869;&#28857;&#26041;&#27861;&#12289;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#12290;&#35299;&#37322;&#20102;&#35768;&#22810;&#20551;&#35774;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#23613;&#31649;&#22312;&#36825;&#37324;&#27809;&#26377;&#22826;&#22810;&#30495;&#27491;&#26032;&#30340;&#19996;&#35199;&#65292;&#20294;&#19968;&#20123;&#29616;&#26377;&#24037;&#20855;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#33719;&#24471;&#26032;&#31639;&#27861;&#12290;&#19968;&#20123;&#30028;&#38480;&#31245;&#24494;&#25913;&#36827;&#20102;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. These notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
&lt;/p&gt;</description></item><item><title>&#24191;&#20041;&#27491;&#20132;Procrustes&#38382;&#39064;&#22312;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#21040;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;SDR&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#32780;&#36866;&#24403;&#21021;&#22987;&#21270;&#19979;&#30340;&#24191;&#20041;&#21151;&#29575;&#26041;&#27861;&#20197;&#32447;&#24615;&#26041;&#24335;&#25910;&#25947;&#20110;SDR&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#12290;</title><link>http://arxiv.org/abs/2106.15493</link><description>&lt;p&gt;
&#20219;&#24847;&#23545;&#25163;&#19979;&#30340;&#24191;&#20041;&#27491;&#20132;Procrustes&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalized Orthogonal Procrustes Problem under Arbitrary Adversaries. (arXiv:2106.15493v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15493
&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#27491;&#20132;Procrustes&#38382;&#39064;&#22312;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#21040;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;SDR&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#32780;&#36866;&#24403;&#21021;&#22987;&#21270;&#19979;&#30340;&#24191;&#20041;&#21151;&#29575;&#26041;&#27861;&#20197;&#32447;&#24615;&#26041;&#24335;&#25910;&#25947;&#20110;SDR&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#27491;&#20132;Procrustes&#38382;&#39064;&#65288;Generalized Orthogonal Procrustes Problem&#65292;GOPP&#65289;&#22312;&#32479;&#35745;&#23398;&#12289;&#25104;&#20687;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#22312;&#23454;&#38469;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#25214;&#21040;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340; NP-hard &#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#23450;&#26494;&#24347;&#65288;Semidefinite Relaxation&#65292;SDR&#65289;&#21644;&#19968;&#31181;&#21517;&#20026;&#24191;&#20041;&#21151;&#29575;&#26041;&#27861;&#65288;Generalized Power Method&#65292;GPM&#65289;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#20449;&#21495;&#21152;&#22122;&#22768;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SDR&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#24182;&#19988;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#24191;&#20041;&#21151;&#29575;&#26041;&#27861;&#20197;&#32447;&#24615;&#26041;&#24335;&#25910;&#25947;&#20110;SDR&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#65292;&#21069;&#25552;&#26159;&#20449;&#22122;&#27604;&#36739;&#22823;&#12290;&#20027;&#35201;&#25216;&#26415;&#22522;&#20110;&#23637;&#31034;GPM&#20013;&#28041;&#21450;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#26412;&#36136;&#19978;&#26159;&#23616;&#37096;&#25910;&#32553;&#26144;&#23556;&#65292;&#28982;&#21518;&#24212;&#29992;&#33879;&#21517;&#30340;Banach&#19981;&#21160;&#28857;&#23450;&#29702;&#23436;&#25104;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20302;&#31209;&#20998;&#35299;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalized orthogonal Procrustes problem (GOPP) plays a fundamental role in several scientific disciplines including statistics, imaging science and computer vision. Despite its tremendous practical importance, it is generally an NP-hard problem to find the least squares estimator. We study the semidefinite relaxation (SDR) and an iterative method named generalized power method (GPM) to find the least squares estimator, and investigate the performance under a signal-plus-noise model. We show that the SDR recovers the least squares estimator exactly and moreover the generalized power method with a proper initialization converges linearly to the global minimizer to the SDR, provided that the signal-to-noise ratio is large. The main technique follows from showing the nonlinear mapping involved in the GPM is essentially a local contraction mapping and then applying the well-known Banach fixed-point theorem finishes the proof. In addition, we analyze the low-rank factorization algorith
&lt;/p&gt;</description></item></channel></rss>