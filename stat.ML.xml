<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16703</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304; -- &#19968;&#20010;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#20960;&#24180;&#21069;&#38590;&#20197;&#24819;&#35937;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25104;&#21151;&#20043;&#22806;&#65292;&#36234;&#26469;&#36234;&#28165;&#26224;&#30340;&#26159;&#65292;&#22312;&#32431;&#39044;&#27979;&#20043;&#22806;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20063;&#26159;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#20986;&#29616;&#20102;&#36825;&#26041;&#38754;&#30340;&#31532;&#19968;&#25209;&#27010;&#24565;&#21644;&#24605;&#24819;&#65292;&#20294;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#26356;&#24120;&#35265;&#30456;&#20851;&#30340;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35268;&#33539;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21508;&#24322;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#32479;&#35745;&#23398;&#27010;&#24565;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
&lt;/p&gt;</description></item></channel></rss>