<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#21487;&#24494;&#26041;&#27861;&#26469;&#35299;&#20915;&#32479;&#35745;&#26368;&#20248;&#20998;&#37197;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25490;&#24207;&#36816;&#31639;&#31526;&#30340;&#19968;&#33324;&#23646;&#24615;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20540;&#20989;&#25968;&#30340;Hadamard&#21487;&#24494;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20989;&#25968;&#20559;&#24494;&#20998;&#27861;&#30452;&#25509;&#25512;&#23548;&#20986;&#20540;&#20989;&#25968;&#36807;&#31243;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2403.18248</link><description>&lt;p&gt;
&#32479;&#35745;&#25512;&#26029;&#20013;&#30340;&#26368;&#20248;&#20998;&#37197;I&#65306;&#35268;&#24459;&#24615;&#21450;&#20854;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference of Optimal Allocations I: Regularities and their Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#21487;&#24494;&#26041;&#27861;&#26469;&#35299;&#20915;&#32479;&#35745;&#26368;&#20248;&#20998;&#37197;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25490;&#24207;&#36816;&#31639;&#31526;&#30340;&#19968;&#33324;&#23646;&#24615;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20540;&#20989;&#25968;&#30340;Hadamard&#21487;&#24494;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20989;&#25968;&#20559;&#24494;&#20998;&#27861;&#30452;&#25509;&#25512;&#23548;&#20986;&#20540;&#20989;&#25968;&#36807;&#31243;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32479;&#35745;&#26368;&#20248;&#20998;&#37197;&#38382;&#39064;&#30340;&#20989;&#25968;&#21487;&#24494;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#25490;&#24207;&#36816;&#31639;&#31526;&#30340;&#19968;&#33324;&#23646;&#24615;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20540;&#20989;&#25968;&#30340;Hadamard&#21487;&#24494;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;Hausdorff&#27979;&#24230;&#30340;&#27010;&#24565;&#20197;&#21450;&#20960;&#20309;&#27979;&#24230;&#35770;&#20013;&#30340;&#38754;&#31215;&#21644;&#20849;&#38754;&#31215;&#31215;&#20998;&#20844;&#24335;&#26159;&#26680;&#24515;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;Hadamard&#21487;&#24494;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20989;&#25968;&#20559;&#24494;&#20998;&#27861;&#30452;&#25509;&#25512;&#23548;&#20986;&#20108;&#20803;&#32422;&#26463;&#26368;&#20248;&#20998;&#37197;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#36807;&#31243;&#20197;&#21450;&#20004;&#27493;ROC&#26354;&#32447;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#23545;&#20984;&#21644;&#23616;&#37096;Lipschitz&#27867;&#20989;&#30340;&#28145;&#21051;&#35265;&#35299;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20248;&#20998;&#37197;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#30340;&#39069;&#22806;&#19968;&#33324;Frechet&#21487;&#24494;&#24615;&#32467;&#26524;&#12290;&#36825;&#20123;&#24341;&#20154;&#20837;&#32988;&#30340;&#21457;&#29616;&#28608;&#21169;&#20102;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18248v1 Announce Type: new  Abstract: In this paper, we develp a functional differentiability approach for solving statistical optimal allocation problems. We first derive Hadamard differentiability of the value function through a detailed analysis of the general properties of the sorting operator. Central to our framework are the concept of Hausdorff measure and the area and coarea integration formulas from geometric measure theory. Building on our Hadamard differentiability results, we demonstrate how the functional delta method can be used to directly derive the asymptotic properties of the value function process for binary constrained optimal allocation problems, as well as the two-step ROC curve estimator. Moreover, leveraging profound insights from geometric functional analysis on convex and local Lipschitz functionals, we obtain additional generic Fr\'echet differentiability results for the value functions of optimal allocation problems. These compelling findings moti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02644</link><description>&lt;p&gt;
&#36890;&#36807;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#30340;&#26041;&#27861;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Variational DAG Estimation via State Augmentation With Stochastic Permutations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02644
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#25193;&#23637;&#21644;&#38543;&#26426;&#25490;&#21015;&#36827;&#34892;&#21464;&#20998;DAG&#20272;&#35745;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#21363;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#65292;&#26159;&#19968;&#20010;&#22312;&#32479;&#35745;&#21644;&#35745;&#31639;&#19978;&#37117;&#24456;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#31561;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#24212;&#29992;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#12290;&#20174;&#27010;&#29575;&#25512;&#26029;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65288;i&#65289;&#34920;&#31034;&#28385;&#36275;DAG&#32422;&#26463;&#30340;&#22270;&#30340;&#20998;&#24067;&#21644;&#65288;ii&#65289;&#20272;&#35745;&#24213;&#23618;&#32452;&#21512;&#31354;&#38388;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;DAG&#21644;&#25490;&#21015;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26500;&#24314;&#32852;&#21512;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#21518;&#39564;&#20272;&#35745;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;&#31163;&#25955;&#20998;&#24067;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#22815;&#36229;&#36234;&#31454;&#20105;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the structure of a Bayesian network, in the form of a directed acyclic graph (DAG), from observational data is a statistically and computationally hard problem with essential applications in areas such as causal discovery. Bayesian approaches are a promising direction for solving this task, as they allow for uncertainty quantification and deal with well-known identifiability issues. From a probabilistic inference perspective, the main challenges are (i) representing distributions over graphs that satisfy the DAG constraint and (ii) estimating a posterior over the underlying combinatorial space. We propose an approach that addresses these challenges by formulating a joint distribution on an augmented space of DAGs and permutations. We carry out posterior estimation via variational inference, where we exploit continuous relaxations of discrete distributions. We show that our approach can outperform competitive Bayesian and non-Bayesian benchmarks on a range of synthetic and re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#20013;&#26680;&#23398;&#20064;&#30340;&#20302;&#31209;&#35299;&#30340;&#24615;&#36136;&#12290;&#22312;&#21482;&#26377;&#20302;&#32500;&#23376;&#31354;&#38388;&#23545;&#21709;&#24212;&#21464;&#37327;&#26377;&#35299;&#37322;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#24471;&#21040;&#31934;&#30830;&#30340;&#20302;&#31209;&#35299;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11736</link><description>&lt;p&gt;
&#22312;Ridge&#22238;&#24402;&#20013;&#65292;&#26680;&#23398;&#20064;&#8220;&#33258;&#21160;&#8221;&#32473;&#20986;&#31934;&#30830;&#30340;&#20302;&#31209;&#35299;
&lt;/p&gt;
&lt;p&gt;
Kernel Learning in Ridge Regression "Automatically" Yields Exact Low Rank Solution. (arXiv:2310.11736v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#20013;&#26680;&#23398;&#20064;&#30340;&#20302;&#31209;&#35299;&#30340;&#24615;&#36136;&#12290;&#22312;&#21482;&#26377;&#20302;&#32500;&#23376;&#31354;&#38388;&#23545;&#21709;&#24212;&#21464;&#37327;&#26377;&#35299;&#37322;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#24471;&#21040;&#31934;&#30830;&#30340;&#20302;&#31209;&#35299;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24418;&#24335;&#20026;$(x,x') \mapsto \phi(\|x-x'\|^2_\Sigma)$&#19988;&#30001;&#21442;&#25968;$\Sigma$&#21442;&#25968;&#21270;&#30340;&#26680;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#26680;&#20989;&#25968;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#23427;&#21516;&#26102;&#20248;&#21270;&#20102;&#39044;&#27979;&#20989;&#25968;&#21644;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#21442;&#25968;$\Sigma$&#12290;&#20174;&#36825;&#20010;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#20013;&#23398;&#21040;&#30340;$\Sigma$&#30340;&#29305;&#24449;&#31354;&#38388;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#21327;&#21464;&#37327;&#31354;&#38388;&#20013;&#21738;&#20123;&#26041;&#21521;&#23545;&#39044;&#27979;&#26159;&#37325;&#35201;&#30340;&#12290;&#20551;&#35774;&#21327;&#21464;&#37327;&#21482;&#36890;&#36807;&#20302;&#32500;&#23376;&#31354;&#38388;&#65288;&#20013;&#24515;&#22343;&#20540;&#23376;&#31354;&#38388;&#65289;&#23545;&#21709;&#24212;&#21464;&#37327;&#26377;&#38750;&#38646;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#19979;&#26377;&#38480;&#26679;&#26412;&#26680;&#23398;&#20064;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#32773;&#20063;&#26159;&#20302;&#31209;&#30340;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#26368;&#23567;&#21270;$\Sigma$&#30340;&#31209;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#34987;&#20013;&#24515;&#22343;&#20540;&#23376;&#31354;&#38388;&#30340;&#32500;&#24230;&#25152;&#38480;&#21046;&#12290;&#36825;&#20010;&#29616;&#35937;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#20302;&#31209;&#29305;&#24615;&#26159;&#22312;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#23545;$\Sigma$&#30340;&#26174;&#24335;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#65292;&#20363;&#22914;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider kernels of the form $(x,x') \mapsto \phi(\|x-x'\|^2_\Sigma)$ parametrized by $\Sigma$. For such kernels, we study a variant of the kernel ridge regression problem which simultaneously optimizes the prediction function and the parameter $\Sigma$ of the reproducing kernel Hilbert space. The eigenspace of the $\Sigma$ learned from this kernel ridge regression problem can inform us which directions in covariate space are important for prediction.  Assuming that the covariates have nonzero explanatory power for the response only through a low dimensional subspace (central mean subspace), we find that the global minimizer of the finite sample kernel learning objective is also low rank with high probability. More precisely, the rank of the minimizing $\Sigma$ is with high probability bounded by the dimension of the central mean subspace. This phenomenon is interesting because the low rankness property is achieved without using any explicit regularization of $\Sigma$, e.g., nuclear
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;LESS&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#24182;&#35757;&#32451;&#23616;&#37096;&#39044;&#27979;&#22120;&#65292;&#28982;&#21518;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#39044;&#27979;&#22120;&#24471;&#21040;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#19988;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.06251</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#23376;&#38598;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Learning with Subset Stacking. (arXiv:2112.06251v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;LESS&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#24182;&#35757;&#32451;&#23616;&#37096;&#39044;&#27979;&#22120;&#65292;&#28982;&#21518;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#39044;&#27979;&#22120;&#24471;&#21040;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#19988;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22238;&#24402;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20174;&#19968;&#32452;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#36755;&#20837;&#21464;&#37327;&#19982;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#24322;&#36136;&#34892;&#20026;&#30340;&#32676;&#20307;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#29983;&#25104;&#20197;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#38598;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#23376;&#38598;&#35757;&#32451;&#19968;&#20010;&#23616;&#37096;&#39044;&#27979;&#22120;&#12290;&#28982;&#21518;&#36825;&#20123;&#39044;&#27979;&#22120;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#25972;&#20307;&#39044;&#27979;&#22120;&#12290;&#25105;&#20204;&#23558;&#27492;&#31639;&#27861;&#31216;&#20026;&#8220;&#23398;&#20064;&#19982;&#23376;&#38598;&#21472;&#21152;&#8221;&#25110;LESS&#65292;&#22240;&#20026;&#23427;&#31867;&#20284;&#20110;&#21472;&#21152;&#22238;&#24402;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;LESS&#19982;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65292;LESS&#26159;&#19968;&#31181;&#26377;&#31454;&#20105;&#21147;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LESS&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#20063;&#38750;&#24120;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#24182;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new regression algorithm that learns from a set of input-output pairs. Our algorithm is designed for populations where the relation between the input variables and the output variable exhibits a heterogeneous behavior across the predictor space. The algorithm starts with generating subsets that are concentrated around random points in the input space. This is followed by training a local predictor for each subset. Those predictors are then combined in a novel way to yield an overall predictor. We call this algorithm ``LEarning with Subset Stacking'' or LESS, due to its resemblance to the method of stacking regressors. We compare the testing performance of LESS with state-of-the-art methods on several datasets. Our comparison shows that LESS is a competitive supervised learning method. Moreover, we observe that LESS is also efficient in terms of computation time and it allows a straightforward parallel implementation.
&lt;/p&gt;</description></item></channel></rss>