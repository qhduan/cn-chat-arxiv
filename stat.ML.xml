<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#25552;&#21319;&#31639;&#27861;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#20855;&#26377;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.02976</link><description>&lt;p&gt;
&#25552;&#21319;&#65292;&#25237;&#31080;&#20998;&#31867;&#22120;&#21644;&#38543;&#26426;&#37319;&#26679;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Boosting, Voting Classifiers and Randomized Sample Compression Schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#25552;&#21319;&#31639;&#27861;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#20855;&#26377;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#21319;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24369;&#23398;&#20064;&#22120;&#26469;&#20135;&#29983;&#19968;&#20010;&#24378;&#23398;&#20064;&#22120;&#12290;&#36825;&#20010;&#33539;&#24335;&#30340;&#26680;&#24515;&#26159;&#23558;&#24378;&#23398;&#20064;&#22120;&#24314;&#27169;&#20026;&#19968;&#20010;&#25237;&#31080;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#24369;&#23398;&#20064;&#22120;&#30340;&#21152;&#26435;&#22810;&#25968;&#25237;&#31080;&#12290;&#23613;&#31649;&#35768;&#22810;&#25104;&#21151;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#22914;&#26631;&#24535;&#24615;&#30340;AdaBoost&#65292;&#20135;&#29983;&#25237;&#31080;&#20998;&#31867;&#22120;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#33021;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#19981;&#22815;&#20248;&#21270;&#65306;&#36804;&#20170;&#20026;&#27490;&#65292;&#24050;&#30693;&#30340;&#20351;&#25237;&#31080;&#20998;&#31867;&#22120;&#36798;&#21040;&#32473;&#23450;&#20934;&#30830;&#24615;&#25152;&#38656;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#30340;&#26368;&#20339;&#30028;&#38480;&#24635;&#26159;&#33267;&#23569;&#21253;&#21547;&#33267;&#22810;&#20004;&#20010;&#23545;&#25968;&#22240;&#23376;&#65292;&#32780;&#36825;&#24050;&#32463;&#36229;&#36807;&#20102;&#19968;&#33324;&#30340;&#24369;&#21040;&#24378;&#23398;&#20064;&#22120;&#25152;&#33021;&#23454;&#29616;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38543;&#26426;&#25552;&#21319;&#31639;&#27861;&#25171;&#30772;&#36825;&#19968;&#38556;&#30861;&#65292;&#35813;&#31639;&#27861;&#36755;&#20986;&#30340;&#25237;&#31080;&#20998;&#31867;&#22120;&#22312;&#26679;&#26412;&#22823;&#23567;&#19978;&#21253;&#21547;&#21333;&#23545;&#25968;&#20381;&#36182;&#30340;&#27867;&#21270;&#38169;&#35823;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23558;&#26679;&#26412;&#21387;&#32553;&#26041;&#27861;&#25193;&#23637;&#21040;&#25903;&#25345;&#38543;&#26426;&#23398;&#20064;&#31639;&#27861;&#26469;&#33719;&#24471;&#36825;&#20010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms ba
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.13524</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#37327;&#23376;&#35745;&#31639;&#65288;MBQC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#29420;&#29305;&#30340;&#33539;&#20363;&#26469;&#35774;&#35745;&#37327;&#23376;&#31639;&#27861;&#12290;&#22312;MBQC&#20013;&#65292;&#30001;&#20110;&#37327;&#23376;&#27979;&#37327;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#33258;&#28982;&#30340;&#25805;&#20316;&#19981;&#26159;&#30830;&#23450;&#24615;&#21644;&#24186;&#27491;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#27010;&#29575;&#38468;&#24102;&#30340;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;MBQC&#30340;&#20027;&#35201;&#31639;&#27861;&#24212;&#29992;&#26159;&#23436;&#20840;&#25269;&#28040;&#36825;&#31181;&#27010;&#29575;&#24615;&#36136;&#65292;&#20197;&#27169;&#25311;&#34920;&#36798;&#22312;&#30005;&#36335;&#27169;&#22411;&#20013;&#30340;&#24186;&#27491;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;MBQC&#31639;&#27861;&#30340;&#24605;&#36335;&#65292;&#35813;&#31639;&#27861;&#25509;&#21463;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#23558;MBQC&#20013;&#30340;&#38543;&#26426;&#38468;&#24102;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#24615;&#26377;&#30410;&#30340;&#33258;&#28982;&#24212;&#29992;&#65292;&#21363;&#29983;&#25104;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29983;&#25104;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25511;&#21046;&#21442;&#25968;&#30340;&#21464;&#20998;MBQC&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#35843;&#25972;&#20801;&#35768;&#22312;&#35745;&#31639;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01930</link><description>&lt;p&gt;
&#23398;&#20064;ECG&#20449;&#21495;&#29305;&#24449;&#30340;&#38750;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning ECG signal features without backpropagation. (arXiv:2307.01930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#26088;&#22312;&#21457;&#29616;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#21407;&#22987;&#25968;&#25454;&#30340;&#26377;&#25928;&#29305;&#24449;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#36825;&#20010;&#26032;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23646;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#29305;&#24449;&#30340;&#32447;&#24615;&#35268;&#24459;&#12290;&#36890;&#36807;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#35268;&#24459;&#22312;&#21069;&#21521;&#26041;&#24335;&#19979;&#29983;&#25104;&#19968;&#20010;&#19982;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21452;&#26679;&#26412;&#26816;&#39564;&#20013;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#32479;&#35745;&#37327;&#65292;&#20197;&#26368;&#22823;&#21270;&#26041;&#24046;&#27491;&#21017;&#21270;&#30340;MMD&#32479;&#35745;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#36229;&#32676;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.07415</link><description>&lt;p&gt;
&#21464;&#37327;&#36873;&#25321;&#22312;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variable Selection for Kernel Two-Sample Tests. (arXiv:2302.07415v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21452;&#26679;&#26412;&#26816;&#39564;&#20013;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#32479;&#35745;&#37327;&#65292;&#20197;&#26368;&#22823;&#21270;&#26041;&#24046;&#27491;&#21017;&#21270;&#30340;MMD&#32479;&#35745;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#36229;&#32676;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#26679;&#26412;&#26816;&#39564;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#26088;&#22312;&#36873;&#25321;&#21306;&#20998;&#20004;&#32452;&#26679;&#26412;&#30340;&#26368;&#26377;&#20449;&#24687;&#21464;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23547;&#27714;&#19968;&#32452;&#21464;&#37327;&#65292;&#20854;&#39044;&#20808;&#30830;&#23450;&#30340;&#22823;&#23567;&#26368;&#22823;&#21270;&#26041;&#24046;&#27491;&#21017;&#21270;&#30340;MMD&#32479;&#35745;&#37327;&#12290;&#36825;&#31181;&#35745;&#31639;&#24418;&#24335;&#20063;&#23545;&#24212;&#20110;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#30340;&#25511;&#21046;&#31867;&#22411;I&#38169;&#35823;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#24322;&#36136;&#31867;&#22411;II&#38169;&#35823;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28151;&#21512;&#25972;&#25968;&#32534;&#31243;&#20844;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#32447;&#24615;&#21644;&#20108;&#27425;&#31867;&#22411;&#20869;&#26680;&#20989;&#25968;&#30340;&#31934;&#30830;&#21644;&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the variable selection problem for two-sample tests, aiming to select the most informative variables to distinguish samples from two groups. To solve this problem, we propose a framework based on the kernel maximum mean discrepancy (MMD). Our approach seeks a group of variables with a pre-specified size that maximizes the variance-regularized MMD statistics. This formulation also corresponds to the minimization of asymptotic type-II error while controlling type-I error, as studied in the literature. We present mixed-integer programming formulations and offer exact and approximation algorithms with performance guarantees for linear and quadratic types of kernel functions. Experimental results demonstrate the superior performance of our framework.
&lt;/p&gt;</description></item></channel></rss>