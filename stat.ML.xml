<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35777;&#26126;signSGD&#31639;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#20998;&#26512;&#20013;&#30340;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;SignRVR&#21644;SignRVM&#31639;&#27861;&#65292;&#24182;&#19988;&#37117;&#20197;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.15976</link><description>&lt;p&gt;
&#38750;&#20984;&#20248;&#21270;&#30340;&#22522;&#20110;&#31526;&#21495;&#38543;&#26426;&#37325;&#25490;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization. (arXiv:2310.15976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15976
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35777;&#26126;signSGD&#31639;&#27861;&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#20998;&#26512;&#20013;&#30340;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;SignRVR&#21644;SignRVM&#31639;&#27861;&#65292;&#24182;&#19988;&#37117;&#20197;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#36890;&#20449;&#25928;&#29575;&#36739;&#39640;&#65292;signSGD&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23545;signSGD&#30340;&#20998;&#26512;&#22522;&#20110;&#20551;&#35774;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25968;&#25454;&#37117;&#26159;&#26377;&#25918;&#22238;&#37319;&#26679;&#30340;&#65292;&#36825;&#19982;&#23454;&#38469;&#23454;&#29616;&#20013;&#25968;&#25454;&#30340;&#38543;&#26426;&#37325;&#25490;&#21644;&#39034;&#24207;&#39304;&#36865;&#36827;&#31639;&#27861;&#30340;&#24773;&#20917;&#30456;&#30683;&#30462;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;signSGD&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#38543;&#26426;&#37325;&#25490;&#65288;SignRR&#65289;&#30340;&#39318;&#20010;&#25910;&#25947;&#32467;&#26524;&#12290;&#32473;&#23450;&#25968;&#25454;&#38598;&#22823;&#23567;$n$&#65292;&#25968;&#25454;&#36845;&#20195;&#27425;&#25968;$T$&#65292;&#21644;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#38480;&#21046;$\sigma^2$&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SignRR&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;signSGD&#30456;&#21516;&#65292;&#20026;$O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ \citep{bernstein2018signsgd}&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102; SignRVR &#21644; SignRVM&#65292;&#20998;&#21035;&#21033;&#29992;&#20102;&#26041;&#24046;&#32422;&#20943;&#26799;&#24230;&#21644;&#21160;&#37327;&#26356;&#26032;&#65292;&#37117;&#20197;$O(\log(nT)/\sqrt{nT})$&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;&#19982;signSGD&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#20013;&#26497;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#19982;&#21516;&#31561;&#25968;&#37327;&#30340;&#26799;&#24230;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
signSGD is popular in nonconvex optimization due to its communication efficiency. Yet, existing analyses of signSGD rely on assuming that data are sampled with replacement in each iteration, contradicting the practical implementation where data are randomly reshuffled and sequentially fed into the algorithm. We bridge this gap by proving the first convergence result of signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the dataset size $n$, the number of epochs of data passes $T$, and the variance bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD \citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which leverage variance-reduced gradients and momentum updates respectively, both converging at $O(\log(nT)/\sqrt{nT})$. In contrast with the analysis of signSGD, our results do not require an extremely large batch size in each iteration to be of the same order a
&lt;/p&gt;</description></item></channel></rss>