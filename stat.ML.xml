<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;</title><link>https://arxiv.org/abs/2402.11736</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;Gibbs&#27979;&#24230;&#30340;&#33945;&#29305;&#21345;&#32599;&#65306;&#27010;&#29575;&#38543;&#26426;&#25918;&#29287;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kernel herding&#23646;&#20110;&#19968;&#31867;&#30830;&#23450;&#24615;&#30340;&#22235;&#20301;&#25968;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#12290;&#23613;&#31649;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#25903;&#25345;&#65292;&#20294;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21363;RKHS&#26159;&#26080;&#38480;&#32500;&#26102;&#65292;&#35777;&#26126;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#20197;&#27604;&#26631;&#20934;&#31215;&#20998;&#33410;&#28857;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#26356;&#24555;&#30340;&#36895;&#29575;&#20943;&#23569;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#31687;&#29702;&#35770;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#20110;&#31215;&#20998;&#33410;&#28857;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#19982;&#26680;&#25918;&#29287;&#30456;&#21516;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#65292;&#24847;&#21619;&#30528;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#19978;&#20855;&#26377;&#26356;&#32039;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#23578;&#26410;&#25552;&#39640;&#36895;&#29575;&#65292;&#20294;&#36825;&#34920;&#26126;&#20102;&#30740;&#31350;Gibbs&#27979;&#24230;&#30340;&#25968;&#23398;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#26680;&#25918;&#29287;&#21450;&#20854;&#21464;&#20307;&#22312;&#35745;&#31639;&#19978;&#30340;&#25913;&#36827;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11736v1 Announce Type: new  Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00539</link><description>&lt;p&gt;
&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#19979;&#30340;Thompson&#25506;&#32034;&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Thompson Exploration with Best Challenger Rule in Best Arm Identification. (arXiv:2310.00539v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32463;&#20856;&#21333;&#21442;&#25968;&#25351;&#25968;&#27169;&#22411;&#19979;&#65292;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#24050;&#26377;&#24456;&#22810;&#31574;&#30053;&#34987;&#25552;&#20986;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#22312;&#27599;&#19968;&#36718;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#21270;&#38382;&#39064;&#21644;/&#25110;&#32773;&#38656;&#35201;&#25506;&#32034;&#19968;&#20010;&#33218;&#33267;&#23569;&#19968;&#23450;&#27425;&#25968;&#65292;&#38500;&#38750;&#26159;&#38024;&#23545;&#39640;&#26031;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#8212;&#8212;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;Thompson&#37319;&#26679;&#26368;&#21021;&#34987;&#32771;&#34385;&#29992;&#20110;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#20063;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#22312;BAI&#20013;&#25506;&#32034;&#33218;&#32780;&#19981;&#24378;&#36843;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20219;&#24847;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#30340;$K$&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#65288;$K\geq 3$&#65289;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#65292;&#20197;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#29305;&#24449;&#26500;&#24314;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16297</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26102;&#21464;&#35843;&#33410;&#22240;&#32032;&#30340;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#20272;&#35745;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Method for Estimation of Causal Excursion Effects to Assess Time-Varying Moderation. (arXiv:2306.16297v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16297
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#65292;&#20197;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#29305;&#24449;&#26500;&#24314;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#26426;&#25552;&#20379;&#30340;&#25968;&#23383;&#21270;&#20581;&#24247;&#24178;&#39044;&#30340;&#21452;&#37325;&#38761;&#21629;&#26174;&#33879;&#22686;&#21152;&#20102;&#31227;&#21160;&#20581;&#24247;&#65288;mHealth&#65289;&#24178;&#39044;&#22312;&#21508;&#20010;&#20581;&#24247;&#31185;&#23398;&#39046;&#22495;&#30340;&#21487;&#21450;&#24615;&#21644;&#37319;&#32435;&#29575;&#12290;&#39034;&#24207;&#38543;&#26426;&#23454;&#39564;&#31216;&#20026;&#24494;&#38543;&#26426;&#35797;&#39564;&#65288;MRTs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#23454;&#35777;&#35780;&#20272;&#36825;&#20123;mHealth&#24178;&#39044;&#32452;&#25104;&#37096;&#20998;&#30340;&#26377;&#25928;&#24615;&#12290;MRTs&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;&#37327;&#65292;&#31216;&#20026;&#8220;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#8221;&#65292;&#20351;&#20581;&#24247;&#31185;&#23398;&#23478;&#33021;&#22815;&#35780;&#20272;&#24178;&#39044;&#25928;&#26524;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#25110;&#36890;&#36807;&#20010;&#20307;&#29305;&#24449;&#12289;&#29615;&#22659;&#25110;&#36807;&#21435;&#30340;&#21453;&#24212;&#26469;&#35843;&#33410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20559;&#31163;&#25928;&#24212;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#35266;&#23519;&#21040;&#30340;&#39640;&#32500;&#21382;&#21490;&#30340;&#29305;&#24449;&#26469;&#26500;&#24314;&#37325;&#35201;&#24178;&#25200;&#21442;&#25968;&#30340;&#24037;&#20316;&#27169;&#22411;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#33258;&#21160;&#29305;&#24449;&#26500;&#24314;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#20854;&#26420;&#32032;&#24212;&#29992;&#23548;&#33268;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Twin revolutions in wearable technologies and smartphone-delivered digital health interventions have significantly expanded the accessibility and uptake of mobile health (mHealth) interventions across various health science domains. Sequentially randomized experiments called micro-randomized trials (MRTs) have grown in popularity to empirically evaluate the effectiveness of these mHealth intervention components. MRTs have given rise to a new class of causal estimands known as "causal excursion effects", which enable health scientists to assess how intervention effectiveness changes over time or is moderated by individual characteristics, context, or responses in the past. However, current data analysis methods for estimating causal excursion effects require pre-specified features of the observed high-dimensional history to construct a working model of an important nuisance parameter. While machine learning algorithms are ideal for automatic feature construction, their naive application
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10259</link><description>&lt;p&gt;
&#22810;&#40657;&#30418;&#39044;&#35328;&#19979;&#30340;&#20027;&#21160;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#20154;&#20204;&#36890;&#24120;&#21482;&#33021;&#25509;&#35302;&#21040;&#22810;&#20010;&#27425;&#20248;&#30340;&#40657;&#30418;&#39044;&#35328;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26368;&#20248;&#30340;&#39044;&#35328;&#65292;&#36825;&#20123;&#39044;&#35328;&#19981;&#33021;&#22312;&#25152;&#26377;&#29366;&#24577;&#19979;&#26222;&#36941;&#20248;&#20110;&#24444;&#27492;&#65292;&#36825;&#32473;&#20027;&#21160;&#20915;&#23450;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#20351;&#29992;&#21738;&#31181;&#39044;&#35328;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#21508;&#33258;&#20272;&#35745;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;MAPS&#21644;MAPS-SE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
&lt;/p&gt;</description></item></channel></rss>