<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01896</link><description>&lt;p&gt;
&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#30340;&#24378;&#40065;&#26834;&#24615;&#30028;&#38480;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Robustness Bounds on the Successful Adversarial Examples: Theory and Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#65288;AE&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#26469;&#35825;&#20351;&#38169;&#20998;&#12290;&#26412;&#25991;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#31867;&#65292;&#30740;&#31350;&#20102;&#25104;&#21151;AE&#30340;&#27010;&#29575;&#19978;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#65292;&#21462;&#20915;&#20110;AE&#30340;&#25200;&#21160;&#33539;&#25968;&#12289;GP&#20013;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26368;&#25509;&#36817;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#19978;&#38480;&#19981;&#21463;&#26679;&#26412;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;ImageNet&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#21464;&#26680;&#20989;&#25968;&#21442;&#25968;&#20250;&#23548;&#33268;&#25104;&#21151;AE&#27010;&#29575;&#19978;&#38480;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01896v1 Announce Type: new  Abstract: Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the Gaussian Process (GP) classification. We proved a new upper bound that depends on AE's perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09122</link><description>&lt;p&gt;
&#28151;&#21512;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed-Output Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#20449;&#21495;&#20998;&#31163;&#26041;&#27861;&#65292;&#20854;&#20013;&#20449;&#21495;&#21487;&#20197;&#26681;&#25454;&#28508;&#21464;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22686;&#21152;&#20102;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#65292;&#20197;&#21253;&#25324;&#27599;&#20010;&#25968;&#25454;&#28857;&#30001;&#24050;&#30693;&#25968;&#37327;&#30340;&#32431;&#32452;&#20998;&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#22810;&#20010;&#36755;&#20837;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#20851;&#20110;&#27599;&#20010;&#35266;&#27979;&#26435;&#37325;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#20998;&#25968;&#32452;&#25104;&#30340;&#24635;&#21644;&#20026;&#19968;&#32422;&#26463;&#21644;&#29992;&#20110;&#20998;&#31867;&#30340;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23545;&#20110;&#20809;&#35889;&#23398;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#25913;&#21464;&#26465;&#20214;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#32431;&#32452;&#20998;&#20449;&#21495;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#23637;&#31034;&#23545;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#24212;&#29992;&#65306;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#28201;&#24230;&#30340;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#21487;&#20197;&#26597;&#35810;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#33218;&#30340;&#21472;&#21152;&#24577;&#26102;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65292;&#20294;&#22312;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#26102;&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2301.08544</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#37327;&#23376;&#36890;&#36947;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Armed Bandits and Quantum Channel Oracles. (arXiv:2301.08544v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31639;&#27861;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#21487;&#20197;&#26597;&#35810;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#33218;&#30340;&#21472;&#21152;&#24577;&#26102;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65292;&#20294;&#22312;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#26102;&#65292;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#26159;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#30340;&#37325;&#35201;&#25903;&#26609;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#24403;&#21487;&#20197;&#22312;&#21472;&#21152;&#24577;&#20013;&#26597;&#35810;&#33218;&#21644;&#22870;&#21169;&#38543;&#26426;&#24615;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#20108;&#27425;&#21152;&#36895;&#65288;&#22312;&#26597;&#35810;&#22797;&#26434;&#24230;&#19978;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36827;&#19968;&#27493;&#30340;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#25105;&#20204;&#21482;&#33021;&#26377;&#38480;&#22320;&#35775;&#38382;&#22870;&#21169;&#30340;&#38543;&#26426;&#24615;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#22312;&#21472;&#21152;&#24577;&#20013;&#26597;&#35810;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#22914;&#27492;&#26102;&#26597;&#35810;&#22797;&#26434;&#24230;&#19982;&#32463;&#20856;&#31639;&#27861;&#30456;&#21516;&#12290;&#36825;&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#32467;&#26524;&#65292;&#21363;&#24403;&#39044;&#27979;&#22120;&#20855;&#26377;&#27491;&#30340;&#22833;&#25928;&#27010;&#29575;&#26102;&#65292;&#23545;&#20110;&#26410;&#32467;&#26500;&#21270;&#25628;&#32034;&#26080;&#27861;&#23454;&#29616;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed bandits are one of the theoretical pillars of reinforcement learning. Recently, the investigation of quantum algorithms for multi-armed bandit problems was started, and it was found that a quadratic speed-up (in query complexity) is possible when the arms and the randomness of the rewards of the arms can be queried in superposition. Here we introduce further bandit models where we only have limited access to the randomness of the rewards, but we can still query the arms in superposition. We show that then the query complexity is the same as for classical algorithms. This generalizes the prior result that no speed-up is possible for unstructured search when the oracle has positive failure probability.
&lt;/p&gt;</description></item></channel></rss>