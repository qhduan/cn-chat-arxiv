<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#27604;&#36739;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#26435;&#37325;&#26469;&#22686;&#24378;&#27979;&#35797;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01537</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#27604;&#36739;&#20013;&#30340;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Variable Selection in Maximum Mean Discrepancy for Interpretable Distribution Comparison. (arXiv:2311.01537v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#27604;&#36739;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#26435;&#37325;&#26469;&#22686;&#24378;&#27979;&#35797;&#30340;&#21151;&#25928;&#65292;&#24182;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#26679;&#26412;&#27979;&#35797;&#26159;&#20026;&#20102;&#21028;&#26029;&#20004;&#20010;&#25968;&#25454;&#38598;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#26679;&#26412;&#27979;&#35797;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#21363;&#35782;&#21035;&#36896;&#25104;&#20004;&#20010;&#20998;&#24067;&#24046;&#24322;&#30340;&#21464;&#37327;&#65288;&#25110;&#32500;&#24230;&#65289;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19982;&#27169;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#38382;&#39064;&#30456;&#20851;&#65292;&#22914;&#25968;&#25454;&#38598;&#28418;&#31227;&#36866;&#24212;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#27169;&#22411;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22522;&#20110;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#65288;MMD&#65289;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#12290;&#25105;&#20204;&#20248;&#21270;&#38024;&#23545;&#21508;&#20010;&#21464;&#37327;&#23450;&#20041;&#30340;&#33258;&#21160;&#30456;&#20851;&#24615;&#26816;&#27979;&#65288;ARD&#65289;&#26435;&#37325;&#65292;&#20197;&#26368;&#22823;&#21270;&#22522;&#20110;MMD&#30340;&#26816;&#39564;&#30340;&#21151;&#29575;&#12290;&#23545;&#20110;&#36825;&#31181;&#20248;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27491;&#21017;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36873;&#25321;&#36866;&#24403;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#30830;&#23450;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#21512;&#24182;&#19981;&#21516;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30830;&#35748;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sample testing decides whether two datasets are generated from the same distribution. This paper studies variable selection for two-sample testing, the task being to identify the variables (or dimensions) responsible for the discrepancies between the two distributions. This task is relevant to many problems of pattern analysis and machine learning, such as dataset shift adaptation, causal inference and model validation. Our approach is based on a two-sample test based on the Maximum Mean Discrepancy (MMD). We optimise the Automatic Relevance Detection (ARD) weights defined for individual variables to maximise the power of the MMD-based test. For this optimisation, we introduce sparse regularisation and propose two methods for dealing with the issue of selecting an appropriate regularisation parameter. One method determines the regularisation parameter in a data-driven way, and the other aggregates the results of different regularisation parameters. We confirm the validity of the pr
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11375</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Neural networks learn to magnify areas near decision boundaries. (arXiv:2301.11375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11375
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#33021;&#22815;&#25918;&#22823;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#25913;&#21892;&#25972;&#20010;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#22914;&#20309;&#22609;&#36896;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#22270;&#35825;&#23548;&#30340;&#40654;&#26364;&#20960;&#20309;&#12290;&#22312;&#23485;&#24230;&#20026;&#26080;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#24341;&#23548;&#39640;&#24230;&#23545;&#31216;&#30340;&#24230;&#37327;&#12290;&#35757;&#32451;&#20998;&#31867;&#20219;&#21153;&#30340;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#25918;&#22823;&#20102;&#27839;&#20915;&#31574;&#36793;&#30028;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#36825;&#20123;&#21464;&#21270;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#25163;&#21160;&#35843;&#25972;&#26680;&#26041;&#27861;&#20197;&#25913;&#21892;&#27867;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how training molds the Riemannian geometry induced by neural network feature maps. At infinite width, neural networks with random parameters induce highly symmetric metrics on input space. Feature learning in networks trained to perform classification tasks magnifies local areas along decision boundaries. These changes are consistent with previously proposed geometric approaches for hand-tuning of kernel methods to improve generalization.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#65292;&#35777;&#26126;&#20102;&#36793;&#32536;&#20998;&#24067; $L^p$ &#25910;&#25947;&#24615;&#21644;&#28151;&#27788;&#29616;&#35937;&#30340;&#22343;&#21248;&#26102;&#38388;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2212.03050</link><description>&lt;p&gt;
&#22343;&#21248;&#26102;&#38388;&#20256;&#25773;&#28151;&#27788;&#30340;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Uniform-in-time propagation of chaos for mean field Langevin dynamics. (arXiv:2212.03050v2 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03050
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#65292;&#35777;&#26126;&#20102;&#36793;&#32536;&#20998;&#24067; $L^p$ &#25910;&#25947;&#24615;&#21644;&#28151;&#27788;&#29616;&#35937;&#30340;&#22343;&#21248;&#26102;&#38388;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#21450;&#20854;&#30456;&#20851;&#31890;&#23376;&#31995;&#32479;&#12290;&#36890;&#36807;&#20551;&#35774;&#33021;&#37327;&#20989;&#25968;&#30340;&#20984;&#24615;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#36793;&#32536;&#20998;&#24067;&#25910;&#25947;&#21040;&#24179;&#22343;&#22330;&#21160;&#21147;&#23398;&#21807;&#19968;&#19981;&#21464;&#27979;&#24230;&#30340; $L^p$ &#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312; $L^2$ Wasserstein &#36317;&#31163;&#21644;&#30456;&#23545;&#29109;&#20004;&#26041;&#38754;&#65292;&#28151;&#27788;&#29616;&#35937;&#30340;&#22343;&#21248;&#26102;&#38388;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the mean field Langevin dynamics and the associated particle system. By assuming the functional convexity of the energy, we obtain the $L^p$-convergence of the marginal distributions towards the unique invariant measure for the mean field dynamics. Furthermore, we prove the uniform-in-time propagation of chaos in both the $L^2$-Wasserstein metric and relative entropy.
&lt;/p&gt;</description></item></channel></rss>