<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2208.14161</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#22240;&#26524;&#20869;&#23481;&#29992;&#20110;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#35299;&#20915;&#20102;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#38024;&#23545;&#26410;&#26631;&#35760;&#30446;&#26631;&#39046;&#22495;&#30340;&#26631;&#31614;&#39044;&#27979;&#20989;&#25968;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#23427;&#24341;&#20837;&#20102;&#26356;&#22823;&#30340;&#39046;&#22495;&#38388;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20026;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#30340;&#28508;&#22312;&#21407;&#22240;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14161v3 Announce Type: replace  Abstract: Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics.   Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.06986</link><description>&lt;p&gt;
&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#21487;&#35777;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#12290;&#28145;&#24230;&#32593;&#32476;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#30340;&#33021;&#21147;&#23545;&#20854;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20173;&#28982;&#19981;&#22815;&#28165;&#26224;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20027;&#35201;&#23616;&#38480;&#20110;&#20004;&#23618;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35777;&#26126;&#30340;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36890;&#36807;&#36880;&#23618;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#19977;&#23618;&#32593;&#32476;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#23427;&#19978;&#30028;&#20102;&#30446;&#26631;&#20855;&#26377;&#29305;&#23450;&#23618;&#27425;&#32467;&#26500;&#26102;&#23454;&#29616;&#20302;&#27979;&#35797;&#38169;&#35823;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#29305;&#23450;&#30340;&#32479;&#35745;&#23398;&#23398;&#20064;&#35774;&#32622;&#20013;&#8212;&#8212;&#21333;&#25351;&#25968;&#27169;&#22411;&#21644;&#20108;&#27425;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#20809;&#32420;&#21033;&#29992;&#27700;&#24211;&#35745;&#31639;&#33539;&#20363;&#36827;&#34892;&#20998;&#31867;&#65292;&#31934;&#24230;&#39640;&#20110;&#30452;&#25509;&#35757;&#32451;&#21407;&#22987;&#22270;&#20687;&#21644;&#20256;&#32479;&#30340;&#20256;&#36755;&#30697;&#38453;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.04745</link><description>&lt;p&gt;
&#22810;&#27169;&#20809;&#32420;&#27700;&#24211;&#35745;&#31639;&#20811;&#26381;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multi-mode fiber reservoir computing overcomes shallow neural networks classifiers. (arXiv:2210.04745v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04745
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#20809;&#32420;&#21033;&#29992;&#27700;&#24211;&#35745;&#31639;&#33539;&#20363;&#36827;&#34892;&#20998;&#31867;&#65292;&#31934;&#24230;&#39640;&#20110;&#30452;&#25509;&#35757;&#32451;&#21407;&#22987;&#22270;&#20687;&#21644;&#20256;&#32479;&#30340;&#20256;&#36755;&#30697;&#38453;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#24207;&#20809;&#23376;&#23398;&#39046;&#22495;&#20013;&#65292;&#24120;&#35265;&#30340;&#30446;&#26631;&#26159;&#23545;&#19981;&#36879;&#26126;&#26448;&#26009;&#36827;&#34892;&#34920;&#24449;&#65292;&#20197;&#25511;&#21046;&#20809;&#30340;&#20256;&#36882;&#25110;&#25191;&#34892;&#25104;&#20687;&#12290;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#22120;&#20214;&#20013;&#65292;&#22810;&#27169;&#20809;&#32420;&#20197;&#20854;&#25104;&#26412;&#25928;&#30410;&#39640;&#12289;&#26131;&#20110;&#25805;&#20316;&#30340;&#29305;&#28857;&#33073;&#39062;&#32780;&#20986;&#65292;&#20351;&#20854;&#22312;&#20960;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#27700;&#24211;&#35745;&#31639;&#33539;&#20363;&#65292;&#23558;&#36825;&#20123;&#20809;&#32420;&#36716;&#21270;&#20026;&#38543;&#26426;&#30828;&#20214;&#25237;&#24433;&#20202;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#39640;&#32500;&#26001;&#28857;&#22270;&#20687;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35777;&#26126;&#65292;&#36890;&#36807;&#35757;&#32451;&#21333;&#20010;&#36923;&#36753;&#22238;&#24402;&#23618;&#23545;&#36825;&#20123;&#38543;&#26426;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#30456;&#27604;&#20043;&#19979;&#30452;&#25509;&#35757;&#32451;&#21407;&#22987;&#22270;&#20687;&#35201;&#26356;&#20026;&#20934;&#30830;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27700;&#24211;&#25152;&#36798;&#21040;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20063;&#39640;&#20110;&#37319;&#29992;&#20256;&#32479;&#30340;&#20256;&#36755;&#30697;&#38453;&#27169;&#22411;&#65292;&#21518;&#32773;&#26159;&#25551;&#36848;&#36890;&#36807;&#26080;&#24207;&#22120;&#20214;&#20256;&#36882;&#20809;&#30340;&#24191;&#27867;&#25509;&#21463;&#24037;&#20855;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#21407;&#22240;&#22312;&#20110;&#27700;&#24211;&#30340;&#21160;&#21147;&#23398;&#20855;&#26377;&#26356;&#39640;&#30340;&#23481;&#37327;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#65292;&#30456;&#23545;&#20110;&#20256;&#36755;&#30697;&#38453;&#30340;&#32447;&#24615;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of disordered photonics, a common objective is to characterize optically opaque materials for controlling light delivery or performing imaging. Among various complex devices, multi-mode optical fibers stand out as cost-effective and easy-to-handle tools, making them attractive for several tasks. In this context, we leverage the reservoir computing paradigm to recast these fibers into random hardware projectors, transforming an input dataset into a higher dimensional speckled image set. The goal of our study is to demonstrate that using such randomized data for classification by training a single logistic regression layer improves accuracy compared to training on direct raw images. Interestingly, we found that the classification accuracy achieved using the reservoir is also higher than that obtained with the standard transmission matrix model, a widely accepted tool for describing light transmission through disordered devices. We find that the reason for such improved perfo
&lt;/p&gt;</description></item></channel></rss>