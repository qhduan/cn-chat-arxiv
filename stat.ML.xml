<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07471</link><description>&lt;p&gt;
&#26377;&#20851;&#26576;&#20123;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#21450;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the nonconvexity of some push-forward constraints and its consequences in machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#25512;&#36827;&#32422;&#26463;&#30340;&#38750;&#20984;&#24615;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
push-forward&#25805;&#20316;&#20351;&#20154;&#33021;&#22815;&#36890;&#36807;&#30830;&#23450;&#24615;&#26144;&#23556;&#37325;&#26032;&#20998;&#37197;&#27010;&#29575;&#27979;&#24230;&#12290;&#23427;&#22312;&#32479;&#35745;&#21644;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65306;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#65288;&#29305;&#21035;&#26159;&#26469;&#33258;&#26368;&#20248;&#36755;&#36816;&#12289;&#29983;&#25104;&#24314;&#27169;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65289;&#21253;&#25324;&#20316;&#20026;&#27169;&#22411;&#19978;&#30340;&#25512;&#36827;&#26465;&#20214;&#25110;&#22788;&#32602;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#20851;&#20110;&#36825;&#20123;&#32422;&#26463;&#30340;&#65288;&#38750;&#65289;&#20984;&#24615;&#21450;&#20854;&#23545;&#30456;&#20851;&#23398;&#20064;&#38382;&#39064;&#30340;&#24433;&#21709;&#30340;&#19968;&#33324;&#29702;&#35770;&#35265;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#32452;&#20989;&#25968;&#65288;&#23558;&#19968;&#20010;&#27010;&#29575;&#27979;&#24230;&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;&#30340;&#26144;&#23556;&#65307;&#35825;&#23548;&#19981;&#21516;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30456;&#31561;&#36755;&#20986;&#20998;&#24067;&#30340;&#26144;&#23556;&#65289;&#30340;&#65288;&#38750;&#65289;&#20984;&#24615;&#30340;&#19968;&#31995;&#21015;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#36825;&#31361;&#20986;&#20102;&#23545;&#20110;&#22823;&#22810;&#25968;&#27010;&#29575;&#27979;&#24230;&#32780;&#35328;&#65292;&#36825;&#20123;&#25512;&#36827;&#32422;&#26463;&#26159;&#38750;&#20984;&#30340;&#12290;&#22312;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#32467;&#26524;&#22914;&#20309;&#26263;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07471v1 Announce Type: cross  Abstract: The push-forward operation enables one to redistribute a probability measure through a deterministic map. It plays a key role in statistics and optimization: many learning problems (notably from optimal transport, generative modeling, and algorithmic fairness) include constraints or penalties framed as push-forward conditions on the model. However, the literature lacks general theoretical insights on the (non)convexity of such constraints and its consequences on the associated learning problems. This paper aims at filling this gap. In a first part, we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions: the maps transporting one probability measure to another; the maps inducing equal output distributions across distinct probability measures. This highlights that for most probability measures, these push-forward constraints are not convex. In a second time, we show how this result impli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01762</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel. (arXiv:2311.01762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#23427;&#22312;&#25968;&#25454;&#20013;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#21442;&#25968;&#20013;&#26159;&#32447;&#24615;&#30340;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#38381;&#24335;&#35299;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;&#30697;&#38453;&#27714;&#36870;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#23545;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#24102;&#23485;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#33267;&#38646;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#30340;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#24102;&#23485;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#24102;&#23485;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21160;&#24577;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#20197;&#21450;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#32500;&#28151;&#21512;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#21563;&#21512;&#12290;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#20250;&#22312;&#27599;&#19968;&#23618;&#21457;&#29983;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#20250;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;</title><link>http://arxiv.org/abs/2310.03010</link><description>&lt;p&gt;
&#39640;&#32500;&#24230; SGD &#19982;&#26032;&#20852;&#30340;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#30456;&#21563;&#21512;
&lt;/p&gt;
&lt;p&gt;
High-dimensional SGD aligns with emerging outlier eigenspaces. (arXiv:2310.03010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21160;&#24577;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#20197;&#21450;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#32500;&#28151;&#21512;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#21563;&#21512;&#12290;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#20250;&#22312;&#27599;&#19968;&#23618;&#21457;&#29983;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#20250;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21644;&#32463;&#39564;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#35889;&#30340;&#32852;&#21512;&#28436;&#21270;&#65292;&#23545;&#35757;&#32451;&#21160;&#24577;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#22810;&#31867;&#39640;&#32500;&#28151;&#21512;&#21644;1&#25110;2&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#20010;&#20856;&#22411;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;SGD&#36712;&#36857;&#36805;&#36895;&#19982;&#28023;&#26862;&#30697;&#38453;&#21644;&#26799;&#24230;&#30697;&#38453;&#30340;&#26032;&#20852;&#20302;&#31209;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#30456;&#21563;&#21512;&#12290;&#27492;&#22806;&#65292;&#22312;&#22810;&#23618;&#35774;&#32622;&#20013;&#65292;&#36825;&#31181;&#23545;&#40784;&#21457;&#29983;&#22312;&#27599;&#19968;&#23618;&#65292;&#26368;&#21518;&#19968;&#23618;&#30340;&#24322;&#24120;&#29305;&#24449;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28436;&#21270;&#65292;&#24182;&#19988;&#22312;SGD&#25910;&#25947;&#21040;&#20122;&#20248;&#20998;&#31867;&#22120;&#26102;&#34920;&#29616;&#20986;&#31209;&#32570;&#20047;&#12290;&#36825;&#20026;&#36807;&#21435;&#21313;&#24180;&#20013;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#28023;&#26862;&#30697;&#38453;&#21644;&#20449;&#24687;&#30697;&#38453;&#30340;&#35889;&#30340;&#24191;&#27867;&#25968;&#20540;&#30740;&#31350;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17570</link><description>&lt;p&gt;
&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#12289;&#39640;&#25928;&#12289;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#24050;&#37096;&#32626;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#30456;&#27604;&#20043;&#21069;&#20381;&#36182;&#20110;&#22266;&#23450;&#26679;&#26412;&#37327;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#24207;&#36143;&#30340;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#36319;&#36394;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20063;&#20801;&#35768;&#25968;&#25454;&#36890;&#36807;&#27010;&#29575;&#31574;&#30053;&#36827;&#34892;&#25910;&#38598;&#65292;&#32780;&#19981;&#26159;&#20174;&#20154;&#21475;&#20013;&#22343;&#21248;&#37319;&#26679;&#12290;&#36825;&#20351;&#24471;&#23457;&#35745;&#21487;&#20197;&#22312;&#20026;&#20854;&#20182;&#30446;&#30340;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#12290;&#27492;&#22806;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#38543;&#26102;&#38388;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#23376;&#20154;&#32676;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22240;&#27169;&#22411;&#21464;&#26356;&#25110;&#22522;&#30784;&#20154;&#32676;&#21464;&#26356;&#23548;&#33268;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110; anytime-valid &#25512;&#26029;&#21644;&#21338;&#24328;&#32479;&#35745;&#23398;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;"&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#27979;&#35797;"&#26694;&#26550;&#12290;&#36825;&#20123;&#32852;&#31995;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#24555;&#36895;&#21644;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#38024;&#23545;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#65292;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.18233</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference on eigenvectors of non-symmetric matrices. (arXiv:2303.18233v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#38750;&#23545;&#31216;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#38024;&#23545;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#65292;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;Tyler&#65288;1981&#65289;&#30340;&#21487;&#23545;&#31216;&#21270;&#26465;&#20214;&#24182;&#38750;&#24314;&#31435;&#28176;&#36817;&#25512;&#26029;&#29305;&#24449;&#21521;&#37327;&#31243;&#24207;&#25152;&#24517;&#38656;&#30340;&#12290; &#25105;&#20204;&#20026;&#23436;&#20840;&#21521;&#37327;&#21644;&#27599;&#20010;&#31995;&#25968;&#20551;&#35774;&#20998;&#21035;&#24314;&#31435;&#20102; Wald &#21644; t &#26816;&#39564;&#30340;&#20998;&#24067;&#29702;&#35770;&#12290; &#25105;&#20204;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#26469;&#28304;&#20110;&#38750;&#23545;&#31216;&#30697;&#38453;&#30340;&#29305;&#24449;&#25237;&#24433;&#12290; &#36890;&#36807;&#23558;&#25237;&#24433;&#34920;&#31034;&#20026;&#20174;&#22522;&#30784;&#30697;&#38453;&#21040;&#20854;&#35889;&#25968;&#25454;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#26512;&#25668;&#21160;&#29702;&#35770;&#25214;&#21040;&#20102;&#23548;&#25968;&#12290; &#36825;&#20123;&#32467;&#26524;&#28436;&#31034;&#20102; Sun&#65288;1991&#65289;&#30340;&#35299;&#26512;&#25668;&#21160;&#29702;&#35770;&#26159;&#22810;&#20803;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#31181;&#26377;&#29992;&#24037;&#20855;&#65292;&#24182;&#19988;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#20316;&#20026;&#19968;&#31181;&#24212;&#29992;&#65292;&#25105;&#20204;&#20026;&#30001;&#26377;&#21521;&#22270;&#24341;&#21457;&#30340;&#37051;&#25509;&#30697;&#38453;&#20272;&#35745;&#30340; Bonacich &#20013;&#24515;&#24615;&#23450;&#20041;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper argues that the symmetrisability condition in Tyler(1981) is not necessary to establish asymptotic inference procedures for eigenvectors. We establish distribution theory for a Wald and t-test for full-vector and individual coefficient hypotheses, respectively. Our test statistics originate from eigenprojections of non-symmetric matrices. Representing projections as a mapping from the underlying matrix to its spectral data, we find derivatives through analytic perturbation theory. These results demonstrate how the analytic perturbation theory of Sun(1991) is a useful tool in multivariate statistics and are of independent interest. As an application, we define confidence sets for Bonacich centralities estimated from adjacency matrices induced by directed graphs.
&lt;/p&gt;</description></item></channel></rss>