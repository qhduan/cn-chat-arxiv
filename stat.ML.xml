<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11940</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent. (arXiv:2401.11940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#23569;&#37327;&#34987;&#30772;&#22351;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#20855;&#26377;&#20302;&#32990;&#29366;&#31209;&#32467;&#26500;&#30340;&#24352;&#37327;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#20284;&#20110;Burer-Monteiro&#65288;BM&#65289;&#26041;&#27861;&#30340;&#20998;&#35299;&#36807;&#31243;&#30340;&#39640;&#25928;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#26412;&#26041;&#27861;&#28041;&#21450;&#23558;&#19968;&#20010;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#65288;FGD&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#28040;&#38500;&#20102;t-SVD&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20445;&#35777;FGD&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.03696</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#22823;&#31867;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;/&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21464;&#20998;&#20248;&#21270;&#24615;&#65288;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#31639;&#23376;&#21644;k-&#24179;&#38754;&#21464;&#25442;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#34920;&#31034;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#35828;&#26126;&#22312;&#36825;&#20123;Banach&#31354;&#38388;&#19978;&#25552;&#20986;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#30340;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#24687;&#24687;&#30456;&#20851;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#31070;&#32463;&#32593;&#32476;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#21253;&#25324;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#12289;&#33539;&#25968;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;&#22312;&#34180;&#26495;/&#22810;&#27425;&#35856;&#27874;&#26679;&#26465;&#29702;&#35770;&#20013;&#25214;&#21040;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#22312;&#20869;&#30340;&#22810;&#31181;&#32463;&#20856;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.13059</link><description>&lt;p&gt;
&#29992;&#29109;&#36817;&#20284;&#30340;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13059
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#26080;&#27861;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20197;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21333;&#23792;&#30340;&#39640;&#26031;&#20998;&#24067;&#36890;&#24120;&#34987;&#36873;&#25321;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#65292;&#24456;&#38590;&#36924;&#36817;&#22810;&#23792;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#22914;&#20309;&#36817;&#20284;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35745;&#31639;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30495;&#23454;&#29109;&#19982;&#36817;&#20284;&#29109;&#20043;&#38388;&#30340;&#36817;&#20284;&#35823;&#24046;&#65292;&#20197;&#20415;&#25581;&#31034;&#25105;&#20204;&#30340;&#36817;&#20284;&#20309;&#26102;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36817;&#20284;&#35823;&#24046;&#30001;&#39640;&#26031;&#28151;&#21512;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#19982;&#26041;&#24046;&#20043;&#21644;&#30340;&#27604;&#29575;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#24403;&#39640;&#26031;&#28151;&#21512;&#32452;&#20214;&#30340;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36817;&#20284;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
&lt;/p&gt;</description></item></channel></rss>