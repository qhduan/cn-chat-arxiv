<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06173</link><description>&lt;p&gt;
SMC&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#24182;&#34892;&#24378;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SMC Is All You Need: Parallel Strong Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06173
&lt;/p&gt;
&lt;p&gt;
SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#20998;&#24067;&#21482;&#33021;&#25353;&#27604;&#20363;&#24120;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#20256;&#32479;&#30340;&#19968;&#33268;Bayesian&#26041;&#27861;&#65292;&#22914;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(SMC)&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#65292;&#20855;&#26377;&#26080;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#24182;&#34892;&#30340;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(pSMC)&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#20855;&#26377;&#24182;&#34892;&#24378;&#25193;&#23637;&#24615;&#65292;&#21363;&#22914;&#26524;&#20801;&#35768;&#24322;&#27493;&#36827;&#31243;&#25968;&#37327;&#22686;&#38271;&#65292;&#26102;&#38388;&#22797;&#26434;&#24615;(&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23384;)&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;pSMC&#20855;&#26377;MSE$=O(1/NR)$&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$N$&#34920;&#31034;&#27599;&#20010;&#22788;&#29702;&#22120;&#20013;&#30340;&#36890;&#20449;&#26679;&#26412;&#25968;&#37327;&#65292;$R$&#34920;&#31034;&#22788;&#29702;&#22120;&#25968;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#36866;&#24403;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;$N$&#65292;&#24403;$R\rightarrow \infty$&#26102;&#65292;&#35813;&#26041;&#27861;&#20197;&#22266;&#23450;&#26377;&#38480;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;Cost$=O(1)$&#25910;&#25947;&#21040;&#26080;&#31351;&#23567;&#31934;&#24230;MSE$=O(\varepsilon^2)$&#65292;&#27809;&#26377;&#25928;&#29575;&#27844;&#28431;&#65292;&#21363;&#35745;&#31639;&#22797;&#26434;&#24615;Cost$=O(\varepsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00162</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#25506;&#32034;&#32972;&#21518;&#30340;&#31070;&#35805;
&lt;/p&gt;
&lt;p&gt;
Behind the Myth of Exploration in Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#26159;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#22312;&#23454;&#36341;&#20013;&#24517;&#39035;&#22312;&#23398;&#20064;&#30446;&#26631;&#20013;&#21253;&#21547;&#25506;&#32034;&#39033;&#12290;&#23613;&#31649;&#36825;&#20123;&#39033;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25506;&#32034;&#29615;&#22659;&#30340;&#20869;&#22312;&#38656;&#27714;&#36827;&#34892;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#20004;&#31181;&#19981;&#21516;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20351;&#24471;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#20445;&#25345;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#20272;&#35745;&#65292;&#22686;&#21152;&#20102;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26368;&#32456;&#25552;&#20379;&#26368;&#20248;&#31574;&#30053;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#25928;&#24212;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#31361;&#20986;&#20102;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#20123;&#31574;&#30053;&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
&lt;/p&gt;</description></item></channel></rss>