<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2110.06257</link><description>&lt;p&gt;
&#20174;&#26377;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Conditionally Stationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.06257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65292;&#21363;&#20174;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#28508;&#22312;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;AI&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#25361;&#25112;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#32972;&#26223;&#19979;&#65292;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#20855;&#26377;&#23436;&#20840;&#35266;&#27979;&#21464;&#37327;&#21644;/&#25110;&#26469;&#33258;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#30340;&#21463;&#38480;&#22330;&#26223;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#26469;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#21363;&#22312;&#26465;&#20214;&#19978;&#26159;&#24179;&#31283;&#30340;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#38750;&#24179;&#31283;&#34892;&#20026;&#34987;&#24314;&#27169;&#20026;&#22312;&#19968;&#32452;&#65288;&#21487;&#33021;&#26159;&#38544;&#34255;&#30340;&#65289;&#29366;&#24577;&#21464;&#37327;&#19978;&#30340;&#24179;&#31283;&#24615;&#12290;&#21629;&#21517;&#20026;State-Dependent Causal Inference&#65288;SDCI&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#21487;&#35777;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#35777;&#26126;&#22312;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#19979;&#65292;&#24182;&#22312;&#23384;&#22312;&#38544;&#34255;&#29366;&#24577;&#26102;&#32463;&#39564;&#24615;&#22320;&#23454;&#29616;&#12290;&#21518;&#32773;&#36890;&#36807;&#23545;&#21512;&#25104;&#32447;&#24615;&#31995;&#32479;&#21644;&#38750;&#32447;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#30340;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;SDCI&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#32447;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.06257v2 Announce Type: replace  Abstract: Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2001.01095</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;: &#36890;&#36807;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Independence Testing via Maximum and Average Distance Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#22810;&#20803;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#34920;&#24449;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#36793;&#38469;&#30456;&#20851;&#32500;&#24230;&#25968;&#37327;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20248;&#21183;&#65292;&#26816;&#26597;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#38646;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#26816;&#27979;&#31243;&#24207;&#12290;&#24471;&#20986;&#30340;&#26816;&#39564;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#20316;&#20026;&#24213;&#23618;&#24230;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#20803;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#26368;&#22823;&#36317;&#31163;&#30456;&#20851;&#24615;&#12289;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#21644;&#21407;&#22987;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#20197;&#26816;&#27979;&#20154;&#31867;&#34880;&#27974;&#20013;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#21644;&#32957;&#27700;&#24179;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04952</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Training Methods for Scalable Neural Implicit Sampler. (arXiv:2306.04952v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#38750;&#26631;&#20934;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#26159;&#31185;&#23398;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#39532;&#23572;&#31185;&#22827;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#21487;&#20445;&#35777;&#20174;&#36825;&#20123;&#20998;&#24067;&#20013;&#28176;&#36827;&#26080;&#20559;&#37319;&#26679;&#65292;&#20294;&#22312;&#22788;&#29702;&#39640;&#32500;&#30446;&#26631;&#26102;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#29983;&#25104;&#19968;&#25209;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#30452;&#25509;&#23558;&#26131;&#20110;&#37319;&#26679;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#30340;&#31070;&#32463;&#21464;&#25442;&#65292;&#21487;&#20197;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;&#20026;&#20102;&#35757;&#32451;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65306;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#12290;&#21069;&#32773;&#26368;&#23567;&#21270;Kullback-Leibler&#25955;&#24230;&#65292;&#32780;&#21518;&#32773;&#21017;&#26368;&#23567;&#21270;Fisher&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches like Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we propose an efficient and scalable neural implicit sampler that overcomes these limitations. Our sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit sampler, we introduce two novel methods: the KL training method and the Fisher training method. The former minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence. By empl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.09900</link><description>&lt;p&gt;
&#26080;&#20132;&#21472;&#31574;&#30053;&#23398;&#20064;&#65306;&#24754;&#35266;&#21644;&#24191;&#20041;&#32463;&#39564;Bernstein&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#65292;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#21040;&#30340;&#35266;&#27979;&#65288;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#25110;&#26159;&#36866;&#24212;&#28436;&#21464;&#30340;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#23398;&#20064;&#32473;&#23450;&#31867;&#21035;&#20013;&#30340;&#26368;&#20248;&#20010;&#24615;&#21270;&#20915;&#31574;&#35268;&#21017;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#21363;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#25152;&#26377;&#20010;&#24615;&#21270;&#29305;&#24449;&#30340;&#25152;&#26377;&#21160;&#20316;&#30340;&#20542;&#21521;&#24615;&#19979;&#30028;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26368;&#22351;&#30340;&#20542;&#21521;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#19981;&#21463;&#25511;&#21046;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#22826;&#29616;&#23454;&#65292;&#29305;&#21035;&#26159;&#24403;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#19988;&#20542;&#21521;&#24615;&#20943;&#24369;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#20248;&#21270;&#31574;&#30053;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#8212;&#8212;&#32780;&#19981;&#26159;&#28857;&#20272;&#35745;&#12290;LCBs&#36890;&#36807;&#37327;&#21270;&#22686;&#24378;&#20498;&#25968;&#20542;&#21521;&#26435;&#37325;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
&lt;/p&gt;</description></item></channel></rss>