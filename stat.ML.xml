<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#26159;&#19968;&#20010;&#24179;&#22343;&#22797;&#26434;&#24230;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.05765</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the average-case complexity of learning output distributions of quantum circuits. (arXiv:2305.05765v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#26159;&#19968;&#20010;&#24179;&#22343;&#22797;&#26434;&#24230;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#30340;&#36755;&#20986;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22312;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#19979;&#65292;&#35813;&#38382;&#39064;&#30340;&#24179;&#22343;&#22797;&#26434;&#24230;&#26159;&#22256;&#38590;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#28145;&#24230;&#20026;$d$&#12289;&#30001;$n$&#20010;&#37327;&#23376;&#27604;&#29305;&#26500;&#25104;&#30340;&#30742;&#22681;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;&#22312;&#36229;&#23545;&#25968;&#30005;&#36335;&#28145;&#24230;$d=\omega(\log(n))$&#26102;&#65292;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892;&#36229;&#22810;&#39033;&#24335;&#27425;&#25968;&#30340;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;&#24658;&#23450;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#23384;&#22312;&#19968;&#20010;$d=O(n)$&#65292;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#36827;&#34892;$\Omega(2^n)$&#27425;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;$O(2^{-n})$&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#22312;&#26080;&#38480;&#30005;&#36335;&#28145;&#24230;$d\to\infty$&#26102;&#65292;&#20219;&#20309;&#23398;&#20064;&#31639;&#27861;&#37117;&#38656;&#35201;&#36827;&#34892;$2^{2^{\Omega(n)}}$&#27425;&#26597;&#35810;&#25165;&#33021;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#23454;&#29616;$2^{-2^{\Omega(n)}}$&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#36741;&#21161;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;......&#65288;&#25991;&#31456;&#20869;&#23481;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
In this work, we show that learning the output distributions of brickwork random quantum circuits is average-case hard in the statistical query model. This learning model is widely used as an abstract computational model for most generic learning algorithms. In particular, for brickwork random quantum circuits on $n$ qubits of depth $d$, we show three main results:  - At super logarithmic circuit depth $d=\omega(\log(n))$, any learning algorithm requires super polynomially many queries to achieve a constant probability of success over the randomly drawn instance.  - There exists a $d=O(n)$, such that any learning algorithm requires $\Omega(2^n)$ queries to achieve a $O(2^{-n})$ probability of success over the randomly drawn instance.  - At infinite circuit depth $d\to\infty$, any learning algorithm requires $2^{2^{\Omega(n)}}$ many queries to achieve a $2^{-2^{\Omega(n)}}$ probability of success over the randomly drawn instance.  As an auxiliary result of independent interest, we show 
&lt;/p&gt;</description></item></channel></rss>