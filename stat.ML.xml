<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.06942</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#30005;&#32593;&#30417;&#27979;&#21644;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#19968;&#20195;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#65292;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#25512;&#26029;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;AI&#25903;&#25345;&#30340;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#30340;&#30417;&#27979;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#22522;&#20110;SCADA&#21644;&#21516;&#27493;&#30456;&#37327;&#25216;&#26415;&#26500;&#24314;&#30340;&#24191;&#22495;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06942v1 Announce Type: cross  Abstract: Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28085;&#30422;&#25152;&#26377;&#23454;&#38469;&#24773;&#20917;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#27169;&#22411;&#30830;&#20999;&#27867;&#21270;&#20445;&#35777;&#65292;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20256;&#36755;&#25104;&#26412;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11981</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#40065;&#26834;&#27169;&#22411;&#30340;&#36890;&#29992;&#27867;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Universal Generalization Guarantees for Wasserstein Distributionally Robust Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28085;&#30422;&#25152;&#26377;&#23454;&#38469;&#24773;&#20917;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#27169;&#22411;&#30830;&#20999;&#27867;&#21270;&#20445;&#35777;&#65292;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20256;&#36755;&#25104;&#26412;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#35757;&#32451;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21560;&#24341;&#20154;&#26041;&#24335;&#65292;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#32479;&#35745;&#20998;&#26512;&#35777;&#26126;&#65292;&#22522;&#20110;Wasserstein&#27169;&#31946;&#38598;&#26500;&#24314;&#30340;&#40065;&#26834;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#25171;&#30772;&#20102;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#65292;&#20197;&#36817;&#20284;&#20195;&#20215;&#33719;&#24471;&#65292;&#25110;&#32773;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#39564;&#35777;&#30340;&#20551;&#35774;&#19979;&#33719;&#24471;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#28085;&#30422;&#25152;&#26377;&#23454;&#38469;&#24773;&#20917;&#30340;&#30830;&#20999;&#27867;&#21270;&#20445;&#35777;&#65292;&#21253;&#25324;&#20219;&#20309;&#20256;&#36755;&#25104;&#26412;&#20989;&#25968;&#21644;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#33021;&#26159;&#38750;&#20984;&#21644;&#38750;&#24179;&#28369;&#30340;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#23558;&#38750;&#24179;&#28369;&#20998;&#26512;&#29702;&#35770;&#19982;&#32463;&#20856;&#38598;&#20013;&#32467;&#26524;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#35777;&#26126;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#19968;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#25299;&#23637;&#33267;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11981v1 Announce Type: cross  Abstract: Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that robust models built from Wasserstein ambiguity sets have nice generalization guarantees, breaking the curse of dimensionality. However, these results are obtained in specific cases, at the cost of approximations, or under assumptions difficult to verify in practice. In contrast, we establish, in this article, exact generalization guarantees that cover all practical cases, including any transport cost function and any loss function, potentially non-convex and nonsmooth. For instance, our result applies to deep learning, without requiring restrictive assumptions. We achieve this result through a novel proof technique that combines nonsmooth analysis rationale with classical concentration results. Our approach is general enough to ext
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#20197;&#21450;Wasserstein&#36317;&#31163;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An analysis of the noise schedule for score-based generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22122;&#22768;&#35843;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#20197;&#21450;Wasserstein&#36317;&#31163;&#30340;&#25913;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26088;&#22312;&#36890;&#36807;&#20165;&#20351;&#29992;&#30446;&#26631;&#25968;&#25454;&#30340;&#22122;&#22768;&#25200;&#21160;&#26679;&#26412;&#26469;&#23398;&#20064;&#24471;&#20998;&#20989;&#25968;&#65292;&#20174;&#32780;&#20272;&#35745;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;KL&#25955;&#24230;&#21644;Wasserstein&#36317;&#31163;&#26469;&#34913;&#37327;&#29983;&#25104;&#36136;&#37327;&#12290;&#33267;&#20170;&#20026;&#27490;&#65292;&#25152;&#26377;&#29616;&#26377;&#32467;&#26524;&#37117;&#26159;&#38024;&#23545;&#26102;&#38388;&#22343;&#21248;&#21464;&#21270;&#30340;&#22122;&#22768;&#35843;&#24230;&#24471;&#21040;&#30340;&#12290;&#22312;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#30446;&#26631;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#30340;&#19978;&#30028;&#65292;&#26126;&#30830;&#20381;&#36182;&#20110;&#20219;&#20309;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#20551;&#35774;&#24471;&#20998;&#26159;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;Wasserstein&#36317;&#31163;&#35823;&#24046;&#30028;&#38480;&#65292;&#21033;&#29992;&#20102;&#26377;&#21033;&#30340;&#25910;&#32553;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#19978;&#30028;&#33258;&#21160;&#35843;&#33410;&#22122;&#22768;&#35843;&#24230;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target. Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances.  All existing results  have been obtained so far for time-homogeneous speed of the noise schedule.  Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. Assuming that the score is Lipschitz continuous, we provide an improved error bound in Wasserstein distance, taking advantage of favourable underlying contraction mechanisms. We also propose an algorithm to automatically tune the noise schedule using the proposed upper bound. We illustrate empirically the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12824</link><description>&lt;p&gt;
MAPPING: &#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage. (arXiv:2401.12824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32487;&#25215;&#24182;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#21382;&#21490;&#19978;&#30340;&#20559;&#35265;&#21644;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#20020;&#24202;&#35786;&#26029;&#12289;&#37329;&#34701;&#20449;&#36151;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#65292;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#22797;&#21046;&#21040;&#20855;&#26377;&#25299;&#25169;&#20381;&#36182;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#22270;&#23398;&#20064;&#36890;&#24120;&#20559;&#22909;&#20110;&#20351;&#29992;&#25104;&#23545;&#32422;&#26463;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#20294;&#26080;&#27861;&#20811;&#26381;&#32500;&#24230;&#38480;&#21046;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65307;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#22788;&#29702;&#25216;&#26415;&#19978;&#26469;&#24378;&#21046;&#24182;&#35843;&#25972;&#20844;&#24179;&#24615;&#65292;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;GNN&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;GNN&#24448;&#24448;&#20542;&#21521;&#20110;&#22686;&#24378;&#20844;&#24179;&#24615;&#25110;&#22686;&#21152;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20108;&#32773;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#26435;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. Furthermore, previous work on GNNs tend to enhance either fairness or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12395</link><description>&lt;p&gt;
&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Diffusion Models. (arXiv:2310.12395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGMs)&#36890;&#36807;&#36845;&#20195;&#22320;&#20351;&#29992;&#25200;&#21160;&#30446;&#26631;&#20989;&#25968;&#30340;&#24471;&#20998;&#20989;&#25968;&#26469;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#38381;&#24335;&#22320;&#35780;&#20272;&#36825;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#30001;&#27492;&#24471;&#21040;&#30340;SGMs&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#19981;&#33021;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#36825;&#31181;&#36817;&#20284;&#30340;&#35823;&#24046;&#26377;&#21161;&#20110;&#25512;&#24191;&#65292;&#28982;&#32780;&#31070;&#32463;SGMs&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20195;&#20215;&#39640;&#65292;&#32780;&#19988;&#23545;&#20110;&#36825;&#31181;&#35823;&#24046;&#25552;&#20379;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#26469;&#33719;&#24471;&#19968;&#20010;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;SGMs&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#39640;&#25928;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#12290;&#21033;&#29992;&#36825;&#20010;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06943</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of hyperparameters on variable selection in random forests. (arXiv:2309.06943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#36866;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#21644;&#21464;&#37327;&#36873;&#25321;&#12290;&#20808;&#21069;&#30740;&#31350;&#20102;RF&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#23545;&#39044;&#27979;&#24615;&#33021;&#21644;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#65292;&#20294;&#36229;&#21442;&#25968;&#23545;&#22522;&#20110;RF&#30340;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#29702;&#35770;&#20998;&#24067;&#21644;&#23454;&#35777;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#36827;&#34892;&#20102;&#20004;&#20010;&#27169;&#25311;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;Vita&#21644;Boruta&#21464;&#37327;&#36873;&#25321; procedures &#22312;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#65288;&#25935;&#24863;&#24615;&#65289;&#30340;&#21516;&#26102;&#25511;&#21046;&#34394;&#35686;&#29575;&#65288;FDR&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#35201;&#27604;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25277;&#21462;&#31574;&#30053;&#21644;&#26368;&#23567;&#32456;&#31471;&#33410;&#28857;&#22823;&#23567;&#26356;&#33021;&#24433;&#21709;&#36873;&#25321; procedures&#12290;RF&#36229;&#21442;&#25968;&#30340;&#21512;&#36866;&#35774;&#32622;&#21462;&#20915;&#20110;
&lt;/p&gt;
&lt;p&gt;
Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.14335</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26680;&#20998;&#24067;&#22238;&#24402;&#23398;&#20064;&#29702;&#35770;&#19982;&#20004;&#38454;&#27573;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved learning theory for kernel distribution regression with two-stage sampling. (arXiv:2308.14335v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#28085;&#30422;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#26680;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#39318;&#36873;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26680;&#20998;&#24067;&#22238;&#24402;&#22312;&#35745;&#31639;&#19978;&#26159;&#26377;&#21033;&#30340;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#29702;&#35770;&#30340;&#25903;&#25345;&#12290;&#35813;&#29702;&#35770;&#36824;&#35299;&#20915;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21482;&#26377;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#23884;&#20837;&#30340;&#26680;&#65292;&#36825;&#20123;&#26680;&#21253;&#21547;&#20102;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#20837;&#30340;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#20998;&#26512;&#25552;&#20379;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#23545;&#19977;&#20010;&#37325;&#35201;&#30340;&#26680;&#31867;&#21035;&#25104;&#31435;&#65292;&#36825;&#20123;&#26680;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#24179;&#22343;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution regression problem encompasses many important statistics and machine learning tasks, and arises in a large range of applications. Among various existing approaches to tackle this problem, kernel methods have become a method of choice. Indeed, kernel distribution regression is both computationally favorable, and supported by a recent learning theory. This theory also tackles the two-stage sampling setting, where only samples from the input distributions are available. In this paper, we improve the learning theory of kernel distribution regression. We address kernels based on Hilbertian embeddings, that encompass most, if not all, of the existing approaches. We introduce the novel near-unbiased condition on the Hilbertian embeddings, that enables us to provide new error bounds on the effect of the two-stage sampling, thanks to a new analysis. We show that this near-unbiased condition holds for three important classes of kernels, based on optimal transport and mean embedd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09254</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#22686;&#24378;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;&#29983;&#25104;&#34394;&#26500;&#20107;&#23454;&#30340;&#25285;&#24551;&#65292;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#29305;&#21035;&#24378;&#35843;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#39044;&#27979;&#38598;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#26041;&#24335;&#37327;&#21270;GLM&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#38598;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#20540;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#39044;&#27979;&#38598;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20173;&#28385;&#36275;PAC&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#20845;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16703</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304; -- &#19968;&#20010;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#20960;&#24180;&#21069;&#38590;&#20197;&#24819;&#35937;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25104;&#21151;&#20043;&#22806;&#65292;&#36234;&#26469;&#36234;&#28165;&#26224;&#30340;&#26159;&#65292;&#22312;&#32431;&#39044;&#27979;&#20043;&#22806;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20063;&#26159;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#20986;&#29616;&#20102;&#36825;&#26041;&#38754;&#30340;&#31532;&#19968;&#25209;&#27010;&#24565;&#21644;&#24605;&#24819;&#65292;&#20294;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#26356;&#24120;&#35265;&#30456;&#20851;&#30340;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35268;&#33539;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21508;&#24322;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#32479;&#35745;&#23398;&#27010;&#24565;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#22312;&#24213;&#23618;&#31639;&#23376;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#31532;&#20108;&#31181;&#31639;&#27861;&#26159;&#19968;&#31181;&#21152;&#36895;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#35299;&#30340;&#23384;&#22312;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2301.03113</link><description>&lt;p&gt;
&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#26681;&#26597;&#25214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Randomized Block-Coordinate Optimistic Gradient Algorithms for Root-Finding Problems. (arXiv:2301.03113v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#22312;&#24213;&#23618;&#31639;&#23376;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#31532;&#20108;&#31181;&#31639;&#27861;&#26159;&#19968;&#31181;&#21152;&#36895;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#35299;&#30340;&#23384;&#22312;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#36817;&#20284;&#27714;&#35299;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#20063;&#31216;&#20026;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#20351;&#29992;&#24658;&#23450;&#30340;&#27493;&#38271;&#65292;&#38750;&#21152;&#36895;&#31639;&#27861;&#65292;&#22312;&#24213;&#23618;&#31639;&#23376;G&#28385;&#36275;Lipschitz&#36830;&#32493;&#24615;&#21644;&#24369;Minty&#35299;&#26465;&#20214;&#26102;&#65292;&#23427;&#22312;&#25968;&#23398;&#26399;&#26395;E[||Gx^k||^2]&#19978;&#36798;&#21040;O(1/k)&#30340;&#26368;&#20248;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;E[&#183;]&#34920;&#31034;&#26399;&#26395;&#65292;k&#20026;&#36845;&#20195;&#35745;&#25968;&#22120;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#65292;&#22312;G&#30340;&#22841;&#36924;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#22312;E[||Gx^k||^2]&#21644;E[||x^{k+1} x^{k}||^2]&#19978;&#20998;&#21035;&#36798;&#21040;O(1/k^2)&#21644;o(1/k^2)&#30340;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#36845;&#20195;&#24207;&#21015;{x^k}&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#65292;&#20197;&#21450;&#22312;&#27492;&#35299;&#22788;Gx^k&#30340;&#27169;&#30340;&#24179;&#26041;&#22312;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop two new randomized block-coordinate optimistic gradient algorithms to approximate a solution of nonlinear equations in large-scale settings, which are called root-finding problems. Our first algorithm is non-accelerated with constant stepsizes, and achieves $\mathcal{O}(1/k)$ best-iterate convergence rate on $\mathbb{E}[ \Vert Gx^k\Vert^2]$ when the underlying operator $G$ is Lipschitz continuous and satisfies a weak Minty solution condition, where $\mathbb{E}[\cdot]$ is the expectation and $k$ is the iteration counter. Our second method is a new accelerated randomized block-coordinate optimistic gradient algorithm. We establish both $\mathcal{O}(1/k^2)$ and $o(1/k^2)$ last-iterate convergence rates on both $\mathbb{E}[ \Vert Gx^k\Vert^2]$ and $\mathbb{E}[ \Vert x^{k+1} x^{k}\Vert^2]$ for this algorithm under the co-coerciveness of $G$. In addition, we prove that the iterate sequence $\{x^k\}$ converges to a solution almost surely, and $\Vert Gx^k\Vert^2$ at
&lt;/p&gt;</description></item></channel></rss>