<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12859</link><description>&lt;p&gt;
&#20855;&#26377;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#21407;&#22987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Primal Methods for Variational Inequality Problems with Functional Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#22240;&#20854;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#36816;&#31609;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#22791;&#21463;&#35748;&#21487;&#12290; &#39318;&#27425;&#26041;&#27861;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290; &#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#25110;&#32447;&#24615;&#26368;&#23567;&#21270;&#23637;&#24320;&#22120;&#26469;&#23548;&#33322;&#21487;&#34892;&#38598;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20250;&#22312;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290; &#35299;&#20915;&#36825;&#20123;&#21151;&#33021;&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;Lagrange&#20989;&#25968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#19978;&#12290; &#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#23384;&#22312;&#24182;&#19988;&#20107;&#20808;&#20102;&#35299;&#26368;&#20339;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#12290; &#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12859v1 Announce Type: cross  Abstract: Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.11665</link><description>&lt;p&gt;
&#32479;&#35745;&#23545;&#27969;&#24418;&#20551;&#35774;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Statistical exploration of the Manifold Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.11665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;&#29702;&#35770;&#65292;&#23427;&#35748;&#20026;&#21517;&#20041;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#23454;&#38469;&#19978;&#38598;&#20013;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#20013;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#36825;&#31181;&#36890;&#29992;&#19988;&#38750;&#24120;&#31616;&#21333;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#20016;&#23500;&#32780;&#26377;&#26102;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#12289;&#30456;&#20851;&#24615;&#21644;&#24179;&#31283;&#24615;&#31561;&#22522;&#26412;&#27010;&#24565;&#12290;&#36825;&#20026;&#20026;&#20160;&#20040;&#27969;&#24418;&#20551;&#35774;&#22312;&#36825;&#20040;&#22810;&#24773;&#20917;&#19979;&#20284;&#20046;&#25104;&#31435;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#22312;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>