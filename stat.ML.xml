<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17852</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;&#20559;&#35265;&#27491;&#20132;&#30340;&#26041;&#24335;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness through Transforming Data Orthogonal to Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20915;&#31574;&#65292;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24453;&#36935;&#19981;&#24179;&#31561;&#12290;&#23613;&#31649;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#22810;&#20803;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#23545;&#20915;&#31574;&#32467;&#26524;&#30340;&#24494;&#22937;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#21363;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#26088;&#22312;&#28040;&#38500;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20013;&#32852;&#21512;&#27491;&#24577;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#21363;&#21487;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;OB&#31639;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06525</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#26080;&#38480;&#23485;&#22270;&#21367;&#31215;&#32593;&#32476;&#21450;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Flexible infinite-width graph convolutional networks and the importance of representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#24120;&#35265;&#29702;&#35770;&#26041;&#27861;&#26159;&#36827;&#34892;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#65292;&#27492;&#26102;&#36755;&#20986;&#25104;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#24067;&#12290;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#12290;&#28982;&#32780;&#65292;NNGP&#20869;&#26680;&#26159;&#22266;&#23450;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;&#23569;&#37327;&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#19982;&#26377;&#38480;&#23485;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#36890;&#24120;&#34987;&#35748;&#20026;&#33021;&#22815;&#34920;&#29616;&#33391;&#22909;&#65292;&#27491;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20351;&#20854;&#22312;&#29702;&#35770;&#19978;&#21487;&#22788;&#29702;&#30340;&#21516;&#26102;&#65292;NNGP&#21487;&#33021;&#20250;&#28040;&#38500;&#20351;&#20854;&#24037;&#20316;&#33391;&#22909;&#30340;&#22240;&#32032;&#65288;&#34920;&#31034;&#23398;&#20064;&#65289;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#24517;&#35201;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#24037;&#20855;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#12290;&#36825;&#19982;NNGP&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#20026;&#23427;&#26159;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#24182;&#20351;&#29992;&#20869;&#26680;&#65292;&#20294;&#23427;&#24102;&#26377;&#19968;&#20010;&#8220;&#26059;&#38062;&#8221;&#26469;&#25511;&#21046;&#34920;&#31034;&#23398;&#20064;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well precisely because they are able to learn representations. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of graph classification tasks. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.05019</link><description>&lt;p&gt;
SA-Solver&#65306;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#20122;&#24403;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25913;&#36827;&#30340;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#32771;&#34385;&#35299;&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#37319;&#26679;&#21487;&#20197;&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#23545;&#38543;&#26426;&#37319;&#26679;&#30340;&#32508;&#21512;&#20998;&#26512;&#65306;&#26041;&#24046;&#25511;&#21046;&#30340;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#32447;&#24615;&#22810;&#27493;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SA-Solver&#65292;&#23427;&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SA-Solver&#23454;&#29616;&#20102;&#65306;1&#65289;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#24615;&#33021;&#65307;2&#65289;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.17501</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient uniform approximation using Random Vector Functional Link networks. (arXiv:2306.17501v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;(RVFL)&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#36825;&#31181;&#26550;&#26500;&#21482;&#38656;&#35201;&#23398;&#20064;&#22806;&#37096;&#26435;&#37325;&#65292;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#31616;&#21270;&#20026;&#32447;&#24615;&#20248;&#21270;&#20219;&#21153;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#20854;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#23613;&#31649;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;&#20197;$L_2$&#26041;&#24335;&#21487;&#20197;&#23454;&#29616;&#36825;&#26679;&#30340;&#36924;&#36817;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#24773;&#20917;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#30340;&#19979;&#30028;&#65292;&#21462;&#20915;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#12289;&#26399;&#26395;&#30340;&#20934;&#30830;&#24230;&#21644;&#36755;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#26681;&#26893;&#20110;&#27010;&#29575;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Random Vector Functional Link (RVFL) network is a depth-2 neural network with random inner weights and biases. As only the outer weights of such architectures need to be learned, the learning process boils down to a linear optimization task, allowing one to sidestep the pitfalls of nonconvex optimization problems. In this paper, we prove that an RVFL with ReLU activation functions can approximate Lipschitz continuous functions provided its hidden layer is exponentially wide in the input dimension. Although it has been established before that such approximation can be achieved in $L_2$ sense, we prove it for $L_\infty$ approximation error and Gaussian inner weights. To the best of our knowledge, our result is the first of this kind. We give a nonasymptotic lower bound for the number of hidden layer nodes, depending on, among other things, the Lipschitz constant of the target function, the desired accuracy, and the input dimension. Our method of proof is rooted in probability theory an
&lt;/p&gt;</description></item></channel></rss>