<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.16981</link><description>&lt;p&gt;
&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Simple Binary Hypothesis Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#20110;&#21051;&#30011;&#31616;&#21333;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#32622;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#20108;&#20803;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#21306;&#20998;&#20004;&#20010;&#20998;&#24067;$p$&#21644;$q$&#25152;&#38656;&#30340;&#26368;&#23567;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#25968;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20043;&#19968;&#36827;&#34892;&#65306;(i) &#26080;&#20808;&#39564;&#35774;&#32622;&#65292;&#31867;&#22411;-I&#38169;&#35823;&#26368;&#22823;&#20026;$\alpha$&#65292;&#31867;&#22411;-II&#38169;&#35823;&#26368;&#22823;&#20026;$\beta$; &#25110;&#32773; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#36125;&#21494;&#26031;&#38169;&#35823;&#26368;&#22823;&#20026;$\delta$&#65292;&#20808;&#39564;&#20998;&#24067;&#20026;$(\alpha, 1-\alpha)$&#12290; &#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#22312;$\alpha = \beta$&#65288;&#26080;&#20808;&#39564;&#65289;&#25110;$\alpha = 1/2$&#65288;&#36125;&#21494;&#26031;&#65289;&#26102;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#65292;&#24182;&#19988;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#29992;$p$&#21644;$q$&#20043;&#38388;&#30340;Hellinger&#25955;&#24230;&#26469;&#21051;&#30011;&#65292;&#30452;&#21040;&#20056;&#27861;&#24120;&#25968;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#29992;&#26469;&#21051;&#30011;&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20056;&#27861;&#24120;&#25968;&#29420;&#31435;&#20110;$p$&#12289;$q$&#21644;&#25152;&#26377;&#38169;&#35823;&#21442;&#25968;&#65289;&#65292;&#36866;&#29992;&#20110;&#65306;(i) &#20808;&#39564;&#35774;&#32622;&#20013;&#25152;&#26377;$0 \le \alpha, \beta \le 1/8$&#65307;&#20197;&#21450; (ii) &#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25152;&#26377;$\delta \le \alpha/4$&#12290; &#29305;&#21035;&#22320;&#65292;&#35813;&#20844;&#24335;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16981v1 Announce Type: cross  Abstract: The sample complexity of simple binary hypothesis testing is the smallest number of i.i.d. samples required to distinguish between two distributions $p$ and $q$ in either: (i) the prior-free setting, with type-I error at most $\alpha$ and type-II error at most $\beta$; or (ii) the Bayesian setting, with Bayes error at most $\delta$ and prior distribution $(\alpha, 1-\alpha)$. This problem has only been studied when $\alpha = \beta$ (prior-free) or $\alpha = 1/2$ (Bayesian), and the sample complexity is known to be characterized by the Hellinger divergence between $p$ and $q$, up to multiplicative constants. In this paper, we derive a formula that characterizes the sample complexity (up to multiplicative constants that are independent of $p$, $q$, and all error parameters) for: (i) all $0 \le \alpha, \beta \le 1/8$ in the prior-free setting; and (ii) all $\delta \le \alpha/4$ in the Bayesian setting. In particular, the formula admits eq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item></channel></rss>