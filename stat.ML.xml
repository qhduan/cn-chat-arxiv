<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24230;&#29305;&#24449;&#21305;&#37197;&#31639;&#27861;&#39640;&#25928;&#21305;&#37197;&#38543;&#26426;&#19981;&#22343;&#21248;&#22270;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#26368;&#23567;&#24179;&#22343;&#24230;&#21644;&#26368;&#23567;&#30456;&#20851;&#24615;&#36798;&#21040;&#19968;&#23450;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.10441</link><description>&lt;p&gt;
&#36890;&#36807;&#24230;&#29305;&#24449;&#39640;&#25928;&#21305;&#37197;&#38543;&#26426;&#19981;&#22343;&#21248;&#22270;
&lt;/p&gt;
&lt;p&gt;
Efficiently matching random inhomogeneous graphs via degree profiles. (arXiv:2310.10441v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24230;&#29305;&#24449;&#21305;&#37197;&#31639;&#27861;&#39640;&#25928;&#21305;&#37197;&#38543;&#26426;&#19981;&#22343;&#21248;&#22270;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#26368;&#23567;&#24179;&#22343;&#24230;&#21644;&#26368;&#23567;&#30456;&#20851;&#24615;&#36798;&#21040;&#19968;&#23450;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24674;&#22797;&#20004;&#20010;&#30456;&#20851;&#30340;&#38543;&#26426;&#22270;&#20043;&#38388;&#28508;&#22312;&#39030;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#22270;&#20855;&#26377;&#26497;&#19981;&#22343;&#21248;&#19988;&#26410;&#30693;&#30340;&#19981;&#21516;&#39030;&#28857;&#23545;&#20043;&#38388;&#30340;&#36793;&#27010;&#29575;&#12290;&#22312;Ding&#12289;Ma&#12289;Wu&#21644;Xu(2021)&#25552;&#20986;&#30340;&#24230;&#29305;&#24449;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25193;&#23637;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21305;&#37197;&#31639;&#27861;&#65292;&#21482;&#35201;&#26368;&#23567;&#24179;&#22343;&#24230;&#33267;&#23569;&#20026;$\Omega(\log^{2} n)$&#65292;&#26368;&#23567;&#30456;&#20851;&#24615;&#33267;&#23569;&#20026;$1 - O(\log^{-2} n)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of recovering the latent vertex correspondence between two correlated random graphs with vastly inhomogeneous and unknown edge probabilities between different pairs of vertices. Inspired by and extending the matching algorithm via degree profiles by Ding, Ma, Wu and Xu (2021), we obtain an efficient matching algorithm as long as the minimal average degree is at least $\Omega(\log^{2} n)$ and the minimal correlation is at least $1 - O(\log^{-2} n)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;&#19968;&#20010;&#29305;&#23450;&#20844;&#24335;&#25152;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07779</link><description>&lt;p&gt;
&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#30340;&#22312;&#32447;&#31639;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence analysis of online algorithms for vector-valued kernel regression. (arXiv:2309.07779v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;&#19968;&#20010;&#29305;&#23450;&#20844;&#24335;&#25152;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20316;&#20026;&#20808;&#39564;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20174;&#22122;&#22768;&#21521;&#37327;&#20540;&#25968;&#25454;&#20013;&#36924;&#36817;&#22238;&#24402;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#22312;&#22312;&#32447;&#31639;&#27861;&#20013;&#65292;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#36880;&#20010;&#21487;&#29992;&#65292;&#24182;&#20381;&#27425;&#22788;&#29702;&#20197;&#26500;&#24314;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#20851;&#27880;&#36825;&#31181;&#22312;&#32447;&#36924;&#36817;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;$C^2(m+1)^{-s/(2+s)}$&#32465;&#23450;&#65292;&#20854;&#20013;$m$&#20026;&#24403;&#19979;&#22788;&#29702;&#30340;&#25968;&#25454;&#25968;&#37327;&#65292;&#21442;&#25968;$0&lt;s\leq 1$&#34920;&#31034;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#39069;&#22806;&#20809;&#28369;&#24615;&#20551;&#35774;&#65292;&#24120;&#25968;$C$&#21462;&#20915;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#26041;&#24046;&#12289;&#22238;&#24402;&#20989;&#25968;&#30340;&#20809;&#28369;&#24615;&#20197;&#21450;&#31639;&#27861;&#30340;&#20854;&#20182;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating the regression function from noisy vector-valued data by an online learning algorithm using an appropriate reproducing kernel Hilbert space (RKHS) as prior. In an online algorithm, i.i.d. samples become available one by one by a random process and are successively processed to build approximations to the regression function. We are interested in the asymptotic performance of such online approximation algorithms and show that the expected squared error in the RKHS norm can be bounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processed data, the parameter $0&lt;s\leq 1$ expresses an additional smoothness assumption on the regression function and the constant $C$ depends on the variance of the input noise, the smoothness of the regression function and further parameters of the algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06230</link><description>&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models. (arXiv:2309.06230v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#26512;&#24341;&#21457;&#20102;&#23545;&#21333;&#25351;&#25968;&#27169;&#22411;&#65288;SIMs&#65289;&#21644;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;SIMs&#20026;&#39640;&#32500;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#28789;&#27963;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#32780;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#26088;&#22312;&#20174;&#22823;&#37327;&#30340;&#39044;&#27979;&#22240;&#23376;&#20013;&#25214;&#21040;&#31232;&#30095;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#25918;&#23485;&#36873;&#25321;&#65292;&#20294;&#19981;&#33021;&#24471;&#21040;&#26368;&#20339;&#23376;&#38598;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#38024;&#23545;&#39640;&#32500;SIMs&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#35745;&#31639;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#35299;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20960;&#20046;&#32943;&#23450;&#20855;&#26377;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#34394;&#25311;&#23646;&#24615;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#19968;&#20010;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#22238;&#24402;&#31995;&#25968;&#30340;&#25903;&#25345;&#22823;&#23567;&#65292;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#35774;&#35823;&#24046;&#20998;&#24067;&#25110;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specif
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06578</link><description>&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Kernel Ridge Regression Inference. (arXiv:2302.06578v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;(KRR)&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#25490;&#21517;&#12289;&#22270;&#20687;&#21644;&#22270;&#34920;&#22312;&#20869;&#30340;&#19968;&#33324;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#23398;&#26657;&#20998;&#37197;&#20013;&#30340;&#25490;&#24207;&#20248;&#20808;&#32423;&#21015;&#34920;&#65292;&#20294;KRR&#30340;&#25512;&#26029;&#29702;&#35770;&#23578;&#26410;&#23436;&#20840;&#30693;&#24713;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#32463;&#27982;&#23398;&#21644;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#38024;&#23545;&#19968;&#33324;&#22238;&#24402;&#22120;&#30340;&#23574;&#38160;&#12289;&#19968;&#33268;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#20026;&#20102;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#20030;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#31216;&#21270;&#26469;&#28040;&#38500;&#20559;&#24046;&#24182;&#38480;&#21046;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#31243;&#24207;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#37096;&#20998;&#21644;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#22343;&#21248;&#39640;&#26031;&#21644;&#33258;&#20030;&#32806;&#21512;&#12290;&#36825;&#20123;&#25512;&#23548;&#26263;&#31034;&#20102;&#22522;&#20110;RKHS&#21333;&#20301;&#29699;&#30340;&#32463;&#39564;&#36807;&#31243;&#30340;&#24378;&#36924;&#36817;&#65292;&#23545;&#35206;&#30422;&#25968;&#20855;&#26377;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide uniform inference and confidence bands for kernel ridge regression (KRR), a widely-used non-parametric regression estimator for general data types including rankings, images, and graphs. Despite the prevalence of these data -e.g., ranked preference lists in school assignment -- the inferential theory of KRR is not fully known, limiting its role in economics and other scientific domains. We construct sharp, uniform confidence sets for KRR, which shrink at nearly the minimax rate, for general regressors. To conduct inference, we develop an efficient bootstrap procedure that uses symmetrization to cancel bias and limit computational overhead. To justify the procedure, we derive finite-sample, uniform Gaussian and bootstrap couplings for partial sums in a reproducing kernel Hilbert space (RKHS). These imply strong approximation for empirical processes indexed by the RKHS unit ball with logarithmic dependence on the covering number. Simulations verify coverage. We use our proce
&lt;/p&gt;</description></item></channel></rss>