<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.17512</link><description>&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#26041;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#23450;&#20041;&#28508;&#22312;&#21521;&#37327;&#30340;&#27880;&#24847;&#21147;&#26469;&#23558;&#20854;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#30340;&#8220;Latte Transformer&#8221;&#27169;&#22411;&#21487;&#29992;&#20110;&#21452;&#21521;&#21644;&#21333;&#21521;&#20219;&#21153;&#65292;&#22240;&#26524;&#29256;&#26412;&#20801;&#35768;&#19968;&#31181;&#22312;&#25512;&#29702;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#36882;&#24402;&#23454;&#29616;&#12290;&#26631;&#20934;transformer&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;Latte Transformer&#35745;&#31639;&#19979;&#19968;&#20010;&#26631;&#35760;&#25152;&#38656;&#30340;&#26102;&#38388;&#26159;&#24658;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#34920;&#29616;&#21487;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#27880;&#24847;&#21147;&#23454;&#38469;&#21487;&#34892;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2202.09134</link><description>&lt;p&gt;
&#22312;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#37327;&#21270;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#20010;&#20855;&#20307;&#27169;&#22411;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20123;&#35266;&#23519;&#65292;&#20294;&#20063;&#24471;&#20986;&#20102;&#24847;&#22806;&#30340;&#21457;&#29616;&#65306;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#32780;&#19981;&#26159;&#20943;&#23569;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#39044;&#27979;&#39118;&#38505;&#12290;&#23427;&#21487;&#20197;&#20805;&#24403;&#27491;&#21017;&#21270;&#22120;&#65292;&#20294;&#22312;&#26576;&#20123;&#39640;&#32500;&#38382;&#39064;&#20013;&#21364;&#26080;&#27861;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25913;&#21464;&#32463;&#39564;&#39118;&#38505;&#30340;&#21452;&#37325;&#19979;&#38477;&#23792;&#20540;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20998;&#26512;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#34987;&#36171;&#20104;&#30340;&#20960;&#20010;&#23646;&#24615;&#35201;&#20040;&#26159;&#30495;&#30340;&#65292;&#35201;&#20040;&#26159;&#20551;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;-&#29305;&#21035;&#26159;&#25968;&#25454;&#20998;&#24067;&#65292;&#20272;&#35745;&#22120;&#30340;&#23646;&#24615;&#20197;&#21450;&#26679;&#26412;&#22823;&#23567;&#65292;&#22686;&#24378;&#25968;&#37327;&#21644;&#32500;&#25968;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#24037;&#20855;&#26159;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
&lt;/p&gt;</description></item></channel></rss>