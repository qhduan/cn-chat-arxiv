<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.09778</link><description>&lt;p&gt;
&#26799;&#24230;&#30495;&#30340;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Gradient is All You Need?. (arXiv:2306.09778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#35299;&#37322;&#20026;&#26799;&#24230;&#19979;&#38477;&#30340;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#12290;&#27492;&#20248;&#21270;&#26041;&#27861;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20016;&#23500;&#31867;&#21035;&#30340;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#23376;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#32467;&#21512;&#26799;&#24230;&#19979;&#38477;&#30475;&#20316;&#38543;&#26426;&#26494;&#24347;&#26041;&#27861;&#65292;&#26469;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#36890;&#36807;&#31890;&#23376;&#20043;&#38388;&#30340;&#36890;&#35759;&#65292;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#38646;&#38454;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#20302;&#25928;&#25110;&#19981;&#20855;&#22791;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20016;&#23500;&#31867;&#21035;&#19979;&#20840;&#23616;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradi
&lt;/p&gt;</description></item></channel></rss>