<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11343</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Transfer Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#20004;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#36981;&#23432;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#20005;&#26684;&#21046;&#23450;&#20102;\textit{&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;}&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#26377;&#19968;&#20010;&#21463;&#20449;&#20219;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#22312;&#36825;&#20010;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#32463;&#20856;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21363;&#21333;&#21464;&#37327;&#22343;&#20540;&#20272;&#35745;&#12289;&#20302;&#32500;&#32447;&#24615;&#22238;&#24402;&#21644;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#12290;&#36890;&#36807;&#30740;&#31350;&#26497;&#23567;&#20540;&#29575;&#24182;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#30340;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;&#26159;&#24050;&#24314;&#31435;&#30340;&#23616;&#37096;&#21644;&#20013;&#22830;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#38544;&#31169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11343v1 Announce Type: new  Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of 
&lt;/p&gt;</description></item><item><title>CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;</title><link>https://arxiv.org/abs/2403.07728</link><description>&lt;p&gt;
CAS: &#19968;&#31181;&#20855;&#26377;FCR&#25511;&#21046;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#31526;&#21512;&#39044;&#27979;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07728
&lt;/p&gt;
&lt;p&gt;
CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#26041;&#24335;&#19979;&#21518;&#36873;&#25321;&#39044;&#27979;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#23558;&#36164;&#28304;&#32791;&#36153;&#22312;&#19981;&#37325;&#35201;&#30340;&#21333;&#20301;&#19978;&#65292;&#22312;&#25253;&#21578;&#20854;&#39044;&#27979;&#21306;&#38388;&#20043;&#21069;&#23545;&#24403;&#21069;&#20010;&#20307;&#36827;&#34892;&#21021;&#27493;&#36873;&#25321;&#22312;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#30001;&#20110;&#22312;&#32447;&#36873;&#25321;&#23548;&#33268;&#25152;&#36873;&#39044;&#27979;&#21306;&#38388;&#20013;&#23384;&#22312;&#26102;&#38388;&#22810;&#37325;&#24615;&#65292;&#22240;&#27492;&#25511;&#21046;&#23454;&#26102;&#35823;&#35206;&#30422;&#38472;&#36848;&#29575;&#65288;FCR&#65289;&#26469;&#27979;&#37327;&#24179;&#22343;&#35823;&#35206;&#30422;&#35823;&#24046;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CAS&#65288;&#36866;&#24212;&#24615;&#36873;&#25321;&#21518;&#26657;&#20934;&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21253;&#35065;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#21644;&#22312;&#32447;&#36873;&#25321;&#35268;&#21017;&#65292;&#20197;&#36755;&#20986;&#21518;&#36873;&#25321;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#22914;&#26524;&#36873;&#25321;&#20102;&#24403;&#21069;&#20010;&#20307;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#26469;&#26500;&#24314;&#26657;&#20934;&#38598;&#65292;&#28982;&#21518;&#20026;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20026;&#26657;&#20934;&#38598;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26500;&#36896;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10818</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#30340;&#20984;&#26367;&#20195;&#21697;&#30340;&#19968;&#33268;&#24615;&#21644;&#32500;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Trading off Consistency and Dimensionality of Convex Surrogates for the Mode
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10818
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#24517;&#39035;&#23558;&#32467;&#26524;&#23884;&#20837;&#21040;&#33267;&#23569;&#26377;$n-1$&#32500;&#30340;&#23454;&#25968;&#31354;&#38388;&#20013;&#65292;&#20197;&#35774;&#35745;&#19968;&#31181;&#19968;&#33268;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20250;&#23548;&#33268;"&#27491;&#30830;"&#30340;&#20998;&#31867;&#65292;&#32780;&#19981;&#21463;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#31561;&#38656;&#35201;&#22823;&#37327;n&#26102;&#65292;&#20248;&#21270;n-1&#32500;&#26367;&#20195;&#24120;&#24120;&#26159;&#26840;&#25163;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#22914;&#20309;&#26435;&#34913;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#20197;&#21450;&#22312;&#21333;&#32431;&#24418;&#19978;&#32422;&#26463;&#19968;&#33268;&#24615;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36319;&#38543;&#36807;&#21435;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#23884;&#20837;&#36807;&#31243;&#65292;&#23558;&#32467;&#26524;&#26144;&#23556;&#21040;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#30340;&#39030;&#28857;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27599;&#20010;&#28857;&#36136;&#37327;&#20998;&#24067;&#21608;&#22260;&#23384;&#22312;&#21333;&#32431;&#24418;&#30340;&#20840;&#32500;&#23376;&#38598;&#65292;&#20854;&#20013;&#19968;&#33268;&#24615;&#25104;&#31435;&#65292;&#20294;&#26159;&#65292;&#23569;&#20110;n-1&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#19968;&#31181;&#29616;&#35937;&#24615;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10818v1 Announce Type: new  Abstract: In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomeno
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#25104;&#26412;&#20989;&#25968;&#26469;&#23547;&#25214;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03970</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Optimal partitioning of directed acyclic graphs with dependent costs between clusters. (arXiv:2308.03970v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#25104;&#26412;&#20989;&#25968;&#26469;&#23547;&#25214;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#25512;&#26029;&#22330;&#26223;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#12289;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#22522;&#30784;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21010;&#20998;&#25104;&#38598;&#32676;&#26469;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#65292;&#26368;&#20248;&#21010;&#20998;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#35201;&#20248;&#21270;&#30340;&#25104;&#26412;&#21462;&#20915;&#20110;&#38598;&#32676;&#20869;&#30340;&#33410;&#28857;&#20197;&#21450;&#36890;&#36807;&#29238;&#33410;&#28857;&#21644;/&#25110;&#23376;&#33410;&#28857;&#36830;&#25509;&#30340;&#38598;&#32676;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#20381;&#36182;&#38598;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#20381;&#36182;&#38598;&#32676;&#30340;&#26368;&#20248;&#38598;&#32676;&#26144;&#23556;&#12290;&#22312;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#20219;&#24847;&#23450;&#20041;&#30340;&#27491;&#25104;&#26412;&#20989;&#25968;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;DCMAP&#25910;&#25947;&#20110;&#25214;&#21040;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#23545;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#20989;&#25968;&#30340;&#19968;&#20010;&#28023;&#33609;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;&#23545;&#20110;&#19968;&#20010;25&#20010;&#21644;50&#20010;&#33410;&#28857;&#30340;DBN&#65292;&#25628;&#32034;&#31354;&#38388;&#22823;&#23567;&#20998;&#21035;&#20026;$9.91\times 10^9$&#21644;$1.5$
&lt;/p&gt;
&lt;p&gt;
Many statistical inference contexts, including Bayesian Networks (BNs), Markov processes and Hidden Markov Models (HMMS) could be supported by partitioning (i.e.~mapping) the underlying Directed Acyclic Graph (DAG) into clusters. However, optimal partitioning is challenging, especially in statistical inference as the cost to be optimised is dependent on both nodes within a cluster, and the mapping of clusters connected via parent and/or child nodes, which we call dependent clusters. We propose a novel algorithm called DCMAP for optimal cluster mapping with dependent clusters. Given an arbitrarily defined, positive cost function based on the DAG and cluster mappings, we show that DCMAP converges to find all optimal clusters, and returns near-optimal solutions along the way. Empirically, we find that the algorithm is time-efficient for a DBN model of a seagrass complex system using a computation cost function. For a 25 and 50-node DBN, the search space size was $9.91\times 10^9$ and $1.5
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#24773;&#20917;&#19979;&#32570;&#22833;&#26631;&#31614;&#19988;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12789</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#22240;&#26524;&#25512;&#26029;&#65306;&#38754;&#21521;&#34928;&#20943;&#37325;&#21472;&#30340;&#36873;&#25321;&#20559;&#24046;&#19979;&#21487;&#27867;&#21270;&#30340;&#21452;&#31283;&#20272;&#35745;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Causal Inference: Generalizable and Double Robust Inference for Average Treatment Effects under Selection Bias with Decaying Overlap. (arXiv:2305.12789v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#24773;&#20917;&#19979;&#32570;&#22833;&#26631;&#31614;&#19988;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23384;&#22312;&#21487;&#33021;&#32570;&#22833;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#30340;ATE&#20272;&#35745;&#38382;&#39064;&#12290;&#26631;&#35760;&#25351;&#31034;&#31526;&#30340;&#26465;&#20214;&#20542;&#21521;&#24471;&#20998;&#20801;&#35768;&#20381;&#36182;&#20110;&#21327;&#21464;&#37327;&#65292;&#24182;&#19988;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#30340;&#34928;&#20943;&#32780;&#34928;&#20943;&#8212;&#8212;&#20174;&#32780;&#20801;&#35768;&#26410;&#26631;&#35760;&#25968;&#25454;&#22823;&#23567;&#27604;&#26631;&#35760;&#25968;&#25454;&#22823;&#23567;&#22686;&#38271;&#24471;&#26356;&#24555;&#12290;&#36825;&#31181;&#24773;&#20917;&#22635;&#34917;&#20102;&#21322;&#30417;&#30563;&#65288;SS&#65289;&#21644;&#32570;&#22833;&#25968;&#25454;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20801;&#35768;&#36873;&#25321;&#20559;&#24046;&#30340;&#38543;&#26426;&#32570;&#22833;&#65288;MAR&#65289;&#26426;&#21046;&#8212;&#8212;&#36825;&#36890;&#24120;&#22312;&#26631;&#20934;&#30340;SS&#25991;&#29486;&#20013;&#26159;&#31105;&#27490;&#30340;&#65292;&#24182;&#19988;&#22312;&#32570;&#22833;&#25968;&#25454;&#25991;&#29486;&#20013;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#27491;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;ATE&#30340;&#19968;&#33324;&#21452;&#31283;DR-DMAR&#65288;decaying&#65289;SS&#20272;&#35745;&#22120;&#65292;&#36825;&#31181;&#20272;&#35745;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Average treatment effect (ATE) estimation is an essential problem in the causal inference literature, which has received significant recent attention, especially with the presence of high-dimensional confounders. We consider the ATE estimation problem in high dimensions when the observed outcome (or label) itself is possibly missing. The labeling indicator's conditional propensity score is allowed to depend on the covariates, and also decay uniformly with sample size - thus allowing for the unlabeled data size to grow faster than the labeled data size. Such a setting fills in an important gap in both the semi-supervised (SS) and missing data literatures. We consider a missing at random (MAR) mechanism that allows selection bias - this is typically forbidden in the standard SS literature, and without a positivity condition - this is typically required in the missing data literature. We first propose a general doubly robust 'decaying' MAR (DR-DMAR) SS estimator for the ATE, which is cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20351;&#24471;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#24182;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#26377;&#25928;&#30340;&#21516;&#26102;&#20445;&#25345;&#20302;&#24265;&#30340;&#35745;&#31639;&#20195;&#20215;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;&#20195;&#20215;&#20989;&#25968; $\phi$&#12290;</title><link>http://arxiv.org/abs/2206.03345</link><description>&lt;p&gt;
&#38024;&#23545;&#36229;&#21442;&#25968;&#21270;&#30340;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification. (arXiv:2206.03345v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20351;&#24471;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#24182;&#22312;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#26377;&#25928;&#30340;&#21516;&#26102;&#20445;&#25345;&#20302;&#24265;&#30340;&#35745;&#31639;&#20195;&#20215;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;&#20195;&#20215;&#20989;&#25968; $\phi$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#38750;&#20984;&#20989;&#25968;$f(X)=\phi(XX^{T})$&#30340;&#26041;&#27861;&#65292;&#20854;&#20013; $\phi$&#26159;&#19968;&#20010;&#24179;&#28369;&#20984;&#30340;$n\times n$&#30697;&#38453;&#19978;&#19979;&#25991;&#30340;&#20195;&#20215;&#20989;&#25968;&#12290;&#34429;&#28982;&#20165;&#26377;&#20108;&#38454;&#20572;&#30041;&#28857;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#34987;&#35777;&#26126;&#25214;&#21040;&#65292;&#20294;&#22914;&#26524; $X$ &#30340;&#31209;&#32570;&#22833;&#65292;&#37027;&#20040;&#23427;&#30340;&#31209;&#32570;&#22833;&#23558;&#35777;&#26126;&#23427;&#26159;&#20840;&#23616;&#26368;&#20248;&#30340;&#12290;&#36825;&#31181;&#35748;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#24517;&#28982;&#38656;&#35201;&#24403;&#21069;&#36845;&#20195;$X$&#30340;&#25628;&#32034;&#31209; $r$ &#36229;&#36807;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;$X^{\star}$ &#30340;&#31209;$r^{\star}$&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36229;&#21442;&#25968;&#21270;&#26174;&#33879;&#20943;&#24930;&#20102;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174; $r=r^{\star}$ &#26102;&#30340;&#32447;&#24615;&#36895;&#24230;&#38477;&#20026; $r&gt;r^{\star}$ &#26102;&#30340;&#20122;&#32447;&#24615;&#36895;&#24230;&#65292;&#21363;&#20351; $\phi$ &#26159;&#24378;&#20984;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23558;&#36229;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#24674;&#22797;&#21040;&#32447;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#24615;&#35777;&#26126;&#20381;&#26087;&#26377;&#25928;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#30697;&#38453;&#20056;&#27861;&#21644;&#27714;&#36870;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#24378;&#20984;&#30340;$&#966;$&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider using gradient descent to minimize the nonconvex function $f(X)=\phi(XX^{T})$ over an $n\times r$ factor matrix $X$, in which $\phi$ is an underlying smooth convex cost function defined over $n\times n$ matrices. While only a second-order stationary point $X$ can be provably found in reasonable time, if $X$ is additionally rank deficient, then its rank deficiency certifies it as being globally optimal. This way of certifying global optimality necessarily requires the search rank $r$ of the current iterate $X$ to be overparameterized with respect to the rank $r^{\star}$ of the global minimizer $X^{\star}$. Unfortunately, overparameterization significantly slows down the convergence of gradient descent, from a linear rate with $r=r^{\star}$ to a sublinear rate when $r&gt;r^{\star}$, even when $\phi$ is strongly convex. In this paper, we propose an inexpensive preconditioner that restores the convergence rate of gradient descent back to linear in the overparameterized case, while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2111.10275</link><description>&lt;p&gt;
&#24102;&#26377;&#26680;&#30340;&#22797;&#21512;&#36866;&#21512;&#24615;&#26816;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Composite Goodness-of-fit Tests with Kernels. (arXiv:2111.10275v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38169;&#35823;&#35828;&#26126;&#21487;&#33021;&#20250;&#23545;&#27010;&#29575;&#27169;&#22411;&#30340;&#23454;&#29616;&#36896;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20419;&#20351;&#24320;&#21457;&#20986;&#19968;&#20123;&#30452;&#25509;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26356;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#26159;&#21542;&#38656;&#35201;&#21462;&#20915;&#20110;&#27169;&#22411;&#26159;&#21542;&#30495;&#30340;&#38169;&#35823;&#65292;&#30446;&#21069;&#32570;&#20047;&#36890;&#29992;&#30340;&#26041;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#21512;&#26816;&#39564;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26469;&#33258;&#26576;&#20123;&#21442;&#25968;&#27169;&#22411;&#26063;&#20013;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#21033;&#29992;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#26680;Stein&#24046;&#24322;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#12290;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#21253;&#25324;&#24403;&#21442;&#25968;&#27169;&#22411;&#30340;&#23494;&#24230;&#24050;&#30693;&#38500;&#26631;&#20934;&#21270;&#24120;&#25968;&#22806;&#65292;&#25110;&#32773;&#22914;&#26524;&#27169;&#22411;&#37319;&#29992;&#27169;&#25311;&#22120;&#24418;&#24335;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27491;&#30830;&#30340;&#27169;&#22411;&#35268;&#33539;&#30340;&#38646;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;&#21442;&#25968;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31435;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#24322;&#24120;&#26816;&#27979;&#24212;&#29992;&#26696;&#20363;&#28436;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the param
&lt;/p&gt;</description></item></channel></rss>