<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38160;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20986;&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#12289;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#22686;&#22823;&#23398;&#20064;&#29575;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#21457;&#29983;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2311.02076</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26222;&#36866;&#38160;&#24230;&#21160;&#21147;&#23398;&#65306;&#22266;&#23450;&#28857;&#20998;&#26512;&#12289;&#31283;&#23450;&#36793;&#30028;&#21644;&#28151;&#27788;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos. (arXiv:2311.02076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#38160;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20986;&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#12289;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#22686;&#22823;&#23398;&#20064;&#29575;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#21457;&#29983;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#20013;&#65292;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;&#38160;&#24230;&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23637;&#31034;&#20986;&#21508;&#31181;&#31283;&#20581;&#30340;&#29616;&#35937;&#12290;&#36825;&#21253;&#25324;&#26089;&#26399;&#26102;&#38388;&#38454;&#27573;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#38160;&#24230;&#21487;&#33021;&#20943;&#23567;&#65288;&#38477;&#20302;&#38160;&#24230;&#65289;&#65292;&#20197;&#21450;&#21518;&#26399;&#34892;&#20026;&#65292;&#22914;&#36880;&#28176;&#22686;&#21152;&#30340;&#38160;&#21270;&#21644;&#31283;&#23450;&#36793;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;2&#23618;&#32447;&#24615;&#32593;&#32476;&#65288;UV&#27169;&#22411;&#65289;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#25152;&#26377;&#20851;&#38190;&#38160;&#24230;&#29616;&#35937;&#12290;&#36890;&#36807;&#20998;&#26512;&#20989;&#25968;&#31354;&#38388;&#20013;&#21160;&#21147;&#23398;&#22266;&#23450;&#28857;&#30340;&#32467;&#26500;&#21644;&#20989;&#25968;&#26356;&#26032;&#30340;&#21521;&#37327;&#22330;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#38160;&#24230;&#36235;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(i)&#26089;&#26399;&#38160;&#24230;&#38477;&#20302;&#21644;&#36880;&#28176;&#22686;&#21152;&#38160;&#21270;&#30340;&#26426;&#21046;&#65292;(ii)&#31283;&#23450;&#36793;&#30028;&#25152;&#38656;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450; (iii)&#24403;&#23398;&#20064;&#29575;&#22686;&#21152;&#26102;&#65292;&#31283;&#23450;&#36793;&#30028;&#27969;&#24418;&#19978;&#30340;&#20493;&#22686;&#28151;&#27788;&#36335;&#24452;.
&lt;/p&gt;
&lt;p&gt;
In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, and (iii) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32508;&#21512; Tweedie &#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20391;&#20449;&#24687;&#30340;&#32467;&#26500;&#30693;&#35782;&#65292;&#22312;&#32771;&#34385;&#20102;&#36741;&#21161;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27491;&#24577;&#22343;&#20540;&#30340;&#22797;&#21512;&#20272;&#35745;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05883</link><description>&lt;p&gt;
&#20351;&#29992;&#20391;&#20449;&#24687;&#30340;&#32463;&#39564;&#36125;&#21494;&#26031;&#20272;&#35745;&#65306;&#19968;&#31181;&#38750;&#21442;&#25968;&#32508;&#21512; Tweedie &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Empirical Bayes Estimation with Side Information: A Nonparametric Integrative Tweedie Approach. (arXiv:2308.05883v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32508;&#21512; Tweedie &#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20391;&#20449;&#24687;&#30340;&#32467;&#26500;&#30693;&#35782;&#65292;&#22312;&#32771;&#34385;&#20102;&#36741;&#21161;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27491;&#24577;&#22343;&#20540;&#30340;&#22797;&#21512;&#20272;&#35745;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32771;&#34385;&#21040;&#20391;&#20449;&#24687;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#27491;&#24577;&#22343;&#20540;&#30340;&#22797;&#21512;&#20272;&#35745;&#38382;&#39064;&#12290;&#21033;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32508;&#21512; Tweedie&#65288;NIT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#22810;&#21464;&#37327;&#36741;&#21161;&#25968;&#25454;&#20013;&#32534;&#30721;&#30340;&#32467;&#26500;&#30693;&#35782;&#21512;&#24182;&#21040;&#22797;&#21512;&#20272;&#35745;&#30340;&#31934;&#24230;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20984;&#20248;&#21270;&#24037;&#20855;&#30452;&#25509;&#20272;&#35745;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#23558;&#32467;&#26500;&#32422;&#26463;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23545; NIT &#30340;&#28176;&#36817;&#39118;&#38505;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102; NIT &#25910;&#25947;&#21040; Oracle &#20272;&#35745;&#22120;&#30340;&#36895;&#29575;&#12290;&#38543;&#30528;&#36741;&#21161;&#25968;&#25454;&#30340;&#32500;&#24230;&#22686;&#21152;&#65292;&#25105;&#20204;&#20934;&#30830;&#22320;&#37327;&#21270;&#20102;&#20272;&#35745;&#39118;&#38505;&#30340;&#25913;&#21892;&#20197;&#21450;&#25910;&#25947;&#36895;&#24230;&#30340;&#24694;&#21270;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; NIT &#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of compound estimation of normal means while accounting for the presence of side information. Leveraging the empirical Bayes framework, we develop a nonparametric integrative Tweedie (NIT) approach that incorporates structural knowledge encoded in multivariate auxiliary data to enhance the precision of compound estimation. Our approach employs convex optimization tools to estimate the gradient of the log-density directly, enabling the incorporation of structural constraints. We conduct theoretical analyses of the asymptotic risk of NIT and establish the rate at which NIT converges to the oracle estimator. As the dimension of the auxiliary data increases, we accurately quantify the improvements in estimation risk and the associated deterioration in convergence rate. The numerical performance of NIT is illustrated through the analysis of both simulated and real data, demonstrating its superiority over existing methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2002.09377</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#24418;&#19979;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Misspecification-robust likelihood-free inference in high dimensions. (arXiv:2002.09377v3 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.09377
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#21644;&#24046;&#24322;&#20989;&#25968;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#32479;&#35745;&#27169;&#22411;&#30340;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#23454;&#36341;&#20013;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#35745;&#31639;&#65288;ABC&#65289;&#25512;&#26029;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#35823;&#24046;&#20998;&#24067;&#33258;&#30001;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25193;&#23637;&#26041;&#27861;&#26469;&#27010;&#29575;&#21270;&#22320;&#36924;&#36817;&#24046;&#24322;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#20110;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#21442;&#25968;&#20351;&#29992;&#21333;&#29420;&#30340;&#37319;&#38598;&#20989;&#25968;&#21644;&#24046;&#24322;&#20989;&#25968;&#26469;&#23454;&#29616;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#12290;&#26377;&#25928;&#30340;&#21152;&#24615;&#37319;&#38598;&#32467;&#26500;&#19982;&#25351;&#25968;&#25439;&#22833;-&#20284;&#28982;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35823;&#24046;&#27169;&#22411;&#35828;&#26126;&#30340;&#40065;&#26834;&#24615;&#24378;&#30340;&#36793;&#38469;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The me
&lt;/p&gt;</description></item></channel></rss>