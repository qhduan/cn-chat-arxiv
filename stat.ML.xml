<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11249</link><description>&lt;p&gt;
&#20851;&#20110;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fisher-Rao Gradient of the Evidence Lower Bound. (arXiv:2307.11249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#20063;&#31216;&#20026;&#33258;&#28982;&#26799;&#24230;&#65292;&#23427;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29702;&#35770;&#12289;Helmholtz&#26426;&#21644;&#33258;&#30001;&#33021;&#21407;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;ELBO&#30340;&#33258;&#28982;&#26799;&#24230;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#33258;&#28982;&#26799;&#24230;&#30456;&#20851;&#65292;&#21518;&#32773;&#26159;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#12290;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#20013;&#26799;&#24230;&#30340;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#24213;&#23618;&#27169;&#22411;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#22312;&#32473;&#23450;&#20984;&#32422;&#26463;&#19979;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20986;&#21463;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#25552;&#20986;&#26032;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#65292;&#24182;&#24212;&#29992;&#20110;&#31232;&#30095;&#30697;&#38453;&#31561;&#24773;&#22659;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32479;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15121</link><description>&lt;p&gt;
&#22312;&#20984;&#32422;&#26463;&#19979;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning linear dynamical systems under convex constraints. (arXiv:2303.15121v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#32473;&#23450;&#20984;&#32422;&#26463;&#19979;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20986;&#21463;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#25552;&#20986;&#26032;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#65292;&#24182;&#24212;&#29992;&#20110;&#31232;&#30095;&#30697;&#38453;&#31561;&#24773;&#22659;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32479;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#21333;&#20010;&#36712;&#36857;&#20013;&#35782;&#21035;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#26410;&#23545;&#31995;&#32479;&#30697;&#38453; $A^* \in \mathbb{R}^{n \times n}$ &#36827;&#34892;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#65292;&#24182;&#23545;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056; (OLS) &#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#20551;&#35774;&#21487;&#29992;&#20808;&#21069;&#30340; $A^*$ &#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#21487;&#20197;&#22312;&#21253;&#21547; $A^*$ &#30340;&#20984;&#38598; $\mathcal{K}$ &#20013;&#25429;&#33719;&#12290;&#23545;&#20110;&#38543;&#21518;&#30340;&#21463;&#32422;&#26463;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#35299;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986; Frobenius &#33539;&#25968;&#19979;&#20381;&#36182;&#20110; $\mathcal{K}$ &#22312; $A^*$ &#22788;&#20999;&#38181;&#30340;&#23616;&#37096;&#22823;&#23567;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#32467;&#26524;&#30340;&#26377;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#23454;&#20363;&#21270;&#20026;&#20197;&#19979;&#35774;&#32622;&#65306;(i) $\mathcal{K}$ &#26159; $\mathbb{R}^{n \times n}$ &#20013;&#30340; $d$ &#32500;&#23376;&#31354;&#38388;&#65292;&#25110;&#32773; (ii) $A^*$ &#26159; $k$ &#31232;&#30095;&#30340;&#65292;$\mathcal{K}$ &#26159;&#36866;&#24403;&#32553;&#25918;&#30340; $\ell_1$ &#29699;&#12290;&#22312; $d, k \ll n^2$ &#30340;&#21306;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#35823;&#24046;&#30028;&#23545;&#20110;&#30456;&#21516;&#30340;&#32479;&#35745;&#21644;&#22122;&#22768;&#20551;&#35774;&#27604; OLS &#20272;&#35745;&#22120;&#33719;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of identification of linear dynamical systems from a single trajectory. Recent results have predominantly focused on the setup where no structural assumption is made on the system matrix $A^* \in \mathbb{R}^{n \times n}$, and have consequently analyzed the ordinary least squares (OLS) estimator in detail. We assume prior structural information on $A^*$ is available, which can be captured in the form of a convex set $\mathcal{K}$ containing $A^*$. For the solution of the ensuing constrained least squares estimator, we derive non-asymptotic error bounds in the Frobenius norm which depend on the local size of the tangent cone of $\mathcal{K}$ at $A^*$. To illustrate the usefulness of this result, we instantiate it for the settings where, (i) $\mathcal{K}$ is a $d$ dimensional subspace of $\mathbb{R}^{n \times n}$, or (ii) $A^*$ is $k$-sparse and $\mathcal{K}$ is a suitably scaled $\ell_1$ ball. In the regimes where $d, k \ll n^2$, our bounds improve upon those obta
&lt;/p&gt;</description></item></channel></rss>