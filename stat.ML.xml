<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#20998;&#25968;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#65292;&#24320;&#21457;&#20102;&#38024;&#23545;&#30495;&#23454;&#22238;&#24402;&#20989;&#25968;&#38750;&#20809;&#28369;&#24773;&#20917;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#25104;&#21151;&#22788;&#29702;&#20102;&#35813;&#20989;&#25968;&#31867;&#22312;$L_2$-&#20998;&#25968; Sobolev &#31354;&#38388;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#19978;&#30028;&#20026;$n^{-\frac{2s}{2s+d}}$&#12290;</title><link>https://arxiv.org/abs/2402.14985</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25968;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#36827;&#34892;&#38750;&#20809;&#28369;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Nonsmooth Nonparametric Regression via Fractional Laplacian Eigenmaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14985
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#25968;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#65292;&#24320;&#21457;&#20102;&#38024;&#23545;&#30495;&#23454;&#22238;&#24402;&#20989;&#25968;&#38750;&#20809;&#28369;&#24773;&#20917;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#25104;&#21151;&#22788;&#29702;&#20102;&#35813;&#20989;&#25968;&#31867;&#22312;$L_2$-&#20998;&#25968; Sobolev &#31354;&#38388;&#20013;&#30340;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#19978;&#30028;&#20026;$n^{-\frac{2s}{2s+d}}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30495;&#23454;&#22238;&#24402;&#20989;&#25968;&#19981;&#19968;&#23450;&#24179;&#28369;&#30340;&#24773;&#20917;&#24320;&#21457;&#20102;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#20998;&#25968;&#25289;&#26222;&#25289;&#26031;&#65292;&#24182;&#26088;&#22312;&#22788;&#29702;&#30495;&#23454;&#22238;&#24402;&#20989;&#25968;&#20301;&#20110;$L_2$-&#20998;&#25968; Sobolev &#31354;&#38388;&#65288;&#38454;&#25968;&#20026;$s\in (0,1)$&#65289;&#30340;&#24773;&#20917;&#12290;&#35813;&#20989;&#25968;&#31867;&#26159;&#19968;&#20010; Hilbert &#31354;&#38388;&#65292;&#20301;&#20110;&#24179;&#26041;&#21487;&#31215;&#20989;&#25968;&#31354;&#38388;&#21644;&#19968;&#38454; Sobolev &#31354;&#38388;&#20043;&#38388;&#65292;&#21253;&#25324;&#20998;&#25968;&#24130;&#20989;&#25968;&#12289;&#20998;&#27573;&#24120;&#25968;&#25110;&#22810;&#39033;&#24335;&#20989;&#25968;&#20197;&#21450;&#23574;&#23792;&#20989;&#25968;&#20316;&#20026;&#20856;&#22411;&#31034;&#20363;&#12290;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#26679;&#26412;&#20869;&#22343;&#26041;&#20272;&#35745;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#20855;&#26377;$n^{-\frac{2s}{2s+d}}$&#30340;&#38454;&#65292;&#20854;&#20013;$d$&#26159;&#32500;&#25968;&#65292;$s$&#26159;&#21069;&#36848;&#39034;&#24207;&#21442;&#25968;&#65292;$n$&#26159;&#35266;&#27979;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#25152;&#24320;&#21457;&#26041;&#27861;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14985v1 Announce Type: cross  Abstract: We develop nonparametric regression methods for the case when the true regression function is not necessarily smooth. More specifically, our approach is using the fractional Laplacian and is designed to handle the case when the true regression function lies in an $L_2$-fractional Sobolev space with order $s\in (0,1)$. This function class is a Hilbert space lying between the space of square-integrable functions and the first-order Sobolev space consisting of differentiable functions. It contains fractional power functions, piecewise constant or polynomial functions and bump function as canonical examples. For the proposed approach, we prove upper bounds on the in-sample mean-squared estimation error of order $n^{-\frac{2s}{2s+d}}$, where $d$ is the dimension, $s$ is the aforementioned order parameter and $n$ is the number of observations. We also provide preliminary empirical results validating the practical performance of the developed
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14515</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#21644;&#26497;&#22823;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14515
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#38376;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#19982;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;QNN&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#38480;&#20613;&#37324;&#21494;&#32423;&#25968;&#65292;&#20854;&#20013;&#39057;&#29575;&#38598;&#34987;&#31216;&#20026;&#39057;&#35889;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#39057;&#35889;&#24182;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#27169;&#22411;&#65292;&#23384;&#22312;&#21508;&#31181;&#26497;&#22823;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#20445;&#25345;&#39057;&#35889;&#30340;&#20855;&#26377;&#30456;&#21516;&#38754;&#31215;$A = RL$&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#21452;&#23556;&#65292;&#20854;&#20013;$R$&#34920;&#31034;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#65292;$L$&#34920;&#31034;&#23618;&#25968;&#65292;&#25105;&#20204;&#22240;&#27492;&#31216;&#20043;&#20026;&#38754;&#31215;&#20445;&#25345;&#21464;&#25442;&#19979;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#30340;&#22312;&#32467;&#26524;&#20013;$R$&#21644;$L$&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#22823;&#39057;&#35889;&#30340;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2001.01095</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;: &#36890;&#36807;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Independence Testing via Maximum and Average Distance Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#22810;&#20803;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#34920;&#24449;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#36793;&#38469;&#30456;&#20851;&#32500;&#24230;&#25968;&#37327;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20248;&#21183;&#65292;&#26816;&#26597;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#38646;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#26816;&#27979;&#31243;&#24207;&#12290;&#24471;&#20986;&#30340;&#26816;&#39564;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#20316;&#20026;&#24213;&#23618;&#24230;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#20803;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#26368;&#22823;&#36317;&#31163;&#30456;&#20851;&#24615;&#12289;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#21644;&#21407;&#22987;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#20197;&#26816;&#27979;&#20154;&#31867;&#34880;&#27974;&#20013;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#21644;&#32957;&#27700;&#24179;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.07475</link><description>&lt;p&gt;
&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23548;&#33268;&#65288;&#36817;&#20284;&#65289;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#30340;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#20197;&#36817;&#20284;&#22320;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#36817;&#20284;&#20110;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#26469;&#33258;&#20110;&#23545;&#24179;&#26041;&#25439;&#22833;&#30340;&#35757;&#32451;&#12290;&#19982;&#20043;&#21069;&#19987;&#38376;&#38024;&#23545;&#25351;&#25968;&#23614;&#25439;&#22833;&#24182;&#20351;&#29992;&#20013;&#38388;&#25903;&#25345;&#21521;&#37327;&#26426;&#20844;&#24335;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#22522;&#20110;Ji&#21644;Telgarsky&#65288;2021&#65289;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#30340;&#26032;&#36817;&#20284;&#31561;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#24674;&#22797;&#20102;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#19979;&#25351;&#25968;&#23614;&#25439;&#22833;&#30340;&#29616;&#26377;&#31934;&#30830;&#31561;&#25928;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#32039;&#23494;&#24615;&#30340;&#35777;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35777;&#25454;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.16402</link><description>&lt;p&gt;
GradSkip&#65306;&#20855;&#26377;&#26356;&#22909;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#36890;&#20449;&#21152;&#36895;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. (arXiv:2210.16402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#36890;&#20449;&#20043;&#21069;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26799;&#24230;&#31867;&#22411;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#20102;&#32422;&#21313;&#24180;&#65292;&#20294;&#26412;&#22320;&#35757;&#32451;&#30340;&#21152;&#36895;&#24615;&#36136;&#22312;&#29702;&#35770;&#19978;&#36824;&#26410;&#24471;&#21040;&#23436;&#20840;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;Mishchenko&#31561;&#20154;(2022 International Conference on Machine Learning)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#35777;&#26126;&#20102;&#24403;&#26412;&#22320;&#35757;&#32451;&#24471;&#21040;&#27491;&#30830;&#25191;&#34892;&#26102;&#65292;&#20250;&#23548;&#33268;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#65292;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#36825;&#19968;&#28857;&#25104;&#31435;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;ProxSkip&#35201;&#27714;&#25152;&#26377;&#23458;&#25143;&#31471;&#22312;&#27599;&#27425;&#36890;&#20449;&#36718;&#20013;&#25191;&#34892;&#30456;&#21516;&#25968;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#12290;&#28789;&#24863;&#26469;&#33258;&#24120;&#35782;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#36890;&#36807;&#29468;&#27979;&#35748;&#20026;&#25317;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#24212;&#35813;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#23601;&#33021;&#23436;&#25104;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing the clients to perform multiple local gradient-type training steps prior to communication. While methods of this type have been studied for about a decade, the empirically observed acceleration properties of local training eluded all attempts at theoretical understanding. In a recent breakthrough, Mishchenko et al. (ICML 2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their method ProxSkip requires all clients to take the same number of local training steps in each communication round. Inspired by a common sense intuition, we start our investigation by conjecturing that clients with ``less important'' data should be able to get away with fewer local training steps without this impacting the overall communication c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2010.11750</link><description>&lt;p&gt;
&#37327;&#21270;&#24322;&#26500;&#36716;&#31227;&#30340;&#31934;&#30830;&#39640;&#32500;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers. (arXiv:2010.11750v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26102;&#20351;&#29992;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20160;&#20040;&#26102;&#20505;&#23558;&#26469;&#33258;&#20004;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#21512;&#24182;&#27604;&#21333;&#29420;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26356;&#22909;&#65311;&#30452;&#35266;&#19978;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#36716;&#31227;&#25928;&#24212;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#65292;&#22914;&#26679;&#26412;&#22823;&#23567;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#36825;&#31181;&#36716;&#31227;&#25928;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#27604;&#36739;&#32852;&#21512;&#23398;&#20064;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#20043;&#38388;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19968;&#20010;&#20219;&#21153;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#20219;&#21153;&#20855;&#26377;&#27604;&#36739;&#20248;&#21183;&#21462;&#20915;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30830;&#20999;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#31867;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#19968;&#20123;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#21152;&#26102;&#65292;&#22266;&#23450;&#27604;&#20363;&#12290;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#20197;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of learning one task with samples from another task has received much interest recently. In this paper, we ask a fundamental question: when is combining data from two tasks better than learning one task alone? Intuitively, the transfer effect from one task to another task depends on dataset shifts such as sample sizes and covariance matrices. However, quantifying such a transfer effect is challenging since we need to compare the risks between joint learning and single-task learning, and the comparative advantage of one over the other depends on the exact kind of dataset shift between both tasks. This paper uses random matrix theory to tackle this challenge in a linear regression setting with two tasks. We give precise asymptotics about the excess risks of some commonly used estimators in the high-dimensional regime, when the sample sizes increase proportionally with the feature dimension at fixed ratios. The precise asymptotics is provided as a function of the sample sizes 
&lt;/p&gt;</description></item></channel></rss>