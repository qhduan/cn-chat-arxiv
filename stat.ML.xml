<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;</title><link>https://arxiv.org/abs/2208.04284</link><description>&lt;p&gt;
&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#33324;&#21270;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Rademacher Complexity-based Generalization Bounds for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#20998;&#31867;&#23569;&#37327;&#31867;&#21035;&#22270;&#20687;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#30340;&#21457;&#23637;&#23545;&#20110;&#39640;&#32500;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#26159;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Rademacher&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;CNNs&#30340;&#32593;&#32476;&#38271;&#24230;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35832;&#22914;ReLU&#65292;Leaky ReLU&#65292;Parametric Rectifier Linear Unit&#65292;Sigmoid&#21644;Tanh&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.05797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Ling&#31561;&#20154;&#30740;&#31350;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;DEQ&#65292;&#35813;&#20107;&#23454;&#20173;&#28982;&#25104;&#31435;&#12290;&#30001;&#20110;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#38480;&#21046;&#24179;&#34913;&#28857;&#30340;Gram&#30697;&#38453;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#24635;&#20307;Gram&#30697;&#38453;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#20855;&#26377;Hermite&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#26032;&#24418;&#24335;&#30340;&#21452;&#37325;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
&lt;/p&gt;</description></item></channel></rss>