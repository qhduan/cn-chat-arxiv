<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;</title><link>https://arxiv.org/abs/2403.18355</link><description>&lt;p&gt;
&#30417;&#30563;&#22810;&#26680;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Supervised Multiple Kernel Learning approaches for multi-omics data integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18355
&lt;/p&gt;
&lt;p&gt;
MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#25216;&#26415;&#30340;&#36827;&#23637;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#32452;&#23398;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22810;&#31181;&#24322;&#36136;&#25968;&#25454;&#28304;&#30340;&#38598;&#25104;&#30446;&#21069;&#26159;&#29983;&#29289;&#23398;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#28789;&#27963;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#22810;&#32452;&#23398;&#36755;&#20837;&#30340;&#22810;&#26679;&#24615;&#65292;&#23613;&#31649;&#23427;&#22312;&#22522;&#22240;&#32452;&#25968;&#25454;&#25366;&#25496;&#20013;&#26159;&#19968;&#31181;&#19981;&#24120;&#29992;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26680;&#34701;&#21512;&#31574;&#30053;&#30340;&#26032;&#39062;MKL&#26041;&#27861;&#12290;&#20026;&#20102;&#20174;&#36755;&#20837;&#26680;&#30340;&#20803;&#26680;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#26080;&#30417;&#30563;&#38598;&#25104;&#31639;&#27861;&#35843;&#25972;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#30417;&#30563;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#29992;&#20110;&#26680;&#34701;&#21512;&#21644;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;MKL&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#26356;&#22797;&#26434;&#12289;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;&#12290;&#22810;&#26680;&#23398;&#20064;&#20026;&#22810;&#32452;&#23398;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18355v1 Announce Type: cross  Abstract: Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our resu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11339</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#22270;&#23545;&#31216;&#24615;&#25171;&#30772;&#36827;&#34892;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11339
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36229;&#22270;&#30001;&#19968;&#32452;&#33410;&#28857;&#20197;&#21450;&#31216;&#20026;&#36229;&#36793;&#30340;&#33410;&#28857;&#23376;&#38598;&#21512;&#32452;&#25104;&#12290;&#26356;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#19968;&#20010;&#36229;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#32570;&#22833;&#30340;&#36229;&#36793;&#30340;&#20219;&#21153;&#12290;&#20026;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#23398;&#20064;&#30340;&#36229;&#36793;&#34920;&#31034;&#22312;&#21516;&#26500;&#19979;&#19981;&#22833;&#21435;&#21306;&#20998;&#33021;&#21147;&#26102;&#20855;&#26377;&#23436;&#20840;&#34920;&#36798;&#24615;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#22120;&#21463;&#21040;&#24191;&#20041;Weisfeiler Lehman-1&#65288;GWL-1&#65289;&#31639;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#65292;&#23427;&#26159;Weisfeiler Lehman-1&#31639;&#27861;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;GWL-1&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#12290;&#20107;&#23454;&#19978;&#65292;&#20855;&#26377;&#30456;&#21516;GWL-1&#20540;&#33410;&#28857;&#30340;&#35825;&#23548;&#23376;&#36229;&#22270;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#24050;&#32463;&#22312;GPU&#20869;&#23384;&#19978;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#35782;&#21035;&#20986;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#29305;&#23450;&#27491;&#21017;&#23376;&#36229;&#22270;&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11339v1 Announce Type: new  Abstract: A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06763</link><description>&lt;p&gt;
&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;&#21487;&#25193;&#23637;&#26680;&#36923;&#36753;&#22238;&#24402;&#65306;&#29702;&#35770;&#20998;&#26512;&#21644;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#26102;&#65292;&#32463;&#24120;&#38754;&#20020;&#23384;&#20648;&#38656;&#27714;&#21644;&#27169;&#22411;&#20013;&#28041;&#21450;&#30340;&#22823;&#37327;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26680;&#36923;&#36753;&#22238;&#24402;&#12290;&#30740;&#31350;&#39318;&#20808;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#65306;i) &#23545;KLR&#35299;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;ii) &#32473;&#20986;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;KLR&#35299;&#30340;&#19978;&#30028;&#65292;&#24182;&#26368;&#21518;&#25551;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;Nystr\"om KLR&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#29305;&#21270;&#12290;&#20043;&#21518;&#65292;&#23545;Nystr\"om KLR&#36827;&#34892;&#20102;&#35745;&#31639;&#39564;&#35777;&#12290;&#27979;&#35797;&#20102;&#22235;&#31181;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#26412;&#22343;&#21248;&#37319;&#26679;&#12289;k-means&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#26464;&#26438;&#24471;&#20998;&#30340;&#20004;&#31181;&#38750;&#22343;&#21248;&#26041;&#27861;&#12290;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi
&lt;/p&gt;</description></item><item><title>PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03687</link><description>&lt;p&gt;
Pard: &#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#29992;&#20110;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03687
&lt;/p&gt;
&lt;p&gt;
PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#65292;&#20294;&#20854;&#31616;&#21333;&#26377;&#25928;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#32622;&#25442;&#19981;&#21464;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#22270;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#21644;&#25104;&#21315;&#19978;&#19975;&#27493;&#30340;&#21435;&#22122;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PARD&#65292;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;PARD&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#26080;&#38656;&#20851;&#27880;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38598;&#21512;&#19981;&#21516;&#65292;&#22270;&#20013;&#30340;&#20803;&#32032;&#24182;&#19981;&#26159;&#23436;&#20840;&#26080;&#24207;&#30340;&#65292;&#33410;&#28857;&#21644;&#36793;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#37096;&#20998;&#39034;&#24207;&#12290;&#21033;&#29992;&#36825;&#20010;&#37096;&#20998;&#39034;&#24207;&#65292;PARD&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30340;&#27010;&#29575;&#20026;c&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#25104;&#26412;&#20989;&#25968;&#26469;&#23547;&#25214;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03970</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Optimal partitioning of directed acyclic graphs with dependent costs between clusters. (arXiv:2308.03970v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#25104;&#26412;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#36827;&#34892;&#26368;&#20248;&#20998;&#21306;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#25104;&#26412;&#20989;&#25968;&#26469;&#23547;&#25214;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20013;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#25512;&#26029;&#22330;&#26223;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#12289;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#22522;&#30784;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#21010;&#20998;&#25104;&#38598;&#32676;&#26469;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#65292;&#26368;&#20248;&#21010;&#20998;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#35201;&#20248;&#21270;&#30340;&#25104;&#26412;&#21462;&#20915;&#20110;&#38598;&#32676;&#20869;&#30340;&#33410;&#28857;&#20197;&#21450;&#36890;&#36807;&#29238;&#33410;&#28857;&#21644;/&#25110;&#23376;&#33410;&#28857;&#36830;&#25509;&#30340;&#38598;&#32676;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#20381;&#36182;&#38598;&#32676;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMAP&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#20381;&#36182;&#38598;&#32676;&#30340;&#26368;&#20248;&#38598;&#32676;&#26144;&#23556;&#12290;&#22312;&#22522;&#20110;DAG&#21644;&#38598;&#32676;&#26144;&#23556;&#30340;&#20219;&#24847;&#23450;&#20041;&#30340;&#27491;&#25104;&#26412;&#20989;&#25968;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;DCMAP&#25910;&#25947;&#20110;&#25214;&#21040;&#25152;&#26377;&#26368;&#20248;&#38598;&#32676;&#65292;&#24182;&#22312;&#36884;&#20013;&#36820;&#22238;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#23545;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#20989;&#25968;&#30340;&#19968;&#20010;&#28023;&#33609;&#22797;&#26434;&#31995;&#32479;&#30340;DBN&#27169;&#22411;&#20855;&#26377;&#26102;&#38388;&#25928;&#29575;&#24615;&#12290;&#23545;&#20110;&#19968;&#20010;25&#20010;&#21644;50&#20010;&#33410;&#28857;&#30340;DBN&#65292;&#25628;&#32034;&#31354;&#38388;&#22823;&#23567;&#20998;&#21035;&#20026;$9.91\times 10^9$&#21644;$1.5$
&lt;/p&gt;
&lt;p&gt;
Many statistical inference contexts, including Bayesian Networks (BNs), Markov processes and Hidden Markov Models (HMMS) could be supported by partitioning (i.e.~mapping) the underlying Directed Acyclic Graph (DAG) into clusters. However, optimal partitioning is challenging, especially in statistical inference as the cost to be optimised is dependent on both nodes within a cluster, and the mapping of clusters connected via parent and/or child nodes, which we call dependent clusters. We propose a novel algorithm called DCMAP for optimal cluster mapping with dependent clusters. Given an arbitrarily defined, positive cost function based on the DAG and cluster mappings, we show that DCMAP converges to find all optimal clusters, and returns near-optimal solutions along the way. Empirically, we find that the algorithm is time-efficient for a DBN model of a seagrass complex system using a computation cost function. For a 25 and 50-node DBN, the search space size was $9.91\times 10^9$ and $1.5
&lt;/p&gt;</description></item></channel></rss>