<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>TreeDOX&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#21644;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13836</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#29992;&#20110;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Tree-based Learning for High-Fidelity Prediction of Chaos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13836
&lt;/p&gt;
&lt;p&gt;
TreeDOX&#26159;&#19968;&#31181;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#21644;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#36827;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21464;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#65306;TreeDOX&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#36807;&#24230;&#23884;&#20837;&#20316;&#20026;&#26174;&#24335;&#30701;&#26399;&#35760;&#24518;&#65292;&#20197;&#21450;&#39069;&#22806;&#26641;&#22238;&#24402;&#22120;&#26469;&#25191;&#34892;&#29305;&#24449;&#38477;&#32500;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;Henon&#26144;&#23556;&#65292;Lorenz&#21644;Kuramoto-Sivashinsky&#31995;&#32479;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#30340;Southern Oscillation Index&#23637;&#31034;&#20102;TreeDOX&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13836v1 Announce Type: new  Abstract: Model-free forecasting of the temporal evolution of chaotic systems is crucial but challenging. Existing solutions require hyperparameter tuning, significantly hindering their wider adoption. In this work, we introduce a tree-based approach not requiring hyperparameter tuning: TreeDOX. It uses time delay overembedding as explicit short-term memory and Extra-Trees Regressors to perform feature reduction and forecasting. We demonstrate the state-of-the-art performance of TreeDOX using the Henon map, Lorenz and Kuramoto-Sivashinsky systems, and the real-world Southern Oscillation Index.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#33021;&#22815;&#24456;&#22909;&#24212;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#24182;&#23436;&#32654;&#22320;&#36866;&#24212;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.05928</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#32447;&#24615;&#20851;&#31995;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#22122;&#22768;&#32447;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#33021;&#22815;&#24456;&#22909;&#24212;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#24182;&#23436;&#32654;&#22320;&#36866;&#24212;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#25351;&#25554;&#20540;&#27169;&#22411;&#22312;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#30340;&#29616;&#35937;&#65292;&#26368;&#26089;&#20986;&#29616;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#23454;&#35777;&#35266;&#23519;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#25554;&#20540;&#35757;&#32451;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#26469;&#33258;&#20110;&#26126;&#26174;&#20998;&#31163;&#30340;&#31867;&#26465;&#20214;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#20801;&#35768;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#19968;&#23450;&#27604;&#20363;&#34987;&#23545;&#25163;&#31713;&#25913;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29305;&#28857;&#65306;&#23427;&#20204;&#21487;&#20197;&#34987;&#39537;&#21160;&#21040;&#38646;&#35757;&#32451;&#35823;&#24046;&#65292;&#23436;&#32654;&#22320;&#25311;&#21512;&#20219;&#20309;&#26377;&#22122;&#22768;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#12290;&#19982;&#20043;&#21069;&#20851;&#20110;&#33391;&#24615;&#36807;&#25311;&#21512;&#38656;&#35201;&#32447;&#24615;&#25110;&#22522;&#20110;&#26680;&#30340;&#39044;&#27979;&#22120;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22312;&#27169;&#22411;&#21644;&#23398;&#20064;&#21160;&#24577;&#37117;&#26159;&#22522;&#26412;&#38750;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent. To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve minimax optimal test error. In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.
&lt;/p&gt;</description></item></channel></rss>