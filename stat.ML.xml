<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.11908</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#28857;&#26641;&#30340;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#24314;&#31435;&#22312;&#20256;&#32479;&#38543;&#26426;&#26862;&#26519;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#26680;&#21152;&#26435;&#31639;&#27861;&#26469;&#26500;&#24314;&#20272;&#31639;&#22120;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26641;&#29983;&#38271;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#22522;&#20110;&#23450;&#28857;&#36845;&#20195;&#36817;&#20284;&#34920;&#31034;&#26799;&#24230;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#20026;&#27492;&#24320;&#21457;&#20102;&#28176;&#36817;&#29702;&#35770;&#12290;&#36825;&#26377;&#25928;&#22320;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#37327;&#30340;&#32500;&#24230;&#36866;&#20013;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#12290;&#22312;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21046;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#21644;&#32467;&#26500;&#20449;&#24687;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Distributionally robust risk evaluation with a causality constraint and structural information. (arXiv:2203.10571v3 [q-fin.MF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#22240;&#26524;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#12290;&#22312;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#21046;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25968;&#25454;&#30340;&#26399;&#26395;&#20989;&#25968;&#20540;&#30340;&#20998;&#24067;&#40065;&#26834;&#35780;&#20272;&#12290;&#19968;&#32452;&#22791;&#36873;&#24230;&#37327;&#36890;&#36807;&#22240;&#26524;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#23545;&#20598;&#24615;&#65292;&#24182;&#23558;&#22240;&#26524;&#32422;&#26463;&#37325;&#26500;&#20026;&#26080;&#38480;&#32500;&#27979;&#35797;&#20989;&#25968;&#31354;&#38388;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#29992;Rademacher&#22797;&#26434;&#24230;&#35777;&#26126;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#32467;&#26500;&#20449;&#24687;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#38480;&#21046;&#27169;&#31946;&#38598;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20598;&#24418;&#24335;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#23545;&#23454;&#29616;&#27874;&#21160;&#29575;&#21644;&#32929;&#25351;&#30340;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#32463;&#20856;&#26368;&#20248;&#20256;&#36755;&#20844;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies distributionally robust evaluation of expected function values over temporal data. A set of alternative measures is characterized by the causal optimal transport. We prove the strong duality and recast the causality constraint as minimization over an infinite-dimensional test function space. We approximate test functions by neural networks and prove the sample complexity with Rademacher complexity. Moreover, when structural information is available to further restrict the ambiguity set, we prove the dual formulation and provide efficient optimization methods. Empirical analysis of realized volatility and stock indices demonstrates that our framework offers an attractive alternative to the classic optimal transport formulation.
&lt;/p&gt;</description></item></channel></rss>