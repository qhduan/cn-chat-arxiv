<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12562</link><description>&lt;p&gt;
&#36890;&#36807;&#33719;&#21462;&#36171;&#26435;&#65306;&#25903;&#25345;&#23567;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Equity through Access: A Case for Small-scale Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24471;&#30410;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#35745;&#31639;&#21147;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#36164;&#28304;&#34987;&#29992;&#20110;&#35757;&#32451;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#12289;&#25968;&#25454;&#12289;&#33021;&#28304;&#21644;&#30899;&#25490;&#25918;&#26041;&#38754;&#28040;&#32791;&#24040;&#22823;&#12290;&#36825;&#20123;&#25104;&#26412;&#27491;&#22312;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#26032;&#22411;&#20934;&#20837;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22312;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#36164;&#28304;&#26377;&#38480;&#30340;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#35270;&#20102;&#29616;&#26377;&#35270;&#35273;&#20219;&#21153;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;DL&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#24615;&#33021;&#19982;&#36164;&#28304;&#21333;&#20803;&#30340;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;PePR&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#65288;&#36328;&#24230;&#20174;1M&#21040;130M&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65289;&#21644;&#19977;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#26377;&#20851;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20851;&#31995;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11351</link><description>&lt;p&gt;
&#22522;&#20110;SDP&#30340;&#20108;&#20998;&#22270;&#32858;&#31867;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An SDP-based Branch-and-Cut Algorithm for Biclustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#32858;&#31867;&#65292;&#20063;&#31216;&#20026;&#20849;&#32858;&#31867;&#12289;&#22359;&#32858;&#31867;&#25110;&#21452;&#21521;&#32858;&#31867;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#21516;&#26102;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#32452;&#65292;&#20351;&#24471;&#21516;&#19968;&#32452;&#20869;&#30340;&#34892;&#21644;&#21015;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#20316;&#20026;&#20108;&#20998;&#22270;&#32858;&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21152;&#26435;&#23436;&#20840;&#20108;&#20998;&#22270;&#20013;&#35782;&#21035; $k$ &#20010;&#19981;&#30456;&#20132;&#30340;&#23436;&#20840;&#20108;&#37096;&#23376;&#22270;&#65288;&#31216;&#20026;&#21452;&#22242;&#65289;&#65292;&#20351;&#23427;&#20204;&#30340;&#23494;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#12290;&#23545;&#20110;&#19978;&#30028;&#20363;&#31243;&#65292;&#25105;&#20204;&#32771;&#34385;&#21322;&#23450;&#35268;&#21010;&#25918;&#26494;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21152;&#24378;&#30028;&#38480;&#30340;&#26377;&#25928;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#38454;&#26041;&#27861;&#20197;&#20999;&#24179;&#38754;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#25918;&#26494;&#38382;&#39064;&#12290;&#23545;&#20110;&#19979;&#30028;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#22823;&#26435;&#21305;&#37197;&#33293;&#20837;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15513</link><description>&lt;p&gt;
&#35843;&#25972;&#22256;&#24785;&#24230;&#24182;&#35745;&#31639;&#22522;&#20110;&#37319;&#26679;&#30340;t-SNE&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#24120;&#29992;&#30340;&#31649;&#36947;&#21033;&#29992;&#20108;&#32500;&#21487;&#35270;&#21270;&#65292;&#20363;&#22914;&#36890;&#36807;t&#20998;&#24067;&#37051;&#36817;&#38543;&#26426;&#23884;&#20837;&#65288;t-SNE&#65289;&#12290;&#20294;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#20250;&#29983;&#25104;&#27425;&#20248;&#30340;&#23884;&#20837;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#19981;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#12290;&#23558;&#36825;&#20123;&#21442;&#25968;&#22686;&#21152;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35745;&#31639;&#23545;&#20110;&#23454;&#38469;&#24037;&#20316;&#27969;&#31243;&#26469;&#35828;&#22826;&#26114;&#36149;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#37319;&#26679;&#30340;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24517;&#39035;&#35880;&#24910;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#21462;&#20915;&#20110;&#37319;&#26679;&#29575;&#21644;&#39044;&#26399;&#30340;&#26368;&#32456;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item></channel></rss>