<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.02004</link><description>&lt;p&gt;
&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#21450;log-Sobolev&#21644;Talagrand&#19981;&#31561;&#24335;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02004
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#19968;&#33324;&#21270;&#30340;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20197;&#21450;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;(PGD)~(Kuntz&#31561;&#20154;&#65292;2023)&#30340;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#38480;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#25955;&#21270;&#33258;&#30001;&#33021;&#26799;&#24230;&#27969;&#33719;&#24471;&#30340;&#22823;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#20110;&#28385;&#36275;&#19968;&#33324;&#21270;log-Sobolev&#21644;Polyak-Lojasiewicz&#19981;&#31561;&#24335;&#65288;LSI&#21644;PLI&#65289;&#30340;&#27169;&#22411;&#65292;&#27969;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#21040;&#33258;&#30001;&#33021;&#30340;&#26497;&#23567;&#21270;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#20248;&#36755;&#36816;&#25991;&#29486;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#65288;LSI&#24847;&#21619;&#30528;Talagrand&#19981;&#31561;&#24335;&#65289;&#21450;&#20854;&#22312;&#20248;&#21270;&#25991;&#29486;&#20013;&#30340;&#23545;&#24212;&#29289;&#65288;PLI&#24847;&#21619;&#30528;&#25152;&#35859;&#30340;&#20108;&#27425;&#22686;&#38271;&#26465;&#20214;&#65289;&#25193;&#23637;&#24182;&#24212;&#29992;&#21040;&#25105;&#20204;&#30340;&#26032;&#35774;&#32622;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25512;&#24191;&#20102;Bakry-Emery&#23450;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#24378;&#20985;&#23545;&#25968;&#20284;&#28982;&#30340;&#27169;&#22411;&#65292;LSI/PLI&#30340;&#27010;&#25324;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#20302;&#22266;&#26377;&#25968;&#25454;&#32500;&#24230;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#20272;&#35745;&#23494;&#24230;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#28041;&#21450;&#25968;&#25454;&#21644;&#28508;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#32467;&#26524;&#19982;&#30446;&#26631;&#30340;&#26399;&#26395;Wasserstein-1&#36317;&#31163;&#30340;&#32553;&#25918;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.15801</link><description>&lt;p&gt;
&#20851;&#20110;&#29992;&#20110;&#20302;&#22266;&#26377;&#25968;&#25454;&#32500;&#24230;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#30340;&#32479;&#35745;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Properties of Generative Adversarial Models for Low Intrinsic Data Dimension. (arXiv:2401.15801v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#20302;&#22266;&#26377;&#25968;&#25454;&#32500;&#24230;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#30340;&#32479;&#35745;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#20272;&#35745;&#23494;&#24230;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#28041;&#21450;&#25968;&#25454;&#21644;&#28508;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#32467;&#26524;&#19982;&#30446;&#26631;&#30340;&#26399;&#26395;Wasserstein-1&#36317;&#31163;&#30340;&#32553;&#25918;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#20854;&#32479;&#35745;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#20173;&#28982;&#30456;&#23545;&#24754;&#35266;&#12290;&#29305;&#21035;&#26159;&#22312;&#24212;&#29992;GANs&#30340;&#25968;&#25454;&#20998;&#24067;&#65288;&#22914;&#33258;&#28982;&#22270;&#20687;&#65289;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#20854;&#22312;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20855;&#26377;&#22266;&#26377;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#20294;&#36825;&#22312;&#29616;&#26377;&#20998;&#26512;&#20013;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#21453;&#26144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#25512;&#23548;&#20851;&#20110;&#25968;&#25454;&#21644;&#28508;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#24230;&#30340;&#32479;&#35745;&#20445;&#35777;&#26469;&#24357;&#21512;GANs&#21450;&#20854;&#21452;&#21521;&#21464;&#20307;BiGANs&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#20998;&#26512;&#22320;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#26377;&#26469;&#33258;&#26410;&#30693;&#30446;&#26631;&#20998;&#24067;&#30340; n &#20010;&#26679;&#26412;&#65292;&#24182;&#19988;&#36873;&#25321;&#20102;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#37027;&#20040;&#20174;&#30446;&#26631;&#20013;&#20272;&#35745;&#24471;&#20986;&#30340;&#26399;&#26395; Wasserstein-1 &#36317;&#31163;&#20250;&#25353;&#29031; $O(n^{-1/d_\mu })$ &#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable empirical successes of Generative Adversarial Networks (GANs), the theoretical guarantees for their statistical accuracy remain rather pessimistic. In particular, the data distributions on which GANs are applied, such as natural images, are often hypothesized to have an intrinsic low-dimensional structure in a typically high-dimensional feature space, but this is often not reflected in the derived rates in the state-of-the-art analyses. In this paper, we attempt to bridge the gap between the theory and practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs), by deriving statistical guarantees on the estimated densities in terms of the intrinsic dimension of the data and the latent space. We analytically show that if one has access to $n$ samples from the unknown target distribution and the network architectures are properly chosen, the expected Wasserstein-1 distance of the estimates from the target scales as $O\left( n^{-1/d_\mu } \right)$
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.20360</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#65306;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#24102;&#26377;&#25209;&#24402;&#19968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#22522;&#26412;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#12289;&#21152;&#36895;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20960;&#20010;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#31215;&#20998;&#65289;&#12289;&#20248;&#21270;&#29702;&#35770;&#65288;&#21253;&#25324;Kurdyka-Lojasiewicz&#19981;&#31561;&#24335;&#65289;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#20070;&#30340;&#26368;&#21518;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#19968;&#20123;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Galerkin&#26041;&#27861;&#12290;&#24076;&#26395;&#26412;&#20070;&#33021;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10301</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#31361;&#20986;&#20316;&#29992;&#65306;&#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#24403;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#20986;&#29616;&#12290;&#34429;&#28982;&#35768;&#22810;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#30456;&#24403;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#26159;&#30450;&#30446;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#35201;&#30340;&#26159;&#28548;&#28165;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#20855;&#22791;&#33391;&#22909;&#30446;&#26631;&#24615;&#33021;&#30340;&#20551;&#35774;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#39044;&#27979;&#20013;&#20855;&#22791;&#26465;&#20214;&#19981;&#21464;&#30340;&#32452;&#20214;&#65288;CICs&#65289;&#30340;&#23384;&#22312;&#20551;&#35774;&#65292;&#36825;&#20123;&#32452;&#20214;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#20445;&#25345;&#26465;&#20214;&#19981;&#21464;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CICs&#65292;&#36890;&#36807;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;CIP&#65289;&#21487;&#20197;&#20272;&#35745;&#65292;&#20855;&#22791;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25552;&#20379;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#30340;&#19977;&#20010;&#31361;&#20986;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CICs&#30340;&#26032;&#31639;&#27861;&#65292;&#21363;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;IW-CIP&#65289;&#65292;&#23427;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
&lt;/p&gt;</description></item></channel></rss>