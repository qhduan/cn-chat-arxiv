<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>InVA&#26159;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#20010;&#22270;&#20687;&#26469;&#36827;&#34892;&#39044;&#27979;&#25512;&#29702;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;VAE&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02734</link><description>&lt;p&gt;
InVA: &#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02734
&lt;/p&gt;
&lt;p&gt;
InVA&#26159;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#20010;&#22270;&#20687;&#26469;&#36827;&#34892;&#39044;&#27979;&#25512;&#29702;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;VAE&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25506;&#32034;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#25104;&#20687;&#27169;&#24335;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#30740;&#31350;&#22522;&#20110;&#22810;&#20010;&#22270;&#20687;&#26469;&#25512;&#26029;&#22270;&#20687;&#30340;&#39044;&#27979;&#25512;&#29702;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#20511;&#29992;&#22810;&#20010;&#25104;&#20687;&#27169;&#24335;&#20043;&#38388;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#26412;&#25991;&#24314;&#31435;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25991;&#29486;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;InVA&#65289;&#26041;&#27861;&#65292;&#23427;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#30340;&#22810;&#20010;&#22270;&#20687;&#20013;&#20511;&#29992;&#20449;&#24687;&#26469;&#32472;&#21046;&#22270;&#20687;&#30340;&#39044;&#27979;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#32467;&#26524;&#22270;&#20687;&#19982;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#32852;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#35745;&#31639;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;InVA&#30456;&#23545;&#20110;&#36890;&#24120;&#19981;&#20801;&#35768;&#20511;&#29992;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#20449;&#24687;&#30340;VAE&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \texttt{InVA} over VAEs, which typically do not allow borrowing information between input imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item></channel></rss>