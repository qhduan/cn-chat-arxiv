<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04114</link><description>&lt;p&gt;
SCAFFLSA&#65306;&#37327;&#21270;&#21644;&#28040;&#38500;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#21644;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;FedLSA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#20998;&#26512;&#12290;&#25105;&#20204;&#26126;&#30830;&#37327;&#21270;&#20102;&#24322;&#36136;&#20195;&#29702;&#26412;&#22320;&#35757;&#32451;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FedLSA&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#19982;&#25152;&#38656;&#31934;&#24230; $\epsilon$ &#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCAFFLSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;FedLSA&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#26657;&#27491;&#26412;&#22320;&#35757;&#32451;&#30340;&#20559;&#24046;&#65292;&#24182;&#22312;&#19981;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#24212;&#30340;&#22797;&#26434;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.15502</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bayesian Tests. (arXiv:2401.15502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#26426;&#23494;&#25968;&#25454;&#36827;&#34892;&#31185;&#23398;&#20551;&#35774;&#26816;&#39564;&#30340;&#39046;&#22495;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#22522;&#30707;&#12290;&#22312;&#25253;&#21578;&#31185;&#23398;&#21457;&#29616;&#26102;&#65292;&#24191;&#27867;&#37319;&#29992;&#36125;&#21494;&#26031;&#26816;&#39564;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;P&#20540;&#30340;&#20027;&#35201;&#25209;&#35780;&#65292;&#21363;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#26080;&#27861;&#37327;&#21270;&#23545;&#31454;&#20105;&#20551;&#35774;&#30340;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22522;&#20110;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#22522;&#30784;&#19978;&#33258;&#28982;&#20135;&#29983;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25512;&#26029;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#22240;&#23376;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24314;&#27169;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#23454;&#36136;&#24615;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#22312;&#25152;&#25552;&#26694;&#26550;&#19979;&#30830;&#31435;&#36125;&#21494;&#26031;&#22240;&#23376;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#23558;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#25955;&#24230;&#26041;&#27861;&#21644;Wasserstein&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#20197;&#21516;&#26102;&#25200;&#21160;&#20284;&#28982;&#21644;&#32467;&#26524;&#30340;&#26368;&#20248;&#23545;&#25239;&#20998;&#24067;&#12290;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05414</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#32479;&#19968;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unifying Distributionally Robust Optimization via Optimal Transport Theory. (arXiv:2308.05414v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#23558;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#25955;&#24230;&#26041;&#27861;&#21644;Wasserstein&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#20197;&#21516;&#26102;&#25200;&#21160;&#20284;&#28982;&#21644;&#32467;&#26524;&#30340;&#26368;&#20248;&#23545;&#25239;&#20998;&#24067;&#12290;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#23545;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65306;&#22522;&#20110;&#25955;&#24230;&#21644;&#22522;&#20110;Wasserstein&#30340;&#26041;&#27861;&#12290;&#25955;&#24230;&#26041;&#27861;&#20351;&#29992;&#20284;&#28982;&#27604;&#26469;&#24314;&#27169;&#38169;&#37197;&#65292;&#32780;&#21518;&#32773;&#20351;&#29992;&#23454;&#38469;&#32467;&#26524;&#30340;&#36317;&#31163;&#25110;&#25104;&#26412;&#26469;&#24314;&#27169;&#38169;&#37197;&#12290;&#22312;&#36825;&#20123;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816; (OT) &#21644;&#26465;&#20214;&#30697;&#32422;&#26463;&#30340;&#26694;&#26550;&#20013;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#24471;&#26368;&#20248;&#23545;&#25239;&#20998;&#24067;&#21516;&#26102;&#25200;&#21160;&#20284;&#28982;&#21644;&#32467;&#26524;&#65292;&#24182;&#22312;&#22522;&#32447;&#27169;&#22411;&#21644;&#23545;&#25239;&#27169;&#22411;&#20043;&#38388;&#20135;&#29983;&#19968;&#20010;&#26368;&#20248; (&#20174;&#26368;&#20248;&#36755;&#36816;&#24847;&#20041;&#19978;) &#30340;&#32806;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#20960;&#20010;&#23545;&#20598;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#34892;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, there has been considerable interest in two prominent approaches for Distributionally Robust Optimization (DRO): Divergence-based and Wasserstein-based methods. The divergence approach models misspecification in terms of likelihood ratios, while the latter models it through a measure of distance or cost in actual outcomes. Building upon these advances, this paper introduces a novel approach that unifies these methods into a single framework based on optimal transport (OT) with conditional moment constraints. Our proposed approach, for example, makes it possible for optimal adversarial distributions to simultaneously perturb likelihood and outcomes, while producing an optimal (in an optimal transport sense) coupling between the baseline model and the adversarial model.Additionally, the paper investigates several duality results and presents tractable reformulations that enhance the practical applicability of this unified framework.
&lt;/p&gt;</description></item></channel></rss>