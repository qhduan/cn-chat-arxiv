<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#39537;&#21160;&#24773;&#22659;&#20013;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#23548;&#33268;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#35823;&#23548;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03868</link><description>&lt;p&gt;
&#28966;&#28857;&#32622;&#20449;: &#24102;&#26377;&#36873;&#25321;&#26465;&#20214;&#35206;&#30422;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#39537;&#21160;&#24773;&#22659;&#20013;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#23548;&#33268;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#35823;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20307;&#39044;&#27979;&#24314;&#31435;&#22312;&#36793;&#32536;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#38388;&#19978;&#65292;&#35813;&#21306;&#38388;&#20197;&#26576;&#31181;&#35268;&#23450;&#30340;&#27010;&#29575;&#35206;&#30422;&#20102;&#38543;&#26426;&#25277;&#21462;&#30340;&#26032;&#27979;&#35797;&#28857;&#30340;&#26410;&#30693;&#32467;&#26524;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24120;&#35265;&#24773;&#20917;&#26159;&#65292;&#22312;&#30475;&#21040;&#27979;&#35797;&#21333;&#20803;&#21518;&#65292;&#20174;&#19994;&#32773;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20915;&#23450;&#20851;&#27880;&#21738;&#20123;&#27979;&#35797;&#21333;&#20803;&#65292;&#24182;&#24076;&#26395;&#37327;&#21270;&#28966;&#28857;&#21333;&#20803;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#36825;&#20123;&#28966;&#28857;&#21333;&#20803;&#30340;&#36793;&#32536;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;&#21487;&#33021;&#20250;&#22240;&#36873;&#25321;&#20559;&#24046;&#32780;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#31934;&#30830;&#35206;&#30422;&#30340;&#39044;&#27979;&#38598;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#35206;&#30422;&#26159;&#26377;&#26465;&#20214;&#20110;&#25152;&#36873;&#21333;&#20803;&#30340;&#12290;&#20854;&#19968;&#33324;&#24418;&#24335;&#36866;&#29992;&#20110;&#20219;&#24847;&#36873;&#25321;&#35268;&#21017;&#65292;&#24182;&#23558;Mondrian&#25972;&#20307;&#39044;&#27979;&#25512;&#24191;&#21040;&#22810;&#20010;&#27979;&#35797;&#21333;&#20803;&#21644;&#38750;&#31561;&#21464;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#22810;&#20010;&#29616;&#23454;&#30340;&#36873;&#25321;&#35268;&#21017;&#35745;&#31639;&#20102;&#36866;&#29992;&#20110;&#25105;&#20204;&#26694;&#26550;&#30340;&#35745;&#31639;&#25928;&#29575;&#23454;&#29616;&#65292;&#21253;&#25324;top-K&#36873;&#25321;&#12289;&#20248;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03868v1 Announce Type: cross  Abstract: Conformal prediction builds marginally valid prediction intervals which cover the unknown outcome of a randomly drawn new test point with a prescribed probability. In practice, a common scenario is that, after seeing the test unit(s), practitioners decide which test unit(s) to focus on in a data-driven manner, and wish to quantify the uncertainty for the focal unit(s). In such cases, marginally valid prediction intervals for these focal units can be misleading due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage conditional on the unit being selected. Its general form works for arbitrary selection rules, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We then work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization
&lt;/p&gt;</description></item><item><title>DHQRN&#21487;&#20197;&#39044;&#27979;&#26356;&#19968;&#33324;&#30340;Huber&#20998;&#20301;&#25968;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10306</link><description>&lt;p&gt;
&#28145;&#24230;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Huber quantile regression networks. (arXiv:2306.10306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10306
&lt;/p&gt;
&lt;p&gt;
DHQRN&#21487;&#20197;&#39044;&#27979;&#26356;&#19968;&#33324;&#30340;Huber&#20998;&#20301;&#25968;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#24212;&#29992;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25110;&#32477;&#23545;&#35823;&#24046;&#35780;&#20998;&#20989;&#25968;&#26469;&#25253;&#21578;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#22343;&#20540;&#25110;&#20013;&#20301;&#25968;&#12290;&#21457;&#20986;&#26356;&#22810;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20989;&#25968;&#65288;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#65289;&#30340;&#37325;&#35201;&#24615;&#24050;&#34987;&#35748;&#20026;&#26159;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36890;&#36807;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#21644;ERNN&#65289;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#32593;&#32476;&#65288;DHQRN&#65289;&#65292;&#23427;&#23558;QRNN&#21644;ERNN&#23884;&#22871;&#20026;&#36793;&#32536;&#24773;&#20917;&#12290; DHQRN&#21487;&#20197;&#39044;&#27979;Huber&#20998;&#20301;&#25968;&#65292;&#36825;&#26159;&#26356;&#19968;&#33324;&#30340;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#23884;&#22871;&#36215;&#26469;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#20989;&#25968;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#19982;Huber&#20998;&#20301;&#25968;&#21151;&#33021;&#19968;&#33268;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;DHQRN&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;&#25151;&#20215;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#20854;&#20182;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20960;&#20010;&#35823;&#24046;&#25351;&#26631;&#20013;&#65292;DHQRN&#32988;&#36807;&#20854;&#20182;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical machine learning regression applications aim to report the mean or the median of the predictive probability distribution, via training with a squared or an absolute error scoring function. The importance of issuing predictions of more functionals of the predictive probability distribution (quantiles and expectiles) has been recognized as a means to quantify the uncertainty of the prediction. In deep learning (DL) applications, that is possible through quantile and expectile regression neural networks (QRNN and ERNN respectively). Here we introduce deep Huber quantile regression networks (DHQRN) that nest QRNNs and ERNNs as edge cases. DHQRN can predict Huber quantiles, which are more general functionals in the sense that they nest quantiles and expectiles as limiting cases. The main idea is to train a deep learning algorithm with the Huber quantile regression function, which is consistent for the Huber quantile functional. As a proof of concept, DHQRN are applied to predict hou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#25152;&#23548;&#33268;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#21516;&#26102;&#20250;&#25913;&#36827;&#27867;&#21270;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.17592</link><description>&lt;p&gt;
(&#36817;&#20284;)&#32676;&#31561;&#21464;&#24615;&#19979;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Approximation-Generalization Trade-offs under (Approximate) Group Equivariance. (arXiv:2305.17592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#25152;&#23548;&#33268;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#21516;&#26102;&#20250;&#25913;&#36827;&#27867;&#21270;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#24050;&#25104;&#20026;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#24120;&#35268;&#35774;&#35745;&#20934;&#21017;&#12290;&#20363;&#22914;&#65292;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#34507;&#30333;&#36136;&#21644;&#33647;&#29289;&#35774;&#35745;&#31561;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26222;&#36941;&#24863;&#35273;&#26159;&#65292;&#23558;&#30456;&#20851;&#23545;&#31216;&#24615;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20250;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26377;&#20154;&#35748;&#20026;&#65292;&#24403;&#25968;&#25454;&#21644;/&#25110;&#27169;&#22411;&#21482;&#33021;&#34920;&#29616;&#20986;$\textit{&#36817;&#20284;}$&#25110;$\textit{&#37096;&#20998;}$&#23545;&#31216;&#24615;&#26102;&#65292;&#26368;&#20248;&#25110;&#26368;&#22909;&#24615;&#33021;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27169;&#22411;&#23545;&#40784;&#20110;&#25968;&#25454;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#30452;&#35273;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#19968;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#33324;&#30340;&#25968;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#23558;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#35201;&#27714;&#21464;&#25442;&#26159;&#26377;&#38480;&#30340;&#65292;&#29978;&#33267;&#19981;&#38656;&#35201;&#24418;&#25104;&#23436;&#25972;&#30340;....
&lt;/p&gt;
&lt;p&gt;
The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\textit{approximate}$ or $\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.11740</link><description>&lt;p&gt;
&#20851;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#21892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#30340;&#25968;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20854;&#31532;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#24403;&#22312;&#20687;ImageNet&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#31532;&#19968;&#23618;&#24448;&#24448;&#23398;&#20064;&#21040;&#19982;&#26041;&#21521;&#36793;&#36890;&#28388;&#27874;&#22120;&#38750;&#24120;&#30456;&#20284;&#30340;&#21442;&#25968;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;Gabor&#28388;&#27874;&#22120;&#36827;&#34892;&#23376;&#37319;&#26679;&#21367;&#31215;&#23481;&#26131;&#20986;&#29616;&#28151;&#21472;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#36755;&#20837;&#30340;&#23567;&#20559;&#31227;&#25935;&#24863;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#22823;&#27744;&#21270;&#31639;&#23376;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#20351;&#20854;&#20960;&#20046;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#23376;&#37319;&#26679;&#21367;&#31215;&#21518;&#26368;&#22823;&#27744;&#21270;&#30340;&#20301;&#31227;&#31283;&#23450;&#24615;&#24230;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#28388;&#27874;&#22120;&#30340;&#39057;&#29575;&#21644;&#26041;&#21521;&#22312;&#23454;&#29616;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#21452;&#26641;&#22797;&#23567;&#27874;&#21253;&#21464;&#25442;&#30340;&#30830;&#23450;&#24615;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21363;&#31163;&#25955;Gabor&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on improving the mathematical interpretability of convolutional neural networks (CNNs) in the context of image classification. Specifically, we tackle the instability issue arising in their first layer, which tends to learn parameters that closely resemble oriented band-pass filters when trained on datasets like ImageNet. Subsampled convolutions with such Gabor-like filters are prone to aliasing, causing sensitivity to small input shifts. In this context, we establish conditions under which the max pooling operator approximates a complex modulus, which is nearly shift invariant. We then derive a measure of shift invariance for subsampled convolutions followed by max pooling. In particular, we highlight the crucial role played by the filter's frequency and orientation in achieving stability. We experimentally validate our theory by considering a deterministic feature extractor based on the dual-tree complex wavelet packet transform, a particular case of discrete Gabor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26368;&#23567;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#26032;&#20272;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;&#26680;&#33539;&#25968;&#32602;&#39033;&#26377;&#21161;&#20110;&#35299;&#20915;&#20302;&#31209;&#22238;&#24402;&#22120;&#19979;&#30340;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#27169;&#22411;&#30340;&#28508;&#22312;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#37325;&#35201;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1810.10987</link><description>&lt;p&gt;
&#38754;&#26495;&#22238;&#24402;&#27169;&#22411;&#30340;&#26680;&#33539;&#25968;&#35268;&#21017;&#21270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nuclear Norm Regularized Estimation of Panel Regression Models. (arXiv:1810.10987v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1810.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26368;&#23567;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#26032;&#20272;&#35745;&#26041;&#27861;&#65292;&#20854;&#20013;&#26680;&#33539;&#25968;&#32602;&#39033;&#26377;&#21161;&#20110;&#35299;&#20915;&#20302;&#31209;&#22238;&#24402;&#22120;&#19979;&#30340;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#27169;&#22411;&#30340;&#28508;&#22312;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#37325;&#35201;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#30340;&#38754;&#26495;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#26032;&#20272;&#35745;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26368;&#23567;&#21270;&#27531;&#24046;&#24179;&#26041;&#21644;&#65292;&#24102;&#26377;&#26680;&#65288;&#36857;&#65289;&#33539;&#25968;&#35268;&#21017;&#21270;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26368;&#23567;&#21270;&#27531;&#24046;&#30340;&#26680;&#33539;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#20010;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#19982;&#29616;&#26377;&#30340;&#26368;&#23567;&#20108;&#20056;&#65288;LS&#65289;&#20272;&#35745;&#22120;&#30456;&#27604;&#20855;&#26377;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#23450;&#20041;&#20026;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#26680;&#33539;&#25968;&#32602;&#39033;&#26377;&#21161;&#20110;&#35299;&#20915;&#20132;&#20114;&#22266;&#23450;&#25928;&#24212;&#27169;&#22411;&#30340;&#28508;&#22312;&#35782;&#21035;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#22238;&#24402;&#22120;&#26159;&#20302;&#31209;&#30340;&#19988;&#22240;&#32032;&#25968;&#37327;&#26410;&#30693;&#26102;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26680;&#33539;&#25968;&#35268;&#21017;&#21270;&#20272;&#35745;&#22120;&#26500;&#36896;&#28176;&#36817;&#31561;&#25928;&#20110;Bai&#65288;2009&#24180;&#65289;&#21644;Moon&#21644;Weidner&#65288;2017&#24180;&#65289;&#26368;&#23567;&#20108;&#20056;&#65288;LS&#65289;&#20272;&#35745;&#22120;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate panel regression models with interactive fixed effects. We propose two new estimation methods that are based on minimizing convex objective functions. The first method minimizes the sum of squared residuals with a nuclear (trace) norm regularization. The second method minimizes the nuclear norm of the residuals. We establish the consistency of the two resulting estimators. Those estimators have a very important computational advantage compared to the existing least squares (LS) estimator, in that they are defined as minimizers of a convex objective function. In addition, the nuclear norm penalization helps to resolve a potential identification problem for interactive fixed effect models, in particular when the regressors are low-rank and the number of the factors is unknown. We also show how to construct estimators that are asymptotically equivalent to the least squares (LS) estimator in Bai (2009) and Moon and Weidner (2017) by using our nuclear norm regul
&lt;/p&gt;</description></item></channel></rss>