<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#30740;&#31350;&#20102;Metropolis-within-Gibbs&#26041;&#26696;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24314;&#31435;&#20102;&#19982;&#25968;&#20540;&#35777;&#25454;&#23494;&#20999;&#19968;&#33268;&#30340;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#20108;&#20803;&#22238;&#24402;&#21644;&#31163;&#25955;&#35266;&#23519;&#25193;&#25955;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09416</link><description>&lt;p&gt;
Metropolis-within-Gibbs&#26041;&#26696;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09416
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Metropolis-within-Gibbs&#26041;&#26696;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24314;&#31435;&#20102;&#19982;&#25968;&#20540;&#35777;&#25454;&#23494;&#20999;&#19968;&#33268;&#30340;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#20108;&#20803;&#22238;&#24402;&#21644;&#31163;&#25955;&#35266;&#23519;&#25193;&#25955;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33324;&#30340;&#22352;&#26631;&#36880;&#27493;MCMC&#26041;&#26696;&#65288;&#22914;Metropolis-within-Gibbs&#25277;&#26679;&#22120;&#65289;&#65292;&#36825;&#20123;&#26041;&#26696;&#36890;&#24120;&#29992;&#20110;&#25311;&#21512;&#36125;&#21494;&#26031;&#38750;&#20849;&#36717;&#20998;&#23618;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;&#30456;&#24212;&#30340;&#65288;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;Gibbs&#25277;&#26679;&#22120;&#30340;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#36890;&#36807;&#26465;&#20214;&#23548;&#32435;&#30340;&#27010;&#24565;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#27969;&#34892;&#30340;Metropolis-within-Gibbs&#26041;&#26696;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65288;&#25968;&#25454;&#28857;&#21644;&#21442;&#25968;&#21516;&#26102;&#22686;&#21152;&#65289;&#30340;&#38750;&#20849;&#36717;&#20998;&#23618;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#32473;&#23450;&#38543;&#26426;&#25968;&#25454;&#29983;&#25104;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#25968;&#20540;&#35777;&#25454;&#23494;&#20999;&#19968;&#33268;&#30340;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#36824;&#35752;&#35770;&#20102;&#22312;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#20108;&#20803;&#22238;&#24402;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#31163;&#25955;&#35266;&#23519;&#25193;&#25955;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#21463;&#36825;&#31867;&#32479;&#35745;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20851;&#20110;&#36817;&#20284;&#23548;&#32435;&#21644;&#25200;&#21160;&#30340;&#29420;&#31435;&#20852;&#36259;&#30340;&#36741;&#21161;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09416v1 Announce Type: cross  Abstract: We study general coordinate-wise MCMC schemes (such as Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian non-conjugate hierarchical models. We relate their convergence properties to the ones of the corresponding (potentially not implementable) Gibbs sampler through the notion of conditional conductance. This allows us to study the performances of popular Metropolis-within-Gibbs schemes for non-conjugate hierarchical models, in high-dimensional regimes where both number of datapoints and parameters increase. Given random data-generating assumptions, we establish dimension-free convergence results, which are in close accordance with numerical evidences. Applications to Bayesian models for binary regression with unknown hyperparameters and discretely observed diffusions are also discussed. Motivated by such statistical applications, auxiliary results of independent interest on approximate conductances and perturba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12143</link><description>&lt;p&gt;
&#31616;&#21333;&#26426;&#21046;&#29992;&#20110;&#34920;&#31034;&#12289;&#32034;&#24341;&#21644;&#25805;&#20316;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20998;&#31867;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#36825;&#28041;&#21450;&#35774;&#32622;&#27169;&#22411;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#23427;&#20197;&#36866;&#24212;&#20855;&#26377;&#26631;&#35760;&#27010;&#24565;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#19968;&#20010;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#30697;&#38453;&#32479;&#35745;&#37327;&#26469;&#29983;&#25104;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#12290;&#36825;&#20123;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19968;&#32452;&#27010;&#24565;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20174;&#36825;&#20123;&#31614;&#21517;&#20013;&#23398;&#20064;&#35813;&#32467;&#26500;&#26469;&#36882;&#24402;&#22320;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#24403;&#27010;&#24565;"&#30456;&#20132;"&#26102;&#65292;&#27010;&#24565;&#30340;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;"&#30456;&#20132;"&#27010;&#24565;&#20013;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#20027;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#20445;&#25345;&#19968;&#20010;&#27010;&#24565;&#23383;&#20856;&#65292;&#20197;&#20415;&#36755;&#20837;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#24182;&#34987;&#36335;&#30001;&#21040;&#19982;&#36755;&#20837;&#30340;(&#28508;&#22312;)&#29983;&#25104;&#30456;&#20851;&#30340;&#27010;&#24565;&#38598;&#21512;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
&lt;/p&gt;</description></item></channel></rss>