<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10456</link><description>&lt;p&gt;
&#36890;&#36807;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling for Tabular Data via Penalized Optimal Transport Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#23398;&#20064;&#34920;&#26684;&#25968;&#25454;&#20013;&#34892;&#30340;&#27010;&#29575;&#20998;&#24067;&#24182;&#29983;&#25104;&#30495;&#23454;&#30340;&#21512;&#25104;&#26679;&#26412;&#30340;&#20219;&#21153;&#26082;&#20851;&#38190;&#21448;&#38750;&#24179;&#20961;&#12290;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#20854;&#21069;&#36523;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#20197;&#21450;Wasserstein&#36317;&#31163;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#22266;&#26377;&#19981;&#31283;&#23450;&#24615;&#65292;WGAN&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POTNet&#65288;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#12289;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#36793;&#38469;&#24809;&#32602;Wasserstein&#65288;MPW&#65289;&#25439;&#22833;&#30340;&#29983;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;POTNet&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10456v1 Announce Type: cross  Abstract: The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.20708</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26399;&#26395;&#25913;&#36827;&#30340;&#24847;&#22806;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Unexpected Improvements to Expected Improvement for Bayesian Optimization. (arXiv:2310.20708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#21487;&#20197;&#35828;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#27969;&#34892;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24456;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;EI&#30340;&#24615;&#33021;&#24448;&#24448;&#34987;&#19968;&#20123;&#26032;&#26041;&#27861;&#36229;&#36234;&#12290;&#23588;&#20854;&#26159;&#65292;EI&#21450;&#20854;&#21464;&#31181;&#22312;&#24182;&#34892;&#21644;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#33719;&#24471;&#20540;&#22312;&#35768;&#22810;&#21306;&#22495;&#20013;&#25968;&#20540;&#19978;&#21464;&#20026;&#38646;&#12290;&#24403;&#35266;&#27979;&#27425;&#25968;&#22686;&#21152;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#22256;&#38590;&#36890;&#24120;&#20250;&#22686;&#21152;&#65292;&#23548;&#33268;&#24615;&#33021;&#22312;&#25991;&#29486;&#20013;&#19981;&#19968;&#33268;&#19988;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20122;&#20248;&#21270;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogEI&#65292;&#36825;&#26159;&#19968;&#31867;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#12290;&#19982;&#26631;&#20934;EI&#30456;&#27604;&#65292;&#36825;&#20123;LogEI&#20989;&#25968;&#30340;&#25104;&#21592;&#35201;&#20040;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#35201;&#20040;&#20855;&#26377;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#20540;&#30149;&#24577;&#22312;&#8220;&#32463;&#20856;&#8221;&#20998;&#26512;EI&#12289;&#26399;&#26395;&#36229;&#20307;&#31215;&#25913;&#36827;&#65288;EHVI&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05185</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.
&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#30340;&#21487;&#25193;&#23637;&#21452;&#23618;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#26159;&#24378;&#20984;&#25110;&#26080;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24809;&#32602;&#37325;&#26500;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#21452;&#23618;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;PBGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PBGD&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
&lt;/p&gt;</description></item></channel></rss>