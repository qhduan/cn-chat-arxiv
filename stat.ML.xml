<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04161</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35268;&#33539;&#20998;&#26512;&#26694;&#26550;&#65306;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#27169;&#22411;&#22312;&#27492;&#36807;&#31243;&#20013;&#36890;&#36807;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#65292;&#20801;&#35768;&#29702;&#35770;&#21644;&#31995;&#32479;&#23454;&#39564;&#26469;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12289;Transformer&#26550;&#26500;&#12289;&#23398;&#21040;&#30340;&#20998;&#24067;&#21644;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#25968;&#25454;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.07409</link><description>&lt;p&gt;
&#12298;&#36229;&#36234;&#25209;&#22788;&#29702;&#20108;&#20803;&#20998;&#31867;&#30340;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#12299;
&lt;/p&gt;
&lt;p&gt;
Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arunachalam&#21644;de Wolf&#65288;2018&#65289;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#21644;&#31946;&#28034;&#35774;&#32622;&#19979;&#65292;&#37327;&#23376;&#25209;&#22788;&#29702;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19982;&#30456;&#24212;&#30340;&#32463;&#20856;&#26679;&#26412;&#22797;&#26434;&#24615;&#20855;&#26377;&#30456;&#21516;&#30340;&#24418;&#24335;&#21644;&#25968;&#37327;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26126;&#26174;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;Dawid&#21644;Tewari&#65288;2022&#65289;&#32463;&#20856;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23545;&#25163;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
&lt;/p&gt;</description></item></channel></rss>