<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.15527</link><description>&lt;p&gt;
&#20381;&#20174;&#22312;&#32447;&#27169;&#22411;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Conformal online model aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22312;&#32447;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#20174;&#39044;&#27979;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#20570;&#20986;&#24378;&#28872;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#23427;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23558;&#28857;&#39044;&#27979;&#36716;&#25442;&#25104;&#20855;&#26377;&#39044;&#23450;&#20041;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#30340;&#38598;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20381;&#20174;&#39044;&#27979;&#21482;&#22312;&#20107;&#20808;&#30830;&#23450;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36215;&#20316;&#29992;&#12290;&#20381;&#20174;&#39044;&#27979;&#20013;&#30456;&#23545;&#36739;&#23569;&#28041;&#21450;&#30340;&#38382;&#39064;&#26159;&#27169;&#22411;&#36873;&#25321;&#21644;/&#25110;&#32858;&#21512;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#24212;&#35813;&#22914;&#20309;&#20381;&#20174;&#21270;&#20247;&#22810;&#39044;&#27979;&#26041;&#27861;&#65288;&#38543;&#26426;&#26862;&#26519;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#27491;&#21017;&#21270;&#32447;&#24615;&#27169;&#22411;&#31561;&#65289;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#20174;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#26469;&#33258;&#22810;&#20010;&#31639;&#27861;&#30340;&#39044;&#27979;&#38598;&#36827;&#34892;&#25237;&#31080;&#65292;&#20854;&#20013;&#26681;&#25454;&#36807;&#21435;&#34920;&#29616;&#35843;&#25972;&#27169;&#22411;&#19978;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15527v1 Announce Type: cross  Abstract: Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any black-box prediction model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;</title><link>https://arxiv.org/abs/2403.13748</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#22240;&#23376;&#21270;&#39640;&#26031;&#36817;&#20284;&#30340;&#24046;&#24322;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13748
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20998;&#24067;$p$&#65292;&#38382;&#39064;&#26159;&#20174;&#19968;&#20123;&#26356;&#26131;&#22788;&#29702;&#30340;&#26063;$\mathcal{Q}$&#20013;&#35745;&#31639;&#26368;&#20339;&#36817;&#20284;$q$&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;Kullback-Leibler (KL)&#25955;&#24230;&#26469;&#25214;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20854;&#20182;&#26377;&#25928;&#30340;&#25955;&#24230;&#36873;&#25321;&#65292;&#24403;$\mathcal{Q}$&#19981;&#21253;&#21547;$p$&#26102;&#65292;&#27599;&#20010;&#25955;&#24230;&#37117;&#25903;&#25345;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39640;&#26031;&#30340;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#34987;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36817;&#20284;&#25152;&#24433;&#21709;&#30340;VI&#32467;&#26524;&#20013;&#65292;&#25955;&#24230;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;VI&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#22914;&#26041;&#24046;&#12289;&#31934;&#24230;&#21644;&#29109;&#65292;&#36827;&#34892;\textit{&#25490;&#24207;}&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#34920;&#26126;&#26080;&#27861;&#36890;&#36807;&#22240;&#23376;&#21270;&#36817;&#20284;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;&#65307;&#22240;&#27492;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#30340;&#27169;&#24335;&#24674;&#22797;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#27169;&#24335;&#21644;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#21644;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#36215;&#21040;&#20102;&#30456;&#21516;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10158</link><description>&lt;p&gt;
&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#20013;&#30340;&#27169;&#24335;&#24674;&#22797;&#21450;&#20854;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Pattern Recovery in Penalized and Thresholded Estimation and its Geometry. (arXiv:2307.10158v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10158
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24809;&#32602;&#21270;&#21644;&#38408;&#20540;&#21270;&#20272;&#35745;&#30340;&#27169;&#24335;&#24674;&#22797;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#27169;&#24335;&#21644;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#21644;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#36215;&#21040;&#20102;&#30456;&#21516;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24809;&#32602;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#24809;&#32602;&#39033;&#30001;&#23454;&#20540;&#30340;&#22810;&#38754;&#20307;&#35268;&#33539;&#32473;&#20986;&#65292;&#20854;&#20013;&#21253;&#25324;&#35832;&#22914;LASSO&#65288;&#20197;&#21450;&#20854;&#35768;&#22810;&#21464;&#20307;&#22914;&#24191;&#20041;LASSO&#65289;&#12289;SLOPE&#12289;OSCAR&#12289;PACS&#31561;&#26041;&#27861;&#12290;&#27599;&#20010;&#20272;&#35745;&#22120;&#21487;&#20197;&#25581;&#31034;&#26410;&#30693;&#21442;&#25968;&#21521;&#37327;&#30340;&#19981;&#21516;&#32467;&#26500;&#25110;&#8220;&#27169;&#24335;&#8221;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22522;&#20110;&#27425;&#24494;&#20998;&#30340;&#27169;&#24335;&#30340;&#19968;&#33324;&#27010;&#24565;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#34913;&#37327;&#20854;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#27169;&#24335;&#24674;&#22797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#24335;&#20197;&#27491;&#27010;&#29575;&#34987;&#35813;&#36807;&#31243;&#26816;&#27979;&#21040;&#30340;&#26368;&#23567;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;&#21487;&#36798;&#24615;&#26465;&#20214;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26356;&#24378;&#30340;&#26080;&#22122;&#22768;&#24674;&#22797;&#26465;&#20214;&#12290;&#23545;&#20110;LASSO&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#26159;&#20351;&#27169;&#24335;&#24674;&#22797;&#30340;&#27010;&#29575;&#22823;&#20110;1/2&#25152;&#24517;&#38656;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#22122;&#22768;&#24674;&#22797;&#36215;&#21040;&#20102;&#23436;&#20840;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25193;&#23637;&#21644;&#32479;&#19968;&#20102;&#20114;&#19981;&#34920;&#31034;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the framework of penalized estimation where the penalty term is given by a real-valued polyhedral gauge, which encompasses methods such as LASSO (and many variants thereof such as the generalized LASSO), SLOPE, OSCAR, PACS and others. Each of these estimators can uncover a different structure or ``pattern'' of the unknown parameter vector. We define a general notion of patterns based on subdifferentials and formalize an approach to measure their complexity. For pattern recovery, we provide a minimal condition for a particular pattern to be detected by the procedure with positive probability, the so-called accessibility condition. Using our approach, we also introduce the stronger noiseless recovery condition. For the LASSO, it is well known that the irrepresentability condition is necessary for pattern recovery with probability larger than $1/2$ and we show that the noiseless recovery plays exactly the same role, thereby extending and unifying the irrepresentability conditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36125;&#32034;&#22827;&#36807;&#31243;&#25512;&#24191;&#21040;&#26102;&#31354;&#39046;&#22495;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#26102;&#31354;&#37325;&#24314;&#12290;&#36890;&#36807;&#26367;&#25442;&#38543;&#26426;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#36793;&#32536;&#29305;&#24449;&#24182;&#27169;&#25311;&#21160;&#24577;&#21464;&#21270;&#22270;&#20687;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16378</link><description>&lt;p&gt;
&#36125;&#32034;&#22827;&#20808;&#39564;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#26102;&#31354;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Besov Priors for Bayesian Inverse Problems. (arXiv:2306.16378v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36125;&#32034;&#22827;&#36807;&#31243;&#25512;&#24191;&#21040;&#26102;&#31354;&#39046;&#22495;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#26102;&#31354;&#37325;&#24314;&#12290;&#36890;&#36807;&#26367;&#25442;&#38543;&#26426;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#36793;&#32536;&#29305;&#24449;&#24182;&#27169;&#25311;&#21160;&#24577;&#21464;&#21270;&#22270;&#20687;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31185;&#23398;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20419;&#20351;&#23545;&#25429;&#25417;&#25968;&#25454;&#29305;&#24449;&#65288;&#22914;&#31361;&#21464;&#25110;&#26126;&#26174;&#23545;&#27604;&#24230;&#65289;&#30340;&#36866;&#24403;&#32479;&#35745;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;&#35768;&#22810;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#38656;&#35201;&#20174;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22855;&#24322;&#24615;&#30340;&#26102;&#38388;&#30456;&#20851;&#23545;&#35937;&#24207;&#21015;&#20013;&#36827;&#34892;&#26102;&#31354;&#37325;&#24314;&#65292;&#22914;&#24102;&#26377;&#36793;&#32536;&#30340;&#21160;&#24577;&#35745;&#31639;&#26426;&#26029;&#23618;&#24433;&#20687;&#65288;CT&#65289;&#22270;&#20687;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#25552;&#20379;&#36807;&#24230;&#24179;&#28369;&#30340;&#20808;&#39564;&#20505;&#36873;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#38543;&#26426;&#31995;&#25968;&#30340;&#23567;&#27874;&#23637;&#24320;&#23450;&#20041;&#30340;&#36125;&#32034;&#22827;&#36807;&#31243;&#65288;BP&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#36825;&#31867;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#30340;&#26356;&#21512;&#36866;&#30340;&#20808;&#39564;&#12290;BP&#22312;&#25104;&#20687;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;GP&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#20135;&#29983;&#20445;&#30041;&#36793;&#32536;&#29305;&#24449;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#33258;&#21160;&#22320;&#32435;&#20837;&#21160;&#24577;&#21464;&#21270;&#22270;&#20687;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#23558;BP&#25512;&#24191;&#21040;&#26102;&#31354;&#39046;&#22495;&#65288;STBP&#65289;&#65292;&#36890;&#36807;&#22312;&#23567;&#27874;&#23637;&#24320;&#20013;&#26367;&#25442;&#38543;&#26426;&#31995;&#25968;&#65292;&#23454;&#29616;&#20102;&#26102;&#31354;&#30456;&#20851;&#24615;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast development in science and technology has driven the need for proper statistical tools to capture special data features such as abrupt changes or sharp contrast. Many applications in the data science seek spatiotemporal reconstruction from a sequence of time-dependent objects with discontinuity or singularity, e.g. dynamic computerized tomography (CT) images with edges. Traditional methods based on Gaussian processes (GP) may not provide satisfactory solutions since they tend to offer over-smooth prior candidates. Recently, Besov process (BP) defined by wavelet expansions with random coefficients has been proposed as a more appropriate prior for this type of Bayesian inverse problems. While BP outperforms GP in imaging analysis to produce edge-preserving reconstructions, it does not automatically incorporate temporal correlation inherited in the dynamically changing images. In this paper, we generalize BP to the spatiotemporal domain (STBP) by replacing the random coefficients in 
&lt;/p&gt;</description></item></channel></rss>