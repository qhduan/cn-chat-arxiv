<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;</title><link>https://arxiv.org/abs/2402.03991</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#20869;&#21464;&#21270;&#23567;&#20250;&#23548;&#33268;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03991
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#65306;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#24448;&#24448;&#36817;&#20284;&#20026;&#20302;&#31209;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25110;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20013;&#21435;&#38500;&#30456;&#23545;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20302;&#31209;&#20559;&#24046;&#30340;&#29702;&#35770;&#30740;&#31350;&#37117;&#28041;&#21450;&#21040;&#31616;&#21270;&#30340;&#32447;&#24615;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24102;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#36890;&#29992;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#31070;&#32463;&#31209;&#23849;&#28291;&#29616;&#35937;&#65292;&#23427;&#23558;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#30340;&#20302;&#31209;&#20559;&#24046;&#19982;&#32593;&#32476;&#30340;&#31070;&#32463;&#23849;&#28291;&#29305;&#24615;&#32852;&#31995;&#36215;&#26469;&#65306;&#38543;&#30528;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20013;&#27599;&#19968;&#23618;&#30340;&#31209;&#21576;&#27604;&#20363;&#36882;&#20943;&#65292;&#19982;&#21069;&#38754;&#23618;&#30340;&#38544;&#34255;&#31354;&#38388;&#23884;&#20837;&#30340;&#31867;&#20869;&#21464;&#21270;&#25104;&#21453;&#27604;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supporte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.19640</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimates for Pairwise Learning with Deep ReLU Networks. (arXiv:2305.19640v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#23545;&#23398;&#20064;&#25351;&#30340;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#32771;&#34385;&#19968;&#23545;&#26679;&#26412;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#24182;&#20272;&#35745;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#12290;&#23545;&#20110;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#26465;&#20214;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#20854;&#35823;&#24046;&#20272;&#35745;&#30340;&#38454;&#25968;&#20026;O&#65288;&#65288;Vlog&#65288;n&#65289;/ n&#65289;1 /&#65288;2-&#946;&#65289;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#65292;&#22312;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#28385;&#36275;&#26576;&#20123;&#20809;&#28369;&#24615;&#27491;&#21017;&#24615;&#26102;&#65292;&#26368;&#20248;&#30028;&#38480;&#36798;&#21040;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#65292;&#24046;&#36317;&#20165;&#20026;&#23545;&#25968;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise learning refers to learning tasks where a loss takes a pair of samples into consideration. In this paper, we study pairwise learning with deep ReLU networks and estimate the excess generalization error. For a general loss satisfying some mild conditions, a sharp bound for the estimation error of order $O((V\log(n) /n)^{1/(2-\beta)})$ is established. In particular, with the pairwise least squares loss, we derive a nearly optimal bound of the excess generalization error which achieves the minimax lower bound up to a logrithmic term when the true predictor satisfies some smoothness regularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#20174;&#39640;&#32500;&#24179;&#28369;&#30446;&#26631;&#20998;&#24067;&#37319;&#26679;&#26102;&#65292;Metropolized Hamiltonian Monte Carlo (HMC)&#27604;Metropolis-adjusted Langevin&#31639;&#27861;&#65288;MALA&#65289;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.04724</link><description>&lt;p&gt;
Metropolized Hamiltonian Monte Carlo&#20309;&#26102;&#33021;&#35777;&#26126;&#20248;&#20110;Metropolis-adjusted Langevin&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
When does Metropolized Hamiltonian Monte Carlo provably outperform Metropolis-adjusted Langevin algorithm?. (arXiv:2304.04724v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#20174;&#39640;&#32500;&#24179;&#28369;&#30446;&#26631;&#20998;&#24067;&#37319;&#26679;&#26102;&#65292;Metropolized Hamiltonian Monte Carlo (HMC)&#27604;Metropolis-adjusted Langevin&#31639;&#27861;&#65288;MALA&#65289;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;Metropolized Hamiltonian Monte Carlo (HMC)&#30340;&#28151;&#21512;&#26102;&#38388;&#65292;&#20351;&#29992;leapfrog&#31215;&#20998;&#22120;&#20174;$\mathbb{R}^d$&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#35813;&#20998;&#24067;&#30340;&#23545;&#25968;&#23494;&#24230;&#24179;&#28369;&#65292;&#20855;&#26377;Frobenius&#33539;&#25968;&#19978;&#30340;&#26446;&#26222;&#24076;&#33576;&#40657;&#22622;&#65292;&#24182;&#28385;&#36275;&#31561;&#21608;&#24615;&#12290;&#25105;&#20204;&#23558;&#26799;&#24230;&#22797;&#26434;&#24230;&#38480;&#21046;&#20026;&#20174;&#19968;&#20010;&#26262;&#21551;&#21160;&#36798;&#21040;$\epsilon$&#35823;&#24046;&#30340;&#24635;&#21464;&#24322;&#36317;&#31163;&#25152;&#38656;&#30340;$\tilde O(d^{1/4}\text{polylog}(1/\epsilon))$&#65292;&#24182;&#23637;&#31034;&#20102;&#36873;&#25321;&#27604;1&#26356;&#22823;&#30340;leapfrog&#27493;&#25968;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#36229;&#36234;Wu&#31561;&#20154;&#65288;2022&#65289;&#23545;Metropolis-adjusted Langevin algorithm (MALA)&#30340;&#20998;&#26512;&#65292;&#20854;&#22312;&#32500;&#24230;&#20381;&#36182;&#24615;&#19978;&#26159;$\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#36830;&#32493;HMC&#21160;&#24577;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#21464;&#37327;&#30340;&#31163;&#25955;&#21270;&#30340;&#32852;&#21512;&#20998;&#24067;&#36817;&#20284;&#19981;&#21464;&#12290;&#24403;&#36890;&#36807;leapfrog&#27493;&#25968;&#30340;&#24402;&#32435;&#26469;&#23637;&#31034;&#36825;&#20010;&#20851;&#38190;&#29305;&#24449;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#21508;&#31181;&#37327;&#30340;&#30697;&#30340;&#20272;&#35745;&#65292;&#36825;&#20123;&#37327;&#22312;&#38480;&#21046;Metropolized HMC&#30340;&#28151;&#21512;&#26102;&#38388;&#26102;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#22312;MALA&#20013;&#24050;&#30693;&#30340;&#31867;&#20284;&#32467;&#26524;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#39640;&#32500;&#24179;&#28369;&#30446;&#26631;&#20998;&#24067;&#26102;&#65292;&#20351;&#29992;&#20855;&#26377;&#22823;&#37327;leapfrog&#27493;&#39588;&#30340;Metropolized HMC&#21487;&#33021;&#27604;&#20351;&#29992;MALA&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with the leapfrog integrator to sample from a distribution on $\mathbb{R}^d$ whose log-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies isoperimetry. We bound the gradient complexity to reach $\epsilon$ error in total variation distance from a warm start by $\tilde O(d^{1/4}\text{polylog}(1/\epsilon))$ and demonstrate the benefit of choosing the number of leapfrog steps to be larger than 1. To surpass previous analysis on Metropolis-adjusted Langevin algorithm (MALA) that has $\tilde{O}(d^{1/2}\text{polylog}(1/\epsilon))$ dimension dependency in Wu et al. (2022), we reveal a key feature in our proof that the joint distribution of the location and velocity variables of the discretization of the continuous HMC dynamics stays approximately invariant. This key feature, when shown via induction over the number of leapfrog steps, enables us to obtain estimates on moments of various quantities tha
&lt;/p&gt;</description></item></channel></rss>