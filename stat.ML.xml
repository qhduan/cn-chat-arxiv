<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#23558;Koopman&#31639;&#23376;&#26694;&#26550;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;Nystro&#776;m&#36924;&#36817;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#20854;&#29702;&#35770;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.02811</link><description>&lt;p&gt;
&#20855;&#26377;Koopman&#31639;&#23376;&#23398;&#20064;&#21644;Nystro&#776;m&#26041;&#27861;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Linear quadratic control of nonlinear systems with Koopman operator learning and the Nystr\"om method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Koopman&#31639;&#23376;&#26694;&#26550;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;Nystro&#776;m&#36924;&#36817;&#23454;&#29616;&#20102;&#23545;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#20854;&#29702;&#35770;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Koopman&#31639;&#23376;&#26694;&#26550;&#22914;&#20309;&#19982;&#26680;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#26377;&#25928;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#34429;&#28982;&#26680;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#24456;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#23376;&#31354;&#38388;&#65288;Nystro&#776;m&#36924;&#36817;&#65289;&#22914;&#20309;&#23454;&#29616;&#24040;&#22823;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#25512;&#23548;&#20986;&#20851;&#20110;Nystro&#776;m&#36924;&#36817;&#25928;&#26524;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#30456;&#20851;&#35299;&#30340;&#36817;&#20284;Riccati&#31639;&#23376;&#21644;&#35843;&#33410;&#22120;&#30446;&#26631;&#37117;&#20197;$ m^{-1/2} $&#30340;&#36895;&#29575;&#25910;&#25947;&#65292;&#20854;&#20013;$ m $&#26159;&#38543;&#26426;&#23376;&#31354;&#38388;&#22823;&#23567;&#12290;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#25968;&#20540;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02811v1 Announce Type: cross  Abstract: In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nystr\"om approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nystr\"om approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05250</link><description>&lt;p&gt;
&#29992;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#31616;&#21270;GNN&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Simplifying GNN Performance with Low Rank Kernel Models. (arXiv:2310.05250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#36817;&#30340;&#35889;GNN&#26041;&#27861;&#23545;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65288;SSNC&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#35768;&#22810;&#24403;&#21069;&#30340;GNN&#26550;&#26500;&#21487;&#33021;&#36807;&#20110;&#31934;&#32454;&#35774;&#35745;&#12290;&#30456;&#21453;&#65292;&#31616;&#21333;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#35889;&#22495;&#20013;&#24212;&#29992;&#65292;&#21487;&#20197;&#21462;&#20195;&#35768;&#22810;&#21463;&#28145;&#24230;&#23398;&#20064;&#21551;&#21457;&#30340;GNN&#35774;&#35745;&#12290;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#20284;&#20046;&#38750;&#24120;&#36866;&#21512;&#21508;&#31181;&#22270;&#31867;&#22411;&#65292;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;SSNC&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;GNN&#26041;&#27861;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#36827;&#21487;&#33021;&#37096;&#20998;&#24402;&#22240;&#20110;&#35780;&#20272;&#24815;&#20363;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19982;GNN&#35889;&#36807;&#28388;&#25216;&#26415;&#30456;&#20851;&#30340;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many of the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques. Code available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
&lt;/p&gt;</description></item></channel></rss>