<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32039;&#33268;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#25968;&#25454;&#65292;&#24182;&#19988;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25299;&#25169;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.08456</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#25345;&#32493;&#22270;&#30340;&#27979;&#24230;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Statistical learning on measures: an application to persistence diagrams. (arXiv:2303.08456v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32039;&#33268;&#31354;&#38388;&#19978;&#30340;&#27979;&#24230;&#25968;&#25454;&#65292;&#24182;&#19988;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25299;&#25169;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20108;&#20803;&#26377;&#30417;&#30563;&#23398;&#20064;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#32039;&#33268;&#31354;&#38388; $\mathcal{X}$ &#19978;&#30340;&#27979;&#24230;&#65292;&#32780;&#19981;&#26159;&#22312;&#26377;&#38480;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#25968;&#25454;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454; $D_N = (\mu_1, Y_1), \ldots, (\mu_N, Y_N)$ &#65292;&#20854;&#20013; $\mu_i$ &#26159; $\mathcal{X}$ &#19978;&#30340;&#27979;&#24230;&#65292; $Y_i$ &#26159; $0$ &#25110; $1$ &#20013;&#30340;&#26631;&#31614;&#12290;&#23545;&#20110; $\mathcal{X}$ &#19978;&#30340;&#22522;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512; $\mathcal{F}$ &#65292;&#25105;&#20204;&#22312;&#27979;&#24230;&#31354;&#38388;&#20013;&#26500;&#24314;&#30456;&#24212;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#26032;&#20998;&#31867;&#22120;&#31867;&#30340; Rademacher &#22797;&#26434;&#24615;&#30340;&#19978;&#19979;&#30028;&#65292;&#23427;&#21487;&#20197;&#31616;&#21333;&#22320;&#29992; $\mathcal{F}$ &#31867;&#30456;&#20851;&#37327;&#26469;&#34920;&#36798;&#12290;&#22914;&#26524; $\mu_i$ &#26159;&#26377;&#38480;&#38598;&#19978;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#37027;&#20040;&#36825;&#20010;&#20998;&#31867;&#20219;&#21153;&#23601;&#20250;&#21464;&#25104;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#26356;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#31181;&#26694;&#26550;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a binary supervised learning classification problem where instead of having data in a finite-dimensional Euclidean space, we observe measures on a compact space $\mathcal{X}$. Formally, we observe data $D_N = (\mu_1, Y_1), \ldots, (\mu_N, Y_N)$ where $\mu_i$ is a measure on $\mathcal{X}$ and $Y_i$ is a label in $\{0, 1\}$. Given a set $\mathcal{F}$ of base-classifiers on $\mathcal{X}$, we build corresponding classifiers in the space of measures. We provide upper and lower bounds on the Rademacher complexity of this new class of classifiers that can be expressed simply in terms of corresponding quantities for the class $\mathcal{F}$. If the measures $\mu_i$ are uniform over a finite set, this classification task boils down to a multi-instance learning problem. However, our approach allows more flexibility and diversity in the input data we can deal with. While such a framework has many possible applications, this work strongly emphasizes on classifying data via topological d
&lt;/p&gt;</description></item></channel></rss>