<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03467</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#30340;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#25910;&#25947;&#36895;&#24230;&#32473;&#20986;&#20102;&#23450;&#37327;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;&#40654;&#26364;&#26799;&#24230;&#27969;&#21644;&#25193;&#25955;&#36807;&#31243;&#8212;&#8212;&#40654;&#26364;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21033;&#29992;&#38543;&#26426;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#23567;&#23398;&#20064;&#29575;&#33539;&#22260;&#20869;&#65292;RSGD&#21487;&#20197;&#36817;&#20284;&#20026;&#30001;&#26080;&#31351;&#32500;&#32500;&#32435;&#36807;&#31243;&#39537;&#21160;&#30340;RSMF&#30340;&#35299;&#12290;RSMF&#32771;&#34385;&#21040;&#20102;RSGD&#30340;&#38543;&#26426;&#27874;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#30830;&#23450;&#24615;&#40654;&#26364;&#26799;&#24230;&#27969;&#30340;&#36924;&#36817;&#39034;&#24207;&#12290;RSGD&#20351;&#29992;&#20102;&#37325;&#20256;&#36882;&#26144;&#23556;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25351;&#25968;&#26144;&#23556;&#30340;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#36817;&#20284;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#36924;&#36817;&#30340;&#24369;&#35823;&#24046;&#36827;&#34892;&#20102;&#23450;&#37327;&#30028;&#23450;&#65292;&#22312;&#37325;&#20256;&#36882;&#26144;&#23556;&#12289;&#27969;&#24418;&#20960;&#20309;&#21644;&#26799;&#24230;&#30340;&#38543;&#26426;&#20272;&#35745;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give quantitative estimates for the rate of convergence of Riemannian stochastic gradient descent (RSGD) to Riemannian gradient flow and to a diffusion process, the so-called Riemannian stochastic modified flow (RSMF). Using tools from stochastic differential geometry we show that, in the small learning rate regime, RSGD can be approximated by the solution to the RSMF driven by an infinite-dimensional Wiener process. The RSMF accounts for the random fluctuations of RSGD and, thereby, increases the order of approximation compared to the deterministic Riemannian gradient flow. The RSGD is build using the concept of a retraction map, that is, a cost efficient approximation of the exponential map, and we prove quantitative bounds for the weak error of the diffusion approximation under assumptions on the retraction map, the geometry of the manifold, and the random estimators of the gradient.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20108;&#20803;&#21644;&#26377;&#24207;&#39033;&#30446;&#21709;&#24212;&#20013;&#20272;&#35745;&#31232;&#30095;&#22240;&#23376;&#36733;&#33655;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#35299;&#20915;&#20102;&#26410;&#30693;&#28508;&#22312;&#22240;&#23376;&#32500;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17820</link><description>&lt;p&gt;
&#31232;&#30095;&#36125;&#21494;&#26031;&#22810;&#32500;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Sparse Bayesian Multidimensional Item Response Theory. (arXiv:2310.17820v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20108;&#20803;&#21644;&#26377;&#24207;&#39033;&#30446;&#21709;&#24212;&#20013;&#20272;&#35745;&#31232;&#30095;&#22240;&#23376;&#36733;&#33655;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#35299;&#20915;&#20102;&#26410;&#30693;&#28508;&#22312;&#22240;&#23376;&#32500;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;MIRT&#65289;&#34987;&#24212;&#29992;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#20351;&#29992;&#65292;&#20197;&#23547;&#25214;&#38382;&#21367;&#25968;&#25454;&#20013;&#21709;&#24212;&#27169;&#24335;&#32972;&#21518;&#30340;&#21487;&#35299;&#37322;&#65288;&#31232;&#30095;&#65289;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#20110;&#36825;&#31181;&#31232;&#30095;&#24615;&#21457;&#29616;&#24037;&#20855;&#30340;&#38656;&#27714;&#23578;&#26410;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20803;&#21644;&#26377;&#24207;&#39033;&#30446;MIRT&#30340;&#36125;&#21494;&#26031;&#24179;&#21488;&#65292;&#20854;&#38656;&#35201;&#26368;&#23569;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#21487;&#24182;&#34892;&#21270;&#30340;&#29305;&#24615;&#65292;&#22312;&#30456;&#23545;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;MIRT&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;MCMC&#27169;&#25311;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26082;&#36153;&#26102;&#21448;&#38590;&#20197;&#36890;&#36807;&#39069;&#22806;&#30340;&#38408;&#20540;&#35774;&#23450;&#23454;&#29616;&#31934;&#30830;&#30340;&#31232;&#30095;&#24674;&#22797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20108;&#20803;&#21644;&#26377;&#24207;&#39033;&#30446;&#21709;&#24212;&#20013;&#20272;&#35745;&#31232;&#30095;&#22240;&#23376;&#36733;&#33655;&#12290;&#25105;&#20204;&#21033;&#29992;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#35299;&#20915;&#20102;&#26410;&#30693;&#28508;&#22312;&#22240;&#23376;&#32500;&#24230;&#30340;&#30475;&#20284;&#19981;&#21487;&#36926;&#36234;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#20272;&#35745;&#22240;&#23376;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#26059;&#36716;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Item Response Theory (MIRT) is sought-after widely by applied researchers looking for interpretable (sparse) explanations underlying response patterns in questionnaire data. There is, however, an unmet demand for such sparsity discovery tools in practice. Our paper develops a Bayesian platform for binary and ordinal item MIRT which requires minimal tuning and scales well on relatively large datasets due to its parallelizable features. Bayesian methodology for MIRT models has traditionally relied on MCMC simulation, which cannot only be slow in practice, but also often renders exact sparsity recovery impossible without additional thresholding. In this work, we develop a scalable Bayesian EM algorithm to estimate sparse factor loadings from binary and ordinal item responses. We address the seemingly insurmountable problem of unknown latent factor dimensionality with tools from Bayesian nonparametrics which enable estimating the number of factors. Rotations to sparsity throug
&lt;/p&gt;</description></item></channel></rss>