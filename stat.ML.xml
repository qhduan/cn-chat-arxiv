<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#20613;&#37324;&#21494;&#31995;&#25968;&#35299;&#20915;&#29109;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#26080;&#38480;&#32500;Banach&#31354;&#38388;&#20013;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2302.00982</link><description>&lt;p&gt;
&#22312;Banach&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#26368;&#20248;&#36755;&#36816;&#29992;&#20110;&#22810;&#20803;&#20998;&#20301;&#25968;&#30340;&#27491;&#21017;&#21270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimal transport in Banach Spaces for regularized estimation of multivariate quantiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00982
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#20613;&#37324;&#21494;&#31995;&#25968;&#35299;&#20915;&#29109;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#26080;&#38480;&#32500;Banach&#31354;&#38388;&#20013;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20004;&#20010;&#32477;&#23545;&#36830;&#32493;&#27010;&#29575;&#27979;&#24230;$\mu$&#21644;$\nu$&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#36755;&#36816;&#65288;EOT&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#33945;&#26085;-&#22350;&#25176;&#32599;&#32500;&#22855;&#20998;&#20301;&#25968;&#30340;&#29305;&#23450;&#35774;&#32622;&#21551;&#21457;&#65292;&#20854;&#20013;&#28304;&#27979;&#24230;$\mu$&#35201;&#20040;&#26159;&#21333;&#20301;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#35201;&#20040;&#26159;&#29699;&#38754;&#22343;&#21248;&#20998;&#24067;&#12290;&#21033;&#29992;&#28304;&#27979;&#24230;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20854;&#20613;&#37324;&#21494;&#31995;&#25968;&#26469;&#21442;&#25968;&#21270;&#22350;&#25176;&#32599;&#32500;&#22855;&#23545;&#20598;&#21183;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#20943;&#23569;&#21040;&#20004;&#20010;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#26469;&#23454;&#29616;&#27714;&#35299;EOT&#30340;&#24555;&#36895;&#25968;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#22312;&#21462;&#20540;&#20110;&#26080;&#38480;&#32500;Banach&#31354;&#38388;&#20013;&#30340;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#24615;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00982v2 Announce Type: replace-cross  Abstract: We introduce a new stochastic algorithm for solving entropic optimal transport (EOT) between two absolutely continuous probability measures $\mu$ and $\nu$. Our work is motivated by the specific setting of Monge-Kantorovich quantiles where the source measure $\mu$ is either the uniform distribution on the unit hypercube or the spherical uniform distribution. Using the knowledge of the source measure, we propose to parametrize a Kantorovich dual potential by its Fourier coefficients. In this way, each iteration of our stochastic algorithm reduces to two Fourier transforms that enables us to make use of the Fast Fourier Transform (FFT) in order to implement a fast numerical method to solve EOT. We study the almost sure convergence of our stochastic algorithm that takes its values in an infinite-dimensional Banach space. Then, using numerical experiments, we illustrate the performances of our approach on the computation of regular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10189</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#23398;&#20064;&#39640;&#32500;&#38750;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions. (arXiv:2306.10189v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;$d$&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;$n$&#20010;&#36712;&#36857;&#24555;&#29031;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#31995;&#32479;&#38656;&#35201;&#23398;&#20064;$d$&#20010;&#20989;&#25968;&#12290;&#38500;&#38750;&#20855;&#26377;&#39069;&#22806;&#30340;&#31995;&#32479;&#23646;&#24615;&#30693;&#35782;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#23545;&#31216;&#24615;&#65292;&#21542;&#21017;&#26174;&#24335;&#30340;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25552;&#20379;&#30340;&#38544;&#24335;&#20844;&#24335;&#23398;&#20064;&#30340;&#32447;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;ODE&#37325;&#20889;&#20026;&#26356;&#24369;&#30340;&#31215;&#20998;&#24418;&#24335;&#65292;&#25105;&#20204;&#38543;&#21518;&#36827;&#34892;&#26368;&#23567;&#21270;&#24182;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#21521;&#37327;&#22330;&#20381;&#36182;&#20110;&#19982;&#35299;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;$d$&#21487;&#33021;&#36229;&#36807;100&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20174;&#22270;&#20687;&#25968;&#25454;&#23398;&#20064;&#38750;&#21442;&#25968;&#19968;&#38454;&#25311;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a nonparametric system of ordinary differential equations (ODEs) from $n$ trajectory snapshots in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach to learning using the implicit formulation provided by vector-valued Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral form, which we subsequently minimize, we derive our learning algorithm. The minimization problem's solution for the vector field relies on multivariate occupation kernel functions associated with the solution trajectories. We validate our approach through experiments on highly nonlinear simulated and real data, where $d$ may exceed 100. We further demonstrate the versatility of the proposed method by learning a nonparametric first order quasilinear partial differential 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15612</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#39640;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#34987;&#24191;&#27867;&#29992;&#20316;&#26367;&#20195;&#20989;&#25968;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#32473;&#23450;&#36755;&#20837;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20989;&#25968;&#35780;&#20272;&#30340;&#26174;&#24335;&#20998;&#24067;&#12290;&#38500;&#20102;&#22522;&#20110;&#27010;&#29575;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#25552;&#20986;&#26469;&#20272;&#35745;&#30456;&#23545;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#30456;&#23545;&#25509;&#36817;&#21644;&#30456;&#23545;&#36828;&#31163;&#30340;&#20004;&#32452;&#23494;&#24230;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#36825;&#20004;&#32452;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23494;&#24230;&#27604;&#12290;&#28982;&#32780;&#65292;&#27492;&#31574;&#30053;&#20013;&#20351;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
&lt;/p&gt;</description></item></channel></rss>