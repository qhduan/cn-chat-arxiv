<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.01356</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#20123;&#25152;&#35859;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20010;&#37327;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#25991;&#29486;&#20013;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21363;&#36873;&#25321;&#38543;&#26426;&#26435;&#37325;&#24182;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;Lipschitz&#24120;&#25968;&#21051;&#30011;&#21040;&#19968;&#20010;&#32477;&#23545;&#25968;&#20540;&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#24120;&#25968;&#30340;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#21305;&#37197;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
&lt;/p&gt;</description></item></channel></rss>