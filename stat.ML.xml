<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#19978;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#28857;&#38388;&#36317;&#25351;&#25968;&#32423;&#23567;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#25351;&#20986;&#29616;&#26377;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2302.00834</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#25554;&#20540;&#30340;&#23574;&#38160;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at Irregularly Spaced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#35268;&#21017;&#38388;&#38548;&#25968;&#25454;&#19978;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#28857;&#38388;&#36317;&#25351;&#25968;&#32423;&#23567;&#30340;&#24773;&#20917;&#19979;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#25351;&#20986;&#29616;&#26377;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#28145;&#24230;ReLU&#32593;&#32476;&#22914;&#20309;&#22312;&#21333;&#20301;&#29699;&#20013;&#30340;$N$&#20010;&#25968;&#25454;&#28857;&#19978;&#36827;&#34892;&#20540;&#30340;&#25554;&#20540;&#65292;&#36825;&#20123;&#28857;&#20043;&#38388;&#30456;&#36317;$\delta$&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;$\delta$&#22312;$N$&#25351;&#25968;&#32423;&#23567;&#30340;&#21306;&#22495;&#20013;&#38656;&#35201;$\Omega(N)$&#20010;&#21442;&#25968;&#65292;&#36825;&#32473;&#20986;&#20102;&#35813;&#21306;&#22495;&#30340;&#23574;&#38160;&#32467;&#26524;&#65292;&#22240;&#20026;$O(N)$&#20010;&#21442;&#25968;&#24635;&#26159;&#36275;&#22815;&#30340;&#12290; &#36825;&#20063;&#34920;&#26126;&#29992;&#20110;&#35777;&#26126;VC&#32500;&#24230;&#19979;&#30028;&#30340;&#20301;&#25552;&#21462;&#25216;&#26415;&#26080;&#27861;&#24212;&#29992;&#20110;&#19981;&#35268;&#21017;&#38388;&#38548;&#30340;&#25968;&#25454;&#28857;&#12290;&#26368;&#21518;&#65292;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#23884;&#20837;&#31471;&#28857;&#22788;&#20026;Sobolev&#31354;&#38388;&#23454;&#29616;&#30340;&#36817;&#20284;&#36895;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00834v2 Announce Type: replace  Abstract: We study the interpolation power of deep ReLU neural networks. Specifically, we consider the question of how efficiently, in terms of the number of parameters, deep ReLU networks can interpolate values at $N$ datapoints in the unit ball which are separated by a distance $\delta$. We show that $\Omega(N)$ parameters are required in the regime where $\delta$ is exponentially small in $N$, which gives the sharp result in this regime since $O(N)$ parameters are always sufficient. This also shows that the bit-extraction technique used to prove lower bounds on the VC dimension cannot be applied to irregularly spaced datapoints. Finally, as an application we give a lower bound on the approximation rates that deep ReLU neural networks can achieve for Sobolev spaces at the embedding endpoint.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.17772</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Classification Trees Robust to Distribution Shifts. (arXiv:2310.17772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26368;&#20248;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;/&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#31867;&#26641;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#32463;&#24120;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#20986;&#29616;&#65292;&#20363;&#22914;&#20844;&#20849;&#21355;&#29983;&#21644;&#31038;&#20250;&#24037;&#20316;&#65292;&#20854;&#20013;&#25968;&#25454;&#36890;&#24120;&#26159;&#36890;&#36807;&#33258;&#25105;&#25253;&#21578;&#30340;&#35843;&#26597;&#25910;&#38598;&#30340;&#65292;&#36825;&#20123;&#35843;&#26597;&#23545;&#38382;&#39064;&#30340;&#34920;&#36848;&#26041;&#24335;&#12289;&#35843;&#26597;&#36827;&#34892;&#30340;&#26102;&#38388;&#21644;&#22320;&#28857;&#12289;&#20197;&#21450;&#21463;&#35775;&#32773;&#19982;&#35843;&#26597;&#21592;&#20998;&#20139;&#20449;&#24687;&#30340;&#33298;&#36866;&#31243;&#24230;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#30340;&#23398;&#20064;&#26368;&#20248;&#40065;&#26834;&#20998;&#31867;&#26641;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#20064;&#26368;&#20248;&#40065;&#26834;&#26641;&#30340;&#38382;&#39064;&#21487;&#20197;&#31561;&#20215;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#20855;&#26377;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#30446;&#26631;&#30340;&#21333;&#38454;&#27573;&#28151;&#21512;&#25972;&#25968;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31561;&#20215;&#22320;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#32447;&#24615;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32422;&#26463;&#29983;&#25104;&#30340;&#23450;&#21046;&#35299;&#20915;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint gene
&lt;/p&gt;</description></item></channel></rss>