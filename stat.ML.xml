<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11646</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#38477;&#20302;&#30340;&#33609;&#22270;&#36827;&#34892;&#38750;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Estimation via Variance-Reduced Sketching. (arXiv:2401.11646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#27169;&#22411;&#22312;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#32479;&#35745;&#21487;&#38752;&#24615;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#21464;&#24471;&#19981;&#22815;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Variance-Reduced Sketching&#65288;VRS&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#30340;&#21516;&#26102;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#22810;&#21464;&#37327;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#26080;&#38480;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#24182;&#20511;&#37492;&#20102;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#23637;&#31034;&#20102;VRS&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#20013;&#65292;VRS&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#21644;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08852</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm. (arXiv:2308.08852v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#27169;&#22411;&#22312;&#20174;&#29983;&#29289;&#20998;&#26512;&#21040;&#25512;&#33616;&#31995;&#32479;&#31561;&#20247;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26680;&#24515;&#33410;&#28857;&#30340;&#22270;&#24418;&#27169;&#22411;&#22312;&#25968;&#25454;&#32500;&#24230;&#36739;&#22823;&#26102;&#35745;&#31639;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#39640;&#25928;&#20272;&#35745;&#26680;&#24515;&#22270;&#24418;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#29983;&#25104;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#28857;&#65292;&#28982;&#21518;&#20351;&#29992;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#36827;&#34892;&#28909;&#21551;&#21160;&#65292;&#20197;&#35745;&#31639;&#20986;&#33021;&#22815;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#31934;&#30830;&#21040;&#36275;&#22815;&#31243;&#24230;&#30340;&#35299;&#12290;&#24191;&#20041;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#30830;&#20445;&#20102;&#35813;&#31639;&#27861;&#33021;&#22815;&#38750;&#24120;&#39640;&#25928;&#22320;&#33719;&#24471;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#39564;&#20013;&#65292;&#35813;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#39640;&#32500;&#20219;&#21153;&#20013;&#65292;&#23427;&#21487;&#20197;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#36798;&#21040;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. The sparsity structure of the generalized Jacobian ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still ach
&lt;/p&gt;</description></item></channel></rss>