<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01779</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play image restoration with Stochastic deNOising REgularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#31639;&#27861;&#26159;&#19968;&#31867;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#22270;&#20687;&#21453;&#28436;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22270;&#20687;&#24674;&#22797;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#23569;&#22122;&#38899;&#30340;&#22270;&#20687;&#19978;&#30340;&#19968;&#31181;&#38750;&#26631;&#20934;&#30340;&#21435;&#22122;&#22120;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#19982;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#30683;&#30462;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#65292;&#21435;&#22122;&#22120;&#20165;&#24212;&#29992;&#20110;&#37325;&#26032;&#21152;&#22122;&#30340;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PnP&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#65292;&#23427;&#20165;&#22312;&#22122;&#22768;&#27700;&#24179;&#36866;&#24403;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#12290;&#23427;&#22522;&#20110;&#26174;&#24335;&#30340;&#38543;&#26426;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#21450;&#20854;&#36864;&#28779;&#25193;&#23637;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#21147;&#23849;&#28291;&#38382;&#39064;&#65292;&#21457;&#29616;&#29305;&#24449;&#24402;&#19968;&#21270;&#21487;&#20197;&#38450;&#27490;&#27492;&#38382;&#39064;&#30340;&#20986;&#29616;&#65292;&#20026;&#35299;&#20915;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.16109</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#19968;&#21270;&#38450;&#27490;&#38750;&#23545;&#27604;&#23398;&#20064;&#21160;&#21147;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics. (arXiv:2309.16109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#21147;&#23849;&#28291;&#38382;&#39064;&#65292;&#21457;&#29616;&#29305;&#24449;&#24402;&#19968;&#21270;&#21487;&#20197;&#38450;&#27490;&#27492;&#38382;&#39064;&#30340;&#20986;&#29616;&#65292;&#20026;&#35299;&#20915;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#30340;&#20004;&#20010;&#27491;&#35270;&#22270;&#22312;&#25968;&#25454;&#34920;&#31034;&#31354;&#38388;&#20013;&#36890;&#36807;&#21560;&#24341;&#21147;&#20351;&#23427;&#20204;&#30456;&#20284;&#65292;&#32780;&#36890;&#36807;&#25490;&#26021;&#21147;&#20351;&#23427;&#20204;&#36828;&#31163;&#36127;&#26679;&#26412;&#12290;&#38750;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;BYOL&#21644;SimSiam&#31561;&#25163;&#27573;&#21435;&#38500;&#20102;&#36127;&#26679;&#26412;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;&#34429;&#28982;&#30001;&#20110;&#32570;&#20047;&#25490;&#26021;&#21147;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#23849;&#28291;&#25104;&#19968;&#20010;&#21333;&#28857;&#65292;&#20294;&#30000;&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#20998;&#26512;&#25581;&#31034;&#65292;&#22914;&#26524;&#25968;&#25454;&#22686;&#24378;&#36275;&#22815;&#24378;&#20110;&#27491;&#21017;&#21270;&#65292;&#21017;&#34920;&#31034;&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#24120;&#29992;&#30340;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;&#21363;&#22312;&#34913;&#37327;&#34920;&#31034;&#30456;&#20284;&#24615;&#20043;&#21069;&#36827;&#34892;&#30340;&#24402;&#19968;&#21270;&#25805;&#20316;&#65292;&#22240;&#27492;&#36807;&#24378;&#30340;&#27491;&#21017;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#21160;&#21147;&#23849;&#28291;&#65292;&#36825;&#22312;&#29305;&#24449;&#24402;&#19968;&#21270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#33258;&#28982;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a self-supervised representation learning framework, where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Non-contrastive learning, represented by BYOL and SimSiam, further gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. (2021) revealed through the learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly-used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may collapse the dynamics, which is an unnatural behavior under the presence of feature normalizat
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01054</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01054
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#20272;&#35745;&#65288;SSNL&#65289;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#27169;&#25311;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;SSNL&#36890;&#36807;&#25311;&#21512;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23556;&#24207;&#21015;&#31070;&#32463;&#20284;&#28982;&#65288;SSNL&#65289;&#20272;&#35745;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27169;&#22411;&#20013;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20989;&#25968;&#24182;&#19988;&#21482;&#33021;&#20351;&#29992;&#21487;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#26102;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;SSNL&#25311;&#21512;&#19968;&#20010;&#38477;&#32500;&#30340;&#20840;&#23556;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26367;&#20195;&#20284;&#28982;&#20989;&#25968;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#21253;&#25324;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#25110;&#21464;&#20998;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;SSNL&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#20284;&#28982;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#21253;&#21547;&#26080;&#20449;&#24687;&#25968;&#25454;&#32500;&#24230;&#25110;&#20301;&#20110;&#36739;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;SSNL&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#23427;&#36890;&#24120;&#20248;&#20110;&#22312;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#29616;&#20195;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#19968;&#39033;&#26469;&#33258;&#22825;&#20307;&#29289;&#29702;&#23398;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20363;&#23376;&#19978;&#23545;&#30913;&#22330;&#27169;&#22411;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22312;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.14530</link><description>&lt;p&gt;
&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimation in Mixed-Membership Stochastic Block Models. (arXiv:2307.14530v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#22312;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#29616;&#20195;&#32593;&#32476;&#31185;&#23398;&#20013;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#20854;&#24212;&#29992;&#21487;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#25214;&#21040;&#65292;&#20174;&#34507;&#30333;&#36136;&#24314;&#27169;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#35770;&#25991;&#30740;&#31350;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#21363;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#23646;&#20110;&#22810;&#20010;&#31038;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30001;Airoldi&#31561;&#20154;&#65288;2008&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;&#28151;&#21512;&#25104;&#21592;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;MMSB&#65289;&#12290;MMSB&#22312;&#22270;&#20013;&#23545;&#37325;&#21472;&#31038;&#21306;&#32467;&#26500;&#25552;&#20379;&#20102;&#30456;&#24403;&#19968;&#33324;&#30340;&#35774;&#32622;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#38382;&#39064;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#32593;&#32476;&#20013;&#37325;&#24314;&#31038;&#21306;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#36825;&#20010;&#19979;&#30028;&#21305;&#37197;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#32467;&#26524;&#22312;&#23545;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#30340;&#30456;&#24403;&#26222;&#36941;&#26465;&#20214;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35828;&#26126;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#22312;&#24213;&#23618;&#31639;&#23376;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#31532;&#20108;&#31181;&#31639;&#27861;&#26159;&#19968;&#31181;&#21152;&#36895;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#35299;&#30340;&#23384;&#22312;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2301.03113</link><description>&lt;p&gt;
&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#35299;&#20915;&#26681;&#26597;&#25214;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Randomized Block-Coordinate Optimistic Gradient Algorithms for Root-Finding Problems. (arXiv:2301.03113v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#30340;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#22312;&#24213;&#23618;&#31639;&#23376;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#31532;&#20108;&#31181;&#31639;&#27861;&#26159;&#19968;&#31181;&#21152;&#36895;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#35299;&#30340;&#23384;&#22312;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#36817;&#20284;&#27714;&#35299;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#20063;&#31216;&#20026;&#26681;&#26597;&#25214;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#20351;&#29992;&#24658;&#23450;&#30340;&#27493;&#38271;&#65292;&#38750;&#21152;&#36895;&#31639;&#27861;&#65292;&#22312;&#24213;&#23618;&#31639;&#23376;G&#28385;&#36275;Lipschitz&#36830;&#32493;&#24615;&#21644;&#24369;Minty&#35299;&#26465;&#20214;&#26102;&#65292;&#23427;&#22312;&#25968;&#23398;&#26399;&#26395;E[||Gx^k||^2]&#19978;&#36798;&#21040;O(1/k)&#30340;&#26368;&#20248;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;E[&#183;]&#34920;&#31034;&#26399;&#26395;&#65292;k&#20026;&#36845;&#20195;&#35745;&#25968;&#22120;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38543;&#26426;&#22359;&#22352;&#26631;&#20048;&#35266;&#26799;&#24230;&#31639;&#27861;&#65292;&#22312;G&#30340;&#22841;&#36924;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#22312;E[||Gx^k||^2]&#21644;E[||x^{k+1} x^{k}||^2]&#19978;&#20998;&#21035;&#36798;&#21040;O(1/k^2)&#21644;o(1/k^2)&#30340;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#36845;&#20195;&#24207;&#21015;{x^k}&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#65292;&#20197;&#21450;&#22312;&#27492;&#35299;&#22788;Gx^k&#30340;&#27169;&#30340;&#24179;&#26041;&#22312;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop two new randomized block-coordinate optimistic gradient algorithms to approximate a solution of nonlinear equations in large-scale settings, which are called root-finding problems. Our first algorithm is non-accelerated with constant stepsizes, and achieves $\mathcal{O}(1/k)$ best-iterate convergence rate on $\mathbb{E}[ \Vert Gx^k\Vert^2]$ when the underlying operator $G$ is Lipschitz continuous and satisfies a weak Minty solution condition, where $\mathbb{E}[\cdot]$ is the expectation and $k$ is the iteration counter. Our second method is a new accelerated randomized block-coordinate optimistic gradient algorithm. We establish both $\mathcal{O}(1/k^2)$ and $o(1/k^2)$ last-iterate convergence rates on both $\mathbb{E}[ \Vert Gx^k\Vert^2]$ and $\mathbb{E}[ \Vert x^{k+1} x^{k}\Vert^2]$ for this algorithm under the co-coerciveness of $G$. In addition, we prove that the iterate sequence $\{x^k\}$ converges to a solution almost surely, and $\Vert Gx^k\Vert^2$ at
&lt;/p&gt;</description></item></channel></rss>