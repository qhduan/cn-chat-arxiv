<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12668</link><description>&lt;p&gt;
&#38543;&#26426;&#21270;&#26082;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#21448;&#21487;&#20197;&#20943;&#23569;&#26041;&#24046;&#65306;&#38543;&#26426;&#26862;&#26519;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12668
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65292;&#39318;&#27425;&#22312;\cite{breiman2001random}&#20013;&#25351;&#20986;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#20284;&#20046;&#27604;&#35013;&#34955;&#27861;&#20943;&#23569;&#20102;&#20559;&#24046;&#12290;&#21463;\cite{mentch2020randomization}&#19968;&#31687;&#26377;&#36259;&#30340;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20316;&#32773;&#35748;&#20026;&#38543;&#26426;&#26862;&#26519;&#20943;&#23569;&#20102;&#26377;&#25928;&#33258;&#30001;&#24230;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#29615;&#22659;&#19979;&#25165;&#33021;&#32988;&#36807;&#35013;&#34955;&#38598;&#25104;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#22914;&#20309;&#33021;&#22815;&#25581;&#31034;&#34987;&#35013;&#34955;&#27861;&#24573;&#35270;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#65292;&#22312;&#23384;&#22312;&#36825;&#31181;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#20559;&#24046;&#36824;&#33021;&#20943;&#23567;&#26041;&#24046;&#65292;&#24182;&#19988;&#24403;&#20449;&#22122;&#27604;&#39640;&#26102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34920;&#29616;&#24840;&#21457;&#22909;&#20110;&#35013;&#34955;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#20026;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#22312;&#21508;&#31181;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;&#38543;&#26426;&#26862;&#26519;&#19982;&#35013;&#34955;&#38598;&#25104;&#22312;&#27599;&#27425;&#20998;&#21106;&#27880;&#20837;&#30340;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#24046;&#24322;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#36824;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12668v1 Announce Type: cross  Abstract: We study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.14073</link><description>&lt;p&gt;
&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65306;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#21644;&#39640;&#26031;&#24615;&#20551;&#35774;&#19979;&#31283;&#23450;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#31639;&#36825;&#20010;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#20215;&#20110;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;GPU&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
&lt;/p&gt;</description></item></channel></rss>