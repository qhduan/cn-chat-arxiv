<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31867;&#20998;&#24067;&#34429;&#28982;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#21364;&#26080;&#27861;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.00267</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#21487;&#23398;&#20064;&#30340;&#20998;&#24067;&#31867;&#37117;&#33021;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Learnable Distribution Classes are Privately Learnable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#19968;&#31867;&#20998;&#24067;&#34429;&#28982;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#21364;&#26080;&#27861;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#19968;&#31867;&#20998;&#24067;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#21487;&#20197;&#20197;&#24635;&#21464;&#24046;&#36317;&#31163;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#22312;&#65288;&#949;&#65292;&#948;&#65289;-&#24046;&#20998;&#38544;&#31169;&#19979;&#26080;&#27861;&#23398;&#20064;&#12290;&#36825;&#25512;&#32763;&#20102;Ashtiani&#30340;&#19968;&#20010;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\varepsilon, \delta)$-differential privacy. This refutes a conjecture of Ashtiani.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item></channel></rss>