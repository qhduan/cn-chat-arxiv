<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#27599;&#36718;&#25237;&#24433;&#30340;&#25968;&#37327;&#26469;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08911</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Methods for Non-stationary Online Learning. (arXiv:2309.08911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08911
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#27599;&#36718;&#25237;&#24433;&#30340;&#25968;&#37327;&#26469;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#34987;&#25552;&#20986;&#20316;&#20026;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20004;&#20010;&#21407;&#21017;&#24615;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20248;&#21270;&#23427;&#20204;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#23618;&#22312;&#32447;&#38598;&#25104;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#24615;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#32500;&#25252;&#19968;&#32452;&#22522;&#23398;&#20064;&#22120;&#65292;&#24182;&#37319;&#29992;&#20803;&#31639;&#27861;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#36319;&#36394;&#26368;&#20339;&#23398;&#20064;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#23618;&#32467;&#26500;&#24341;&#21457;&#20102;&#20851;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25285;&#24551; -&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21516;&#26102;&#32500;&#25252;$\mathcal{O}(\log T)$&#20010;&#22522;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#19968;&#20010;$T$&#36718;&#22312;&#32447;&#28216;&#25103;&#65292;&#22240;&#27492;&#27599;&#36718;&#25191;&#34892;&#22810;&#27425;&#25237;&#24433;&#21040;&#21487;&#34892;&#22495;&#19978;&#65292;&#24403;&#22495;&#24456;&#22797;&#26434;&#26102;&#65292;&#36825;&#25104;&#20026;&#35745;&#31639;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#23558;&#27599;&#36718;&#30340;&#25237;&#24433;&#27425;&#25968;&#20174;$\mathcal{O}(\log T)$&#38477;&#20302;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Non-stationary online learning has drawn much attention in recent years. In particular, dynamic regret and adaptive regret are proposed as two principled performance measures for online convex optimization in non-stationary environments. To optimize them, a two-layer online ensemble is usually deployed due to the inherent uncertainty of the non-stationarity, in which a group of base-learners are maintained and a meta-algorithm is employed to track the best one on the fly. However, the two-layer structure raises the concern about the computational complexity -- those methods typically maintain $\mathcal{O}(\log T)$ base-learners simultaneously for a $T$-round online game and thus perform multiple projections onto the feasible domain per round, which becomes the computational bottleneck when the domain is complicated. In this paper, we present efficient methods for optimizing dynamic regret and adaptive regret, which reduce the number of projections per round from $\mathcal{O}(\log T)$ t
&lt;/p&gt;</description></item></channel></rss>