<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20869;&#37096;&#20998;&#24067;&#25110;&#22806;&#37096;&#20998;&#24067;&#65292;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14058</link><description>&lt;p&gt;
&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hypothesis-Driven Deep Learning for Out of Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20869;&#37096;&#20998;&#24067;&#25110;&#22806;&#37096;&#20998;&#24067;&#65292;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#36879;&#26126;&#40657;&#30418;&#31995;&#32479;&#30340;&#39044;&#27979;&#32463;&#24120;&#29992;&#20110;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#65292;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#26679;&#26412;&#30340;&#26041;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#31181;&#24230;&#37327;&#21644;&#27979;&#35797;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OoD&#65289;&#25968;&#25454;&#21644;&#20998;&#24067;&#20869;&#65288;InD&#65289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#26032;&#26679;&#26412;&#26159;InD&#36824;&#26159;OoD&#12290;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#36807;&#30340;DNN&#21644;&#19968;&#20123;&#36755;&#20837;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;DNN&#39304;&#36865;&#36755;&#20837;&#24182;&#35745;&#31639;&#19968;&#32452;OoD&#24230;&#37327;&#65292;&#31216;&#20026;&#28508;&#22312;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;OoD&#26816;&#27979;&#38382;&#39064;&#34920;&#36848;&#20026;&#28508;&#22312;&#21709;&#24212;&#20043;&#38388;&#30340;&#20551;&#35774;&#26816;&#39564;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25490;&#21015;&#30340;&#37325;&#26032;&#37319;&#26679;&#26469;&#25512;&#26029;&#22312;&#38646;&#20551;&#35774;&#19979;&#35266;&#23519;&#21040;&#30340;&#28508;&#22312;&#21709;&#24212;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14058v1 Announce Type: new  Abstract: Predictions of opaque black-box systems are frequently deployed in high-stakes applications such as healthcare. For such applications, it is crucial to assess how models handle samples beyond the domain of training data. While several metrics and tests exist to detect out-of-distribution (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use. In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD. Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses. We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#20013;&#30340;&#23884;&#22871;&#26399;&#26395;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.16776</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#22871;MLMC&#23545;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods. (arXiv:2401.16776v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#20013;&#30340;&#23884;&#22871;&#26399;&#26395;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;SNPE&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#27169;&#22411;&#12290;&#23427;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#30340;&#27169;&#25311;&#26469;&#23398;&#20064;&#21518;&#39564;&#12290;&#20316;&#20026;&#19968;&#31181;SNPE&#25216;&#26415;&#65292;Greenberg&#31561;&#20154;&#65288;2019&#65289;&#25552;&#20986;&#30340;&#33258;&#21160;&#21518;&#39564;&#21464;&#25442;&#65288;APT&#65289;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;APT&#26041;&#27861;&#21253;&#21547;&#35745;&#31639;&#38590;&#20197;&#22788;&#29702;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#23545;&#25968;&#30340;&#26399;&#26395;&#65292;&#21363;&#23884;&#22871;&#26399;&#26395;&#12290;&#23613;&#31649;&#21407;&#23376;APT&#36890;&#36807;&#31163;&#25955;&#21270;&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20998;&#26512;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#20272;&#35745;&#30456;&#20851;&#30340;&#23884;&#22871;&#26399;&#26395;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230;&#30340;&#23884;&#22871;&#20272;&#35745;&#26159;&#26377;&#20559;&#30340;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Sequential neural posterior estimation (SNPE) techniques have been recently proposed for dealing with simulation-based models with intractable likelihoods. They are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As a SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs notably and scales to high dimensional data. However, the APT method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic APT was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we propose a nested APT method to estimate the involved nested expectation instead. This facilitates establishing the convergence analysis. Since the nested estimators for the loss function and its gradient are biased, we make
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05679</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#36827;&#34892;&#32858;&#31867;&#65306;&#33021;&#26377;&#22810;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Clustering with minimum spanning trees: How good can it be?. (arXiv:2303.05679v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#36827;&#34892;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#30340;&#24847;&#20041;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;MST-based&#21010;&#20998;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#20854;&#20182;&#38750;MST&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;MST&#26041;&#27861;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#29983;&#25104;&#26641;&#65288;MST&#65289;&#22312;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#21487;&#20197;&#25552;&#20379;&#26041;&#20415;&#30340;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#24182;&#19988;&#35745;&#31639;&#30456;&#23545;&#36739;&#24555;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;MST&#22312;&#20302;&#32500;&#31354;&#38388;&#30340;&#20998;&#21306;&#25968;&#25454;&#32858;&#31867;&#20219;&#21153;&#20013;&#30340;&#24847;&#20041;&#31243;&#24230;&#12290;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#65288;oracle&#65289;&#31639;&#27861;&#19982;&#22823;&#37327;&#22522;&#20934;&#25968;&#25454;&#30340;&#19987;&#23478;&#26631;&#31614;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#19978;&#38480;&#65292;&#25105;&#20204;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#24635;&#20307;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#19981;&#26159;&#25552;&#20986;&#21478;&#19968;&#20010;&#21482;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#65292;&#32780;&#26159;&#22238;&#39038;&#12289;&#30740;&#31350;&#12289;&#25193;&#23637;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#26368;&#26032;MST-based&#21010;&#20998;&#26041;&#26696;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#26032;&#30340;&#21644;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;Genie&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#24448;&#24448;&#20248;&#20110;&#38750;MST&#31639;&#27861;&#65292;&#22914;k-means&#65292;&#39640;&#26031;&#28151;&#21512;&#65292;&#35889;&#32858;&#31867;&#65292;Birch&#65292;&#22522;&#20110;&#23494;&#24230;&#21644;&#32463;&#20856;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#36824;&#26159;&#21457;&#29616;MST&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#22914;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum spanning trees (MSTs) provide a convenient representation of datasets in numerous pattern recognition activities. Moreover, they are relatively fast to compute. In this paper, we quantify the extent to which they can be meaningful in partitional data clustering tasks in low-dimensional spaces. By identifying the upper bounds for the agreement between the best (oracle) algorithm and the expert labels from a large battery of benchmark data, we discover that MST methods are overall very competitive. Next, instead of proposing yet another algorithm that performs well on a limited set of examples, we review, study, extend, and generalise existing, state-of-the-art MST-based partitioning schemes. This leads to a few new and noteworthy approaches. Overall, Genie and the information-theoretic methods often outperform the non-MST algorithms such as k-means, Gaussian mixtures, spectral clustering, Birch, density-based, and classical hierarchical agglomerative procedures. Nevertheless, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2209.02935</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#32858;&#31867;&#20934;&#30830;&#24230;&#65306;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#30340;&#22806;&#37096;&#32858;&#31867;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#20869;&#37096;&#24230;&#37327;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#19968;&#20010;&#26368;&#22909;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#21306;&#20998;&#20986;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#21644;&#31995;&#32479;&#24615;&#34920;&#29616;&#19981;&#20339;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#32858;&#31867;&#31639;&#27861;&#20351;&#29992;&#20869;&#37096;&#25110;&#22806;&#37096;&#26377;&#25928;&#24230;&#37327;&#36827;&#34892;&#35780;&#20272;&#12290;&#20869;&#37096;&#24230;&#37327;&#37327;&#21270;&#25152;&#24471;&#20998;&#21306;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20363;&#22914;&#65292;&#31751;&#32039;&#23494;&#24230;&#30340;&#24179;&#22343;&#31243;&#24230;&#25110;&#28857;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20419;&#20351;&#30340;&#32858;&#31867;&#26377;&#26102;&#21487;&#33021;&#26159;&#26080;&#24847;&#20041;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22806;&#37096;&#24230;&#37327;&#23558;&#31639;&#27861;&#30340;&#36755;&#20986;&#19982;&#30001;&#19987;&#23478;&#25552;&#20379;&#30340;&#21442;&#32771;&#30495;&#23454;&#20998;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24120;&#29992;&#30340;&#32463;&#20856;&#20998;&#21306;&#30456;&#20284;&#24615;&#35780;&#20998;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#20114;&#20449;&#24687;&#12289;Fowlkes-Mallows&#25110;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968;&#65292;&#32570;&#23569;&#19968;&#20123;&#21487;&#21462;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#65292;&#23427;&#20204;&#19981;&#33021;&#27491;&#30830;&#35782;&#21035;&#26368;&#22351;&#24773;&#20917;&#65292;&#20063;&#19981;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
&lt;/p&gt;</description></item></channel></rss>