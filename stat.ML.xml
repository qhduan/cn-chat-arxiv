<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#21464;&#37327;&#30340;&#26497;&#31471;&#22270;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#26465;&#20214;&#22270;&#21644;&#28508;&#21464;&#37327;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.09604</link><description>&lt;p&gt;
&#28151;&#21512;&#21464;&#37327;&#30340;&#26497;&#31471;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Extremal graphical modeling with latent variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#21464;&#37327;&#30340;&#26497;&#31471;&#22270;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#26465;&#20214;&#22270;&#21644;&#28508;&#21464;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22270;&#27169;&#22411;&#32534;&#30721;&#22810;&#21464;&#37327;&#26497;&#31471;&#26465;&#20214;&#29420;&#31435;&#32467;&#26500;&#65292;&#24182;&#20026;&#37327;&#21270;&#32597;&#35265;&#20107;&#20214;&#39118;&#38505;&#25552;&#20379;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#28508;&#21464;&#37327;&#30340;&#21487;&#24310;&#20280;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#20984;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558; H\"usler-Reiss &#31934;&#24230;&#30697;&#38453;&#20998;&#35299;&#20026;&#32534;&#30721;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#22270;&#32467;&#26500;&#30340;&#31232;&#30095;&#37096;&#20998;&#21644;&#32534;&#30721;&#23569;&#37327;&#28508;&#21464;&#37327;&#23545;&#35266;&#23519;&#21464;&#37327;&#30340;&#24433;&#21709;&#30340;&#20302;&#31209;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;\texttt{eglatent}&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#23427;&#33021;&#19968;&#33268;&#22320;&#24674;&#22797;&#26465;&#20214;&#22270;&#20197;&#21450;&#28508;&#21464;&#37327;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09604v1 Announce Type: cross  Abstract: Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of H\"usler-Reiss models, we propose the \texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the H\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of \texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02191</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65306;&#30382;&#32932;&#30149;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23433;&#20840;&#36215;&#35265;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#65292;&#21355;&#29983;&#39046;&#22495;&#30340;AI&#31995;&#32479;&#38656;&#35201;&#32463;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23558;&#20854;&#39044;&#27979;&#32467;&#26524;&#19982;&#20551;&#23450;&#20026;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#65292;&#22522;&#20934;&#20107;&#23454;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#26631;&#20934;&#30340;AI&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36825;&#19968;&#28857;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#65292;&#20294;&#26159;&#23427;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#39640;&#20272;&#26410;&#26469;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#27880;&#37322;&#19981;&#30830;&#23450;&#24615;&#26159;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#27880;&#37322;&#65292;&#20197;&#21450;&#30001;&#20110;&#26377;&#38480;&#30340;&#35266;&#27979;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#30830;&#23450;&#22320;&#32858;&#21512;&#27880;&#37322;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#35270;&#36825;&#31181;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#25110;&#24179;&#22343;&#20540;&#26469;&#32858;&#21512;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#27880;&#37322;&#30340;&#32858;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27880;&#37322;&#30340;&#32858;&#21512;&#26694;&#26550;&#35299;&#37322;&#20026;&#25152;&#35859;&#21487;&#33021;&#24615;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21547;&#26377;&#25209;&#37327;&#26356;&#26032;&#21644;/&#25110;&#36817;&#20284;&#26799;&#24230;&#30340;&#21160;&#37327;&#37325;&#29699;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#28040;&#32791;&#65292;&#21516;&#26102;&#20173;&#33021;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16241</link><description>&lt;p&gt;
&#37319;&#29992;&#25209;&#37327;&#26356;&#26032;&#21644;/&#25110;&#36817;&#20284;&#26799;&#24230;&#30340;&#21160;&#37327;&#37325;&#29699;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of Momentum-Based Heavy Ball Method with Batch Updating and/or Approximate Gradients. (arXiv:2303.16241v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21547;&#26377;&#25209;&#37327;&#26356;&#26032;&#21644;/&#25110;&#36817;&#20284;&#26799;&#24230;&#30340;&#21160;&#37327;&#37325;&#29699;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#26799;&#24230;&#35745;&#31639;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#28040;&#32791;&#65292;&#21516;&#26102;&#20173;&#33021;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;1964&#24180;Polyak&#24341;&#20837;&#30340;&#20984;&#20248;&#21270;&#21644;&#38750;&#20984;&#20248;&#21270;&#20013;&#24191;&#20026;&#20154;&#30693;&#30340;&#8220;&#21160;&#37327;&#37325;&#29699;&#8221;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30830;&#31435;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#24403;&#35201;&#27714;&#35299;&#21442;&#25968;&#30340;&#32500;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#26356;&#26032;&#19968;&#37096;&#20998;&#32780;&#19981;&#26159;&#25152;&#26377;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#65292;&#31216;&#20043;&#20026;&#8220;&#25209;&#37327;&#26356;&#26032;&#8221;&#65292;&#33509;&#19982;&#26799;&#24230;&#27861;&#37197;&#21512;&#20351;&#29992;&#65292;&#21017;&#29702;&#35770;&#19978;&#21482;&#38656;&#35745;&#31639;&#38656;&#35201;&#26356;&#26032;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#32780;&#22312;&#23454;&#38469;&#20013;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31561;&#26041;&#27861;&#20165;&#35745;&#31639;&#37096;&#20998;&#26799;&#24230;&#24182;&#19981;&#33021;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#27599;&#19968;&#27493;&#20013;&#20943;&#23569;CPU&#20351;&#29992;&#37327;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#38454;&#24494;&#20998;&#25110;&#36817;&#20284;&#26799;&#24230;&#20195;&#26367;&#30495;&#23454;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#36817;&#20284;&#26799;&#24230;&#20449;&#24687;&#21644;/&#25110;&#25209;&#37327;&#26356;&#26032;&#30340;&#21160;&#37327;&#37325;&#29699;&#27861;&#20173;&#28982;&#21487;&#20197;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the well-known "Heavy Ball" method for convex and nonconvex optimization introduced by Polyak in 1964, and establish its convergence under a variety of situations. Traditionally, most algorthms use "full-coordinate update," that is, at each step, very component of the argument is updated. However, when the dimension of the argument is very high, it is more efficient to update some but not all components of the argument at each iteration. We refer to this as "batch updating" in this paper.  When gradient-based algorithms are used together with batch updating, in principle it is sufficient to compute only those components of the gradient for which the argument is to be updated. However, if a method such as back propagation is used to compute these components, computing only some components of gradient does not offer much savings over computing the entire gradient. Therefore, to achieve a noticeable reduction in CPU usage at each step, one can use first-order diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#24615;&#36136;&#65292;&#36890;&#36807;&#30740;&#31350;&#27531;&#24046;&#21270;&#29305;&#24449;&#21644;&#34394;&#20551;&#25237;&#24433;&#30340;&#22797;&#26434;&#24615;&#26469;&#25581;&#31034;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.06259</link><description>&lt;p&gt;
&#20102;&#35299;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;: &#20004;&#31181;&#22797;&#26434;&#24615;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Understanding Best Subset Selection: A Tale of Two C(omplex)ities. (arXiv:2301.06259v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#30340;&#21464;&#37327;&#36873;&#25321;&#24615;&#36136;&#65292;&#36890;&#36807;&#30740;&#31350;&#27531;&#24046;&#21270;&#29305;&#24449;&#21644;&#34394;&#20551;&#25237;&#24433;&#30340;&#22797;&#26434;&#24615;&#26469;&#25581;&#31034;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;(BSS)&#20027;&#35201;&#30001;&#20110;&#35745;&#31639;&#29942;&#39048;&#32780;&#22256;&#25200;&#32479;&#35745;&#23398;&#23478;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;&#26368;&#36817;&#65292;&#29616;&#20195;&#35745;&#31639;&#31361;&#30772;&#37325;&#26032;&#28857;&#29123;&#20102;&#23545;BSS&#30340;&#29702;&#35770;&#20852;&#36259;&#24182;&#23548;&#33268;&#20102;&#26032;&#30340;&#21457;&#29616;&#12290;&#26368;&#36817;&#65292;Guo&#31561;&#20154;&#34920;&#26126;&#65292;BSS&#30340;&#27169;&#22411;&#36873;&#25321;&#24615;&#33021;&#21463;&#21040;&#20102;&#40065;&#26834;&#24615;&#35774;&#35745;&#20381;&#36182;&#30340;&#36793;&#30028;&#37327;&#30340;&#25511;&#21046;&#65292;&#19981;&#20687;LASSO&#12289;SCAD&#12289;MCP&#31561;&#29616;&#20195;&#26041;&#27861;&#12290;&#22312;&#20182;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#28608;&#21169;&#19979;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#19979;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#21464;&#37327;&#36873;&#25321;&#24615;&#36136;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38500;&#20102;&#21487;&#36776;&#35782;&#24615;&#36793;&#30028;&#20197;&#22806;&#65292;&#19979;&#21015;&#20004;&#31181;&#22797;&#26434;&#24615;&#24230;&#37327;&#22312;&#34920;&#24449;&#27169;&#22411;&#19968;&#33268;&#24615;&#36793;&#30028;&#26465;&#20214;&#20013;&#36215;&#30528;&#22522;&#26412;&#30340;&#20316;&#29992;&#65306;(a)&#8220;&#27531;&#24046;&#21270;&#29305;&#24449;&#8221;&#30340;&#22797;&#26434;&#24615;&#65292;(b)&#8220;&#34394;&#20551;&#25237;&#24433;&#8221;&#30340;&#22797;&#26434;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20165;&#20381;&#36182;&#20110;&#21487;&#36776;&#35782;&#24615;&#36793;&#30028;&#30340;&#31616;&#21333;&#36793;&#30028;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
For decades, best subset selection (BSS) has eluded statisticians mainly due to its computational bottleneck. However, until recently, modern computational breakthroughs have rekindled theoretical interest in BSS and have led to new findings. Recently, \cite{guo2020best} showed that the model selection performance of BSS is governed by a margin quantity that is robust to the design dependence, unlike modern methods such as LASSO, SCAD, MCP, etc. Motivated by their theoretical results, in this paper, we also study the variable selection properties of best subset selection for high-dimensional sparse linear regression setup. We show that apart from the identifiability margin, the following two complexity measures play a fundamental role in characterizing the margin condition for model consistency: (a) complexity of \emph{residualized features}, (b) complexity of \emph{spurious projections}. In particular, we establish a simple margin condition that depends only on the identifiability mar
&lt;/p&gt;</description></item></channel></rss>