<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07663</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#21644;&#21518;&#39564;&#22349;&#32553;&#38408;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21464;&#20998;&#21518;&#39564;&#32463;&#24120;&#19982;&#20808;&#39564;&#23494;&#20999;&#21563;&#21512;&#65292;&#36825;&#34987;&#31216;&#20026;&#21518;&#39564;&#22349;&#32553;&#65292;&#24433;&#21709;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;VAE&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35843;&#33410;&#30340;&#36229;&#21442;&#25968;beta&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#20998;&#26512;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#36739;&#22823;&#30340;beta&#20250;&#20135;&#29983;&#19968;&#20010;&#38271;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#12290;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#24179;&#21488;&#30340;&#38271;&#24230;&#24310;&#38271;&#65292;&#36229;&#36807;&#19968;&#23450;&#30340;&#38408;&#20540;&#21518;&#21464;&#20026;&#26080;&#31351;&#12290;&#36825;&#24847;&#21619;&#30528;&#19982;&#36890;&#24120;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#19981;&#21516;&#65292;beta&#30340;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#21518;&#39564;&#22349;&#32553;&#65292;&#32780;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#12290;&#22240;&#27492;&#65292;beta&#26159;&#19968;&#20010;&#38656;&#35201;&#35880;&#24910;&#35843;&#25972;&#30340;&#39118;&#38505;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#38408;&#20540;&#65292;&#36873;&#25321;&#23567;&#20110;&#36825;&#20010;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Variational Autoencoder (VAE), the variational posterior often aligns closely with the prior, which is known as posterior collapse and hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter beta has been introduced in the VAE. This paper presents a closed-form expression to assess the relationship between the beta in VAE, the dataset size, the posterior collapse, and the rate-distortion curve by analyzing a minimal VAE in a high-dimensional limit. These results clarify that a long plateau in the generalization error emerges with a relatively larger beta. As the beta increases, the length of the plateau extends and then becomes infinite beyond a certain beta threshold. This implies that the choice of beta, unlike the usual regularization parameters, can induce posterior collapse regardless of the dataset size. Thus, beta is a risky parameter that requires careful tuning. Furthermore, considering the dataset-size dependence on the ra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10187</link><description>&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23376;&#37319;&#26679;&#20316;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#12290;&#36825;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#36890;&#36807;&#23376;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#30340;&#32467;&#26524;&#21040;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#20026;&#20854;&#34987;&#36873;&#25321;&#27010;&#29575;&#30340;&#20498;&#25968;&#12290;&#27599;&#20010;&#28857;&#30340;&#36873;&#25321;&#27010;&#29575;&#30340;&#26435;&#37325;&#23545;&#38544;&#31169;&#30340;&#24433;&#21709;&#24182;&#19981;&#26126;&#26174;&#12290;&#19968;&#26041;&#38754;&#65292;&#36739;&#20302;&#30340;&#36873;&#25321;&#27010;&#29575;&#20250;&#23548;&#33268;&#26356;&#24378;&#30340;&#38544;&#31169;&#25918;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26435;&#37325;&#36234;&#39640;&#65292;&#22312;&#28857;&#34987;&#36873;&#25321;&#26102;&#65292;&#28857;&#23545;&#26426;&#21046;&#36755;&#20986;&#30340;&#24433;&#21709;&#23601;&#36234;&#24378;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36825;&#20004;&#20010;&#24433;&#21709;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#21516;&#26102;&#27604;&#22343;&#21248;&#23376;&#37319;&#26679;&#20855;&#26377;&#26356;&#24378;&#30340;&#38544;&#31169;&#21644;&#26356;&#22909;&#30340;&#25928;&#29992;&#65292;&#24182;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#21644;&#35299;&#20915;&#20102;&#38544;&#31169;&#20248;&#21270;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;...
&lt;/p&gt;
&lt;p&gt;
We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
&lt;/p&gt;</description></item></channel></rss>