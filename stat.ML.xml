<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.04978</link><description>&lt;p&gt;
Stacking&#20316;&#20026;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stacking as Accelerated Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04978
&lt;/p&gt;
&lt;p&gt;
Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stacking&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#23618;&#25968;&#24182;&#36890;&#36807;&#20174;&#26087;&#23618;&#22797;&#21046;&#21442;&#25968;&#26469;&#21021;&#22987;&#21270;&#26032;&#23618;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;Stacking&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#65306;&#21363;&#65292;Stacking&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#35813;&#29702;&#35770;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#25552;&#21319;&#26041;&#27861;&#20013;&#26500;&#24314;&#30340;&#21152;&#27861;&#38598;&#25104;&#31561;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#27599;&#19968;&#36718;&#25552;&#21319;&#36807;&#31243;&#20013;&#21021;&#22987;&#21270;&#26032;&#20998;&#31867;&#22120;&#30340;&#31867;&#20284;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#28508;&#33021;&#20989;&#25968;&#20998;&#26512;&#65292;Stacking&#30830;&#23454;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#26032;&#20013;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our
&lt;/p&gt;</description></item><item><title>&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;</title><link>https://arxiv.org/abs/2402.10482</link><description>&lt;p&gt;
&#29702;&#35299;&#24102;&#26377;&#26631;&#31614;&#22122;&#38899;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#33258;&#33976;&#39311;&#21644;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10482
&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#65288;SD&#65289;&#26159;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#20004;&#20010;&#27169;&#22411;&#20849;&#20139;&#30456;&#21516;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#29702;&#35770;&#19978;&#32771;&#23519;&#20102;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;SD&#65292;&#25506;&#32034;&#20102;&#22810;&#36718;SD&#21644;&#20855;&#26377;&#31934;&#28860;&#25945;&#24072;&#36755;&#20986;&#30340;SD&#65292;&#36825;&#20123;&#28789;&#24863;&#26469;&#33258;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#12290;&#36890;&#36807;&#25512;&#23548;&#23398;&#29983;&#27169;&#22411;&#36755;&#20986;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#65292;&#25105;&#20204;&#21457;&#29616;SD&#26412;&#36136;&#19978;&#26159;&#22312;&#20855;&#26377;&#39640;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#20043;&#38388;&#36827;&#34892;&#26631;&#31614;&#24179;&#22343;&#12290;&#26368;&#21021;&#26377;&#30410;&#30340;&#24179;&#22343;&#21270;&#26377;&#21161;&#20110;&#27169;&#22411;&#19987;&#27880;&#20110;&#19982;&#32473;&#23450;&#23454;&#20363;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SD&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26631;&#31614;&#25439;&#22351;&#26465;&#20214;&#21644;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur
&lt;/p&gt;</description></item></channel></rss>