<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.12367</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#35745;&#20998;&#21305;&#37197;&#31639;&#27861;&#35780;&#20272;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Semisupervised score based matching algorithm to evaluate the effect of public health interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#21305;&#37197;&#31639;&#27861;&#22312;&#35266;&#23519;&#24615;&#30740;&#31350;&#20013;&#8220;&#37197;&#23545;&#8221;&#30456;&#20284;&#30340;&#30740;&#31350;&#21333;&#20803;&#65292;&#20197;&#28040;&#38500;&#30001;&#20110;&#32570;&#20047;&#38543;&#26426;&#24615;&#32780;&#24341;&#36215;&#30340;&#28508;&#22312;&#20559;&#20506;&#21644;&#28151;&#26434;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#26032;&#22411;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#26435;&#37325;$\beta$&#34987;&#35774;&#35745;&#20026;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12367v1 Announce Type: cross  Abstract: Multivariate matching algorithms "pair" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of "pairs" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a "training" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02592</link><description>&lt;p&gt;
&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#35299;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#23610;&#24230;&#27495;&#20041;&#24182;&#20415;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#20248;&#21270;&#25152;&#35859;&#30340;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#65292;&#24378;&#21046;&#20351;&#22823;&#37096;&#20998;&#22240;&#23376;&#24444;&#27492;&#27491;&#20132;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#20132;&#32467;&#26500;&#65292;&#25105;&#20204;&#21033;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#26469;&#20248;&#21270;Stiefel&#27969;&#24418;&#19978;&#30340;&#36825;&#20123;&#22240;&#23376;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;TT&#20998;&#35299;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;RGD&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#24352;&#37327;&#38454;&#25968;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#36895;&#29575;&#20165;&#32463;&#21382;&#32447;&#24615;&#19979;&#38477;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#20174;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;TT&#26684;&#24335;&#24352;&#37327;&#12290;&#20551;&#35774;&#24863;&#30693;&#31639;&#23376;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#65288;RIP&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36890;&#36807;&#35889;&#21021;&#22987;&#21270;&#33719;&#24471;&#65292;RGD&#20063;&#20250;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#30340;&#28508;&#22312;&#31867;&#21035;&#25512;&#26029;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;WGoM&#26356;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10989</link><description>&lt;p&gt;
WGoM&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#24102;&#21152;&#26435;&#21709;&#24212;&#30340;&#20998;&#31867;&#25968;&#25454;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WGoM: A novel model for categorical data with weighted responses. (arXiv:2310.10989v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#30340;&#28508;&#22312;&#31867;&#21035;&#25512;&#26029;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;WGoM&#26356;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graded of Membership&#65288;GoM&#65289;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#20998;&#31867;&#25968;&#25454;&#20013;&#28508;&#22312;&#31867;&#21035;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#24471;&#20010;&#20307;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#28508;&#22312;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#36127;&#25972;&#25968;&#21709;&#24212;&#30340;&#20998;&#31867;&#25968;&#25454;&#65292;&#20351;&#24471;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#12290;&#19982;GoM&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;WGoM&#22312;&#21709;&#24212;&#30697;&#38453;&#30340;&#29983;&#25104;&#19978;&#25918;&#23485;&#20102;GoM&#30340;&#20998;&#24067;&#32422;&#26463;&#65292;&#24182;&#19988;&#27604;GoM&#26356;&#36890;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20272;&#35745;&#21442;&#25968;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#20934;&#30830;&#39640;&#25928;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Graded of Membership (GoM) model is a powerful tool for inferring latent classes in categorical data, which enables subjects to belong to multiple latent classes. However, its application is limited to categorical data with nonnegative integer responses, making it inappropriate for datasets with continuous or negative responses. To address this limitation, this paper proposes a novel model named the Weighted Grade of Membership (WGoM) model. Compared with GoM, our WGoM relaxes GoM's distribution constraint on the generation of a response matrix and it is more general than GoM. We then propose an algorithm to estimate the latent mixed memberships and the other WGoM parameters. We derive the error bounds of the estimated parameters and show that the algorithm is statistically consistent. The algorithmic performance is validated in both synthetic and real-world datasets. The results demonstrate that our algorithm is accurate and efficient, indicating its high potential for practical a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12968</link><description>&lt;p&gt;
&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31751;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31751;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#32676;&#65292;&#20854;&#20013;&#31751;&#22823;&#23567;&#38543;&#30528;&#29289;&#21697;&#24635;&#25968;$n$&#30340;&#22686;&#38271;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22312;LSBM&#20013;&#65292;&#20026;&#27599;&#23545;&#29289;&#21697;&#65288;&#29420;&#31435;&#22320;&#65289;&#35266;&#27979;&#21040;&#19968;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#26631;&#31614;&#26469;&#24674;&#22797;&#31751;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#26399;&#26395;&#34987;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#35823;&#20998;&#31867;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#23454;&#20363;&#29305;&#23450;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#19979;&#37117;&#33021;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#34920;&#29616;&#30340;&#31639;&#27861;&#12290;IAC&#30001;&#19968;&#27425;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#23454;&#20363;&#29305;&#23450;&#30340;&#19979;&#30028;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#21253;&#25324;&#31751;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20165;&#25191;&#34892;&#19968;&#27425;&#35889;&#32858;&#31867;&#65292;IAC&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#37117;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19470</link><description>&lt;p&gt;
&#29992;Johnson-Lindenstrauss&#30697;&#38453;&#36827;&#34892;&#26631;&#31614;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Johnson-Lindenstrauss&#30697;&#38453;&#65288;JLMs&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26497;&#31471;&#22810;&#20803;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;JLM&#30340;&#21015;&#26469;&#23884;&#20837;&#26631;&#31614;&#65292;&#23558;&#19968;&#20010;C&#31867;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;$\cO(\log C)$&#36755;&#20986;&#32500;&#24230;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#36229;&#37327;&#39118;&#38505;&#38480;&#21046;&#65292;&#38416;&#26126;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;Massart&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38477;&#32500;&#30340;&#24809;&#32602;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
&lt;/p&gt;</description></item></channel></rss>