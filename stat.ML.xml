<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16856</link><description>&lt;p&gt;
&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31283;&#20581;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Attentional Graph Neural Networks for Robust Massive Network Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#21457;&#25496;GNNs&#22312;&#22238;&#24402;&#20013;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#23558;GNNs&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20854;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#24443;&#24213;&#25913;&#21464;&#20102;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65306;&#32593;&#32476;&#23450;&#20301;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26032;&#22411;&#32593;&#32476;&#23450;&#20301;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;(NLOS)&#26465;&#20214;&#19979;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#32321;&#29712;&#30340;&#31163;&#32447;&#26657;&#20934;&#25110;NLOS&#35782;&#21035;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;(AGNN)&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;GCN&#26041;&#27861;&#30340;&#26377;&#38480;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16856v2 Announce Type: replace Abstract: In recent years, Graph neural networks (GNNs) have emerged as a prominent tool for classification tasks in machine learning. However, their application in regression tasks remains underexplored. To tap the potential of GNNs in regression, this paper integrates GNNs with attention mechanism, a technique that revolutionized sequential learning tasks with its adaptability and robustness, to tackle a challenging nonlinear regression problem: network localization. We first introduce a novel network localization method based on graph convolutional network (GCN), which exhibits exceptional precision even under severe non-line-of-sight (NLOS) conditions, thereby diminishing the need for laborious offline calibration or NLOS identification. We further propose an attentional graph neural network (AGNN) model, aimed at improving the limited flexibility and mitigating the high sensitivity to the hyperparameter of the GCN-based method. The AGNN co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08150</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20999;&#29255;&#36870;&#22238;&#24402;: &#26497;&#23567;&#26497;&#22823;&#24615;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sliced Inverse Regression: Minimax Optimality and Algorithm. (arXiv:2401.08150v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#38544;&#31169;&#20445;&#25252;&#24050;&#25104;&#20026;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20999;&#29255;&#36870;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#21327;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#23384;&#37325;&#35201;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20223;&#30495;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Proposed by Li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06915</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;: &#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#31616;&#21333;&#21644;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#22312;&#19981;&#21516;&#30340;&#24773;&#22659;&#19979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24179;&#22343;&#26041;&#26696;&#26469;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;SGD&#30340;&#36890;&#29992;&#24179;&#22343;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31867;&#21152;&#26435;&#24179;&#22343;SGD&#35299;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#23637;&#29616;&#20986;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#26368;&#20248;&#26435;&#37325;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;</title><link>http://arxiv.org/abs/2108.11328</link><description>&lt;p&gt;
&#29992;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#21152;&#24615;&#27169;&#22411;&#21644;&#32467;&#26500;&#20132;&#20114;&#39044;&#27979;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21453;&#24212;&#29575;
&lt;/p&gt;
&lt;p&gt;
Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#19968;&#31995;&#21015;&#28789;&#27963;&#19988;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#27169;&#22411;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#33879;&#21517;&#30340; ROAM &#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#35813;&#24212;&#29992;&#20351;&#29992;&#22312;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#35268;&#21010;&#25968;&#25454;&#24211;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#35782;&#21035;&#38590;&#20197;&#35843;&#26597;&#30340;&#21306;&#22495;&#12290;&#21313;&#24180;&#21069;&#32452;&#32455;&#30340;&#19968;&#22330;&#20247;&#21253;&#31454;&#36187;&#34920;&#26126;&#65292;&#22522;&#20110;&#22238;&#24402;&#26641;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#30456;&#24212;&#30340;&#27169;&#22411;&#19981;&#33021;&#29992;&#20110;&#25311;&#23450;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992; $\ell_0$-based &#24809;&#32602;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#23569;&#25968;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#24378;&#23618;&#27425;&#20132;&#20114;&#21512;&#24182;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#22312;Github &#19978;&#24320;&#28304;&#65289;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312; ROAM &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#25913;&#36827;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
&lt;/p&gt;</description></item></channel></rss>