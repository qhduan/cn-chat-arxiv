<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.09600</link><description>&lt;p&gt;
&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Graph Contrastive Learning for Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#22122;&#22768;&#65292;&#36825;&#20250;&#20005;&#37325;&#38477;&#20302;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;GNN&#32534;&#30721;&#22120;&#65292;&#21363;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20302;&#31209;&#27491;&#24120;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#21517;&#20026;LR-GCL&#30340;&#20302;&#31209;GCL&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;LR-GCL&#29983;&#25104;&#30340;&#29305;&#24449;&#65292;&#20351;&#29992;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#31639;&#27861;&#23545;&#22270;&#20013;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;LR-GCL&#21463;&#21040;&#22270;&#25968;&#25454;&#21644;&#20854;&#26631;&#31614;&#30340;&#20302;&#39057;&#24615;&#36136;&#30340;&#21551;&#31034;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#25105;&#20204;&#20851;&#20110;&#36716;&#23548;&#23398;&#20064;&#30340;&#23574;&#38160;&#27867;&#21270;&#30028;&#38480;&#30340;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09600v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL). Our method performs transductive node classification in two steps. First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our kno
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03647</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Fairness for Human-AI Collaboration. (arXiv:2310.03647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03647
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20154;&#31867;&#20915;&#31574;&#32773;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#26102;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#25110;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20844;&#24179;&#31639;&#27861;&#30340;&#36873;&#25321;&#24615;&#36981;&#23432;&#20250;&#30456;&#23545;&#20110;&#20154;&#31867;&#20197;&#21069;&#30340;&#25919;&#31574;&#22686;&#21152;&#27495;&#35270;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#20844;&#24179;&#32467;&#26524;&#38656;&#35201;&#22522;&#26412;&#19981;&#21516;&#30340;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#30830;&#20445;&#23545;&#20915;&#31574;&#32773;&#65288;&#20107;&#20808;&#19981;&#30693;&#36947;&#65289;&#30340;&#36981;&#23432;&#27169;&#24335;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#65292;&#26080;&#35770;&#20154;&#31867;&#30340;&#36981;&#23432;&#27169;&#24335;&#22914;&#20309;&#65292;&#23427;&#20204;&#37117;&#33021;&#30830;&#20445;&#22312;&#20915;&#31574;&#20013;&#25913;&#21892;&#20844;&#24179;&#24615;&#65288;&#24369;&#24418;&#24847;&#20041;&#19978;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#24615;&#33021;&#25913;&#36827;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#31639;&#27861;&#25512;&#33616;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;&#19968;&#20010;&#29305;&#23450;&#20844;&#24335;&#25152;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07779</link><description>&lt;p&gt;
&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#30340;&#22312;&#32447;&#31639;&#27861;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence analysis of online algorithms for vector-valued kernel regression. (arXiv:2309.07779v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#21521;&#37327;&#20540;&#20869;&#26680;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;&#19968;&#20010;&#29305;&#23450;&#20844;&#24335;&#25152;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20316;&#20026;&#20808;&#39564;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20174;&#22122;&#22768;&#21521;&#37327;&#20540;&#25968;&#25454;&#20013;&#36924;&#36817;&#22238;&#24402;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#22312;&#22312;&#32447;&#31639;&#27861;&#20013;&#65292;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#36880;&#20010;&#21487;&#29992;&#65292;&#24182;&#20381;&#27425;&#22788;&#29702;&#20197;&#26500;&#24314;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#20851;&#27880;&#36825;&#31181;&#22312;&#32447;&#36924;&#36817;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RKHS&#33539;&#25968;&#20013;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#21487;&#20197;&#34987;$C^2(m+1)^{-s/(2+s)}$&#32465;&#23450;&#65292;&#20854;&#20013;$m$&#20026;&#24403;&#19979;&#22788;&#29702;&#30340;&#25968;&#25454;&#25968;&#37327;&#65292;&#21442;&#25968;$0&lt;s\leq 1$&#34920;&#31034;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#39069;&#22806;&#20809;&#28369;&#24615;&#20551;&#35774;&#65292;&#24120;&#25968;$C$&#21462;&#20915;&#20110;&#36755;&#20837;&#22122;&#22768;&#30340;&#26041;&#24046;&#12289;&#22238;&#24402;&#20989;&#25968;&#30340;&#20809;&#28369;&#24615;&#20197;&#21450;&#31639;&#27861;&#30340;&#20854;&#20182;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating the regression function from noisy vector-valued data by an online learning algorithm using an appropriate reproducing kernel Hilbert space (RKHS) as prior. In an online algorithm, i.i.d. samples become available one by one by a random process and are successively processed to build approximations to the regression function. We are interested in the asymptotic performance of such online approximation algorithms and show that the expected squared error in the RKHS norm can be bounded by $C^2 (m+1)^{-s/(2+s)}$, where $m$ is the current number of processed data, the parameter $0&lt;s\leq 1$ expresses an additional smoothness assumption on the regression function and the constant $C$ depends on the variance of the input noise, the smoothness of the regression function and further parameters of the algorithm.
&lt;/p&gt;</description></item></channel></rss>