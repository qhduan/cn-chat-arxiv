<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.13255</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#19979;&#22810;&#31867;&#20998;&#31867;&#30340;&#28176;&#36827;&#27867;&#21270;&#31934;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#21327;&#21464;&#37327;&#21452;&#23618;&#27169;&#22411;&#19979;&#65292;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#28176;&#36827;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#25968;&#12289;&#29305;&#24449;&#21644;&#31867;&#21035;&#25968;&#37117;&#21516;&#26102;&#22686;&#38271;&#12290;&#25105;&#20204;&#23436;&#20840;&#35299;&#20915;&#20102;Subramanian&#31561;&#20154;&#22312;'22&#24180;&#25152;&#25552;&#20986;&#30340;&#29468;&#24819;&#65292;&#19982;&#39044;&#27979;&#30340;&#27867;&#21270;&#21306;&#38388;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#30340;&#19979;&#30028;&#31867;&#20284;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#65306;&#23427;&#20204;&#33021;&#22815;&#30830;&#31435;&#35823;&#20998;&#31867;&#29575;&#36880;&#28176;&#36235;&#36817;&#20110;0&#25110;1.&#25105;&#20204;&#32039;&#23494;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#26159;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22238;&#24402;&#22120;&#26368;&#20248;&#30340;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22312;&#28176;&#36827;&#19978;&#27425;&#20248;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31181;&#26032;&#30340;Hanson-Wright&#19981;&#31561;&#24335;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#22810;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#31867;&#22411;&#20998;&#26512;&#22312;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
&lt;/p&gt;</description></item></channel></rss>