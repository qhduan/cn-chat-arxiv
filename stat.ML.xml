<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.10791</link><description>&lt;p&gt;
&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#26680;&#24515;&#12290;&#21021;&#22987;&#21270;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#23567;&#30340;&#21021;&#22987;&#21270;&#36890;&#24120;&#19982;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30456;&#20851;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#23545;&#31616;&#21333;&#35299;&#38544;&#21547;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26089;&#26399;&#23545;&#40784;&#38454;&#27573;&#30340;&#26222;&#36941;&#21644;&#37327;&#21270;&#25551;&#36848;&#65292;&#26368;&#21021;&#30001;Maennel&#31561;&#20154;&#25552;&#20986;&#12290;&#23545;&#20110;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#65292;&#35757;&#32451;&#21160;&#24577;&#30340;&#26089;&#26399;&#38454;&#27573;&#23548;&#33268;&#31070;&#32463;&#20803;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#24341;&#21457;&#20102;&#32593;&#32476;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#36825;&#19982;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#26159;&#20197;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#30446;&#26631;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#20026;&#20195;&#20215;&#30340;&#65306;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#31034;&#20363;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#32593;&#32476;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.08724</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Piecewise Deterministic Markov Processes for Bayesian Neural Networks. (arXiv:2302.08724v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#25512;&#29702;&#36890;&#24120;&#20381;&#36182;&#20110;&#21464;&#20998;&#25512;&#26029;&#22788;&#29702;&#65292;&#36825;&#35201;&#27714;&#36829;&#21453;&#20102;&#29420;&#31435;&#24615;&#21644;&#21518;&#39564;&#24418;&#24335;&#30340;&#20551;&#35774;&#12290;&#20256;&#32479;&#30340;MCMC&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#36866;&#24212;&#20284;&#28982;&#30340;&#23376;&#37319;&#26679;&#65292;&#23548;&#33268;&#35745;&#31639;&#37327;&#22686;&#21152;&#12290;&#26032;&#30340;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;PDMP&#65289;&#37319;&#26679;&#22120;&#20801;&#35768;&#23376;&#37319;&#26679;&#65292;&#20294;&#24341;&#20837;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#19981;&#22343;&#21248;&#27850;&#26494;&#36807;&#31243;&#65288;IPPs&#65289;&#65292;&#20174;&#20013;&#37319;&#26679;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;IPPs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#23558;PDMPs&#24212;&#29992;&#20110;BNNs&#25512;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#22312;&#35745;&#31639;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36817;&#20284;&#25512;&#29702;&#26041;&#26696;&#30456;&#27604;&#65292;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference on modern Bayesian Neural Networks (BNNs) often relies on a variational inference treatment, imposing violated assumptions of independence and the form of the posterior. Traditional MCMC approaches avoid these assumptions at the cost of increased computation due to its incompatibility to subsampling of the likelihood. New Piecewise Deterministic Markov Process (PDMP) samplers permit subsampling, though introduce a model specific inhomogenous Poisson Process (IPPs) which is difficult to sample from. This work introduces a new generic and adaptive thinning scheme for sampling from these IPPs, and demonstrates how this approach can accelerate the application of PDMPs for inference in BNNs. Experimentation illustrates how inference with these methods is computationally feasible, can improve predictive accuracy, MCMC mixing performance, and provide informative uncertainty measurements when compared against other approximate inference schemes.
&lt;/p&gt;</description></item></channel></rss>