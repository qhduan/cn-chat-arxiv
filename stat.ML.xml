<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24102;&#23574;&#23792;&#30340;Wigner&#27169;&#22411;&#20013;&#20351;&#29992;AIC&#31867;&#22411;&#20934;&#21017;&#36827;&#34892;&#19968;&#33268;&#24615;&#27169;&#22411;&#36873;&#25321;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;$\gamma &gt; 2$&#65292;&#35813;&#20934;&#21017;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#65292;&#32780;&#23545;&#20110;$\gamma &lt; 2$&#65292;&#23427;&#20960;&#20046;&#32943;&#23450;&#20250;&#39640;&#20272;&#23574;&#23792;&#25968;&#37327;$k$&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;AIC&#24369;&#19968;&#33268;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26576;&#20010;&#36719;&#26368;&#23567;&#21270;&#22120;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12982</link><description>&lt;p&gt;
&#22312;&#24102;&#23574;&#23792;&#30340;Wigner&#27169;&#22411;&#20013;&#36890;&#36807;AIC&#31867;&#22411;&#20934;&#21017;&#36827;&#34892;&#19968;&#33268;&#24615;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Consistent model selection in the spiked Wigner model via AIC-type criteria. (arXiv:2307.12982v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24102;&#23574;&#23792;&#30340;Wigner&#27169;&#22411;&#20013;&#20351;&#29992;AIC&#31867;&#22411;&#20934;&#21017;&#36827;&#34892;&#19968;&#33268;&#24615;&#27169;&#22411;&#36873;&#25321;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;$\gamma &gt; 2$&#65292;&#35813;&#20934;&#21017;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#65292;&#32780;&#23545;&#20110;$\gamma &lt; 2$&#65292;&#23427;&#20960;&#20046;&#32943;&#23450;&#20250;&#39640;&#20272;&#23574;&#23792;&#25968;&#37327;$k$&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;AIC&#24369;&#19968;&#33268;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26576;&#20010;&#36719;&#26368;&#23567;&#21270;&#22120;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#24102;&#23574;&#23792;&#30340;Wigner&#27169;&#22411;\[ X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \]&#20854;&#20013;$G$&#26159;&#19968;&#20010;$N \times N$&#30340;GOE&#38543;&#26426;&#30697;&#38453;&#65292;&#32780;&#29305;&#24449;&#20540;$\lambda_i$&#37117;&#26159;&#26377;&#23574;&#23792;&#30340;&#65292;&#21363;&#36229;&#36807;&#20102;Baik-Ben Arous-P\'ech\'e (BBP)&#30340;&#38408;&#20540;$\sigma$&#12290;&#25105;&#20204;&#32771;&#34385;&#24418;&#24335;&#20026;\[ -2 \, (\text{&#26368;&#22823;&#21270;&#30340;&#23545;&#25968;&#20284;&#28982;}) + \gamma \, (\text{&#21442;&#25968;&#25968;&#37327;}) \]&#30340;AIC&#31867;&#22411;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#65292;&#29992;&#20110;&#20272;&#35745;&#23574;&#23792;&#25968;&#37327;$k$&#12290;&#23545;&#20110;$\gamma &gt; 2$&#65292;&#19978;&#36848;&#20934;&#21017;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#65292;&#21069;&#25552;&#26159;$\lambda_k &gt; \lambda_{\gamma}$&#65292;&#20854;&#20013;$\lambda_{\gamma}$&#26159;&#20005;&#26684;&#39640;&#20110;BBP&#38408;&#20540;&#30340;&#38408;&#20540;&#65292;&#32780;&#23545;&#20110;$\gamma &lt; 2$&#65292;&#23427;&#20960;&#20046;&#32943;&#23450;&#20250;&#39640;&#20272;$k$&#12290;&#34429;&#28982;AIC&#65288;&#23545;&#24212;&#20110;$\gamma = 2$&#65289;&#24182;&#38750;&#24378;&#19968;&#33268;&#20272;&#35745;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#65292;&#21462;$\gamma = 2 + \delta_N$&#65292;&#20854;&#20013;$\delta_N \to 0$&#19988;$\delta_N \gg N^{-2/3}$&#65292;&#20250;&#24471;&#21040;$k$&#30340;&#24369;&#19968;&#33268;&#20272;&#35745;&#37327;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;AIC&#30340;&#26576;&#20010;&#36719;&#26368;&#23567;&#21270;&#22120;&#26159;&#24378;&#19968;&#33268;&#20272;&#35745;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the spiked Wigner model \[ X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[ -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We also show that a certain soft minimiser of AIC is strongly consistent.
&lt;/p&gt;</description></item></channel></rss>