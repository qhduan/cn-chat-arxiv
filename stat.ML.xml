<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.08531</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08531
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#20294;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#23545;&#20110;Lipschitz&#20984;&#20989;&#25968;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#26368;&#20339;&#30340;$O(\log(1/\delta)\log T/\sqrt{T})$&#25110;$O(\sqrt{\log(1/\delta)/T})$&#26368;&#32456;&#36845;&#20195;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36895;&#29575;&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$\delta$&#26159;&#22833;&#36133;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#30028;&#38480;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#20040;&#23616;&#38480;&#20110;&#32039;&#33268;&#22495;&#65292;&#35201;&#20040;&#38656;&#35201;&#20960;&#20046;&#32943;&#23450;&#26377;&#30028;&#30340;&#22122;&#22768;&#12290;&#24456;&#33258;&#28982;&#22320;&#20250;&#38382;&#65292;&#19981;&#38656;&#35201;&#36825;&#20004;&#20010;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;SGD&#30340;&#26368;&#32456;&#36845;&#20195;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#20445;&#35777;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#38500;&#20102;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#22806;&#65292;&#36824;&#26377;&#24456;&#22810;&#29702;&#35770;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08531v2 Announce Type: replace  Abstract: In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal $O(\log(1/\delta)\log T/\sqrt{T})$ or $O(\sqrt{\log(1/\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\delta$ is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. Besides this important question, there are still lots of theoretical problems lacking an answer. For example, compared with the last-iterate convergence of SGD for non-smoot
&lt;/p&gt;</description></item></channel></rss>