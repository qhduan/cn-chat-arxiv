<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.16858</link><description>&lt;p&gt;
Transductive Learning&#30340;&#23574;&#38160;&#27867;&#21270;&#65306;&#19968;&#31181;Transductive Local Rademacher Complexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16858
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20256;&#32479;&#30340;local rademacher complexity (LRC)&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;transductive&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;LRC&#26041;&#27861;&#22312;&#24402;&#32435;&#35774;&#32622;&#20013;&#30340;&#20998;&#26512;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rademacher complex&#30340;&#23616;&#37096;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;transductive learning&#38382;&#39064;&#65292;&#24182;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#19982;LRC&#30340;&#21457;&#23637;&#31867;&#20284;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29420;&#31435;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#24320;&#22987;&#26500;&#24314;TLRC&#65292;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#35889;&#32858;&#31867;&#30340;&#24378;&#19968;&#33268;&#24615;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#65292;&#24182;&#19988;&#22312;&#35813;&#38408;&#20540;&#20197;&#19979;&#32473;&#20986;&#20272;&#35745;&#26631;&#31614;&#30340;&#26399;&#26395;&#8220;&#19981;&#21305;&#37197;&#29575;&#8221;&#19978;&#30028;&#12290;&#24182;&#19988;&#65292;&#21333;&#27493;&#35889;&#31639;&#27861;&#21487;&#20197;&#22312;&#36229;&#36807;&#35813;&#38408;&#20540;&#26102;&#38750;&#24120;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#22320;&#32473;&#23450;&#27599;&#20010;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2306.06845</link><description>&lt;p&gt;
&#23545;&#31216;&#20108;&#20803;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#35889;&#32858;&#31867;&#30340;&#24378;&#19968;&#33268;&#24615;&#19982;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Strong consistency and optimality of spectral clustering in symmetric binary non-uniform Hypergraph Stochastic Block Model. (arXiv:2306.06845v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06845
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#35889;&#32858;&#31867;&#30340;&#24378;&#19968;&#33268;&#24615;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#65292;&#24182;&#19988;&#22312;&#35813;&#38408;&#20540;&#20197;&#19979;&#32473;&#20986;&#20272;&#35745;&#26631;&#31614;&#30340;&#26399;&#26395;&#8220;&#19981;&#21305;&#37197;&#29575;&#8221;&#19978;&#30028;&#12290;&#24182;&#19988;&#65292;&#21333;&#27493;&#35889;&#31639;&#27861;&#21487;&#20197;&#22312;&#36229;&#36807;&#35813;&#38408;&#20540;&#26102;&#38750;&#24120;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#22320;&#32473;&#23450;&#27599;&#20010;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#22343;&#21248;&#36229;&#22270;&#38543;&#26426;&#22359;&#27169;&#22411;&#19979;&#65292;&#20004;&#20010;&#31561;&#22823;&#23567;&#30340;&#31038;&#21306;&#65288;n/2&#65289;&#20013;&#30340;&#38543;&#26426;&#36229;&#22270;&#19978;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#36793;&#21482;&#20381;&#36182;&#20110;&#20854;&#39030;&#28857;&#30340;&#26631;&#31614;&#65292;&#36793;&#20197;&#19968;&#23450;&#27010;&#29575;&#29420;&#31435;&#20986;&#29616;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#24314;&#31435;&#20102;&#24378;&#19968;&#33268;&#24615;&#30340;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#65292;&#22312;&#35813;&#38408;&#20540;&#20197;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#37117;&#26377;&#24456;&#39640;&#27010;&#29575;&#20250;&#35823;&#20998;&#31867;&#33267;&#23569;&#20004;&#20010;&#39030;&#28857;&#65292;&#32780;&#29305;&#24449;&#21521;&#37327;&#20272;&#35745;&#37327;&#30340;&#26399;&#26395;&#8220;&#19981;&#21305;&#37197;&#29575;&#8221;&#19978;&#30028;&#20026;$n$&#30340;&#38408;&#20540;&#30340;&#36127;&#25351;&#25968;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#36229;&#36807;&#35813;&#38408;&#20540;&#26102;&#65292;&#23613;&#31649;&#24352;&#37327;&#25910;&#32553;&#24341;&#36215;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#21333;&#27493;&#35889;&#31639;&#27861;&#20165;&#22312;&#32473;&#23450;&#25910;&#32553;&#30340;&#37051;&#25509;&#30697;&#38453;&#26102;&#65292;&#21363;&#20351;SDP&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#22833;&#36133;&#65292;&#20063;&#21487;&#20197;&#38750;&#24120;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#22320;&#32473;&#23450;&#27599;&#20010;&#39030;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#24378;&#19968;&#33268;&#24615;&#21487;&#20197;&#36890;&#36807;&#23545;&#25152;&#26377;&#27425;&#20248;&#32858;&#21512;&#20449;&#24687;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the unsupervised classification problem in random hypergraphs under the non-uniform \emph{Hypergraph Stochastic Block Model} (HSBM) with two equal-sized communities ($n/2$), where each edge appears independently with some probability depending only on the labels of its vertices. In this paper, an \emph{information-theoretical} threshold for strong consistency is established. Below the threshold, every algorithm would misclassify at least two vertices with high probability, and the expected \emph{mismatch ratio} of the eigenvector estimator is upper bounded by $n$ to the power of minus the threshold. On the other hand, when above the threshold, despite the information loss induced by tensor contraction, one-stage spectral algorithms assign every vertex correctly with high probability when only given the contracted adjacency matrix, even if \emph{semidefinite programming} (SDP) fails in some scenarios. Moreover, strong consistency is achievable by aggregating information from al
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item></channel></rss>