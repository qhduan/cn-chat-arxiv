<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16054</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
Metric Space Magnitude for Evaluating the Diversity of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16054
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#30340;&#22823;&#23567;&#26159;&#19968;&#31181;&#36817;&#26399;&#24314;&#31435;&#30340;&#19981;&#21464;&#24615;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#25552;&#20379;&#31354;&#38388;&#30340;&#8220;&#26377;&#25928;&#22823;&#23567;&#8221;&#30340;&#34913;&#37327;&#65292;&#24182;&#25429;&#25417;&#21040;&#35768;&#22810;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#20869;&#22312;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24418;&#24335;&#21270;&#20102;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#20989;&#25968;&#20043;&#38388;&#30340;&#26032;&#39062;&#19981;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#25968;&#25454;&#25200;&#21160;&#19979;&#20445;&#35777;&#31283;&#23450;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#20005;&#26684;&#30340;&#22810;&#23610;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#23454;&#39564;&#22871;&#20214;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#12289;&#27169;&#24335;&#23849;&#28291;&#26816;&#27979;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. We show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2311.08376</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#33218;&#30340;&#38598;&#25104;&#25277;&#26679;&#65306;&#23567;&#38598;&#25104;&#36275;&#30691;
&lt;/p&gt;
&lt;p&gt;
Ensemble sampling for linear bandits: small ensembles suffice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#35774;&#23450;&#19979;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#26377;&#29992;&#19988;&#20005;&#35880;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#20132;&#20114;&#20316;&#29992;&#26102;&#38388;&#36328;&#24230;$T$&#30340;$d$&#32500;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#65292;&#37319;&#29992;&#38598;&#25104;&#22823;&#23567;&#20026;$\smash{d \log T}$&#30340;&#38598;&#25104;&#25277;&#26679;&#65292;&#36973;&#21463;&#30340;&#21518;&#24724;&#26368;&#22810;&#20026;$\smash{(d \log T)^{5/2} \sqrt{T}}$&#38454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#20219;&#20309;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#31532;&#19968;&#20010;&#19981;&#35201;&#27714;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#30340;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#38598;&#25104;&#25277;&#26679;&#22833;&#21435;&#24847;&#20041;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#25509;&#36817;$\smash{\sqrt{T}}$&#38454;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#26080;&#38480;&#21160;&#20316;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#20197;&#21450;&#22312;&#20351;&#29992;&#20013;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#19981;&#21464;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.12588</link><description>&lt;p&gt;
&#35299;&#35835;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpreting Equivariant Representations. (arXiv:2401.12588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#31561;&#21464;&#24615;&#20197;&#21450;&#22312;&#20351;&#29992;&#20013;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#19981;&#21464;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#12289;&#25554;&#20540;&#25110;&#29305;&#24449;&#25552;&#21462;&#31561;&#19979;&#28216;&#20219;&#21153;&#65292;&#28508;&#22312;&#34920;&#31034;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19981;&#21464;&#21644;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#24378;&#21046;&#25191;&#34892;&#24402;&#32435;&#20559;&#24046;&#30340;&#24378;&#22823;&#19988;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#28508;&#22312;&#34920;&#31034;&#26102;&#65292;&#24517;&#39035;&#21516;&#26102;&#32771;&#34385;&#31561;&#21464;&#27169;&#22411;&#26045;&#21152;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#32771;&#34385;&#24402;&#32435;&#20559;&#24046;&#20250;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#22312;&#34920;&#31034;&#30340;&#19981;&#21464;&#25237;&#24433;&#21487;&#20197;&#26377;&#25928;&#22320;&#32771;&#34385;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#25321;&#36825;&#26679;&#19968;&#20010;&#25237;&#24433;&#30340;&#21407;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#35265;&#20363;&#23376;&#20013;&#20351;&#29992;&#36825;&#20123;&#21407;&#21017;&#30340;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#29983;&#25104;&#30340;&#32622;&#25442;&#31561;&#21464;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65307;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#35774;&#35745;&#20986;&#19981;&#20135;&#29983;&#20449;&#24687;&#25439;&#22833;&#30340;&#19981;&#21464;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent representations are used extensively for downstream tasks, such as visualization, interpolation or feature extraction of deep learning models. Invariant and equivariant neural networks are powerful and well-established models for enforcing inductive biases. In this paper, we demonstrate that the inductive bias imposed on the by an equivariant model must also be taken into account when using latent representations. We show how not accounting for the inductive biases leads to decreased performance on downstream tasks, and vice versa, how accounting for inductive biases can be done effectively by using an invariant projection of the latent representations. We propose principles for how to choose such a projection, and show the impact of using these principles in two common examples: First, we study a permutation equivariant variational auto-encoder trained for molecule graph generation; here we show that invariant projections can be designed that incur no loss of information in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09123</link><description>&lt;p&gt;
&#20351;&#29992;&#34987;&#21160; Langevin &#21160;&#21147;&#23398;&#30340;&#33258;&#36866;&#24212;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#26159;&#20174;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861; (PSGLD) &#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#26088;&#22312;&#23454;&#29616;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22788;&#30340;&#8220;&#34987;&#21160;&#8221;&#26159;&#25351; PSGLD &#31639;&#27861;(&#36870;&#23398;&#20064;&#36807;&#31243;)&#21487;&#29992;&#30340;&#22122;&#22768;&#28176;&#21464;&#26159;&#30001;&#22806;&#37096;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;(&#27491;&#21521;&#23398;&#20064;&#22120;)&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#28857;&#19978;&#35780;&#20272;&#30340;&#12290;PSGLD &#31639;&#27861;&#22240;&#27492;&#20805;&#24403;&#19968;&#20010;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#21487;&#24674;&#22797;&#27491;&#22312;&#34987;&#27492;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#20998;&#26512;&#20102;&#36825;&#20010;&#34987;&#21160;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#34987;&#21160;&#31639;&#27861;&#21644;&#20854;&#31283;&#23450;&#27979;&#24230;&#20043;&#38388;&#30340; 2-Wasserstein &#36317;&#31163;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#37325;&#24314;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
&lt;/p&gt;</description></item></channel></rss>