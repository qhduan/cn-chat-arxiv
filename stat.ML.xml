<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.02490</link><description>&lt;p&gt;
&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#25910;&#25947;&#29575;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02490
&lt;/p&gt;
&lt;p&gt;
&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#22312;&#23545;&#31216;&#19981;&#21160;&#28857;&#36845;&#20195;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20854;&#36229;&#36234;&#26631;&#20934;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#31383;&#21475;&#24335;&#23433;&#24503;&#26862;&#21152;&#36895;&#65288;AA&#65289;&#31639;&#27861;&#29992;&#20110;&#19981;&#21160;&#28857;&#26041;&#27861;&#65292;$x^{(k+1)}=q(x^{(k)})$&#12290;&#23427;&#39318;&#27425;&#35777;&#26126;&#20102;&#24403;&#31639;&#23376;$q$&#26159;&#32447;&#24615;&#19988;&#23545;&#31216;&#26102;&#65292;&#20351;&#29992;&#20808;&#21069;&#36845;&#20195;&#30340;&#28369;&#21160;&#31383;&#21475;&#30340;&#31383;&#21475;&#24335;AA&#31639;&#27861;&#33021;&#22815;&#25913;&#36827;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#65292;&#36229;&#36807;&#19981;&#21160;&#28857;&#36845;&#20195;&#12290;&#24403;$q$&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#22266;&#23450;&#28857;&#22788;&#20855;&#26377;&#23545;&#31216;&#38597;&#21487;&#27604;&#30697;&#38453;&#26102;&#65292;&#32463;&#36807;&#30053;&#24494;&#20462;&#25913;&#30340;AA&#31639;&#27861;&#34987;&#35777;&#26126;&#23545;&#27604;&#19981;&#21160;&#28857;&#36845;&#20195;&#20855;&#26377;&#31867;&#20284;&#30340;&#26681;&#32447;&#24615;&#25910;&#25947;&#22240;&#23376;&#25913;&#36827;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;Tyler&#30340;M&#20272;&#35745;&#20013;&#65292;AA&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#30340;&#19981;&#21160;&#28857;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02490v2 Announce Type: replace-cross  Abstract: This paper studies the commonly utilized windowed Anderson acceleration (AA) algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the first proof that when the operator $q$ is linear and symmetric the windowed AA, which uses a sliding window of prior iterates, improves the root-linear convergence factor over the fixed-point iterations. When $q$ is nonlinear, yet has a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is proved to have an analogous root-linear convergence factor improvement over fixed-point iterations. Simulations verify our observations. Furthermore, experiments with different data models demonstrate AA is significantly superior to the standard fixed-point methods for Tyler's M-estimation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.01654</link><description>&lt;p&gt;
GANs&#35299;&#20915;&#20998;&#25968;&#20105;&#35758;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30001;&#19968;&#20010;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#21028;&#21035;&#22120;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#26399;&#26395;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#32780;&#21028;&#21035;&#22120;&#21017;&#34987;&#35757;&#32451;&#20197;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#29983;&#25104;&#22120;&#36755;&#20986;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#20998;&#26512;&#29983;&#25104;&#22120;&#20248;&#21270;&#12290;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270; GAN &#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#24471;&#20998;&#36827;&#34892;&#21305;&#37197;&#24471;&#21040;&#30340;&#12290;&#22312;IPM GAN&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#26368;&#20248;&#29983;&#25104;&#22120;&#21305;&#37197;&#24471;&#20998;&#22411;&#20989;&#25968;&#65292;&#21253;&#25324;&#19982;&#25152;&#36873;IPM&#32422;&#26463;&#31354;&#38388;&#30456;&#20851;&#30340;&#26680;&#27969;&#22330;&#12290;&#27492;&#22806;&#65292;IPM-GAN&#20248;&#21270;&#21487;&#20197;&#30475;&#20316;&#26159;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#20013;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#29983;&#25104;&#22120;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#22312;&#26680;&#20989;&#25968;&#19978;&#36827;&#34892;&#21367;&#31215;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) comprise a generator, trained to learn the underlying distribution of the desired data, and a discriminator, trained to distinguish real samples from those output by the generator. A majority of GAN literature focuses on understanding the optimality of the discriminator through integral probability metric (IPM) or divergence based analysis. In this paper, we propose a unified approach to analyzing the generator optimization through variational approach. In $f$-divergence-minimizing GANs, we show that the optimal generator is the one that matches the score of its output distribution with that of the data distribution, while in IPM GANs, we show that this optimal generator matches score-like functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, the IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the 
&lt;/p&gt;</description></item></channel></rss>