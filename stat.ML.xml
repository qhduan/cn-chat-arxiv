<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25286;&#20998;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#32467;&#26500;&#26080;&#20851;&#30340;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;DCDR&#20272;&#35745;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15175</link><description>&lt;p&gt;
&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#65306;&#36229;&#36234;&#20018;&#34892;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Double Cross-fit Doubly Robust Estimators: Beyond Series Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15175
&lt;/p&gt;
&lt;p&gt;
&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25286;&#20998;&#35757;&#32451;&#25968;&#25454;&#24182;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#32467;&#26500;&#26080;&#20851;&#30340;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;DCDR&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#36328;&#25311;&#21512;&#20132;&#21449;&#30340;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22240;&#20854;&#33391;&#22909;&#30340;&#32467;&#26500;&#26080;&#20851;&#38169;&#35823;&#20445;&#35777;&#32780;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#39069;&#22806;&#32467;&#26500;&#65292;&#20363;&#22914;H\"{o}lder&#24179;&#28369;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#29420;&#31435;&#26679;&#26412;&#19978;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25286;&#20998;&#21644;&#19979;&#35843;nuisance&#20989;&#25968;&#20272;&#35745;&#22120;&#26469;&#26500;&#24314;&#26356;&#31934;&#30830;&#30340;&#8220;&#21452;&#20132;&#21449;&#22266;&#23450;&#21452;&#31283;&#20581;&#8221;&#65288;DCDR&#65289;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#26399;&#26465;&#20214;&#21327;&#26041;&#24046;&#30340;DCDR&#20272;&#35745;&#22120;&#65292;&#22312;&#22240;&#26524;&#25512;&#26029;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#20013;&#26159;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#31995;&#21015;&#36880;&#28176;&#26356;&#24378;&#20551;&#35774;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;DCDR&#20272;&#35745;&#22120;&#25552;&#20379;&#26080;&#38656;&#23545;nuisance&#20989;&#25968;&#25110;&#23427;&#20204;&#30340;&#20272;&#35745;&#22120;&#20570;&#20986;&#20551;&#35774;&#30340;&#32467;&#26500;&#26080;&#20851;&#38169;&#35823;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#20551;&#35774;nuisance&#20989;&#25968;&#26159;H\"{o}lder&#24179;&#28369;&#65292;&#20294;&#19981;&#20551;&#35774;&#30693;&#26195;&#30495;&#23454;&#24179;&#28369;&#32423;&#21035;&#25110;&#21327;&#21464;&#37327;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15175v1 Announce Type: cross  Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate densit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#20559;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#20854;&#22312;&#19968;&#31867;&#26925;&#29699;&#19978;&#23454;&#29616;&#20102;&#65288;&#36817;&#20046;&#65289;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#36866;&#24212;&#26410;&#30693;&#36870;&#38382;&#39064;&#24230;&#30340;&#25552;&#21069;&#20572;&#27490;&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.11134</link><description>&lt;p&gt;
&#20989;&#25968;&#20559;&#26368;&#23567;&#20108;&#20056;&#27861;&#65306;&#26368;&#20248;&#25910;&#25947;&#29575;&#21644;&#33258;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Functional Partial Least-Squares: Optimal Rates and Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11134
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#20559;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#20854;&#22312;&#19968;&#31867;&#26925;&#29699;&#19978;&#23454;&#29616;&#20102;&#65288;&#36817;&#20046;&#65289;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#36866;&#24212;&#26410;&#30693;&#36870;&#38382;&#39064;&#24230;&#30340;&#25552;&#21069;&#20572;&#27490;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#26631;&#37327;&#21709;&#24212;&#21644; Hilbert &#31354;&#38388;&#20540;&#39044;&#27979;&#21464;&#37327;&#30340;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#20849;&#36717;&#26799;&#24230;&#26041;&#27861;&#30456;&#20851;&#30340;&#20989;&#25968;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#20272;&#35745;&#30340;&#26032;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#35813;&#20272;&#35745;&#22120;&#22312;&#19968;&#31867;&#26925;&#29699;&#19978;&#23454;&#29616;&#20102;&#65288;&#36817;&#20046;&#65289;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#36870;&#38382;&#39064;&#24230;&#30340;&#25552;&#21069;&#20572;&#27490;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20272;&#35745;&#22120;&#19982;&#20027;&#25104;&#20998;&#22238;&#24402;&#20272;&#35745;&#22120;&#20043;&#38388;&#30340;&#19968;&#20123;&#29702;&#35770;&#21644;&#20223;&#30495;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11134v1 Announce Type: cross  Abstract: We consider the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a well-known ill-posed inverse problem. We propose a new formulation of the functional partial least-squares (PLS) estimator related to the conjugate gradient method. We shall show that the estimator achieves the (nearly) optimal convergence rate on a class of ellipsoids and we introduce an early stopping rule which adapts to the unknown degree of ill-posedness. Some theoretical and simulation comparison between the estimator and the principal component regression estimator is provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10745</link><description>&lt;p&gt;
Mori-Zwanzig&#28508;&#21464;&#31354;&#38388;Koopman&#38381;&#21253;&#29992;&#20110;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#31616;&#21270;&#22797;&#26434;&#21160;&#21147;&#23398;&#29702;&#35299;&#30340;&#23453;&#36149;&#26041;&#27861;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#36924;&#36817;&#26377;&#38480;Koopman&#31639;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#36873;&#25321;&#21512;&#36866;&#30340;&#21487;&#35266;&#23519;&#37327;&#12289;&#38477;&#32500;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20851;&#38190;&#21487;&#35266;&#23519;&#37327;&#26469;&#36924;&#36817;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#38750;&#39532;&#23572;&#21487;&#22827;&#26657;&#27491;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#21464;&#27969;&#24418;&#20013;&#20135;&#29983;&#20102;&#21160;&#21147;&#23398;&#30340;&#23553;&#38381;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2305.12292</link><description>&lt;p&gt;
&#26368;&#20248;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#65306;&#21322;&#23450;&#26494;&#24347;&#21644;&#29305;&#24449;&#21521;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions. (arXiv:2305.12292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#30446;&#30340;&#26159;&#35745;&#31639;&#19968;&#20010;&#22797;&#26434;&#24230;&#26368;&#23567;&#30340;&#30697;&#38453;&#65292;&#20197;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#24674;&#22797;&#32473;&#23450;&#30340;&#19968;&#32452;&#35266;&#27979;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#20247;&#22810;&#24212;&#29992;&#65292;&#22914;&#20135;&#21697;&#25512;&#33616;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#26041;&#27861;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#34429;&#28982;&#39640;&#24230;&#21487;&#25193;&#23637;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#30830;&#23450;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#19981;&#20855;&#22791;&#20219;&#20309;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#19968;&#31181;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#26469;&#37325;&#26032;&#23457;&#35270;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#23548;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#32452;&#31209;&#19968;&#30697;&#38453;&#30340;&#21644;&#65292;&#24182;&#36890;&#36807; Shor &#26494;&#24347;&#26469;&#28608;&#21169;&#27599;&#20010;&#31209;&#19968;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010; 2*2 &#23567;&#30697;&#38453;&#30340;&#34892;&#21015;&#24335;&#20026;&#38646;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#24120;&#24456;&#32039;&#30340;&#20984;&#26494;&#24347;&#31867;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26032;&#20984;&#26494;&#24347;&#26041;&#27861;&#23558;&#26368;&#20248;&#24615;&#24046;&#36317;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible, and has numerous applications such as product recommendation. Unfortunately, existing methods for solving low-rank matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye, by reformulating low-rank problems as convex problems over the non-convex set of projection matrices and implementing a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often tight class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing, via a Shor relaxation, that each two-by-two minor in each rank-one matrix has determinant zero. In numerical experiments, our new convex relaxations decrease the optimali
&lt;/p&gt;</description></item></channel></rss>