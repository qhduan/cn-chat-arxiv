<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>Global-QSGD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#36798;$O(\ sqrt{n})$&#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#12290;</title><link>http://arxiv.org/abs/2305.18627</link><description>&lt;p&gt;
&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#65306;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#23454;&#29992;&#30340;&#26080;&#28014;&#28857;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. (arXiv:2305.18627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18627
&lt;/p&gt;
&lt;p&gt;
Global-QSGD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#32553;&#25918;&#37327;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#36798;$O(\ sqrt{n})$&#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26159;&#25512;&#21160;&#28145;&#24230;&#23398;&#20064;&#36817;&#26399;&#36827;&#23637;&#30340;&#20027;&#35201;&#39537;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#20449;&#24120;&#24120;&#26159;&#31995;&#32479;&#30340;&#20027;&#35201;&#29942;&#39048;&#24182;&#20855;&#26377;&#39640;&#26114;&#30340;&#20195;&#20215;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#26082;&#33021;&#22312;&#32463;&#39564;&#19978;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21448;&#33021;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20840;&#23616;-QSGD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#20840;&#23616;&#32553;&#25918;&#35774;&#35745;&#26469;&#21152;&#36895;&#22522;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;Global-QSGD&#26159;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#20005;&#26684;&#30340;Allreduce&#20860;&#23481;&#21387;&#32553;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#21387;&#32553;&#35823;&#24046;&#21644;&#36890;&#20449;&#33410;&#30465;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26469;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#26080;&#20559;&#24615;&#65292;Global-QSGD&#19981;&#20381;&#36182;&#26114;&#36149;&#30340;&#35823;&#24046;&#21453;&#39304;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;QSGD&#37327;&#21270;&#33021;&#25552;&#20379;&#39640;&#36798;$O(\sqrt{n})$ &#30340;&#39069;&#22806;&#21387;&#32553;&#27604;&#65288;&#20854;&#20013;$n$&#34920;&#31034;&#24037;&#20316;&#32773;&#30340;&#25968;&#37327;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#29702;&#35770;&#20445;&#35777;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20449;&#24687;&#35770;&#21644;&#20984;&#20998;&#26512;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed training is a principal driver of recent advances in deep learning. However, communication often proves costly and becomes the primary bottleneck in these systems. As a result, there is a demand for the design of efficient communication mechanisms that can empirically boost throughput while providing theoretical guarantees. In this work, we introduce Global-QSGD, a novel family of quantization operators, engineered to accelerate distributed training based on global scaling. We demonstrate that Global-QSGD is the first theoretically rigorous Allreduce-compatible compression mechanism that achieves a provable speed-up by striking a balance between compression error and communication savings. Importantly, Global-QSGD does not rely on costly error feedback due to its inherent unbiasedness and offers up to $O(\sqrt{n})$ additional compression ratio compared to the popular QSGD quantization ($n$ represents the number of workers). To obtain theoretical guarantees, we gen
&lt;/p&gt;</description></item></channel></rss>