<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.07715</link><description>&lt;p&gt;
&#36890;&#36807;&#27531;&#24046;&#32553;&#25918;&#23454;&#29616;ResNets&#30340;&#20449;&#21495;&#26368;&#20248;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Optimal signal propagation in ResNets through residual scaling. (arXiv:2305.07715v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Residual&#32593;&#32476;&#65288;ResNets&#65289;&#22312;&#22823;&#28145;&#24230;&#19978;&#27604;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#24341;&#20837;&#36339;&#36807;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#20449;&#21495;&#21521;&#26356;&#28145;&#23618;&#30340;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20026;&#27531;&#24046;&#20998;&#25903;&#28155;&#21152;&#32553;&#25918;&#21442;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#23613;&#31649;&#20182;&#20204;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#36825;&#31181;&#32553;&#25918;&#21442;&#25968;&#29305;&#21035;&#26377;&#21033;&#30340;&#21462;&#20540;&#33539;&#22260;&#65292;&#20294;&#20854;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#21450;&#20854;&#22312;&#32593;&#32476;&#36229;&#21442;&#25968;&#19978;&#30340;&#26222;&#36866;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#23545;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNets&#65289;&#65292;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#22312;&#20449;&#21495;&#20256;&#25773;&#21644;&#36229;&#21442;&#25968;&#35843;&#33410;&#26041;&#38754;&#33719;&#24471;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20026;ResNets&#23548;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#20197;&#30740;&#31350;&#20449;&#21495;&#20256;&#25773;&#21450;&#20854;&#23545;&#27531;&#24046;&#20998;&#25903;&#32553;&#25918;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#23548;&#20986;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#34913;&#37327;&#32593;&#32476;&#23545;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#19968;&#31181;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#22312;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks (ResNets) have significantly better trainability and thus performance than feed-forward networks at large depth. Introducing skip connections facilitates signal propagation to deeper layers. In addition, previous works found that adding a scaling parameter for the residual branch further improves generalization performance. While they empirically identified a particularly beneficial range of values for this scaling parameter, the associated performance improvement and its universality across network hyperparameters yet need to be understood. For feed-forward networks (FFNets), finite-size theories have led to important insights with regard to signal propagation and hyperparameter tuning. We here derive a systematic finite-size theory for ResNets to study signal propagation and its dependence on the scaling for the residual branch. We derive analytical expressions for the response function, a measure for the network's sensitivity to inputs, and show that for deep netwo
&lt;/p&gt;</description></item></channel></rss>