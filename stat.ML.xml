<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.09184</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#23610;&#24230;&#22797;&#26434;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#32500;&#24230;&#30340;&#32479;&#35745;&#27169;&#22411;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#12290;&#36825;&#20010;&#26032;&#30340;&#25968;&#37327;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;2sED&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#19979;&#26041;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#36817;&#20284;&#23545;&#19981;&#21516;&#30340;&#31361;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.04056</link><description>&lt;p&gt;
&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31867;&#31216;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#30340;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#21040;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#19981;&#20381;&#36182;&#20110;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#22823;&#22411;&#27969;&#24418;&#31070;&#32463;&#32593;&#32476;(MNNs)&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#27969;&#24418;&#28388;&#27874;-&#32452;&#21512;&#32593;&#32476;&#12290;&#36825;&#20010;&#31867;&#21035;&#21253;&#25324;&#20102;Wang&#12289;Ruiz&#21644;Ribeiro&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;MNNs&#65292;&#27969;&#24418;&#25955;&#23556;&#21464;&#25442;(&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;)&#65292;&#20197;&#21450;&#20854;&#20182;&#26377;&#36259;&#30340;&#20043;&#21069;&#22312;&#25991;&#29486;&#20013;&#26410;&#32771;&#34385;&#30340;&#31034;&#20363;&#65292;&#22914;Kipf&#21644;Welling&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27969;&#24418;&#31561;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#23545;&#27969;&#24418;&#26377;&#20840;&#23616;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#22312;&#26679;&#26412;&#28857;&#25968;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#33021;&#22815;&#20445;&#35777;&#25910;&#25947;&#21040;&#20854;&#36830;&#32493;&#26497;&#38480;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;(&#20027;&#35201;&#20851;&#27880;&#29305;&#23450;&#30340;MNN&#32467;&#26500;&#21644;&#22270;&#26500;&#24314;)&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#28388;&#27874;&#22120;&#25968;&#37327;&#12290;&#32780;&#19988;&#65292;&#23427;&#34920;&#29616;&#20986;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10189</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#23398;&#20064;&#39640;&#32500;&#38750;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions. (arXiv:2306.10189v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;$d$&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;$n$&#20010;&#36712;&#36857;&#24555;&#29031;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#31995;&#32479;&#38656;&#35201;&#23398;&#20064;$d$&#20010;&#20989;&#25968;&#12290;&#38500;&#38750;&#20855;&#26377;&#39069;&#22806;&#30340;&#31995;&#32479;&#23646;&#24615;&#30693;&#35782;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#23545;&#31216;&#24615;&#65292;&#21542;&#21017;&#26174;&#24335;&#30340;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25552;&#20379;&#30340;&#38544;&#24335;&#20844;&#24335;&#23398;&#20064;&#30340;&#32447;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;ODE&#37325;&#20889;&#20026;&#26356;&#24369;&#30340;&#31215;&#20998;&#24418;&#24335;&#65292;&#25105;&#20204;&#38543;&#21518;&#36827;&#34892;&#26368;&#23567;&#21270;&#24182;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#21521;&#37327;&#22330;&#20381;&#36182;&#20110;&#19982;&#35299;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;$d$&#21487;&#33021;&#36229;&#36807;100&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20174;&#22270;&#20687;&#25968;&#25454;&#23398;&#20064;&#38750;&#21442;&#25968;&#19968;&#38454;&#25311;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a nonparametric system of ordinary differential equations (ODEs) from $n$ trajectory snapshots in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach to learning using the implicit formulation provided by vector-valued Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral form, which we subsequently minimize, we derive our learning algorithm. The minimization problem's solution for the vector field relies on multivariate occupation kernel functions associated with the solution trajectories. We validate our approach through experiments on highly nonlinear simulated and real data, where $d$ may exceed 100. We further demonstrate the versatility of the proposed method by learning a nonparametric first order quasilinear partial differential 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#36827;&#34892;&#25512;&#26029;&#30340;&#24037;&#20316;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20272;&#35745;&#22120;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12553</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#30340;&#39640;&#32500;&#29305;&#24449;&#28176;&#36817;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Inference for Multi-Stage Stationary Treatment Policy with High Dimensional Features. (arXiv:2301.12553v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#36827;&#34892;&#25512;&#26029;&#30340;&#24037;&#20316;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20272;&#35745;&#22120;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#27835;&#30103;&#35268;&#21017;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#37327;&#36523;&#23450;&#21046;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#20989;&#25968;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#19968;&#31867;&#37325;&#35201;&#30340;&#27835;&#30103;&#31574;&#30053;&#26159;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#65292;&#20854;&#20351;&#29992;&#30456;&#21516;&#30340;&#20915;&#31574;&#20989;&#25968;&#26469;&#25351;&#23450;&#27835;&#30103;&#20998;&#37197;&#27010;&#29575;&#65292;&#22312;&#20915;&#31574;&#26102;&#22522;&#20110;&#21516;&#26102;&#21253;&#25324;&#22522;&#32447;&#21464;&#37327;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#65289;&#21644;&#26102;&#21464;&#21464;&#37327;&#65288;&#20363;&#22914;&#24120;&#35268;&#26816;&#27979;&#21040;&#30340;&#30142;&#30149;&#29983;&#29289;&#26631;&#24535;&#29289;&#65289;&#30340;&#19968;&#32452;&#29305;&#24449;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#25991;&#29486;&#23545;&#19982;&#21160;&#24577;&#27835;&#30103;&#31574;&#30053;&#30456;&#20851;&#30340;&#20215;&#20540;&#20989;&#25968;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#20294;&#22312;&#39640;&#32500;&#29305;&#24449;&#21464;&#37327;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#27835;&#30103;&#31574;&#30053;&#26412;&#36523;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#39033;&#24037;&#20316;&#30340;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#22686;&#24378;&#30340;&#20498;&#25968;&#26435;&#37325;&#20272;&#35745;&#22120;&#20272;&#35745;&#22810;&#38454;&#27573;&#38745;&#24577;&#27835;&#30103;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#20215;&#20540;&#20989;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic treatment rules or policies are a sequence of decision functions over multiple stages that are tailored to individual features. One important class of treatment policies for practice, namely multi-stage stationary treatment policies, prescribe treatment assignment probabilities using the same decision function over stages, where the decision is based on the same set of features consisting of both baseline variables (e.g., demographics) and time-evolving variables (e.g., routinely collected disease biomarkers). Although there has been extensive literature to construct valid inference for the value function associated with the dynamic treatment policies, little work has been done for the policies themselves, especially in the presence of high dimensional feature variables. We aim to fill in the gap in this work. Specifically, we first estimate the multistage stationary treatment policy based on an augmented inverse probability weighted estimator for the value function to increase
&lt;/p&gt;</description></item></channel></rss>