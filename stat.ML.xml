<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10070</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#20445;&#32467;&#26500;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Structure-Preserving Kernel Method for Learning Hamiltonian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#21253;&#21547;&#21704;&#23494;&#39039;&#21521;&#37327;&#22330;&#30340;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#35299;&#65292;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#25216;&#26415;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#38656;&#35201;&#21253;&#21547;&#26799;&#24230;&#32447;&#24615;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#35777;&#26126;&#20102;&#24494;&#20998;&#20877;&#29616;&#23646;&#24615;&#21644;&#34920;&#31034;&#23450;&#29702;&#12290;&#20998;&#26512;&#20102;&#20445;&#32467;&#26500;&#26680;&#20272;&#35745;&#22120;&#21644;&#39640;&#26031;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#25552;&#20379;&#20351;&#29992;&#22266;&#23450;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#33391;&#24615;&#33021;&#24471;&#21040;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10070v1 Announce Type: cross  Abstract: A structure-preserving kernel ridge regression method is presented that allows the recovery of potentially high-dimensional and nonlinear Hamiltonian functions out of datasets made of noisy observations of Hamiltonian vector fields. The method proposes a closed-form solution that yields excellent numerical performances that surpass other techniques proposed in the literature in this setup. From the methodological point of view, the paper extends kernel regression methods to problems in which loss functions involving linear functions of gradients are required and, in particular, a differential reproducing property and a Representer Theorem are proved in this context. The relation between the structure-preserving kernel estimator and the Gaussian posterior mean estimator is analyzed. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04952</link><description>&lt;p&gt;
&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Metrics on Markov Equivalence Classes for Evaluating Causal Discovery Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65288;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#30340;&#36755;&#20986;&#22270;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#22270;&#65292;&#35813;&#22270;&#32534;&#30721;&#20102;&#29983;&#25104;&#25968;&#25454;&#36807;&#31243;&#30340;&#22240;&#26524;&#22270;&#30340;&#22270;&#24418;&#20998;&#31163;&#21644;&#36830;&#25509;&#38472;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#24212;&#35813;&#21253;&#25324;&#20998;&#26512;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#24773;&#20917;&#30340;&#20998;&#31163;/&#36830;&#25509;&#31243;&#24230;&#65292;&#20197;&#34913;&#37327;&#36825;&#19968;&#26126;&#30830;&#30446;&#26631;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#21040;&#20004;&#20010;&#22240;&#26524;&#22270;&#30340;&#20998;&#31163;/&#36830;&#25509;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;s/c&#36317;&#31163;&#12289;&#39532;&#23572;&#31185;&#22827;&#36317;&#31163;&#21644;&#24544;&#23454;&#24230;&#36317;&#31163;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29609;&#20855;&#31034;&#20363;&#12289;&#23454;&#35777;&#23454;&#39564;&#21644;&#20266;&#20195;&#30721;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many state-of-the-art causal discovery methods aim to generate an output graph that encodes the graphical separation and connection statements of the causal graph that underlies the data-generating process. In this work, we argue that an evaluation of a causal discovery method against synthetic data should include an analysis of how well this explicit goal is achieved by measuring how closely the separations/connections of the method's output align with those of the ground truth. We show that established evaluation measures do not accurately capture the difference in separations/connections of two causal graphs, and we introduce three new measures of distance called s/c-distance, Markov distance and Faithfulness distance that address this shortcoming. We complement our theoretical analysis with toy examples, empirical experiments and pseudocode.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02520</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#22312;&#24378;&#22122;&#22768;&#20381;&#36182;&#20551;&#35774;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#32771;&#34385;&#22122;&#22768;&#20302;&#31209;&#21152;&#31232;&#30095;&#30697;&#38453;&#24674;&#22797;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20854;&#20013;&#22122;&#22768;&#30697;&#38453;&#21487;&#20197;&#26469;&#33258;&#20219;&#24847;&#20855;&#26377;&#20803;&#32032;&#38388;&#20219;&#24847;&#20381;&#36182;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20851;&#30456;&#20301;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#37117;&#26159;&#32039;&#33268;&#30340;&#65292;&#26082;&#28385;&#36275;&#30830;&#23450;&#24615;&#19979;&#30028;&#21448;&#21305;&#37197;&#26368;&#23567;&#21270;&#39118;&#38505;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#26029;&#35328;&#20004;&#20010;&#20219;&#24847;&#30340;&#20302;&#31209;&#26080;&#20851;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#24517;&#39035;&#22312;&#20854;&#20803;&#32032;&#19978;&#25193;&#25955;&#33021;&#37327;&#65292;&#25442;&#21477;&#35805;&#35828;&#19981;&#33021;&#22826;&#31232;&#30095;&#65292;&#36825;&#25581;&#31034;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#21487;&#33021;&#24341;&#36215;&#29420;&#31435;&#20852;&#36259;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20272;&#35745;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#30340;&#38382;&#39064;&#20013;&#65292;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
&lt;/p&gt;</description></item></channel></rss>