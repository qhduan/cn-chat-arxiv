<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#20215;&#20540;&#65288;IV&#65289;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#24314;&#31435;&#21069;&#30340;&#29305;&#24449;&#36873;&#25321;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13183</link><description>&lt;p&gt;
&#20449;&#24687;&#20215;&#20540;&#65288;IV&#65289;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Hypothesis Testing for Information Value (IV). (arXiv:2309.13183v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#20215;&#20540;&#65288;IV&#65289;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#24314;&#31435;&#21069;&#30340;&#29305;&#24449;&#36873;&#25321;&#25552;&#20379;&#20102;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20215;&#20540;&#65288;IV&#65289;&#26159;&#27169;&#22411;&#24314;&#31435;&#21069;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;&#23454;&#38469;&#26631;&#20934;&#65292;&#20294;&#22522;&#20110;IV&#30340;&#21028;&#26029;&#26159;&#21542;&#19968;&#20010;&#39044;&#27979;&#22240;&#23376;&#20855;&#26377;&#36275;&#22815;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#29702;&#35770;&#20381;&#25454;&#20381;&#28982;&#31070;&#31192;&#19988;&#32570;&#20047;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35813;&#25216;&#26415;&#30340;&#25968;&#23398;&#21457;&#23637;&#21644;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#20960;&#20046;&#27809;&#26377;&#25552;&#21450;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;IV&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20551;&#35774;&#26816;&#39564;&#26041;&#27861;&#26469;&#27979;&#35797;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#35745;&#31639;&#26816;&#39564;&#32479;&#35745;&#37327;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#30740;&#31350;&#20854;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#26041;&#27861;&#24212;&#29992;&#20110;&#38134;&#34892;&#27450;&#35784;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#25105;&#20204;&#32467;&#26524;&#30340;Python&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information value (IV) is a quite popular technique for feature selection prior to the modeling phase. There are practical criteria, but at the same time mysterious and lacking theoretical arguments, based on the IV, to decide if a predictor has sufficient predictive power to be considered in the modeling phase. However, the mathematical development and statistical inference methods for this technique is almost non-existent in the literature. In this work we present a theoretical framework for the IV and propose a non-parametric hypothesis test to test the predictive power. We show how to efficiently calculate the test statistic and study its performance on simulated data. Additionally, we apply our test on bank fraud data and provide a Python library where we implement our results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12344</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35777;&#26126;&#31934;&#30830;&#30340;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#20855;&#26377;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#33267;&#23569;&#21487;&#20197;&#36861;&#28335;&#21040;1936&#24180;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12290;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;&#30456;&#24212;&#30340;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#20294;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#22312;&#23436;&#20840;&#33539;&#22260;&#20869;&#26159;NP&#38590;&#30340;&#12290;&#25152;&#26377;&#26367;&#20195;&#26041;&#27861;&#37117;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#36817;&#20284;&#65292;&#21253;&#25324;&#20351;&#29992;0-1&#25439;&#22833;&#30340;&#20195;&#29702;&#65288;&#20363;&#22914;hinge&#25110;logistic&#25439;&#22833;&#65289;&#25110;&#36817;&#20284;&#30340;&#32452;&#21512;&#25628;&#32034;&#65292;&#36825;&#20123;&#37117;&#19981;&#33021;&#20445;&#35777;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#12290;&#25214;&#21040;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#26377;&#25928;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#30340;&#26500;&#24314;&#36807;&#31243;&#65292;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
&lt;/p&gt;</description></item></channel></rss>