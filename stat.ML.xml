<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2401.03350</link><description>&lt;p&gt;
&#20934;&#30830;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03350
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#20173;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#20107;&#23454;&#19978;&#65292;&#34429;&#28982;&#20107;&#21518;&#26657;&#20934;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20869;&#37096;&#20998;&#24067;&#26657;&#20934;&#65292;&#20294;&#23427;&#20204;&#19981;&#19968;&#23450;&#20063;&#33021;&#25913;&#36827;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20869;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#25216;&#26415;&#23588;&#20854;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38543;&#21518;&#19982;&#20107;&#21518;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G-$\Delta$UQ&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20869;&#22312;&#30340;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#26032;&#39062;&#30340;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#21407;&#21017;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;&#34429;&#28982;&#20027;&#27969;&#35266;&#28857;&#26159;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#20272;&#35745;&#65292;&#38656;&#35201;&#23436;&#20840;&#38543;&#26426;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#21151;&#33021;&#22810;&#26679;&#24615;&#24341;&#20837;&#30340;&#20013;&#35266;&#38170;&#23450;&#21487;&#20197;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.06291</link><description>&lt;p&gt;
&#26368;&#20248;&#24322;&#26500;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits. (arXiv:2306.06291v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26469;&#33258;&#20110;&#20960;&#20010;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#26469;&#28304;&#12290;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20849;&#24615;&#25552;&#39640;&#25928;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#20986;&#29616;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#30456;&#20851;&#21442;&#25968;&#31561;&#20110;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#19968;&#20010;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOLAR&#30340;&#26032;&#22411;&#20108;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#23427;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#23454;&#20363;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#30340;&#36880;&#39033;&#20013;&#20301;&#25968;&#65292;&#28982;&#21518;&#23558;&#23454;&#20363;&#29305;&#23450;&#20272;&#35745;&#20540;&#25910;&#32553;&#21040;&#20013;&#20301;&#25968;&#38468;&#36817;&#26469;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#12290;&#19982;&#29420;&#31435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30456;&#27604;&#65292;MOLAR&#25552;&#39640;&#20102;&#20272;&#35745;&#35823;&#24046;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MOLAR&#24212;&#29992;&#20110;&#24320;&#21457;&#29992;&#20110;&#31232;&#30095;&#24322;&#26500;&#21327;&#21516;&#19978;&#19979;&#25991;&#33218;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#29420;&#31435;&#33218;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#20248;&#20110;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and complex datasets are often collected from several, possibly heterogeneous sources. Collaborative learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here we study collaborative linear regression and contextual bandits, where each instance's associated parameters are equal to a global parameter plus a sparse instance-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing an entry-wise median of the instances' linear regression estimates, and then shrinking the instance-specific estimates towards the median. MOLAR improves the dependence of the estimation error on the data dimension, compared to independent least squares estimates. We then apply MOLAR to develop methods for sparsely heterogeneous collaborative contextual bandits, which lead to improved regret guarantees compared to independent bandit methods. We further show that our 
&lt;/p&gt;</description></item></channel></rss>