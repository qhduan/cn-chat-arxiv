<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.08750</link><description>&lt;p&gt;
&#31070;&#32463;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural reproducing kernel Banach spaces and representer theorems for deep networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30001;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#30340;&#20989;&#25968;&#31354;&#38388;&#26377;&#21161;&#20110;&#29702;&#35299;&#30456;&#24212;&#30340;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#36825;&#20123;&#31354;&#38388;&#37197;&#22791;&#26377;&#24378;&#21046;&#31232;&#30095;&#24615;&#30340;&#33539;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#65292;&#32467;&#21512;&#21464;&#20998;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#35777;&#26126;&#22312;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26377;&#38480;&#26550;&#26500;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;&#27973;&#23618;&#32593;&#32476;&#30340;&#31867;&#20284;&#32467;&#26524;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08750v1 Announce Type: cross  Abstract: Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plaus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#30340;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#65292;&#24182;&#22312;&#22810;&#20010;&#32422;&#26463;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.08016</link><description>&lt;p&gt;
&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Stage-wise Constraints. (arXiv:2401.08016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#30340;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#65292;&#24182;&#22312;&#22810;&#20010;&#32422;&#26463;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32422;&#26463;&#38382;&#39064;&#24517;&#39035;&#28385;&#36275;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#22312;&#38454;&#27573;&#32422;&#26463;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#26174;&#28982;&#65292;&#26399;&#26395;&#32422;&#26463;&#30340;&#35774;&#23450;&#26159;&#23545;&#39640;&#27010;&#29575;&#32422;&#26463;&#30340;&#25918;&#23485;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#32447;&#24615;&#24773;&#20917;&#24320;&#22987;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65288;&#22870;&#21169;&#20989;&#25968;&#65289;&#21644;&#38454;&#27573;&#32422;&#26463;&#65288;&#25104;&#26412;&#20989;&#25968;&#65289;&#37117;&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#38382;&#39064;&#30340;T&#36718;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24605;&#24819;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#32553;&#25918;&#22870;&#21169;&#21644;&#25104;&#26412;&#32622;&#20449;&#21306;&#38388;&#30340;&#21322;&#24452;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35813;&#32422;&#26463;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#22810;&#20010;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#25311;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. Obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. We start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. Our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high proba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21363;&#21487;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#20272;&#35745;&#31232;&#30095;&#22343;&#20540;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15276</link><description>&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#19979;&#30340;&#31232;&#30095;&#22343;&#20540;&#40065;&#26834;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Sparse Mean Estimation via Incremental Learning. (arXiv:2305.15276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21363;&#21487;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#20272;&#35745;&#31232;&#30095;&#22343;&#20540;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#22343;&#20540;&#30340;&#40065;&#26834;&#24615;&#20272;&#35745;&#38382;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#20174;&#37325;&#23614;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#37096;&#20998;&#25439;&#22351;&#26679;&#26412;&#30340;$k$-&#31232;&#30095;&#22343;&#20540;&#12290;&#29616;&#26377;&#20272;&#35745;&#22120;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#21463;&#21040;&#19968;&#20010;&#34987;&#25512;&#27979;&#30340;&#35745;&#31639;&#32479;&#35745;&#26435;&#34913;&#30340;&#38480;&#21046;&#65292;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#38656;&#35201;$\tilde\Omega(k^2)$&#20010;&#26679;&#26412;&#65292;&#32780;&#20854;&#22312;&#32479;&#35745;&#19978;&#26368;&#20248;&#30340;&#23545;&#24212;&#29289;&#21482;&#38656;&#35201;$\tilde O(k)$&#20010;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#35268;&#27169;&#38543;&#30528;&#29615;&#22659;&#30340;&#32500;&#24230;&#22686;&#21152;&#32780;&#24613;&#21095;&#19978;&#21319;&#65292;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#22312;&#36866;&#24230;&#30340;&#26465;&#20214;&#19979;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;&#23427;&#22312;&#20960;&#20046;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#20013;&#36816;&#34892;&#65288;&#30456;&#23545;&#20110;&#29615;&#22659;&#32500;&#24230;&#65289;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;$\tilde O(k)$&#20010;&#26679;&#26412;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22343;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22686;&#37327;&#23398;&#20064;&#29616;&#35937;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#20984;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23558;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of robust sparse mean estimation, where the goal is to estimate a $k$-sparse mean from a collection of partially corrupted samples drawn from a heavy-tailed distribution. Existing estimators face two critical challenges in this setting. First, they are limited by a conjectured computational-statistical tradeoff, implying that any computationally efficient algorithm needs $\tilde\Omega(k^2)$ samples, while its statistically-optimal counterpart only requires $\tilde O(k)$ samples. Second, the existing estimators fall short of practical use as they scale poorly with the ambient dimension. This paper presents a simple mean estimator that overcomes both challenges under moderate conditions: it runs in near-linear time and memory (both with respect to the ambient dimension) while requiring only $\tilde O(k)$ samples to recover the true mean. At the core of our method lies an incremental learning phenomenon: we introduce a simple nonconvex framework that ca
&lt;/p&gt;</description></item></channel></rss>