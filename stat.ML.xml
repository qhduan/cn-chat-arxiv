<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#31283;&#23450;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#29305;&#24449;&#36873;&#25321;&#30340;&#28789;&#25935;&#24230;&#24182;&#26356;&#22909;&#22320;&#26657;&#20934;&#30446;&#26631;&#20551;&#38451;&#24615;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.15877</link><description>&lt;p&gt;
&#38598;&#25104;&#36335;&#24452;&#31283;&#23450;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Integrated path stability selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#31283;&#23450;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#25552;&#39640;&#29305;&#24449;&#36873;&#25321;&#30340;&#28789;&#25935;&#24230;&#24182;&#26356;&#22909;&#22320;&#26657;&#20934;&#30446;&#26631;&#20551;&#38451;&#24615;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#36873;&#25321;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25913;&#21892;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#21457;&#29616;&#31283;&#23450;&#36873;&#25321;&#36807;&#20110;&#20445;&#23432;&#65292;&#23548;&#33268;&#28789;&#25935;&#24230;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#23545;&#26399;&#26395;&#30340;&#20551;&#38451;&#24615;&#25968;&#37327;&#30340;&#29702;&#35770;&#30028;&#38480;E(FP)&#30456;&#23545;&#36739;&#26494;&#65292;&#38590;&#20197;&#30693;&#36947;&#23454;&#36341;&#20013;&#20250;&#26377;&#22810;&#23569;&#20551;&#38451;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#31283;&#23450;&#36335;&#24452;&#32780;&#38750;&#26368;&#22823;&#21270;&#31283;&#23450;&#36335;&#24452;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20135;&#29983;&#20102;&#23545;E(FP)&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#65292;&#23548;&#33268;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#28789;&#25935;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#19982;&#30446;&#26631;E(FP)&#21305;&#37197;&#26041;&#38754;&#26356;&#22909;&#22320;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#21407;&#22987;&#31283;&#23450;&#36873;&#25321;&#31639;&#27861;&#38656;&#35201;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#65292;&#19988;&#20165;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#19968;&#20010;&#36755;&#20837;&#21442;&#25968;&#65292;&#21363;E(FP)&#30340;&#30446;&#26631;&#20540;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24615;&#33021;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15877v1 Announce Type: cross  Abstract: Stability selection is a widely used method for improving the performance of feature selection algorithms. However, stability selection has been found to be highly conservative, resulting in low sensitivity. Further, the theoretical bound on the expected number of false positives, E(FP), is relatively loose, making it difficult to know how many false positives to expect in practice. In this paper, we introduce a novel method for stability selection based on integrating the stability paths rather than maximizing over them. This yields a tighter bound on E(FP), resulting in a feature selection criterion that has higher sensitivity in practice and is better calibrated in terms of matching the target E(FP). Our proposed method requires the same amount of computation as the original stability selection algorithm, and only requires the user to specify one input parameter, a target value for E(FP). We provide theoretical bounds on performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.14890</link><description>&lt;p&gt;
Boosting&#29992;&#20110;&#30028;&#23450;&#26368;&#24046;&#20998;&#31867;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#20934;&#35823;&#24046;&#29575;&#30340;&#24179;&#22343;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#19977;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#21508;&#31867;&#21035;&#30340;&#35823;&#24046;&#29575;&#20998;&#21035;&#20026;10&#65285;&#65292;10&#65285;&#21644;40&#65285;&#65292;&#20854;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#20026;40&#65285;&#65292;&#32780;&#22312;&#31867;&#21035;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#24179;&#22343;&#35823;&#24046;&#29575;&#20026;20&#65285;&#12290;&#26368;&#24046;&#31867;&#21035;&#38169;&#35823;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#24694;&#24615;&#32959;&#30244;&#31867;&#21035;&#20855;&#26377;40&#65285;&#30340;&#38169;&#35823;&#29575;&#32780;&#33391;&#24615;&#21644;&#20581;&#24247;&#31867;&#21035;&#20855;&#26377;10&#65285;&#30340;&#38169;&#35823;&#29575;&#26159;&#19981;&#33021;&#34987;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#19978;&#30028;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#27867;&#21270;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#35757;&#32451;&#38598;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\%, 10\%, and 40\% has a worst-class error rate of 40\%, whereas the average is 20\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\% error rate, while the benign and healthy classes have 10\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#22823;&#26679;&#26412;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08209</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#30340;&#19968;&#33268;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conformal inference for regression on Riemannian Manifolds. (arXiv:2310.08209v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#22823;&#26679;&#26412;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#21450;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#23545;&#27969;&#24418;&#19978;&#30340;&#32479;&#35745;&#23398;&#26377;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#26377;&#22823;&#37327;&#30340;&#24212;&#29992;&#12290;&#22278;&#24418;&#25968;&#25454;&#26159;&#19968;&#20010;&#32463;&#20856;&#31034;&#20363;&#65292;&#20294;&#21327;&#26041;&#24046;&#30697;&#38453;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#24471;&#21040;&#30340;Grassmann&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#31561;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21709;&#24212;&#21464;&#37327;$Y$&#20301;&#20110;&#27969;&#24418;&#19978;&#65292;&#32780;&#21327;&#21464;&#37327;$X$&#20301;&#20110;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#26102;&#65292;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#12290;&#36825;&#25193;&#23637;&#20102;[Lei and Wasserman, 2014]&#20013;&#22312;&#36825;&#19968;&#26032;&#39046;&#22495;&#20013;&#27010;&#36848;&#30340;&#27010;&#24565;&#12290;&#19982;&#19968;&#33268;&#25512;&#26029;&#20013;&#30340;&#20256;&#32479;&#21407;&#21017;&#19968;&#33268;&#65292;&#36825;&#20123;&#39044;&#27979;&#38598;&#26159;&#26080;&#20998;&#24067;&#30340;&#65292;&#34920;&#26126;&#23545;$(X, Y)$&#30340;&#32852;&#21512;&#20998;&#24067;&#27809;&#26377;&#26045;&#21152;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#23427;&#20204;&#20445;&#25345;&#38750;&#21442;&#25968;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression on manifolds, and, more broadly, statistics on manifolds, has garnered significant importance in recent years due to the vast number of applications for this type of data. Circular data is a classic example, but so is data in the space of covariance matrices, data on the Grassmannian manifold obtained as a result of principal component analysis, among many others. In this work we investigate prediction sets for regression scenarios when the response variable, denoted by $Y$, resides in a manifold, and the covariable, denoted by X, lies in Euclidean space. This extends the concepts delineated in [Lei and Wasserman, 2014] to this novel context. Aligning with traditional principles in conformal inference, these prediction sets are distribution-free, indicating that no specific assumptions are imposed on the joint distribution of $(X, Y)$, and they maintain a non-parametric character. We prove the asymptotic almost sure convergence of the empirical version of these regions on th
&lt;/p&gt;</description></item></channel></rss>