<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01855</link><description>&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01855
&lt;/p&gt;
&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#36890;&#24120;&#36890;&#36807;&#26368;&#20248;&#25554;&#20540;(Optimal Interpolation&#65292;OI)&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27169;&#22411;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;(Spatio-temporal Partial Differential Equations&#65292;SPDE)&#21644;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(Gaussian Markov Random Fields&#65292;GMRF)&#20043;&#38388;&#30340;&#32852;&#31995;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#22788;&#29702;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#22823;&#25968;&#25454;&#38598;&#21644;&#29289;&#29702;&#35825;&#23548;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#26368;&#26032;&#36827;&#23637;&#20063;&#20351;&#24471;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23884;&#20837;&#25968;&#25454;&#21516;&#21270;&#21464;&#20998;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#12290;&#37325;&#24314;&#20219;&#21153;&#34987;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#22312;&#21464;&#20998;&#20869;&#37096;&#25104;&#26412;&#20013;&#30340;&#20808;&#39564;&#23398;&#20064;&#38382;&#39064;&#21644;&#21518;&#32773;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#65306;&#20808;&#39564;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#37117;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#33258;&#21160;&#24494;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20123;&#30495;&#23454;&#20540;&#21644;&#37325;&#24314;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2307.09295</link><description>&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#65306;&#19968;&#31181;&#20174;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#31616;&#21333;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09295
&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20844;&#21496;&#20381;&#27425;&#21521;&#19968;&#32676;&#39038;&#23458;&#23637;&#31034;&#26174;&#31034;&#38598;&#65292;&#24182;&#25910;&#38598;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;&#30446;&#26631;&#26159;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#21363;&#23884;&#22871;&#28040;&#38500;(Nested Elimination&#65292;NE)&#65292;&#23427;&#21463;&#21040;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#25152;&#26263;&#31034;&#30340;&#23884;&#22871;&#32467;&#26500;&#30340;&#21551;&#21457;&#12290;NE&#30340;&#32467;&#26500;&#31616;&#21333;&#65292;&#26131;&#20110;&#23454;&#26045;&#65292;&#20855;&#26377;&#23545;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24378;&#22823;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NE&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#65292;&#24182;&#36991;&#20813;&#20102;&#35299;&#20915;&#20219;&#20309;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NE&#30340;&#29305;&#23450;&#23454;&#20363;&#21644;&#38750;&#28176;&#36817;&#24615;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NE&#23454;&#29616;&#20102;&#39640;&#38454;&#26368;&#22351;&#24773;&#20917;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination (NE), which is inspired by the nested structure implied by the information-theoretic lower bound. NE is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of NE. We also show NE achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theore
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04775</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#20197;&#20559;&#24046;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#31867;&#20284;&#20110;Ma&#21644;Chen&#25152;&#24341;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20559;&#24046;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65306;&#65288;i&#65289;&#23558;&#35266;&#27979;&#27169;&#24335;&#35299;&#37322;&#20026;&#23436;&#20840;&#35266;&#27979;&#30340;&#22122;&#22768;&#30697;&#38453;&#65292;&#25105;&#20204;&#23545;&#35266;&#27979;&#27169;&#24335;&#24212;&#29992;&#20256;&#32479;&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#26469;&#20272;&#35745;&#28508;&#22312;&#22240;&#32032;&#20043;&#38388;&#30340;&#36317;&#31163;&#65307; (ii)&#25105;&#20204;&#23545;&#24674;&#22797;&#30340;&#29305;&#24449;&#24212;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#22635;&#34917;&#32570;&#22833;&#35266;&#23519;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#65292;&#36825;&#20123;&#35823;&#24046;&#29575;&#19982;&#30456;&#24212;&#30340;&#30417;&#30563;&#23398;&#20064;&#21442;&#25968;&#29575;&#30456;&#31454;&#20105;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#24615;&#33021;&#19982;&#20351;&#29992;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30456;&#24403;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21453;&#26144;&#20102;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14528</link><description>&lt;p&gt;
&#22522;&#20989;&#25968;&#32534;&#30721;&#25913;&#21892;&#22240;&#23376;&#26426;&#20013;&#25968;&#23383;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy. (arXiv:2305.14528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#26426;(FM)&#21464;&#20307;&#34987;&#24191;&#27867;&#29992;&#20110;&#22823;&#35268;&#27169;&#23454;&#26102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25512;&#29702;&#30340;&#20302;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#29305;&#24449;&#32534;&#30721;&#20026;&#25152;&#36873;&#20989;&#25968;&#38598;&#30340;&#20989;&#25968;&#20540;&#21521;&#37327;&#23558;&#25968;&#20540;&#29305;&#24449;&#32435;&#20837;FM&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factorization machine (FM) variants are widely used for large scale real-time content recommendation systems, since they offer an excellent balance between model accuracy and low computational costs for training and inference. These systems are trained on tabular data with both numerical and categorical columns. Incorporating numerical columns poses a challenge, and they are typically incorporated using a scalar transformation or binning, which can be either learned or chosen a-priori. In this work, we provide a systematic and theoretically-justified way to incorporate numerical features into FM variants by encoding them into a vector of function values for a set of functions of one's choice.  We view factorization machines as approximators of segmentized functions, namely, functions from a field's value to the real numbers, assuming the remaining fields are assigned some given constants, which we refer to as the segment. From this perspective, we show that our technique yields a model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11586</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#34701;&#20837;&#19981;&#30830;&#23450;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20165;&#20551;&#35774;&#27169;&#22411;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20986;&#20855;&#26377;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24314;&#27169;&#20551;&#35774;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#22240;&#32032;&#65292;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20837;&#20301;&#32622;&#21487;&#33021;&#20063;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#24615;&#34701;&#20837;&#21040;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#12290;&#32771;&#34385;&#20004;&#31181;&#21487;&#35266;&#27979;&#37327;&#8212;&#8212;&#20855;&#26377;&#22266;&#23450;&#36755;&#20837;&#30340;&#22122;&#22768;&#27745;&#26579;&#36755;&#20986;&#21644;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#20197;&#25512;&#26029;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#26041;&#27861;&#23558;&#36825;&#20123;&#36755;&#20837;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#20013;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#22238;&#24402;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#35266;&#23519;&#21040;&#19981;&#21516;&#27700;&#24179;&#36755;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#26222;&#36866;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
&lt;/p&gt;</description></item></channel></rss>