<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;</title><link>https://arxiv.org/abs/2403.02233</link><description>&lt;p&gt;
Transformers&#22312;Masked Image Modeling&#20013;&#33021;&#22815;&#35777;&#26126;&#23398;&#20064;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Masked image modeling (MIM)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20174;&#26410;&#23631;&#34109;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#38543;&#26426;&#23631;&#34109;&#30340;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;transformers&#30340;MIM&#30340;&#29702;&#35770;&#29702;&#35299;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#19968;&#23618;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;&#30340;&#29702;&#35770;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14645</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#26684;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Sparse Linear Regression and Lattice Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#65288;SLR&#65289;&#26159;&#32479;&#35745;&#23398;&#20013;&#19968;&#20010;&#30740;&#31350;&#33391;&#22909;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#35774;&#35745;&#30697;&#38453; $X\in\mathbb{R}^{m\times n}$ &#21644;&#21709;&#24212;&#21521;&#37327; $y=X\theta^*+w$&#65292;&#20854;&#20013; $\theta^*$ &#26159; $k$-&#31232;&#30095;&#21521;&#37327;&#65288;&#21363;&#65292;$\|\theta^*\|_0\leq k$&#65289;&#65292;$w$ &#26159;&#23567;&#30340;&#12289;&#20219;&#24847;&#30340;&#22122;&#22768;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010; $k$-&#31232;&#30095;&#30340; $\widehat{\theta} \in \mathbb{R}^n$&#65292;&#20351;&#24471;&#22343;&#26041;&#39044;&#27979;&#35823;&#24046; $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$ &#26368;&#23567;&#21270;&#12290;&#34429;&#28982; $\ell_1$-&#26494;&#24347;&#26041;&#27861;&#22914;&#22522; Pursuit&#12289;Lasso &#21644; Dantzig &#36873;&#25321;&#22120;&#22312;&#35774;&#35745;&#30697;&#38453;&#26465;&#20214;&#33391;&#22909;&#26102;&#35299;&#20915;&#20102; SLR&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#36890;&#29992;&#31639;&#27861;&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#35774;&#32622;&#20013;&#30340;&#22256;&#38590;&#24615;&#30340;&#27491;&#24335;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo
&lt;/p&gt;</description></item></channel></rss>