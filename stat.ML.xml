<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.06578</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Coupling-based Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#65288;&#22914;RealNVP&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#32806;&#21512;&#27969;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#24456;&#26222;&#36941;&#65292;&#20294;&#30001;&#20110;&#20854;&#21463;&#38480;&#30340;&#26550;&#26500;&#65292;&#23545;&#20110;&#32806;&#21512;&#27969;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#23450;&#29702;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20351;&#29992;&#20219;&#24847;&#30149;&#24577;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26500;&#26412;&#36136;&#19978;&#23548;&#33268;&#20307;&#31215;&#20445;&#25345;&#27969;&#65292;&#36825;&#26159;&#19968;&#20010;&#38480;&#21046;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#26222;&#36866;&#24615;&#23450;&#29702;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#20960;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26222;&#36941;&#32463;&#39564;&#65292;&#24182;&#20026;&#36873;&#25321;&#32806;&#21512;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#35266;&#28857;&#65292;&#22635;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.
&lt;/p&gt;</description></item><item><title>SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01855</link><description>&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01855
&lt;/p&gt;
&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#36890;&#24120;&#36890;&#36807;&#26368;&#20248;&#25554;&#20540;(Optimal Interpolation&#65292;OI)&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27169;&#22411;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;(Spatio-temporal Partial Differential Equations&#65292;SPDE)&#21644;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(Gaussian Markov Random Fields&#65292;GMRF)&#20043;&#38388;&#30340;&#32852;&#31995;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#22788;&#29702;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#22823;&#25968;&#25454;&#38598;&#21644;&#29289;&#29702;&#35825;&#23548;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#26368;&#26032;&#36827;&#23637;&#20063;&#20351;&#24471;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23884;&#20837;&#25968;&#25454;&#21516;&#21270;&#21464;&#20998;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#12290;&#37325;&#24314;&#20219;&#21153;&#34987;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#22312;&#21464;&#20998;&#20869;&#37096;&#25104;&#26412;&#20013;&#30340;&#20808;&#39564;&#23398;&#20064;&#38382;&#39064;&#21644;&#21518;&#32773;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#65306;&#20808;&#39564;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#37117;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#33258;&#21160;&#24494;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20123;&#30495;&#23454;&#20540;&#21644;&#37325;&#24314;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#39640;&#32500;&#31232;&#30095;&#35774;&#32622;&#20013;&#21033;&#29992;&#26032;&#30340;&#20248;&#21270;&#31243;&#24207;&#23454;&#29616;&#40065;&#26834;&#31232;&#30095;&#20851;&#32852;&#20272;&#35745;&#65292;&#36890;&#36807;&#22686;&#24191;Lagrange&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#30340;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.17563</link><description>&lt;p&gt;
&#39640;&#25928;&#35745;&#31639;&#31232;&#30095;&#21644;&#40065;&#26834;&#26368;&#22823;&#20851;&#32852;&#20272;&#35745;&#37327;
&lt;/p&gt;
&lt;p&gt;
Efficient Computation of Sparse and Robust Maximum Association Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#39640;&#32500;&#31232;&#30095;&#35774;&#32622;&#20013;&#21033;&#29992;&#26032;&#30340;&#20248;&#21270;&#31243;&#24207;&#23454;&#29616;&#40065;&#26834;&#31232;&#30095;&#20851;&#32852;&#20272;&#35745;&#65292;&#36890;&#36807;&#22686;&#24191;Lagrange&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#30340;&#32452;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#40065;&#26834;&#32479;&#35745;&#20272;&#35745;&#37327;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#36890;&#24120;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#31232;&#30095;&#35774;&#32622;&#20013;&#12290;&#26032;&#30340;&#20248;&#21270;&#31243;&#24207;&#65292;&#20027;&#35201;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24320;&#21457;&#65292;&#20026;&#40065;&#26834;&#32479;&#35745;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#31243;&#24207;&#26469;&#23454;&#29616;&#40065;&#26834;&#31232;&#30095;&#20851;&#32852;&#20272;&#35745;&#12290;&#35813;&#38382;&#39064;&#34987;&#25286;&#20998;&#20026;&#19968;&#20010;&#40065;&#26834;&#20272;&#35745;&#27493;&#39588;&#65292;&#25509;&#30528;&#26159;&#19968;&#20010;&#20313;&#39033;&#35299;&#32806;&#30340;&#65288;&#21452;&#36793;&#65289;&#20984;&#38382;&#39064;&#30340;&#20248;&#21270;&#12290;&#37319;&#29992;&#22686;&#24191;Lagrange&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#19979;&#38477;&#30340;&#32452;&#21512;&#65292;&#36824;&#21253;&#25324;&#36866;&#24403;&#30340;&#32422;&#26463;&#26465;&#20214;&#20197;&#35825;&#23548;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#31639;&#27861;&#31934;&#24230;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#30456;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;&#39640;&#32500;&#23454;&#35777;&#31034;&#20363;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17563v2 Announce Type: replace-cross  Abstract: Although robust statistical estimators are less affected by outlying observations, their computation is usually more challenging. This is particularly the case in high-dimensional sparse settings. The availability of new optimization procedures, mainly developed in the computer science domain, offers new possibilities for the field of robust statistics. This paper investigates how such procedures can be used for robust sparse association estimators. The problem can be split into a robust estimation step followed by an optimization for the remaining decoupled, (bi-)convex problem. A combination of the augmented Lagrangian algorithm and adaptive gradient descent is implemented to also include suitable constraints for inducing sparsity. We provide results concerning the precision of the algorithm and show the advantages over existing algorithms in this context. High-dimensional empirical examples underline the usefulness of this p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15800</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#21644;LIME&#36827;&#34892;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#29305;&#24449;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20998;&#36755;&#20837;&#21464;&#37327;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;SHAP&#21644;LIME&#65292;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#20511;&#37492;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#25490;&#21517;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;RankSHAP&#20445;&#35777;$K$&#20010;&#26368;&#39640;Shapley&#20540;&#20855;&#26377;&#36229;&#36807;$1-\alpha$&#30340;&#27491;&#30830;&#25490;&#24207;&#27010;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#20026;LIME&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#30830;&#20445;&#20197;&#27491;&#30830;&#39034;&#24207;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#20123;&#19981;&#31561;&#24335;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#25104;&#31435;&#65292;&#22312;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15567</link><description>&lt;p&gt;
&#30697;&#38453;&#36229;&#38789;&#21644;&#38543;&#26426;&#30697;&#38453;&#38598;&#20013;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Matrix Supermartingales and Randomized Matrix Concentration Inequalities. (arXiv:2401.15567v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#20123;&#19981;&#31561;&#24335;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#25104;&#31435;&#65292;&#22312;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22810;&#31181;&#23614;&#26465;&#20214;&#19979;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#38789;&#30456;&#20851;&#25110;&#21487;&#20132;&#25442;&#38543;&#26426;&#23545;&#31216;&#30697;&#38453;&#30340;&#26032;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#20999;&#23572;&#35834;&#22827;&#19978;&#30028;&#21644;&#33258;&#24402;&#19968;&#21270;&#37325;&#23614;&#35774;&#32622;&#12290;&#36825;&#20123;&#19981;&#31561;&#24335;&#36890;&#24120;&#20197;&#27931;&#20234;&#32435;&#39034;&#24207;&#34920;&#31034;&#65292;&#24182;&#19988;&#26377;&#26102;&#22312;&#20219;&#24847;&#25968;&#25454;&#30456;&#20851;&#20572;&#27490;&#26102;&#38388;&#37117;&#25104;&#31435;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30697;&#38453;&#36229;&#38789;&#21644;&#26497;&#20540;&#19981;&#31561;&#24335;&#30340;&#29702;&#35770;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new concentration inequalities for either martingale dependent or exchangeable random symmetric matrices under a variety of tail conditions, encompassing standard Chernoff bounds to self-normalized heavy-tailed settings. These inequalities are often randomized in a way that renders them strictly tighter than existing deterministic results in the literature, are typically expressed in the Loewner order, and are sometimes valid at arbitrary data-dependent stopping times.  Along the way, we explore the theory of matrix supermartingales and maximal inequalities, potentially of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16877</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#22635;&#20805;&#21644;&#36890;&#36807;&#26631;&#31614;&#22635;&#20805;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22635;&#20805;&#26041;&#27861;&#26469;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#37117;&#26377;&#26631;&#31614;&#65292;&#20294;&#24120;&#35265;&#30340;&#22635;&#20805;&#26041;&#27861;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#36755;&#20837;&#32780;&#24573;&#30053;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23558;&#26631;&#31614;&#22534;&#21472;&#21040;&#36755;&#20837;&#20013;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#36755;&#20837;&#30340;&#22635;&#20805;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#39044;&#27979;&#30340;&#27979;&#35797;&#26631;&#31614;&#21021;&#22987;&#21270;&#20026;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#22312;&#19968;&#36215;&#36827;&#34892;&#22635;&#20805;&#12290;&#36825;&#26679;&#21487;&#20197;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#32780;&#19988;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#22635;&#20805;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36830;&#32493;&#22411;&#12289;&#20998;&#31867;&#22411;&#25110;&#28151;&#21512;&#22411;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
&lt;/p&gt;</description></item></channel></rss>