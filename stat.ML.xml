<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04417</link><description>&lt;p&gt;
&#25193;&#25955;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#20174;&#22122;&#22768;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#35299;&#37322;&#65292;&#32570;&#20047;&#29702;&#35770;&#20381;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25193;&#25955;&#27169;&#22411;&#21551;&#21457;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#33021;&#32473;&#20986;&#19982;&#20855;&#26377;&#30456;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#23548;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#23578;MNIST&#25968;&#25454;&#38598;&#21644;&#20048;&#22120;&#38899;&#39057;&#25968;&#25454;&#19978;&#29983;&#25104;&#26679;&#26412;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.09222</link><description>&lt;p&gt;
&#21452;&#37325;&#26631;&#20934;&#21270;&#27969;&#65306;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39640;&#26031;&#36807;&#31243;&#34987;&#29992;&#26469;&#24314;&#27169;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20219;&#21153;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#22312;&#20855;&#26377;&#38750;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32422;&#26463;&#20808;&#39564;&#21644;&#22343;&#20540;&#22330;&#21518;&#39564;&#21487;&#33021;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#27969;&#26469;&#37325;&#26032;&#21442;&#25968;&#21270;ODE&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#28789;&#27963;&#12289;&#26356;&#34920;&#36798;&#24615;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#35299;&#26512;&#21487;&#35745;&#31639;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;GP ODE&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#29983;&#25104;&#19968;&#20010;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#12290;&#36890;&#36807;&#36825;&#20123;&#26631;&#20934;&#21270;&#27969;&#30340;&#21452;&#37325;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14172</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#22522;&#20110;&#20809;&#28369;&#24615;&#20808;&#39564;&#25512;&#26029;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#27809;&#26377;&#26126;&#30830;&#36229;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;&#33021;&#22815;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#31616;&#21333;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#65292;&#19981;&#33021;&#31934;&#30830;&#25429;&#25417;&#28508;&#22312;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#35201;&#20040;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#21363;&#39044;&#20808;&#23384;&#22312;&#30340;&#36229;&#22270;&#32467;&#26500;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23616;&#38480;&#20110;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#28369;&#24615;&#20808;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#34920;&#31034;&#36229;&#36793;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#19982;&#21253;&#21547;&#35813;&#36229;&#36793;&#30340;&#36229;&#36793;&#30340;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
&lt;/p&gt;</description></item></channel></rss>