<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;</title><link>https://arxiv.org/abs/2403.07728</link><description>&lt;p&gt;
CAS: &#19968;&#31181;&#20855;&#26377;FCR&#25511;&#21046;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#31526;&#21512;&#39044;&#27979;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07728
&lt;/p&gt;
&lt;p&gt;
CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#26041;&#24335;&#19979;&#21518;&#36873;&#25321;&#39044;&#27979;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#23558;&#36164;&#28304;&#32791;&#36153;&#22312;&#19981;&#37325;&#35201;&#30340;&#21333;&#20301;&#19978;&#65292;&#22312;&#25253;&#21578;&#20854;&#39044;&#27979;&#21306;&#38388;&#20043;&#21069;&#23545;&#24403;&#21069;&#20010;&#20307;&#36827;&#34892;&#21021;&#27493;&#36873;&#25321;&#22312;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#30001;&#20110;&#22312;&#32447;&#36873;&#25321;&#23548;&#33268;&#25152;&#36873;&#39044;&#27979;&#21306;&#38388;&#20013;&#23384;&#22312;&#26102;&#38388;&#22810;&#37325;&#24615;&#65292;&#22240;&#27492;&#25511;&#21046;&#23454;&#26102;&#35823;&#35206;&#30422;&#38472;&#36848;&#29575;&#65288;FCR&#65289;&#26469;&#27979;&#37327;&#24179;&#22343;&#35823;&#35206;&#30422;&#35823;&#24046;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CAS&#65288;&#36866;&#24212;&#24615;&#36873;&#25321;&#21518;&#26657;&#20934;&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21253;&#35065;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#21644;&#22312;&#32447;&#36873;&#25321;&#35268;&#21017;&#65292;&#20197;&#36755;&#20986;&#21518;&#36873;&#25321;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#22914;&#26524;&#36873;&#25321;&#20102;&#24403;&#21069;&#20010;&#20307;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#26469;&#26500;&#24314;&#26657;&#20934;&#38598;&#65292;&#28982;&#21518;&#20026;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20026;&#26657;&#20934;&#38598;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26500;&#36896;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02524</link><description>&lt;p&gt;
&#22312;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20855;&#26377;&#20869;&#22312;&#21487;&#35266;&#27979;&#24615;&#30340;Koopman&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#23450;&#20041;&#30340;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Jet Dynamic Mode Decomposition&#65288;JetDMD&#65289;&#65292;&#21033;&#29992;RKHS&#30340;&#20869;&#22312;&#32467;&#26500;&#21644;&#31216;&#20026;jets&#30340;&#20960;&#20309;&#27010;&#24565;&#26469;&#22686;&#24378;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#19978;&#20248;&#21270;&#20102;&#20256;&#32479;&#30340;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#25968;&#20540;&#20272;&#35745;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#29305;&#27530;&#27491;&#23450;&#20869;&#26680;&#30340;&#25910;&#25947;&#29575;&#35777;&#26126;&#20102;JetDMD&#30340;&#20248;&#36234;&#24615;&#65292;&#20026;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;Koopman&#31639;&#23376;&#30340;&#35889;&#20998;&#26512;&#65292;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#12290;&#36825;&#20010;&#27010;&#24565;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#24182;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14332</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#21040;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#30340;&#23610;&#23544;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#26377;&#25928;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#22522;&#20934;&#32858;&#31867;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26114;&#36149;&#30340;oracle&#26597;&#35810;&#26469;&#35775;&#38382;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20986;&#23558;&#19982;&#22522;&#26412;&#20107;&#23454;&#32467;&#26500;&#19978;&#25509;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#23545;&#22823;&#35268;&#27169;&#32858;&#31867;&#23454;&#20363;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#65288;2&#65289;&#22312;&#36739;&#23567;&#23454;&#20363;&#19978;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#31639;&#27861;&#65292;&#65288;3&#65289;&#20445;&#35777;&#22312;&#23567;&#23454;&#20363;&#19978;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20026;&#19977;&#31181;&#32463;&#20856;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#23610;&#23544;&#27867;&#21270;&#20445;&#35777;&#65306;&#21333;&#38142;&#25509;&#12289;k-means++&#21644;Gonzalez&#30340;k&#20013;&#24515;&#21551;&#21457;&#24335;&#65288;&#19968;&#31181;&#24179;&#28369;&#30340;&#21464;&#31181;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#21644;&#26080;&#38656;&#23494;&#24230;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#20102;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07296</link><description>&lt;p&gt;
&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating the Mixing Coefficients of Geometrically Ergodic Markov Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#31995;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28385;&#36275;&#29305;&#23450;&#26465;&#20214;&#21644;&#26080;&#38656;&#23494;&#24230;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#20102;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#25910;&#25947;&#36895;&#24230;&#21644;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#23454;&#20540;&#20960;&#20309;&#36951;&#20256;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#21333;&#20010;&#946;-&#28151;&#21512;&#31995;&#25968;&#20174;&#19968;&#20010;&#21333;&#19968;&#30340;&#26679;&#26412;&#36335;&#24452;X0&#65292;X1&#65292;...&#65292;Xn&#12290;&#22312;&#23545;&#23494;&#24230;&#30340;&#26631;&#20934;&#20809;&#28369;&#26465;&#20214;&#19979;&#65292;&#21363;&#23545;&#20110;&#27599;&#20010;m&#65292;&#23545;$(X_0,X_m)$&#23545;&#30340;&#32852;&#21512;&#23494;&#24230;&#37117;&#23646;&#20110;&#26576;&#20010;&#24050;&#30693;$s&gt;0$&#30340; Besov &#31354;&#38388;$B^s_{1,\infty}(\mathbb R^2)$&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25105;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;$\mathcal{O}(\log(n) n^{-[s]/(2[s]+2)})$ &#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#30028;&#38480;&#36827;&#34892;&#20102;&#34917;&#20805;&#65292;&#24182;&#22312;&#29366;&#24577;&#31354;&#38388;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#36825;&#20123;&#30028;&#38480;&#30340;&#31867;&#27604;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#23494;&#24230;&#30340;&#20551;&#35774;&#65307;&#39044;&#26399;&#35823;&#24046;&#29575;&#26174;&#31034;&#20026;$\mathcal O(\log(
&lt;/p&gt;
&lt;p&gt;
We propose methods to estimate the individual $\beta$-mixing coefficients of a real-valued geometrically ergodic Markov process from a single sample-path $X_0,X_1, \dots,X_n$. Under standard smoothness conditions on the densities, namely, that the joint density of the pair $(X_0,X_m)$ for each $m$ lies in a Besov space $B^s_{1,\infty}(\mathbb R^2)$ for some known $s&gt;0$, we obtain a rate of convergence of order $\mathcal{O}(\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\footnote{We use $[s]$ to denote the integer part of the decomposition $s=[s]+\{s\}$ of $s \in (0,\infty)$ into an integer term and a {\em strictly positive} remainder term $\{s\} \in (0,1]$.}. We complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite. Naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\mathcal O(\log(
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.00953</link><description>&lt;p&gt;
&#25317;&#26377;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#36153;&#29992;&#26063;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Families of costs with zero and nonnegative MTW tensor in optimal transport. (arXiv:2401.00953v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35745;&#31639;&#20102;&#22312;$\mathbb{R}^n$&#19978;&#20855;&#26377;&#24418;&#24335;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36865;&#38382;&#39064;&#30340;MTW&#24352;&#37327;&#65288;&#25110;&#20132;&#21449;&#26354;&#29575;&#65289;&#12290;&#20854;&#20013;&#65292;$\mathsf{u}$&#26159;&#19968;&#20010;&#20855;&#26377;&#36870;&#20989;&#25968;$\mathsf{s}$&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$x^{\ft}y$&#26159;&#23646;&#20110;$\mathbb{R}^n$&#24320;&#23376;&#38598;&#30340;&#21521;&#37327;$x&#65292;y$&#30340;&#38750;&#36864;&#21270;&#21452;&#32447;&#24615;&#37197;&#23545;&#12290;MTW&#24352;&#37327;&#22312;Kim-McCann&#24230;&#37327;&#19979;&#23545;&#20110;&#38646;&#21521;&#37327;&#30340;&#26465;&#20214;&#26159;&#19968;&#20010;&#22235;&#38454;&#38750;&#32447;&#24615;ODE&#65292;&#21487;&#20197;&#34987;&#31616;&#21270;&#20026;&#20855;&#26377;&#24120;&#25968;&#31995;&#25968;$P$&#21644;$S$&#30340;&#24418;&#24335;&#20026;$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$&#30340;&#32447;&#24615;ODE&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#36870;&#20989;&#25968;&#21253;&#25324;Lambert&#21644;&#24191;&#20041;&#21453;&#21452;&#26354;/&#19977;&#35282;&#20989;&#25968;&#12290;&#24179;&#26041;&#27431;&#27663;&#24230;&#37327;&#21644;$\log$&#22411;&#36153;&#29992;&#26159;&#36825;&#20123;&#35299;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#23478;&#26063;&#30340;&#26368;&#20248;&#26144;&#23556;&#20063;&#26159;&#26174;&#24335;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compute explicitly the MTW tensor (or cross curvature) for the optimal transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form $\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. The square Euclidean metric and $\log$-type costs are equivalent to instances of these solutions. The optimal map for the family is also explicit. For cost functions of a similar form on a hyperboloid model of the hyperbolic space and u
&lt;/p&gt;</description></item><item><title>RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2310.07983</link><description>&lt;p&gt;
RandCom&#65306;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#29992;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07983
&lt;/p&gt;
&lt;p&gt;
RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#36890;&#20449;&#36339;&#36807;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#22240;&#20854;&#22312;&#21152;&#36895;&#36890;&#20449;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#30340;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24378;&#20984;&#30830;&#23450;&#24615;&#35774;&#32622;&#30340;&#38598;&#20013;&#24335;&#36890;&#20449;&#21327;&#35758;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RandCom&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#27010;&#29575;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandCom&#22312;&#38543;&#26426;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#36890;&#36807;&#36890;&#20449;&#27010;&#29575;&#26469;&#28176;&#36817;&#22320;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;RandCom&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#22312;&#38543;&#26426;&#24378;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;RandCom&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#27493;&#38271;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;RandCom&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#30340;&#28508;&#21147;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>