<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.14335</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#26680;&#20998;&#24067;&#22238;&#24402;&#23398;&#20064;&#29702;&#35770;&#19982;&#20004;&#38454;&#27573;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved learning theory for kernel distribution regression with two-stage sampling. (arXiv:2308.14335v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#22238;&#24402;&#38382;&#39064;&#28085;&#30422;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#26680;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#39318;&#36873;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26680;&#20998;&#24067;&#22238;&#24402;&#22312;&#35745;&#31639;&#19978;&#26159;&#26377;&#21033;&#30340;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#29702;&#35770;&#30340;&#25903;&#25345;&#12290;&#35813;&#29702;&#35770;&#36824;&#35299;&#20915;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21482;&#26377;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26680;&#20998;&#24067;&#22238;&#24402;&#30340;&#23398;&#20064;&#29702;&#35770;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#24076;&#23572;&#20271;&#29305;&#23884;&#20837;&#30340;&#26680;&#65292;&#36825;&#20123;&#26680;&#21253;&#21547;&#20102;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#20837;&#30340;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#20998;&#26512;&#25552;&#20379;&#20851;&#20110;&#20004;&#38454;&#27573;&#37319;&#26679;&#25928;&#26524;&#30340;&#26032;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#36817;&#26080;&#20559;&#26465;&#20214;&#23545;&#19977;&#20010;&#37325;&#35201;&#30340;&#26680;&#31867;&#21035;&#25104;&#31435;&#65292;&#36825;&#20123;&#26680;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#24179;&#22343;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution regression problem encompasses many important statistics and machine learning tasks, and arises in a large range of applications. Among various existing approaches to tackle this problem, kernel methods have become a method of choice. Indeed, kernel distribution regression is both computationally favorable, and supported by a recent learning theory. This theory also tackles the two-stage sampling setting, where only samples from the input distributions are available. In this paper, we improve the learning theory of kernel distribution regression. We address kernels based on Hilbertian embeddings, that encompass most, if not all, of the existing approaches. We introduce the novel near-unbiased condition on the Hilbertian embeddings, that enables us to provide new error bounds on the effect of the two-stage sampling, thanks to a new analysis. We show that this near-unbiased condition holds for three important classes of kernels, based on optimal transport and mean embedd
&lt;/p&gt;</description></item><item><title>&#22312;&#23384;&#22312;&#32456;&#32467;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#32463;&#24120;&#24615;&#20107;&#20214;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#39640;&#25928;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#40065;&#26834;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#22240;&#26524;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16571</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#32456;&#32467;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20851;&#20110;&#32463;&#24120;&#24615;&#20107;&#20214;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal inference for the expected number of recurrent events in the presence of a terminal event. (arXiv:2306.16571v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16571
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#32456;&#32467;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#32463;&#24120;&#24615;&#20107;&#20214;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#39640;&#25928;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#40065;&#26834;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#22240;&#26524;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#32456;&#32467;&#20107;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#20851;&#20110;&#32463;&#24120;&#24615;&#20107;&#20214;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#39640;&#25928;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#20272;&#35745;&#30446;&#26631;&#23450;&#20041;&#20026;&#21253;&#25324;&#32463;&#24120;&#24615;&#20107;&#20214;&#30340;&#39044;&#26399;&#25968;&#37327;&#20197;&#21450;&#22312;&#19968;&#31995;&#21015;&#37324;&#31243;&#30865;&#26102;&#38388;&#28857;&#22788;&#35780;&#20272;&#30340;&#22833;&#36133;&#29983;&#23384;&#20989;&#25968;&#30340;&#21521;&#37327;&#12290;&#25105;&#20204;&#22312;&#21491;&#25130;&#23614;&#21644;&#22240;&#26524;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#20272;&#35745;&#30446;&#26631;&#65292;&#20316;&#20026;&#35266;&#23519;&#25968;&#25454;&#30340;&#21151;&#33021;&#24615;&#65292;&#25512;&#23548;&#20102;&#38750;&#21442;&#25968;&#25928;&#29575;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#40065;&#26834;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#36798;&#21040;&#20102;&#30028;&#38480;&#65292;&#24182;&#20801;&#35768;&#38750;&#21442;&#25968;&#20272;&#35745;&#36741;&#21161;&#21442;&#25968;&#12290;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23545;&#22833;&#36133;&#12289;&#25130;&#23614;&#25110;&#35266;&#23519;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#27809;&#26377;&#20570;&#32477;&#23545;&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#24403;&#20998;&#21106;&#20998;&#24067;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#31867;&#21035;&#65292;&#24182;&#22238;&#39038;&#20102;&#24050;&#21457;&#34920;&#20272;&#35745;&#22120;&#22914;&#20309;&#23646;&#20110;&#35813;&#31867;&#21035;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22240;&#26524;&#29983;&#21629;&#21608;&#26399;&#20013;&#19968;&#20123;&#26377;&#36259;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study causal inference and efficient estimation for the expected number of recurrent events in the presence of a terminal event. We define our estimand as the vector comprising both the expected number of recurrent events and the failure survival function evaluated along a sequence of landmark times. We identify the estimand in the presence of right-censoring and causal selection as an observed data functional under coarsening at random, derive the nonparametric efficiency bound, and propose a multiply-robust estimator that achieves the bound and permits nonparametric estimation of nuisance parameters. Throughout, no absolute continuity assumption is made on the underlying probability distributions of failure, censoring, or the observed data. Additionally, we derive the class of influence functions when the coarsening distribution is known and review how published estimators may belong to the class. Along the way, we highlight some interesting inconsistencies in the causal lifetime 
&lt;/p&gt;</description></item></channel></rss>