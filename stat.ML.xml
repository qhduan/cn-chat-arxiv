<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14664</link><description>&lt;p&gt;
&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#36125;&#21494;&#26031;&#31163;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#65292;&#21160;&#20316;&#32463;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#20026;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#26356;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#23398;&#20064;&#65288;OPL&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#26469;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sDM&#65292;&#19968;&#20010;&#20026;OPE&#21644;OPL&#35774;&#35745;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26082;&#26377;&#31639;&#27861;&#22522;&#30784;&#21448;&#26377;&#29702;&#35770;&#22522;&#30784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;sDM&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#21463;&#22312;&#32447;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#24615;&#33021;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20559;&#31163;&#20256;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#30340;&#34920;&#29616;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#22909;&#22788;&#12290;&#23454;&#35777;&#35777;&#25454;&#23637;&#31034;&#20102;sDM&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
&lt;/p&gt;</description></item></channel></rss>