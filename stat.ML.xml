<rss version="2.0"><channel><title>Chat Arxiv stat.ML</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for stat.ML</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.12806</link><description>&lt;p&gt;
DCSI -- &#22522;&#20110;&#20998;&#31163;&#21644;&#36830;&#36890;&#24615;&#30340;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#26631;&#31614;&#26159;&#21542;&#23545;&#24212;&#20110;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#23545;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#12290;&#29616;&#26377;&#25991;&#29486;&#30340;&#32508;&#36848;&#26174;&#31034;&#65292;&#26082;&#26377;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#22797;&#26434;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631; (CVIs) &#37117;&#27809;&#26377;&#20805;&#20998;&#34701;&#20837;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#12290;&#19968;&#31181;&#26032;&#24320;&#21457;&#30340;&#24230;&#37327;&#26041;&#27861; (&#23494;&#24230;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;, DCSI) &#26088;&#22312;&#37327;&#21270;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#24182;&#19988;&#20063;&#21487;&#29992;&#20316; CVI&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DCSI &#19982;&#36890;&#36807;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968; (ARI) &#27979;&#37327;&#30340;DBSCAN&#30340;&#24615;&#33021;&#20043;&#38388;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#23545;&#22810;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#24230;&#32858;&#31867;&#19981;&#36866;&#24403;&#30340;&#37325;&#21472;&#31867;&#21035;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23545;&#32463;&#24120;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;&#65292;DCSI &#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23494;&#24230;&#32858;&#31867;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.07891</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#30340;&#22522;&#26412;&#21407;&#22240;&#20043;&#19968;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24050;&#32463;&#20005;&#26684;&#35777;&#26126;&#65292;&#22312;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31532;&#19968;&#23618;&#36827;&#34892;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#23618;&#36827;&#34892;&#23725;&#22238;&#24402;&#21487;&#20197;&#23548;&#33268;&#29305;&#24449;&#23398;&#20064;&#65307;&#29305;&#24449;&#30697;&#38453;&#30340;&#35889;&#20013;&#20250;&#20986;&#29616;&#20998;&#31163;&#30340;&#19968;&#32500;&#32452;&#20214;&#65292;&#31216;&#20026;&#8220;spike&#8221;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22266;&#23450;&#26799;&#24230;&#19979;&#38477;&#27493;&#38271;&#26102;&#65292;&#36825;&#20010;&#8220;spike&#8221;&#20165;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#20214;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#23398;&#20064;&#38750;&#32447;&#24615;&#32452;&#20214;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#38271;&#26102;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#23454;&#38469;&#19978;&#24341;&#20837;&#20102;&#22810;&#20010;&#19968;&#32500;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#22823;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#35757;&#32451;&#21644;&#27979;&#35797;&#35823;&#24046;&#23436;&#20840;&#30001;&#36825;&#20123;&#8220;spike&#8221;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#26377;&#38480;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;&#21450;&#20854;&#26222;&#36866;&#24615;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26377;&#38480;&#32593;&#32476;&#20013;&#20173;&#28982;&#23384;&#22312;&#30528;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20250;&#21453;&#26144;&#22312;&#36807;&#28193;&#30340;&#26222;&#36866;&#31867;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.02284</link><description>&lt;p&gt;
&#20154;&#24037;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Absorbing Phase Transitions in Artificial Deep Neural Networks. (arXiv:2307.02284v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#26377;&#38480;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;&#21450;&#20854;&#26222;&#36866;&#24615;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26377;&#38480;&#32593;&#32476;&#20013;&#20173;&#28982;&#23384;&#22312;&#30528;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20250;&#21453;&#26144;&#22312;&#36807;&#28193;&#30340;&#26222;&#36866;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33879;&#21517;&#30340;&#24179;&#22343;&#22330;&#29702;&#35770;&#65292;&#23545;&#20110;&#21508;&#31181;&#20307;&#31995;&#30340;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#30340;&#29702;&#35770;&#29702;&#35299;&#24050;&#32463;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#23454;&#38469;&#21644;&#29616;&#23454;&#37325;&#35201;&#24615;&#26356;&#24378;&#30340;&#26377;&#38480;&#32593;&#32476;&#65292;&#32570;&#20047;&#28165;&#26224;&#30452;&#35266;&#30340;&#26694;&#26550;&#26469;&#24310;&#20280;&#25105;&#20204;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#21560;&#25910;&#30456;&#21464;&#20013;&#30340;&#26222;&#36941;&#20020;&#30028;&#29616;&#35937;&#26469;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#30456;&#21464;&#65292;&#24182;&#24378;&#35843;&#20102;&#20307;&#31995;&#26550;&#26500;&#30340;&#24046;&#24322;&#19982;&#30456;&#21464;&#30340;&#26222;&#36866;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;&#26377;&#38480;&#23610;&#24230;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#20102;&#30452;&#35266;&#30340;&#29616;&#35937;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenologic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item></channel></rss>