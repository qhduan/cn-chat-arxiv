# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context](https://arxiv.org/abs/2403.02177) | 提出了一个计划-推理框架，用于在表格上的句子背景中回答用户查询，通过对Llama-2-7B进行微调，构建了ProTrix模型，广泛适用于不同表格任务，并表现出与GPT-3.5-turbo相当的性能水平，可生成准确且忠实的解释。 |
| [^2] | [LLsM: Generative Linguistic Steganography with Large Language Model](https://arxiv.org/abs/2401.15656) | 本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。 |
| [^3] | [Self-Rewarding Language Models.](http://arxiv.org/abs/2401.10020) | 该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。 |

# 详细

[^1]: ProTrix: 使用句子背景构建用于规划和推理表格的模型

    ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context

    [https://arxiv.org/abs/2403.02177](https://arxiv.org/abs/2403.02177)

    提出了一个计划-推理框架，用于在表格上的句子背景中回答用户查询，通过对Llama-2-7B进行微调，构建了ProTrix模型，广泛适用于不同表格任务，并表现出与GPT-3.5-turbo相当的性能水平，可生成准确且忠实的解释。

    

    在各个领域中，表格在传达信息方面起着至关重要的作用，是组织和呈现结构化数据的不可或缺工具。我们提出了一个计划-推理框架，用于回答带有句子背景的表格上的不同类型的用户查询。该框架首先规划上下文中的推理路径，然后将每个步骤分配给基于程序或文本的推理，以达到最终答案。我们根据该框架构建了一个指令调整集TrixtInstruct。我们的数据集涵盖了那些需要结合表格和句子信息来获得规划和推理能力的程序无法解决的查询。我们通过对TrixInstruct上的Llama-2-7B进行微调，提出了ProTrix。我们的实验表明，ProTrix对各种表格任务具有普遍性，并且达到了与GPT-3.5-turbo相当的性能。我们进一步证明ProTrix可以生成准确和忠实的解释来回答复杂的问题。

    arXiv:2403.02177v1 Announce Type: new  Abstract: Tables play a crucial role in conveying information in various domains, serving as indispensable tools for organizing and presenting data in a structured manner. We propose a Plan-then-Reason framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. We construct an instruction tuning set TrixInstruct following the framework. Our dataset cover queries that are program-unsolvable or need combining information from tables and sentences to obtain planning and reasoning abilities. We present ProTrix by finetuning Llama-2-7B on TrixInstruct. Our experiments show that ProTrix generalizes to diverse tabular tasks and achieves comparable performance to GPT-3.5-turbo. We further demonstrate that ProTrix can generate accurate and faithful explanations to answer complex f
    
[^2]: LLsM: 基于大型语言模型的生成式语言隐写术

    LLsM: Generative Linguistic Steganography with Large Language Model

    [https://arxiv.org/abs/2401.15656](https://arxiv.org/abs/2401.15656)

    本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。

    

    语言隐写术（LS）旨在根据秘密信息生成隐写文本（stego）。只有授权接收者才能察觉文本中秘密的存在并提取出来，从而保护隐私。然而，现有方案生成的隐写文本可控性较差，很难包含特定的话语特征，如风格。结果，隐写文本容易被检测出来，危及隐蔽通信。为解决这些问题，本文提出了LLsM，第一个基于大型语言模型（LLM）的LS方法。我们使用一个包含丰富话语特征的大规模构建数据集对LLaMA2进行微调，使得微调后的LLM能够以可控的方式生成具有特定话语特征的文本。然后将话语作为引导信息和秘密一起输入给微调后的LLM，形式为“Prompt”。在此基础上，构建的候选池将进行范围编码。

    Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an
    
[^3]: 自奖励语言模型

    Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])

    [http://arxiv.org/abs/2401.10020](http://arxiv.org/abs/2401.10020)

    该论文提出了自奖励语言模型的概念，通过LLM作为评判者，使用语言模型自己提供训练过程中的奖励。研究表明，该方法不仅可以提高指令遵循能力，还可以为自己提供高质量的奖励。通过对Llama 2 70B模型的三次迭代微调，结果在AlpacaEval 2.0排行榜上超过了其他现有系统。这项工作为实现能够不断自我改进的模型开辟了新的可能性。

    

    我们假设要实现超人级的智能体，未来的模型需要超人级的反馈，以提供足够的训练信号。目前的方法通常是从人类偏好中训练奖励模型，这可能会受到人类表现水平的限制，而且这些独立的冻结奖励模型在LLM训练过程中无法学习改进。在这项工作中，我们研究了自奖励语言模型，其中语言模型本身通过LLM作为评判者的提示在训练过程中提供自己的奖励。我们表明，在迭代DPO训练中，不仅指令遵循能力得到了提高，而且能够为自己提供高质量的奖励。通过对Llama 2 70B进行我们方法的三次迭代的微调，得到的模型在AlpacaEval 2.0排行榜上胜过许多现有系统，包括Claude 2、Gemini Pro和GPT-4 0613。虽然这只是一项初步研究，但这项工作为可能实现能够不断自我改进的模型打开了大门。

    We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
    

