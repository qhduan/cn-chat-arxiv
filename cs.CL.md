# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Incoherent Probability Judgments in Large Language Models.](http://arxiv.org/abs/2401.16646) | 在本论文中，研究人员通过对大型语言模型(LLMs)进行实验证明，这些模型产生的概率判断经常是不连贯的，显示出类似于人类一样的非理性偏差。他们还提出了将自回归LLMs与隐性贝叶斯推断联系起来的解释。 |

# 详细

[^1]: 大型语言模型中的不连贯概率判断

    Incoherent Probability Judgments in Large Language Models. (arXiv:2401.16646v1 [cs.CL])

    [http://arxiv.org/abs/2401.16646](http://arxiv.org/abs/2401.16646)

    在本论文中，研究人员通过对大型语言模型(LLMs)进行实验证明，这些模型产生的概率判断经常是不连贯的，显示出类似于人类一样的非理性偏差。他们还提出了将自回归LLMs与隐性贝叶斯推断联系起来的解释。

    

    针对下一个词预测训练的自回归大型语言模型(LLMs)展示出出色的连贯文本生成能力。但它们是否同样擅长形成连贯的概率判断？我们使用概率身份和重复判断来评估LLMs生成的概率判断的连贯性。我们的结果显示，这些模型产生的判断经常是不连贯的，显示出人类一样的概率理论规则偏离。此外，当要求对同一事件进行判断时，LLMs产生的概率判断的均值-方差关系呈现出人类所见到的倒U形状。我们提出这些非理性的偏离可以通过将自回归LLMs与隐性贝叶斯推断联系起来，并与人类概率判断的贝叶斯抽样器模型进行类比来解释。

    Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
    

