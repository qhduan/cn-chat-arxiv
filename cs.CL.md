# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [GPT-4 Understands Discourse at Least as Well as Humans Do](https://arxiv.org/abs/2403.17196) | GPT-4在标准化语篇理解测试中表现出与人类相当的能力，尤其在推断未明确陈述信息方面显示出显著实力 |
| [^2] | ['One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification](https://arxiv.org/abs/2403.06402) | 本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。 |
| [^3] | [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652) | Yi模型系列基于强大的多维能力，通过基于6B和34B预训练模型的扩展，包括聊天模型、长上下文模型、深度放大模型和视觉语言模型，取得了优异的性能。 |
| [^4] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^5] | [TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation](https://arxiv.org/abs/2402.10178) | 提出了一种基于动态任务分解和代理生成的多代理框架(TDAG)，该框架能够在解决复杂的实际问题时提高代理的适应性。同时，引入了ItineraryBench，这是一个在旅行规划中具有精细评估系统的基准测试。 |
| [^6] | [Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition](https://arxiv.org/abs/2401.11431) | 提出了一种名为多数还是少数（MoM）学习的数据不平衡学习方法，针对命名实体识别任务中的多数类别和少数类别之间的挑战，能够提高少数类别的预测性能，而不影响多数类别的性能。 |
| [^7] | [AutoMix: Automatically Mixing Language Models](https://arxiv.org/abs/2310.12963) | AutoMix提出了一种自动选择更大语言模型处理查询的方法，通过少量样本自我验证和元验证器提高了输出的可靠性，可显著提高计算成本和性能的优化，实验证明性能优于基线最多86%. |
| [^8] | [LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning.](http://arxiv.org/abs/2401.15371) | LegalDuet是一种通过双视角法律线索推理模型，使用预训练语言模型学习定制嵌入空间来进行法律判决预测。该模型通过法律案例推理和法律基础推理两个推理链进行判决。在实验中，LegalDuet在CAIL2018数据集上表现出最先进的性能，并超过了基线模型。 |
| [^9] | [Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks.](http://arxiv.org/abs/2401.10070) | 该论文提出了一种通信高效的个性化联邦学习框架，通过引入轻量级的LoRA模块进行客户端调整和与服务器的交互，以最小化通信开销，以及使用K最近邻分类器的全局模型来实现个性化并克服数据异构问题。 |
| [^10] | [Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification.](http://arxiv.org/abs/2401.03158) | 四步推理（QLFR）框架是一种通过引入句法和语义丰富的CoT来提升短文本分类任务中大型语言模型（LLMs）性能的方法。 |
| [^11] | [Multi-Agent Consensus Seeking via Large Language Models.](http://arxiv.org/abs/2310.20151) | 本文研究了基于大型语言模型的多智能体系统中的一致性寻求问题。研究发现，在没有明确指导的情况下，智能体主要使用平均策略进行一致性寻求，同时还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。 |
| [^12] | [JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning.](http://arxiv.org/abs/2310.02953) | JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。 |
| [^13] | [Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation.](http://arxiv.org/abs/2310.02842) | 本论文提出了一种使用智能多任务适应混合提示的方法来解决LLM在处理异质任务和数据分布时的问题。研究者设计了智能门控功能，用于识别嵌入在不同提示组中的相关技能，并根据目标任务的需求动态分配组合专家。该方法对任何模型压缩技术都不受限制，提高了任务处理的效率。 |
| [^14] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^15] | [PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models.](http://arxiv.org/abs/2307.09254) | 本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。 |

# 详细

[^1]: GPT-4至少能够像人类一样理解语篇

    GPT-4 Understands Discourse at Least as Well as Humans Do

    [https://arxiv.org/abs/2403.17196](https://arxiv.org/abs/2403.17196)

    GPT-4在标准化语篇理解测试中表现出与人类相当的能力，尤其在推断未明确陈述信息方面显示出显著实力

    

    我们测试了一种领先的AI系统GPT-4是否像人类一样理解语篇，使用了一项标准化的语篇理解测试。参与者会被呈现简短的故事，然后回答八个是/否问题，探究他们对故事的理解。这些问题的格式旨在评估直接性（陈述 vs. 暗示）和显著性（主要观点 vs. 细节）的独立影响。鉴于人类表现水平非常高，GPT-4的表现略好于人类，但并无统计学显著差异。GPT-4和人类都表现出强大的能力，能够推断故事中未明确陈述的信息，这是对理解力的重要测试。

    arXiv:2403.17196v1 Announce Type: new  Abstract: We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.
    
[^2]: 一刀切不适用：学习在文本分类中使用多少例为了改进上下文学习

    'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification

    [https://arxiv.org/abs/2403.06402](https://arxiv.org/abs/2403.06402)

    本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。

    

    arXiv:2403.06402v1 发表类型：新 Abstract: 自然语言处理（NLP）中的预测模型已经从从头训练模型发展到使用标记数据微调预训练模型。这种微调的极端形式涉及到上下文学习（ICL），其中一个预先训练的生成模型的输出（冻结的解码器参数）只受到输入字符串的变化（称为指令或提示）的控制。ICL的一个重要组成部分是在提示中使用少量标记数据实例作为示例。尽管现有工作在推理过程中为每个数据实例使用静态数量的示例，但在本文中，我们提出了一种动态调整示例数量的新方法。这类似于k最近邻（k-NN）分类器中使用可变大小邻域的方法。在我们提出的自适应ICL（AICL）的工作流程中，对于特定数据实例进行推理时使用的演示数量是动态调整的。

    arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
    
[^3]: Yi: 由 01.AI 推出的开放基础模型

    Yi: Open Foundation Models by 01.AI

    [https://arxiv.org/abs/2403.04652](https://arxiv.org/abs/2403.04652)

    Yi模型系列基于强大的多维能力，通过基于6B和34B预训练模型的扩展，包括聊天模型、长上下文模型、深度放大模型和视觉语言模型，取得了优异的性能。

    

    我们介绍了Yi模型系列，这是一系列具有强大多维能力的语言和多模态模型。Yi模型系列基于6B和34B的预训练语言模型，然后我们将它们扩展为聊天模型、200K长上下文模型、深度放大模型和视觉语言模型。我们的基础模型在诸如MMLU之类的各种基准测试中表现出色，而我们微调过的聊天模型在AlpacaEval和Chatbot Arena等主要评估平台上具有较高的人类偏好率。通过依赖于我们的可扩展超级计算基础设施和经典的Transformer架构，我们认为Yi模型的性能主要归因于其数据质量，这是由我们的数据工程工作所带来的。对于预训练，我们使用级联的数据去重和质量过滤流水线构建了3100亿个英文和中文语料库的标记。对于微调，我们对小规模模型进行了改进

    arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
    
[^4]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^5]: TDAG:一种基于动态任务分解和代理生成的多代理框架

    TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation

    [https://arxiv.org/abs/2402.10178](https://arxiv.org/abs/2402.10178)

    提出了一种基于动态任务分解和代理生成的多代理框架(TDAG)，该框架能够在解决复杂的实际问题时提高代理的适应性。同时，引入了ItineraryBench，这是一个在旅行规划中具有精细评估系统的基准测试。

    

    Large Language Models (LLMs)如ChatGPT的出现，启发了基于LLM的代理的开发，能够解决复杂的实际问题。然而，这些代理在执行任务时常常由于方法论约束（如错误传播和适应性受限）而遇到困难。为了解决这个问题，我们提出了一种基于动态任务分解和代理生成的多代理框架(TDAG)。该框架动态地将复杂任务分解为更小的子任务，并将每个子任务分配给一个特别生成的子代理，从而提高了在多样化和不可预测的实际任务中的适应性。同时，现有的基准测试往往缺乏评估复杂、多步骤任务中递增进展所需的细粒度。为了应对这个问题，我们在旅行规划的背景下引入了ItineraryBench，它具有互连、逐渐复杂的任务和细粒度的评估系统。

    arXiv:2402.10178v1 Announce Type: new  Abstract: The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench i
    
[^6]: 多数还是少数：用于命名实体识别的数据不平衡学习方法

    Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition

    [https://arxiv.org/abs/2401.11431](https://arxiv.org/abs/2401.11431)

    提出了一种名为多数还是少数（MoM）学习的数据不平衡学习方法，针对命名实体识别任务中的多数类别和少数类别之间的挑战，能够提高少数类别的预测性能，而不影响多数类别的性能。

    

    数据不平衡在各种机器学习（ML）任务中是一个重要挑战，特别是在自然语言处理（NLP）中的命名实体识别（NER）任务。NER表现出一种长尾分布的数据不平衡，其中有许多少数类别（即实体类别）和一个单一的多数类别（即O类别）。这种不平衡导致将实体类别误分类为O类别。为了解决这个问题，我们提出了一种简单且有效的学习方法，命名为多数还是少数（MoM）学习。MoM学习将只有地面事实为多数类别的样本所计算的损失融入到传统ML模型的损失中。对四个NER数据集（日语和英语）的评估实验表明，MoM学习提高了少数类别的预测性能，同时不牺牲多数类别的性能，并且比广为人知的最新技术更有效。

    arXiv:2401.11431v2 Announce Type: replace  Abstract: Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). This imbalance leads to misclassifications of the entity classes as the O-class. To tackle this issue, we propose a simple and effective learning method named majority or minority (MoM) learning. MoM learning incorporates the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments on four NER datasets (Japanese and English) showed that MoM learning improves prediction performance of the minority classes without sacrificing the performance of the majority class and is more effective than widely known and state-of-the-art
    
[^7]: AutoMix: 自动混合语言模型

    AutoMix: Automatically Mixing Language Models

    [https://arxiv.org/abs/2310.12963](https://arxiv.org/abs/2310.12963)

    AutoMix提出了一种自动选择更大语言模型处理查询的方法，通过少量样本自我验证和元验证器提高了输出的可靠性，可显著提高计算成本和性能的优化，实验证明性能优于基线最多86%.

    

    大型语言模型(LLMs)现在可以通过各种尺寸和配置的云API提供商获得。虽然这种多样性提供了广泛的选择，但有效利用这些选项以优化计算成本和性能仍然具有挑战性。在这项工作中，我们提出了AutoMix，一种根据较小LM的输出的近似正确性来策略性地将查询路由到更大LM的方法。AutoMix的核心是一种少量样本的自我验证机制，它可以估计输出的可靠性而无需训练。鉴于验证可能存在噪声，我们在AutoMix中使用了元验证器来提高这些评估的准确性。我们在五个基于上下文的推理数据集上使用LLAMA2-13B和GPT-4进行实验，结果表明AutoMix超越了已建立的基线，每单位成本的增量效益提高了最多86%。我们的代码和数据可在https://github.c找到

    arXiv:2310.12963v3 Announce Type: replace  Abstract: Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86%. Our code and data are available at https://github.c
    
[^8]: LegalDuet: 通过双视角法律线索推理学习有效的法律判决预测表示

    LegalDuet: Learning Effective Representations for Legal Judgment Prediction through a Dual-View Legal Clue Reasoning. (arXiv:2401.15371v1 [cs.CL])

    [http://arxiv.org/abs/2401.15371](http://arxiv.org/abs/2401.15371)

    LegalDuet是一种通过双视角法律线索推理模型，使用预训练语言模型学习定制嵌入空间来进行法律判决预测。该模型通过法律案例推理和法律基础推理两个推理链进行判决。在实验中，LegalDuet在CAIL2018数据集上表现出最先进的性能，并超过了基线模型。

    

    大多数现有的法律判决预测（LJP）模型侧重于发现刑事事实描述中的法律线索。然而，在现实场景中，专业法官不仅需要吸收过去判决的法律案例经验，还依赖于从专业法律知识中学到的专业法律基础推理。本文提出了一种名为LegalDuet的模型，该模型预训练语言模型以学习用于进行法律判决的定制嵌入空间。它提出了一种双视角法律线索推理机制，由两个推理链组成：1）法律案例推理，根据从类比/混淆的法律案例中学到的判决经验进行法律判决；2）法律基础推理，通过匹配刑事案件和法律决定之间的法律线索。实验证明，LegalDuet在CAIL2018数据集上实现了最先进的性能，并且超过了基线模型。

    Most existing Legal Judgment Prediction (LJP) models focus on discovering the legal triggers in the criminal fact description. However, in real-world scenarios, a professional judge not only needs to assimilate the law case experience that thrives on past sentenced legal judgments but also depends on the professional legal grounded reasoning that learned from professional legal knowledge. In this paper, we propose a LegalDuet model, which pretrains language models to learn a tailored embedding space for making legal judgments. It proposes a dual-view legal clue reasoning mechanism, which derives from two reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments according to the judgment experiences learned from analogy/confusing legal cases; 2) Legal Ground Reasoning, which lies in matching the legal clues between criminal cases and legal decisions. Our experiments show that LegalDuet achieves state-of-the-art performance on the CAIL2018 dataset and outperforms bas
    
[^9]: 通信高效的个性化联邦学习在语音转文本任务中的应用

    Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks. (arXiv:2401.10070v1 [cs.CL])

    [http://arxiv.org/abs/2401.10070](http://arxiv.org/abs/2401.10070)

    该论文提出了一种通信高效的个性化联邦学习框架，通过引入轻量级的LoRA模块进行客户端调整和与服务器的交互，以最小化通信开销，以及使用K最近邻分类器的全局模型来实现个性化并克服数据异构问题。

    

    为了保护隐私并满足法规要求，联邦学习在训练语音转文本系统（包括自动语音识别和语音翻译）方面引起了广泛关注。然而，在语音转文本任务中常用的联邦学习方法（即FedAvg）通常面临着大量的通信开销和数据异构导致的性能下降问题。为了解决这些问题，我们提出了一种个性化的联邦语音转文本框架，引入了轻量级的LoRA模块（FedLoRA）用于客户端调整和与服务器进行交互以最小化通信开销，以及全局模型（FedMem）配备了K最近邻分类器，以捕捉客户特定的分布变化以实现个性化并克服数据异构。在CoVoST和GigaSp数据集上基于Conformer和Whisper主干模型进行了大量实验。

    To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSp
    
[^10]: 四步推理（QLFR）框架：推进短文本分类的四步推理

    Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])

    [http://arxiv.org/abs/2401.03158](http://arxiv.org/abs/2401.03158)

    四步推理（QLFR）框架是一种通过引入句法和语义丰富的CoT来提升短文本分类任务中大型语言模型（LLMs）性能的方法。

    

    短文本分类（STC）对于处理和理解当代数字平台上流行的简洁而重要的内容至关重要。STC在抓住语义和句法复杂性方面遇到困难，这个问题在传统的预训练语言模型中很明显。尽管图卷积网络通过整合外部知识库提高了性能，但这些方法受到应用知识质量和范围的限制。最近，大型语言模型（LLMs）和思维链（CoT）的出现显著提高了复杂推理任务的性能。然而，一些研究指出了它们在基础NLP任务中的应用限制。因此，本研究旨在运用CoT来研究LLMs在STC任务中的能力。本研究引入了四步推理（QLFR）框架。这个框架主要包括句法和语义丰富的CoT，有效利用了LLMs的能力。

    Short Text Classification (STC) is crucial for processing and comprehending the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study sought to employ CoT to investigate the capabilities of LLMs in STC tasks. This study introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This framework primarily incorporates Syntactic and Semantic Enrichment CoT, effectiv
    
[^11]: 基于大型语言模型的多智能体一致性寻求

    Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])

    [http://arxiv.org/abs/2310.20151](http://arxiv.org/abs/2310.20151)

    本文研究了基于大型语言模型的多智能体系统中的一致性寻求问题。研究发现，在没有明确指导的情况下，智能体主要使用平均策略进行一致性寻求，同时还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。

    

    大型语言模型（LLM）驱动的多智能体系统在协作解决复杂任务方面展现出了令人期待的能力。本研究考虑了多智能体协作中的一个基本问题：一致性寻求。当多个智能体一起工作时，我们关注的是它们如何通过智能体间的协商达成一致。为此，本研究研究了一个一致性寻求任务，其中每个智能体的状态是一个数值，并且它们通过相互协商来达成一致值。研究发现，当没有明确指导应采用哪种策略时，LLM驱动的智能体主要使用平均策略进行一致性寻求，尽管它们可能偶尔会使用其他策略。此外，本研究还分析了智能体数量、智能体个性和网络拓扑对协商过程的影响。本研究的发现有望为理解LLM驱动的多智能体行为奠定基础。

    Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-
    
[^12]: JsonTuning：面向通用、强大和可控的指令调优

    JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])

    [http://arxiv.org/abs/2310.02953](http://arxiv.org/abs/2310.02953)

    JsonTuning是一种面向通用、强大和可控的指令调优方法，通过利用JSON的结构化特性，帮助模型理解任务要素及其关系，从而扩展了通用性、提高了稳健性，并增强了对输出的控制。

    

    指令调优已成为利用大型语言模型（LLM）能力的关键过程，通过提供明确的任务指令，从而在各种任务中提高性能。然而，目前的文本-文本指令调优（TextTuning）方法由于任务的模糊性和缺乏明确的结构而存在通用性、稳健性和可控性的限制。在本文中，我们提出了JsonTuning，这是一种新的结构到结构的指令调优方法。通过利用JSON的多功能和结构化特性来表示任务，JsonTuning通过帮助模型理解关键任务要素及其关系，扩展了通用性，通过最小化歧义性提高了稳健性，并通过提供对输出的显式控制增强了可控性。我们对不同的语言模型和评估基准进行了全面的比较研究。实验结果表明，JsonTuning在性能上优于TextTuning。

    Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
    
[^13]: 用智能多任务适应混合提示扫描异质性

    Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])

    [http://arxiv.org/abs/2310.02842](http://arxiv.org/abs/2310.02842)

    本论文提出了一种使用智能多任务适应混合提示的方法来解决LLM在处理异质任务和数据分布时的问题。研究者设计了智能门控功能，用于识别嵌入在不同提示组中的相关技能，并根据目标任务的需求动态分配组合专家。该方法对任何模型压缩技术都不受限制，提高了任务处理的效率。

    

    大型语言模型(LLM)有能力解决各种任务，如文本摘要和数学问题，但通常是针对单一任务进行训练。由于高计算成本，当前趋势是使用提示指导调节预先训练的LLM以适应新的下游任务。因此，如何扩展提示调节以同时处理异质任务和数据分布是一个广泛开放的问题。为了解决这一问题，我们建议使用"混合提示"或MoPs，并结合智能门控功能：后者的设计是本文的贡献之一，它可以识别嵌入在不同提示组中的相关技能，并根据目标任务动态分配组合专家(即一组提示)。此外，MoPs在应用任何模型压缩技术时都不受影响——以提高效率。

    Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
    
[^14]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^15]: 用于量化生成式语言模型不确定性的PAC神经预测集学习

    PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])

    [http://arxiv.org/abs/2307.09254](http://arxiv.org/abs/2307.09254)

    本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。

    

    学习和量化模型的不确定性是增强模型可信度的关键任务。由于对生成虚构事实的担忧，最近兴起的生成式语言模型（GLM）特别强调可靠的不确定性量化的需求。本文提出了一种学习神经预测集模型的方法，该方法能够以可能近似正确（PAC）的方式量化GLM的不确定性。与现有的预测集模型通过标量值参数化不同，我们提出通过神经网络参数化预测集，实现更精确的不确定性量化，但仍满足PAC保证。通过在四种类型的语言数据集和六种类型的模型上展示，我们的方法相比标准基准方法平均提高了63％的量化不确定性。

    Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
    

