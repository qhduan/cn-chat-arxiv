# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Differentially Private Knowledge Distillation via Synthetic Text Generation](https://arxiv.org/abs/2403.00932) | 提出一种利用合成数据进行知识蒸馏的差分私密算法 |
| [^2] | [Question Answering Over Spatio-Temporal Knowledge Graph](https://arxiv.org/abs/2402.11542) | 介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标 |

# 详细

[^1]: 通过合成文本生成的差分私密知识蒸馏

    Differentially Private Knowledge Distillation via Synthetic Text Generation

    [https://arxiv.org/abs/2403.00932](https://arxiv.org/abs/2403.00932)

    提出一种利用合成数据进行知识蒸馏的差分私密算法

    

    大型语言模型(LLMs)在许多不同的下游任务中实现了最先进的性能。然而，数据隐私的增加紧迫性要求LLMs在私有数据上使用差分隐私(DP)进行训练。同时，还需要压缩LLMs以在资源受限的设备或延迟敏感的应用中进行真实部署。差分隐私和模型压缩通常必须在实现其目标的过程中权衡效用损失。此外，同时实现这两者可能导致更多的效用损失。为此，我们提出了一种新颖的差分私密知识蒸馏算法，该算法利用了由差分私密LLM生成的合成数据。教师模型的知识以两种方式转移到学生模型上：一种是来自合成数据本身的硬标签，另一种是通过在合成数据上评估的教师模型的输出分布。

    arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
    
[^2]: 基于时空知识图的问答系统

    Question Answering Over Spatio-Temporal Knowledge Graph

    [https://arxiv.org/abs/2402.11542](https://arxiv.org/abs/2402.11542)

    介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标

    

    时空知识图（STKG）通过整合时间和位置信息扩展了知识图（KG）的概念。尽管研究界关注知识图问答（KGQA），但基于STKG的涵盖时空信息的问题回答领域仍未被广泛探讨。此外，缺乏全面的数据集也阻碍了该领域的进展。为解决这一问题，我们提出了STQAD，这是一个包括10,000个自然语言问题的面向时空知识图问答（STKGQA）数据集。不幸的是，各种最先进的KGQA方法在我们的数据集上远未达到令人满意的性能。为此，我们提出了STCQA，一种新的时空KGQA方法，利用了一种名为STComplEx的新型STKG嵌入方法。通过从问题中提取时间和空间信息，我们的问答模型可以更好地理解问题。

    arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
    

