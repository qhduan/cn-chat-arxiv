# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores](https://arxiv.org/abs/2403.00553) | 本论文提出了一种用于衡量文本多样性的标准分数，通过实证研究发现压缩算法可以捕捉类似于$n$-gram重叠同质性得分的信息，并结合多种度量方法来报告分数，适用于不同类型的文本分析。 |
| [^2] | [Evaluating Language Model Agency through Negotiations.](http://arxiv.org/abs/2401.04536) | 本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。 |

# 详细

[^1]: 规范文本多样性的测量：一个工具和对分数的比较分析

    Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores

    [https://arxiv.org/abs/2403.00553](https://arxiv.org/abs/2403.00553)

    本论文提出了一种用于衡量文本多样性的标准分数，通过实证研究发现压缩算法可以捕捉类似于$n$-gram重叠同质性得分的信息，并结合多种度量方法来报告分数，适用于不同类型的文本分析。

    

    大型语言模型生成的输出之间的多样性塑造了人们对其质量和实用性的看法。我们的工作通过实证研究英语文本的多样性得分。我们发现，计算效率高的压缩算法捕捉到与$n$-gram的重叠同质性得分所衡量的信息相似。此外，结合多种度量方法——压缩比、长$n$-gram的自重复、Self-BLEU和BERTScore——足以报告，因为它们彼此之间的相互关联较低。这些分数的适用性超出了生成模型的分析；例如，我们突出了在指导调整数据集和人类生成的文本上的应用。我们发布了一个多样性程度

    arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc
    
[^2]: 通过谈判评估语言模型的代理能力

    Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])

    [http://arxiv.org/abs/2401.04536](http://arxiv.org/abs/2401.04536)

    本研究通过谈判游戏的视角，提出共同评估语言模型（LM）的性能和对齐，以更好地反映真实世界的部署条件，并避免数据泄漏。通过评估多轮次和跨模型交互，我们发现了LM的自我对弈和交叉对弈性能。

    

    公司、组织和政府越来越多地利用语言模型（LM）展示类似代理行为的出色能力。随着LM被采用来执行越来越具有自主性的任务，迫切需要可靠且可扩展的评估基准。当前主要是静态的LM基准无法很好地评估此类动态应用。因此，我们提议通过谈判游戏的视角来共同评估LM的性能和对齐。我们认为这个共同任务更好地反映了真实世界的部署条件，并提供了关于LM决策过程的见解。至关重要的是，谈判游戏使我们能够研究多轮次和跨模型交互，调整复杂性，并避免评估中的意外数据泄漏。我们报告了来自几个主要供应商的六个公开可访问的LM在各种谈判游戏上的结果，评估了自我对弈和交叉对弈性能。值得注意的发现包括：（i）开源模式

    Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
    

