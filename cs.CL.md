# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887) | 层剪枝方法可以在流行的预训练语言模型中实现大部分层的移除而保持性能，同时使用参数高效的微调方法可以进一步减少计算资源，提高推断的内存和延迟。 |
| [^2] | [Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework](https://arxiv.org/abs/2403.08743) | 本文提出了一种基于因果关系的去偏倾框架，通过选择机制指导设计提示来减少大型语言模型(LLMs)产生的社会偏见。 |
| [^3] | [TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning](https://arxiv.org/abs/2403.08694) | TeaMs-RL通过强化学习直接生成基础指导数据集，减少对人类的依赖，提供高质量数据，为单一微调步骤铺平了道路。 |
| [^4] | [CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation](https://arxiv.org/abs/2403.07260) | 提出了一种基于常识知识的大型语言模型联合情感识别对话框架，通过设计提示生成互动者的常识知识并利用这些信息进行预训练，以提高说话者的情感识别准确性。 |
| [^5] | [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/abs/2402.18945) | 论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。 |
| [^6] | [From RAGs to riches: Using large language models to write documents for clinical trials](https://arxiv.org/abs/2402.16406) | 本研究评估了大型语言模型（LLMs）在生成临床试验方案文件部分内容的能力，发现通过检索增强生成（RAG）可显着提高LLM的撰写质量，对LLMs在临床试验相关写作中具有重要意义。 |
| [^7] | [Word Embeddings Revisited: Do LLMs Offer Something New?](https://arxiv.org/abs/2402.11094) | 该论文系统地比较了经典词嵌入技术和基于LLM的词嵌入，发现LLMs倾向于将语义相关的单词更紧密地聚类在一起，并在Bigger Analogy Test Set（BATS）上具有更高的平均准确度。 |
| [^8] | [Inference to the Best Explanation in Large Language Models](https://arxiv.org/abs/2402.10767) | 该论文提出了一个受哲学启发设计的框架IBE-Eval，用于推进对大型语言模型解释的解释和评估，在因果问答实验中显示出高达77%的准确率。 |
| [^9] | [Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering](https://arxiv.org/abs/2402.09911) | 使用伪和多源知识图对大型语言模型进行增强，以改善其幻觉问题和提高性能。通过结合伪图生成和原子级知识验证的框架，在开放式问题回答环境中使用知识图可以提高ROUGE-L分数至少11.5。 |
| [^10] | [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) | 本研究引入了生成表示指令调整（GRIT）方法，通过指令区分生成和嵌入任务，训练一个大型语言模型同时处理这两种任务。与其他模型相比，我们的GritLM 7B在文本嵌入基准测试上达到最新的技术水平，并在多种生成任务中表现出色。通过进一步扩大规模，我们的GritLM 8x7B成为最佳的生成语言模型之一，同时仍然是最好的嵌入模型之一。GRIT的统一也大大提高了RAG在长文档上的速度。 |
| [^11] | [CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations](https://arxiv.org/abs/2402.04236) | 本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。 |
| [^12] | [PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?](https://arxiv.org/abs/2402.02611) | 本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。 |
| [^13] | [FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation](https://arxiv.org/abs/2401.17514) | FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。 |
| [^14] | [Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace](https://arxiv.org/abs/2310.19651) | 本研究通过对指令调整对每个大型语言模型的各项能力（如创意写作、代码生成和逻辑推理）的发展影响进行细致分析，得出了关于数据集规模、参数规模和数据构建方法的指导原则。 |
| [^15] | [Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study.](http://arxiv.org/abs/2401.06603) | 通过双向反馈机制，这个研究探索了大型语言模型(LLMs)和强化学习模型的合作。LLM充当教师，强化学习模型充当学生，它们通过递归互助实现了相互协助。这种合作提供了高级信息和实时反馈，促进了优化。 |
| [^16] | [Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages.](http://arxiv.org/abs/2310.17953) | Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。 |
| [^17] | [Representation Engineering: A Top-Down Approach to AI Transparency.](http://arxiv.org/abs/2310.01405) | 这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。 |
| [^18] | [Helpfulness and Fairness of Task-Oriented Dialogue Systems.](http://arxiv.org/abs/2205.12554) | 本文研究任务导向对话系统的帮助性和公平性。作者定义了对话系统的帮助性，使用分类器自动确定帮助性，并提出使用帮助级别来衡量对话系统的公平性。实验结果表明，现有系统更容易为来自发达国家概念的问题提供帮助。 |

# 详细

[^1]: 深层神经网络层剪枝的不合理无效性

    The Unreasonable Ineffectiveness of the Deeper Layers

    [https://arxiv.org/abs/2403.17887](https://arxiv.org/abs/2403.17887)

    层剪枝方法可以在流行的预训练语言模型中实现大部分层的移除而保持性能，同时使用参数高效的微调方法可以进一步减少计算资源，提高推断的内存和延迟。

    

    我们在流行的预训练语言模型中进行了简单的层剪枝策略的实证研究，发现在移除大部分层（最高达一半）之前，不同问答基准测试的性能几乎没有受到影响。为了剪枝这些模型，我们通过考虑层间的相似性来确定最佳的剪枝层块；然后，为了“修复”损害，我们进行了少量微调。特别地，我们使用参数高效的微调（PEFT）方法，具体包括量化和低秩适配器（QLoRA），这样我们的每个实验都可以在单个A100 GPU上执行。从实际的角度来看，这些结果表明层剪枝方法可以补充其他PEFT策略，从而进一步减少微调的计算资源，另一方面可以提高推断的内存和延迟。从科学的角度来看，该研究表明深层神经网络在某种程度上具有鲁棒性，并且对模型的剪枝没有太大影响。

    arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
    
[^2]: 将LLMs引导到无偏响应：基于因果关系的去偏倾框架

    Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework

    [https://arxiv.org/abs/2403.08743](https://arxiv.org/abs/2403.08743)

    本文提出了一种基于因果关系的去偏倾框架，通过选择机制指导设计提示来减少大型语言模型(LLMs)产生的社会偏见。

    

    大型语言模型（LLMs）很容易产生偏见和歧视性的响应。由于LLMs涉及到重要的决策制定（例如招聘和医疗保健），开发减轻这些偏见的策略至关重要。本文侧重于社会偏见，解决了人口统计信息与LLM输出之间的关联。我们提出了一种基于因果关系的去偏倾框架，利用对LLMs输入的训练语料库的数据生成过程以及LLM推理的内部推理过程的因果理解，通过选择机制指导去偏倾LLM输出的提示设计。我们的框架统一了现有的去偏指示方法，如抑制指令和上下文对比例子，并通过鼓励无偏推理的方法，启示了新的去偏倾方式。我们在真实数据集上的强大实证表现表明，我们的框架可以

    arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
    
[^3]: TeaMs-RL：通过强化学习教授LLMs更好地自我指导方法

    TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning

    [https://arxiv.org/abs/2403.08694](https://arxiv.org/abs/2403.08694)

    TeaMs-RL通过强化学习直接生成基础指导数据集，减少对人类的依赖，提供高质量数据，为单一微调步骤铺平了道路。

    

    大型语言模型（LLMs）的发展通常面临着在强化学习与人类反馈（RLHF）框架中对人类标注员的严重依赖或与自我指导范式相关的频繁且昂贵的外部查询的挑战。在这项工作中，我们转向强化学习（RL）-- 但有所不同。我们偏离了典型的RLHF，后者在指导数据训练后优化LLMs，而我们使用RL直接生成单独足以进行微调的基础指导数据集。我们的方法TeaMs-RL使用一系列文本操作和规则，优先考虑训练数据集的多样化。它促进了高质量数据的生成，而不过于依赖外部先进模型，为单一微调步骤铺平了道路，消除了随后的RLHF阶段的必要性。我们的发现突出了我们方法的关键优势：减少对人类的依赖

    arXiv:2403.08694v1 Announce Type: new  Abstract: The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human inv
    
[^4]: CKERC：基于常识知识的大型语言模型联合情感识别对话

    CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation

    [https://arxiv.org/abs/2403.07260](https://arxiv.org/abs/2403.07260)

    提出了一种基于常识知识的大型语言模型联合情感识别对话框架，通过设计提示生成互动者的常识知识并利用这些信息进行预训练，以提高说话者的情感识别准确性。

    

    对话中的情感识别(ERC)是一项任务，它在对话的上下文中预测话语的情感。它高度依赖于对话语境、说话者身份信息、多方对话场景等。然而，当前最先进的方法（instructERC）仅仅识别说话者，忽略了在对话过程中说话者背后的常识知识(即，听众的反应和说话者的意图等)，这些知识可以深入挖掘说话者信息。为此，我们提出了一种新颖的基于常识知识的大型语言模型联合情感识别对话框架，即CKERC。我们设计提示来生成基于历史话语的对话者常识，结合大型语言模型的互动者常识识别任务进行LLM预训练，以微调说话者隐含线索信息。通过解决以上挑战，我们的方法取得了最先进的成果。

    arXiv:2403.07260v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a task which predicts the emotion of an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, multiparty dialogue scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint large language models with commonsense knowledge framework for emotion recognition in conversation, namely CKERC.We design prompts to generate interlocutors' commonsense based on historical utterances with large language model. And we use the interlocutor commonsense identification task for LLM pre-training to fine-tune speaker implicit clues information.By solving above challenge, our method achieve state-o
    
[^5]: Syntactic Ghost：一种对预训练语言模型进行的无感知通用后门攻击

    Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models

    [https://arxiv.org/abs/2402.18945](https://arxiv.org/abs/2402.18945)

    论文提出了一种名为Syntactic Ghost的新方法，实现了对预训练语言模型进行无感知和通用的后门植入。

    

    预训练语言模型（PLMs）被发现容易受到后门攻击，可以将漏洞转移到各种下游任务中。然而，现有的PLM后门攻击采用明显的触发器，在手动对准的情况下进行，因此在效果、隐匿性和通用性方面无法同时满足期望目标。本文提出了一种新方法，实现了不可见和通用的后门植入，称为Syntactic Ghost（简称为synGhost）。具体来说，该方法敌意地使用具有不同预定义句法结构的毒害样本作为隐蔽触发器，然后将后门植入到预训练表示空间，而不会破坏原始知识。毒害样本的输出表示在特征空间中尽可能均匀地分布，通过对比学习形成广泛的后门。此外，在亮

    arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
    
[^6]: 从 RAGs 到财富：利用大型语言模型为临床试验撰写文件

    From RAGs to riches: Using large language models to write documents for clinical trials

    [https://arxiv.org/abs/2402.16406](https://arxiv.org/abs/2402.16406)

    本研究评估了大型语言模型（LLMs）在生成临床试验方案文件部分内容的能力，发现通过检索增强生成（RAG）可显着提高LLM的撰写质量，对LLMs在临床试验相关写作中具有重要意义。

    

    临床试验需要撰写大量文件，包括协议、同意书、临床研究报告等。大型语言模型（LLMs）有潜力快速生成这些文件的第一个版本，但人们对其输出质量存在担忧。本文评估了LLMs在生成其中一个文件（临床试验方案）的部分内容。研究发现，现成的LLM在内容相关性和术语使用正确性方面表现合理。然而，存在不足之处：特别是临床思维和逻辑，以及参考文献的适当使用。为提高性能，我们使用了检索增强生成（RAG）来提示LLM使用准确的最新信息。通过使用RAG，LLM的撰写质量显著提高，这对LLMs在临床试验相关写作中的实际可用性具有重要意义。

    arXiv:2402.16406v1 Announce Type: new  Abstract: Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.
    
[^7]: 重新审视词嵌入：LLMs是否提供新的东西？

    Word Embeddings Revisited: Do LLMs Offer Something New?

    [https://arxiv.org/abs/2402.11094](https://arxiv.org/abs/2402.11094)

    该论文系统地比较了经典词嵌入技术和基于LLM的词嵌入，发现LLMs倾向于将语义相关的单词更紧密地聚类在一起，并在Bigger Analogy Test Set（BATS）上具有更高的平均准确度。

    

    学习有意义的词嵌入对于训练稳健的语言模型至关重要。最近兴起的大型语言模型（LLMs）为我们提供了许多新的单词/句子/文档嵌入模型。尽管LLMs在各种自然语言处理任务中显示出显着的进步，但仍不清楚性能的提升仅仅是因为规模还是它们生成的底层嵌入与句子-BERT（SBERT）或通用句子编码器（USE）之类的传统编码模型有显著区别。本文通过比较经典词嵌入技术与基于LLM的词嵌入，系统地调查了这个问题，从它们的潜在向量语义方面进行比较。我们的结果显示，LLMs倾向于将语义相关的单词更紧密地聚类在一起，LLMs在Bigger Analogy Test Set（BATS）上的平均准确度也高于经典方法。最后，一些LLMs倾向于产生词嵌入si。

    arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si
    
[^8]: 大型语言模型中的最佳解释推断

    Inference to the Best Explanation in Large Language Models

    [https://arxiv.org/abs/2402.10767](https://arxiv.org/abs/2402.10767)

    该论文提出了一个受哲学启发设计的框架IBE-Eval，用于推进对大型语言模型解释的解释和评估，在因果问答实验中显示出高达77%的准确率。

    

    虽然大型语言模型（LLMs）在现实应用中取得了成功，但它们的基本解释过程仍然知之甚少。本文提出了IBE-Eval，这是一个受哲学关于最佳解释推断（IBE）的启发而设计的框架，旨在推进对LLMs解释的解释和评估。IBE-Eval通过结合包括一致性、简洁性、连贯性和不确定性在内的显式逻辑和语言特征来估计自然语言解释的合理性。在因果问答（CQA）领域进行了大量实验，其中IBE-Eval被要求在多个由LLMs（即GPT 3.5和Llama 2）生成的竞争性因果解释中选择最合理的因果解释。实验证明，IBE-Eval可以成功地以高达77\%的准确率（比随机高约27%）识别最佳解释，优于GPT 3.5作为判定基线的表现。

    arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\appr
    
[^9]: 使用伪和多源知识图增强大型语言模型进行开放式问题回答

    Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering

    [https://arxiv.org/abs/2402.09911](https://arxiv.org/abs/2402.09911)

    使用伪和多源知识图对大型语言模型进行增强，以改善其幻觉问题和提高性能。通过结合伪图生成和原子级知识验证的框架，在开放式问题回答环境中使用知识图可以提高ROUGE-L分数至少11.5。

    

    减轻大型语言模型（LLM）的幻觉并增强它们是一项关键任务。尽管一些现有方法采用了模型自我增强技术，但它们在有效解决未知事实幻觉方面存在不足。使用知识图（KG）增强方法无法同时解决不同KG来源之间的泛化和开放式答案问题的增强。为了解决这些限制，提出了一种结合了伪图生成和原子级知识验证的框架。通过利用伪图生成来实现在开放式问题回答环境中使用KG增强LLM。原子级知识验证利用原子级知识查询和验证来实现在不同KG来源下的泛化能力。与基准相比，该方法在ROUGE-L分数上至少提升了11.5。

    arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende
    
[^10]: 生成表示指令调整

    Generative Representational Instruction Tuning

    [https://arxiv.org/abs/2402.09906](https://arxiv.org/abs/2402.09906)

    本研究引入了生成表示指令调整（GRIT）方法，通过指令区分生成和嵌入任务，训练一个大型语言模型同时处理这两种任务。与其他模型相比，我们的GritLM 7B在文本嵌入基准测试上达到最新的技术水平，并在多种生成任务中表现出色。通过进一步扩大规模，我们的GritLM 8x7B成为最佳的生成语言模型之一，同时仍然是最好的嵌入模型之一。GRIT的统一也大大提高了RAG在长文档上的速度。

    

    所有基于文本的语言问题都可以归结为生成或嵌入。目前的模型只能在其中一种任务上表现良好。我们介绍了生成表示指令调整（GRIT）方法，通过指令来区分生成和嵌入任务，从而训练一个大型语言模型同时处理这两种任务。与其他开放模型相比，我们的GritLM 7B在大规模文本嵌入基准测试（MTEB）上取得了最新的技术水平，并在多种生成任务中超过了同等规模的所有模型。通过进一步扩大规模，GritLM 8x7B在尝试的所有开放生成语言模型中表现最佳，同时仍然是最好的嵌入模型之一。值得注意的是，我们发现GRIT可以与仅在生成或嵌入数据上训练的模型相媲美，因此我们可以在不损失性能的情况下统一两者。除此之外，通过GRIT的统一可以将RAG（检索增强生成）在长文档上的速度提高60%以上。

    arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, 
    
[^11]: CogCoM: 通过一系列的操作训练大规模视觉语言模型，并深入细节

    CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations

    [https://arxiv.org/abs/2402.04236](https://arxiv.org/abs/2402.04236)

    本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。

    

    视觉语言模型（VLM）通过广泛的训练，在将视觉指令与答案对齐方面展示了广泛的可行性。然而，这种确定性的对齐导致模型忽视了关键的视觉推理，并导致在细致的视觉问题和不忠实的响应方面失败。在本文中，我们提出了一种称为“操作链”的机制，使VLM能够通过一系列的操作来解决问题，其中每个操作都指的是对视觉输入的操作，可以是通过先前训练获得的内在能力（例如，基础）或者是模仿类人行为（例如，放大）。这个机制鼓励VLM生成带有证据的视觉推理的忠实的响应，并允许用户在可解释的路径上追踪错误的原因。因此，我们训练了CogCoM，一个具有内置推理机制的17B通用VLM。实验证明，我们的模型达到了最先进的水平。

    Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
    
[^12]: PuzzleBench：LLMs能否解决困难的一阶组合推理问题？

    PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?

    [https://arxiv.org/abs/2402.02611](https://arxiv.org/abs/2402.02611)

    本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。

    

    最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。

    Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
    
[^13]: FEUDA：令人沮丧地简单的基于提示的无监督领域自适应

    FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2401.17514](https://arxiv.org/abs/2401.17514)

    FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。

    

    无监督领域自适应方法的一个主要分支利用来自源领域和目标领域的未标记数据，学习适应的领域不变表示。然而，这些方法存在一定的局限性，鼓励通过持续的预训练使用自监督学习。在基于提示的分类框架中，持续的预训练或学习领域不变表示的必要性仍不清楚，其中一个输入示例由模板修改后，再输入到语言模型（LM）中生成一个标签字符串。为了研究基于提示的无监督领域自适应中的这种新范例，我们提出了一种令人沮丧地简单的无监督领域自适应方法（FEUDA），该方法使用两种不同的指令调整任务，在未标记和标记的示例上训练自回归LM。具体而言，第一个任务通过掩蔽语言建模（MLM）在两个领域的未标记文本上训练LM，第二个任务使用源标记数据进行监督指令调整。

    A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
    
[^14]: 指令调整的动态：大型语言模型的每个能力都有其自己的增长速率

    Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace

    [https://arxiv.org/abs/2310.19651](https://arxiv.org/abs/2310.19651)

    本研究通过对指令调整对每个大型语言模型的各项能力（如创意写作、代码生成和逻辑推理）的发展影响进行细致分析，得出了关于数据集规模、参数规模和数据构建方法的指导原则。

    

    指令调整是一种新兴方法，用于激发大型语言模型（LLMs）的普遍智能。然而，指令数据的创建仍然主要是启发式的，导致现有数据集在数量和质量上存在显着差异。我们的研究通过对数据量、参数大小和数据构建方法如何影响LLM的每个基本能力（如创意写作、代码生成和逻辑推理）的发展进行细致分析，以更好地理解数据构建准则。我们提供了一个经过精心策划的数据集，涵盖了十种能力的超过40k个实例，并研究了具有70亿至330亿参数的经过指令调整的模型。我们的研究揭示了三个主要发现：

    arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a
    
[^15]: 通过双向反馈机制增强大型语言模型和强化学习模型的相互合作：一个案例研究

    Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study. (arXiv:2401.06603v1 [cs.CL])

    [http://arxiv.org/abs/2401.06603](http://arxiv.org/abs/2401.06603)

    通过双向反馈机制，这个研究探索了大型语言模型(LLMs)和强化学习模型的合作。LLM充当教师，强化学习模型充当学生，它们通过递归互助实现了相互协助。这种合作提供了高级信息和实时反馈，促进了优化。

    

    大型语言模型(LLMs)已经展示出对强化学习模型(如规划和推理能力)的显著能力，然而LLMs和强化学习模型之间的合作问题仍然需要解决。在这项研究中，我们采用了师生学习框架来解决这些问题，具体是通过使用强化学习模型提供LLMs反馈，并在合作的多智能体环境中使用LLMs为强化学习模型提供高级信息。在这个框架内，LLM扮演教师角色，而强化学习模型则扮演学生角色。这两个智能体通过递归互助的方式相互协助，如“我帮你帮我帮”等。LLM智能体向强化学习智能体提供抽象信息，实现有效的探索和策略改进。反过来，强化学习智能体向LLM智能体提供反馈，提供有价值的实时信息，帮助生成更有用的标记。这种双向反馈循环促进了优化。

    Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as "I help you help I help." The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization,
    
[^16]: Whisper-MCE: 针对混合语言实现更好性能的Whisper模型微调

    Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])

    [http://arxiv.org/abs/2310.17953](http://arxiv.org/abs/2310.17953)

    Whisper-MCE是使用自己收集的混合粤语和英语音频数据集（MCE）进行训练的Whisper模型微调，相较于基准模型，其在准确捕捉原始音频内容、提高识别准确性和加快识别速度方面具有更优越的能力，尤其在混合语言识别任务中表现出色。

    

    最近，Whisper在英语自动语音识别（ASR）领域已经接近于人类级别的鲁棒性和准确性，但在较小语种和混合语言的语音识别中，仍然需要进一步改进。本文介绍了我们细调的Whisper模型Whisper-MCE的令人瞩目的结果，该模型使用了我们自己收集的混合粤语和英语音频数据集（MCE）进行训练。同时，考虑到词错误率（WER）在较小语种和混合语言环境中评估其有效性时存在挑战，我们提出了一种新颖的评估机制。通过将我们的模型与基准的whisper-large-v2模型进行比较，我们展示了它准确捕捉原始音频内容的能力更强、识别准确性更高、识别速度更快。值得注意的是，我们的模型在识别混合语言的特定任务中胜过其他现有模型。

    Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
    
[^17]: 表示工程化：AI透明化的自上而下方法

    Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01405](http://arxiv.org/abs/2310.01405)

    这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。

    

    本文中，我们确定并描述了表示工程化（RepE）这一新兴领域，这是一种通过借鉴认知神经科学的见解来增强AI系统透明性的方法。RepE将集群级别的表示放在分析的核心，而不是神经元或电路，为我们提供了监测和操纵深度神经网络（DNNs）中高级认知现象的新方法。我们提供了RepE技术的基准和初步分析，显示它们提供了简单而有效的解决方案，用于改善我们对大型语言模型的理解和控制。我们展示了这些方法如何在包括诚实性、无害性、追求权力等一系列与安全相关的问题上发挥作用，展示了自上而下透明性研究的潜力。我们希望这项工作能够促进RepE的进一步探索，并推动AI系统的透明性和安全性的进步。

    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
    
[^18]: 任务导向对话系统的帮助性和公平性研究

    Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12554](http://arxiv.org/abs/2205.12554)

    本文研究任务导向对话系统的帮助性和公平性。作者定义了对话系统的帮助性，使用分类器自动确定帮助性，并提出使用帮助级别来衡量对话系统的公平性。实验结果表明，现有系统更容易为来自发达国家概念的问题提供帮助。

    

    目标导向的对话系统旨在帮助用户实现某些目标，因此人们对其帮助性的感知很重要。然而，目前尚未对目标导向对话系统的人类感知帮助性以及其公平性影响进行深入研究。本文研究了帮助性的计算度量，并通过人类注释构建分类器，自动确定响应的帮助性。我们进一步提出使用对不同用户查询的帮助级别来衡量对话系统的公平性。实验表明，现有系统在三种信息查询场景下更容易为来自发达国家概念的问题提供帮助。

    Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant & coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries th
    

