# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/abs/2402.05123) | 本综述对LLM指导调优的数据选择进行了全面调查。研究发现，数据集的质量在指导调优过程中比数量更为重要，因此许多研究致力于探索从指导数据集中选择高质量子集的方法。课题呈现了一种新的分类体系、介绍了最近的研究进展并详细评估了这些方法。 |

# 详细

[^1]: LLM指导调优的数据选择综述

    A Survey on Data Selection for LLM Instruction Tuning

    [https://arxiv.org/abs/2402.05123](https://arxiv.org/abs/2402.05123)

    本综述对LLM指导调优的数据选择进行了全面调查。研究发现，数据集的质量在指导调优过程中比数量更为重要，因此许多研究致力于探索从指导数据集中选择高质量子集的方法。课题呈现了一种新的分类体系、介绍了最近的研究进展并详细评估了这些方法。

    

    指导调优是训练大型语言模型（LLM）的关键步骤，如何提高指导调优的效果已经引起了增加的关注。现有研究表明，在LLM的指导调优过程中，数据集的质量比数量更为重要。因此，最近许多研究致力于探索从指导数据集中选择高质量子集的方法，旨在降低训练成本并改善LLM的指导能力。本文对LLM指导调优的数据选择进行了综述。首先，介绍了广泛使用的指导数据集。然后，提出了一种新的数据选择方法分类体系，并详细介绍了最近的研究进展，还详细阐述了数据选择方法的评估策略和结果。最后，强调了该任务的开放挑战和新的前景。

    Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.
    

