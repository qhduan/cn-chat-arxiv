# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Topic-based Watermarks for LLM-Generated Text](https://arxiv.org/abs/2404.02138) | 提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。 |
| [^2] | [Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization](https://arxiv.org/abs/2403.17428) | 探讨了利用大型语言模型增强精神科访谈的方法，通过分析朝鲜叛逃者的咨询数据，研究LLMs在划分症状和总结压力因素和症状方面取得高性能。 |
| [^3] | [How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments](https://arxiv.org/abs/2403.11807) | 通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。 |
| [^4] | [Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801) | 使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。 |
| [^5] | [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163) | 生成式人工智能在多模态理解和代码生成方面取得了突破，提出了Design2Code任务并进行了全面基准测试，展示了多模态LLMs直接将视觉设计转换为代码实现的能力。 |
| [^6] | [What Is Missing in Multilingual Visual Reasoning and How to Fix It](https://arxiv.org/abs/2403.01404) | 本文通过在视觉推理任务上的测试，发现了多语言视觉推理中存在的挑战，并提出了针对性的干预措施，包括翻译-测试方法，视觉编程方法和图像字幕生成方法。 |
| [^7] | [Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts](https://arxiv.org/abs/2402.15589) | 本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。 |
| [^8] | [Mitigating Biases of Large Language Models in Stance Detection with Calibration](https://arxiv.org/abs/2402.14296) | 本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。 |
| [^9] | [Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2402.04401) | 这项研究通过个性化参数高效调整模块（PEFT）实现了大规模语言模型（LLM）的民主化，使用户能够拥有和使用他们自己的LLM，解决了传统方法中的定制能力和隐私问题。 |
| [^10] | [ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots](https://arxiv.org/abs/2209.08199) | ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。 |
| [^11] | [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts.](http://arxiv.org/abs/2401.14295) | 这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。 |
| [^12] | [Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination.](http://arxiv.org/abs/2401.05254) | 本文从跨文化的角度研究了美国和中国社交媒体上的情感表达之间的差异。研究发现，与美国Twitter用户相比，中国新浪微博用户在情感强度的变化和激动程度上有更明显的差异。 |
| [^13] | [Joint Learning of Local and Global Features for Aspect-based Sentiment Classification.](http://arxiv.org/abs/2311.01030) | 该论文提出了一种联合学习局部和全局特征的方法，以应对基于方面的情感分类中的问题。通过设计一个包含高斯掩码层和协方差自注意层的局部编码器，在模型中有效地整合了局部上下文和全局特征，并提供了更好的区分能力。 |
| [^14] | [SITTA: A Semantic Image-Text Alignment for Image Captioning.](http://arxiv.org/abs/2307.05591) | SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。 |
| [^15] | [Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering.](http://arxiv.org/abs/2306.09996) | 本文探索使用不同提示策略，重点关注 BLIP2 模型，来提高零样本 VQA 的性能，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。精心设计的问题模板和整合额外视觉线索可以促进 VQA 性能的提高，特别是当它们结合使用时。 |

# 详细

[^1]: 基于主题的LLM生成文本的水印

    Topic-based Watermarks for LLM-Generated Text

    [https://arxiv.org/abs/2404.02138](https://arxiv.org/abs/2404.02138)

    提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。

    

    大型语言模型（LLMs）的最新进展导致了生成的文本输出与人类生成的文本相似度难以分辨。水印算法是潜在工具，通过在LLM生成的输出中嵌入可检测的签名，可以区分LLM生成的文本和人类生成的文本。然而，当前的水印方案在已知攻击下缺乏健壮性。此外，考虑到LLM每天生成数万个文本输出，水印算法需要记忆每个输出才能让检测正常工作，这是不切实际的。本文针对当前水印方案的局限性，提出了针对LLMs的“基于主题的水印算法”概念。

    arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
    
[^2]: 通过症状划分和总结对齐大型语言模型以增强精神科访谈

    Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization

    [https://arxiv.org/abs/2403.17428](https://arxiv.org/abs/2403.17428)

    探讨了利用大型语言模型增强精神科访谈的方法，通过分析朝鲜叛逃者的咨询数据，研究LLMs在划分症状和总结压力因素和症状方面取得高性能。

    

    最近，大型语言模型（LLMs）的进展加速了它们在各个领域的应用。鉴于精神科访谈是专业面试者与被面试者之间目标导向和结构化对话，这是LLMs可以提供实质价值的最未被开发的领域之一。在这里，我们通过分析具有创伤经历和精神健康问题的朝鲜叛逃者的咨询数据，探讨了LLMs用于增强精神科访谈的用途。具体而言，我们研究LLMs是否能够（1）划分表示精神症状的对话部分并命名症状，以及（2）根据访谈对话记录总结压力因素和症状。这里，访谈数据由精神健康专家进行标记，用于训练和评估LLMs。我们的实验结果表明，适当提示的LLMs在症状划分和总结上可以实现高性能。

    arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
    
[^3]: LLM的决策水平在多智能体环境中的评估究竟如何？

    How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments

    [https://arxiv.org/abs/2403.11807](https://arxiv.org/abs/2403.11807)

    通过博弈论视角评估LLMs的决策能力，结果表明GPT-3.5在稳健性方面表现良好，但泛化能力有限，而GPT-4则优于其他模型。

    

    决策是一个复杂的任务，需要各种能力，为评估大型语言模型（LLMs）提供了一个极好的框架。我们的研究通过博弈论的视角探究LLMs的决策能力。我们专注于支持多个智能体同时参与的游戏，引入了我们的框架GAMA-Bench，包括八个经典的多智能体游戏。我们设计了一个评分方案，定量评估模型在这些游戏中的表现。通过GAMA-Bench，我们研究了LLMs的稳健性、泛化能力和增强策略。结果显示，虽然GPT-3.5表现出令人满意的稳健性，但其泛化能力相对有限。然而，通过一些方法如“思维链”，其性能可以得到提高。此外，我们对各种LLMs进行评估，发现GPT-4胜过其他模型。

    arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
    
[^4]: Alpaca对抗Vicuna：使用LLMs揭示LLMs的记忆化

    Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs

    [https://arxiv.org/abs/2403.04801](https://arxiv.org/abs/2403.04801)

    使用LLM代理进行黑盒提示优化方法，揭示了受害代理中更高级别的记忆化，相比直接用训练数据提示目标模型，这种方法更有效，能更好地量化LLMs的记忆化。

    

    在本文中，我们介绍了一种黑盒提示优化方法，该方法利用攻击者LLM代理来揭示受害代理中更高级别的记忆化，与直接用训练数据提示目标模型相比，这是量化LLMs记忆化的主导方法。我们使用迭代的拒绝抽样优化过程来找到基于指令的提示，具有两个主要特征：(1)与训练数据最小重叠，以避免直接向模型呈现解决方案，以及(2)受害模型输出与训练数据的最大重叠，旨在诱使受害者吐出训练数据。我们观察到，我们基于指令的提示生成的输出与训练数据重叠程度比基线前缀后缀测量高出23.7％。我们的发现表明，(1)经过指令调整的模型可以暴露与他们的基本模型一样多的预训练数据。

    arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
    
[^5]: Design2Code：我们离自动化前端工程有多远？

    Design2Code: How Far Are We From Automating Front-End Engineering?

    [https://arxiv.org/abs/2403.03163](https://arxiv.org/abs/2403.03163)

    生成式人工智能在多模态理解和代码生成方面取得了突破，提出了Design2Code任务并进行了全面基准测试，展示了多模态LLMs直接将视觉设计转换为代码实现的能力。

    

    近年来，生成式人工智能在多模态理解和代码生成方面取得了突飞猛进的进展，实现了前所未有的能力。这可以实现一种新的前端开发范式，其中多模态LLMs可能直接将视觉设计转换为代码实现。本文将这一过程形式化为Design2Code任务，并进行全面基准测试。我们手动策划了一个包含484个多样化真实网页的基准测试用例，并开发了一套自动评估指标，以评估当前多模态LLMs能否生成直接渲染为给定参考网页的代码实现，以输入为屏幕截图。我们还结合了全面的人工评估。我们开发了一套多模态提示方法，并展示了它们在GPT-4V和Gemini Pro Vision上的有效性。我们进一步对一个开源的Design2Code-18B模型进行了微调。

    arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
    
[^6]: 多语言视觉推理中的缺失及修复方法

    What Is Missing in Multilingual Visual Reasoning and How to Fix It

    [https://arxiv.org/abs/2403.01404](https://arxiv.org/abs/2403.01404)

    本文通过在视觉推理任务上的测试，发现了多语言视觉推理中存在的挑战，并提出了针对性的干预措施，包括翻译-测试方法，视觉编程方法和图像字幕生成方法。

    

    NLP模型今天在支持多语言和多模态方面取得了进展，提高了对各种用户的可访问性。本文通过在一个视觉推理任务上的测试来评估它们的多语言，多模态能力。我们观察到像GPT-4V这样的专有系统现在在该任务上表现最佳，但与开放模型相比存在差距。令人惊讶的是，GPT-4V在英语和其他语言之间表现出类似的性能，表明在不同语言之间开发公平的系统具有潜力。我们对模型失败进行的分析揭示了使这一任务具有挑战性的三个关键方面：多语言性，复杂推理和多模态性。为了解决这些挑战，我们提出了三种有针对性的干预措施，包括一种翻译-测试方法来解决多语言性，一种视觉编程方法来分解复杂推理，以及一种利用图像字幕生成来解决多模态性的新方法。

    arXiv:2403.01404v1 Announce Type: new  Abstract: NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventio
    
[^7]: 从学术手稿的同行评审叙事中要求LLMs撰写元评论草案

    Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts

    [https://arxiv.org/abs/2402.15589](https://arxiv.org/abs/2402.15589)

    本文研究了使用不同类型/级别的提示来激发三种流行LLM，GPT-3.5、LLaMA2和PaLM2，在学术同行评审过程中自动生成元评论，并进行了详细的定性研究。

    

    学术同行评审过程中最重要但也最繁重的任务之一是撰写元评论，这涉及根据多位专家的同行评审叙事理解学术手稿的核心贡献、优点和缺点，然后将这些专家多视角的看法总结为简洁的整体概述。鉴于生成型AI，尤其是大型语言模型（LLMs）的最新重大发展，我们有充分的理由深入研究LLMs在学术同行评审环境中生成这种元评论的实用性。本文通过使用三种流行的LLM，即GPT-3.5、LLaMA2和PaLM2，执行案例研究，通过基于最近提出的TELeR分类法以不同类型/级别的提示促使它们自动生成元评论。最后，我们对LLM生成的元评论进行了详细的定性研究，并总结了我们的发现。

    arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
    
[^8]: 在立场检测中通过校准减轻大语言模型的偏见

    Mitigating Biases of Large Language Models in Stance Detection with Calibration

    [https://arxiv.org/abs/2402.14296](https://arxiv.org/abs/2402.14296)

    本文提出了一种通过校准来减轻大语言模型在立场检测中偏见的方法，设计了门控校准网络并构建了反事实增强数据，实验证明其效果显著。

    

    大语言模型（LLMs）在许多自然语言处理任务中取得了显著进展。然而，我们的实验证明，在立场检测任务中，LLMs可能会生成偏见立场，这是由于虚假情感-立场相关性和对某些个人和主题的偏好，从而损害了它们的性能。因此，在本文中，我们提出通过校准来减轻LLMs在立场检测中的偏见（MB-Cal）。在其中，设计了一种新颖的门控校准网络，以减轻LLMs产生的立场推理结果上的偏见。此外，为了使校准更准确和可推广，我们构建了反事实增强数据来矫正立场偏见。针对目标和零射击立场检测任务的实验结果表明，所提出的MB-Cal可以有效减轻LLMs的偏见，取得了最先进的结果。

    arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
    
[^9]: 通过个性化参数高效调整实现大规模语言模型的民主化

    Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning

    [https://arxiv.org/abs/2402.04401](https://arxiv.org/abs/2402.04401)

    这项研究通过个性化参数高效调整模块（PEFT）实现了大规模语言模型（LLM）的民主化，使用户能够拥有和使用他们自己的LLM，解决了传统方法中的定制能力和隐私问题。

    

    大规模语言模型（LLM）中的个性化越来越重要，旨在使LLM的交互、内容和推荐与个体用户偏好相一致。最近LLM个性化的进展聚焦于有效的提示设计，通过使用行为历史检索和文本概要等非参数化知识丰富用户查询。然而，由于缺乏模型所有权，这些方法受到了一定的限制，导致定制能力和隐私问题。此外，在复杂和动态用户数据的情况下，它们通常无法准确捕捉用户行为模式。为了解决这些缺点，我们引入了一种名为OPPU的方法，它采用个性化参数高效调整（PEFT）模块来存储用户特定的行为模式和偏好。通过插入用户的个人PEFT参数，他们可以拥有和使用他们的LLM。

    Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parame
    
[^10]: ScreenQA: 移动应用截图上的大规模问答对

    ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots

    [https://arxiv.org/abs/2209.08199](https://arxiv.org/abs/2209.08199)

    ScreenQA提出了一个新的任务和数据集，通过86K个问答对在RICO数据集上注释，旨在评估屏幕阅读理解能力。

    

    我们提出了一个新的任务和数据集ScreenQA，用于通过问答来理解屏幕内容。现有的屏幕数据集要么侧重于结构和组件级别的理解，要么侧重于像导航和任务完成之类的更高级别的组合任务。我们试图通过在RICO数据集上注释86K个问答对来弥合这两者之间的差距，希望能够基准化屏幕阅读理解能力。

    arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
    
[^11]: 推理的拓扑学：揭秘思维链、树和图

    Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])

    [http://arxiv.org/abs/2401.14295](http://arxiv.org/abs/2401.14295)

    这篇论文探讨了结合结构的提示工程在提高大型语言模型推理性能方面的前景，通过思维链、思维树或思维图的设计来引导整体推理过程。通过大量实例，这种范式显著增强了模型在多个任务中的能力。总的来说，论文提供了一个通用蓝图，为未来的发展铺平道路。

    

    自然语言处理（NLP）领域近年来取得了显著进展，特别是在通过创新的提示技术提高大型语言模型（LLM）性能方面。其中，与结构相结合的提示工程被视为一种有前途的范式，其设计如思维链、思维树或思维图等，通过结构指导整体LLM推理过程。通过大量实例的说明，这种范式显著增强了LLM在逻辑或数学推理、规划或创造性写作等各种任务中的能力。为了方便理解这个不断发展的领域并为未来的发展铺平道路，我们设计了一个有效和高效的LLM推理方案的通用蓝图。为此，我们对提示执行流程进行了深入分析，澄清并明确定义了不同的概念。然后我们建立第一个分类系统

    The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
    
[^12]: 中美两国之间基于语言的情绪表达的价值和激动对比：一个跨文化的研究

    Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v1 [cs.CY])

    [http://arxiv.org/abs/2401.05254](http://arxiv.org/abs/2401.05254)

    本文从跨文化的角度研究了美国和中国社交媒体上的情感表达之间的差异。研究发现，与美国Twitter用户相比，中国新浪微博用户在情感强度的变化和激动程度上有更明显的差异。

    

    尽管社交媒体上个体的情感表达已经得到了广泛研究，但研究主要集中在西方环境中。不同文化之间存在着引发情感表达的重要差异。本文研究了美国Twitter和中国新浪微博上的两个主要情感维度（价值和激动）之间的差异。我们研究了美国和中国个体之间的激动和价值之间的功能关系差异，并探讨了相关内容上的差异。此外，我们还对两个平台上的词语使用和话题进行了相关性分析，以解读它们之间的差异。我们观察到，对于Twitter用户来说，负面情绪和正面情绪之间的情感强度变化不太明显，而对于新浪微博用户来说，伴随着情感的上升，激动程度有更明显的升级。从语言特征中，我们发现情感表达方面的差异。

    Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressio
    
[^13]: 联合学习局部和全局特征用于基于方面的情感分类

    Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])

    [http://arxiv.org/abs/2311.01030](http://arxiv.org/abs/2311.01030)

    该论文提出了一种联合学习局部和全局特征的方法，以应对基于方面的情感分类中的问题。通过设计一个包含高斯掩码层和协方差自注意层的局部编码器，在模型中有效地整合了局部上下文和全局特征，并提供了更好的区分能力。

    

    基于方面的情感分类旨在判断句子中给定方面术语所传达的情感极性。情感极性不仅由局部上下文决定，还与远离给定方面术语的词汇相关。最近的基于注意力模型在某些情况下无法足够地区分应该更关注哪些词语。与此同时，基于图的模型正在进入基于方向的情感分类以编码句法依赖树信息。但是这些模型并没有充分利用句法依赖树，因为它们忽视了将依赖关系标签信息有效地整合到表示学习中。在本文中，我们通过有效地建模局部和全局特征来解决这些问题。首先，我们设计了一个包含高斯掩码层和协方差自注意层的局部编码器。高斯掩码层倾向于自适应地调整周围方面术语的感受野，以使其不重要化。

    Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize 
    
[^14]: SITTA: 一种用于图像描述的语义图像文本对齐方法

    SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])

    [http://arxiv.org/abs/2307.05591](http://arxiv.org/abs/2307.05591)

    SITTA是一种用于图像描述的语义图像文本对齐方法，通过构建线性映射成功地将多模态模型和语言模型的嵌入空间对齐，实现了丰富的语言能力和良好的图像-语言映射。

    

    对图像的文本和语义理解对于生成适当的描述非常重要。这需要检测图像中的对象，建模它们之间的关系，评估场景的语义，并将提取的知识表示在语言空间中。为了在保证良好的图像-语言映射的同时实现丰富的语言能力，预训练的语言模型（LMs）被条件化为预训练的多模态（图像-文本）模型，允许使用图像输入。这要求将多模态模型的视觉编码器中检测到的语义与生成性LM的语言表示进行对齐。然而，如何最好地将视觉编码器检测到的语义传递给LM还不清楚。我们介绍了两种构建线性映射的新方法，成功地将两个预训练模型的嵌入空间之间的语义转移。第一种方法是将多模态语言编码器的嵌入空间与生成性LM的嵌入空间进行对齐。

    Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
    
[^15]: 探究零样本和少样本视觉问答提示技术

    Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])

    [http://arxiv.org/abs/2306.09996](http://arxiv.org/abs/2306.09996)

    本文探索使用不同提示策略，重点关注 BLIP2 模型，来提高零样本 VQA 的性能，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。精心设计的问题模板和整合额外视觉线索可以促进 VQA 性能的提高，特别是当它们结合使用时。

    

    视觉问答（VQA）是一项具有挑战性的任务，需要具备理解和推理视觉信息的能力。虽然近期的视觉语言模型取得了进展，但它们在零样本VQA方面仍然存在问题，特别是在处理复杂组合问题和适应新领域，如基于知识的推理方面。本文探讨了各种提示策略的使用，重点关注BLIP2模型，以提高零样本VQA的性能。我们在几个VQA数据集上进行了全面调查，研究了不同问题模板的有效性、少量样本示例的作用、思维链推理的影响以及将图像标题作为额外视觉线索融合的好处。尽管结果各异，但我们的发现表明，精心设计的问题模板和整合额外视觉线索（如图像标题）可以促进VQA性能的提高，特别是当它们结合使用时。

    Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
    

