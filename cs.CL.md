# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012) | 提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。 |
| [^2] | [Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263) | 提出了MELoRA，一种迷你集成低秩适配器，通过使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。 |
| [^3] | [Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models](https://arxiv.org/abs/2402.15481) | 提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。 |
| [^4] | [A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models](https://arxiv.org/abs/2402.13606) | 该论文介绍了对大型语言模型的多语言置信度评估的全面研究，提出了一个专业多语言问答数据集，并研究了这些置信度分数如何增强模型性能，最终提出了一种跨语言置信度估计方法。 |
| [^5] | [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405) | 通过统一的基于分类学指导的指导调整框架，本文提出了一种利用现有分类学进行实体关系微调的方法，有效解决实体集扩展、分类学扩展和种子引导分类学构建三个任务。 |
| [^6] | [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211) | 分析了大型语言模型在情感支持对话中的表现，揭示了其存在的偏好性偏差问题，即对特定策略的过高偏好会阻碍有效的情感支持。 |
| [^7] | [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668) | 对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。 |
| [^8] | [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299) | 本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。 |
| [^9] | [Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora.](http://arxiv.org/abs/2401.14624) | 本论文提出了一种通过大型语言模型来收集特定领域知识的高效方法，通过该方法构建了一个高质量的名为“Knowledge Pile”的数据集，实验证明其显著改善了特定领域的数据稀缺问题。 |
| [^10] | [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models.](http://arxiv.org/abs/2310.13312) | 本研究探索了金融预训练语言模型的语料库多样性对其性能的影响，并通过在多样化的金融语料库上训练的新模型FiLM，取得了比现有模型更好的结果。 |
| [^11] | [Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding.](http://arxiv.org/abs/2309.12646) | 本研究通过利用句子嵌入和语义相似性，解码了双人对话中的情感，并发现在冲突对话中，妻子的情感与语义相似性呈正相关。 |
| [^12] | [Improving Resnet-9 Generalization Trained on Small Datasets.](http://arxiv.org/abs/2309.03965) | 本文提出了一种在小型数据集上改进ResNet-9的方法，通过应用一系列技术来提高其泛化性能，在不到10分钟的时间内，在CIFAR-10数据集的10%子集上达到了88%的准确率。 |
| [^13] | [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization.](http://arxiv.org/abs/2309.03824) | 本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。 |
| [^14] | [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection.](http://arxiv.org/abs/2112.11479) | AtteSTNet是一种基于注意力机制和子词分割的检测混合语言仇恨言论的方法，它不仅与复杂网络相当，而且在各种数据集上性能更好，其极大的简单性和易于维护性是其优点。 |

# 详细

[^1]: 使用大型语言模型生成的相关性判断来预测查询性能

    Query Performance Prediction using Relevance Judgments Generated by Large Language Models

    [https://arxiv.org/abs/2404.01012](https://arxiv.org/abs/2404.01012)

    提出了一种使用自动生成的相关性判断的查询性能预测框架，能够解决先前方法中对不同IR评估指标准确性和解释性的限制。

    

    查询性能预测（QPP）旨在估计搜索系统对查询的检索质量，而无需人工相关性判断。先前的QPP方法通常返回单个标量值，并不要求预测值接近特定的信息检索（IR）评估指标，从而导致以下某些缺点：（i）单个标量无法准确表示不同的IR评估指标，特别是当度量不高度相关时，（ii）单个标量限制了QPP方法的可解释性，因为仅使用标量无法解释QPP结果。为解决这些问题，我们提出了一个使用自动生成的相关性判断的QPP框架（QPP-GenRE），将QPP分解为独立的子任务，即对排名列表中每个项目对给定查询的相关性进行判断。这样我们可以使用生成的相关性判断来预测任何IR评估指标。

    arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
    
[^2]: 迷你集成低秩适配器用于参数高效微调

    Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.17263](https://arxiv.org/abs/2402.17263)

    提出了MELoRA，一种迷你集成低秩适配器，通过使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。

    

    参数高效微调（PEFT）是一种用于定制预训练大型语言模型（LLMs）的流行方法，尤其是在模型规模和任务多样性增加的情况下。低秩适应（LoRA）基于这样一个思想，即适应过程在本质上是低维的，即可以用相对较少的参数表示重要的模型变化。然而，与全参数微调相比，降低秩会遇到特定任务的泛化误差方面的挑战。我们提出了MELoRA，一种迷你集成低秩适配器，使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。其核心思想是冻结原始的预训练权重，并训练一组仅具有少量参数的迷你LoRA。这可以捕捉迷你LoRA之间的重要多样性程度，从而促进更好的泛化能力。

    arXiv:2402.17263v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theor
    
[^3]: 偏见和反复无常：衡量大型语言模型社会歧视的统计框架

    Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models

    [https://arxiv.org/abs/2402.15481](https://arxiv.org/abs/2402.15481)

    提出了Prejudice-Caprice Framework（PCF）来全面衡量LLMs中的歧视，考虑了它们在不同上下文中的一贯偏见偏好和偏好变化。

    

    arXiv:2402.15481v1 公告类型: 新的 摘要: 大型语言模型（LLMs）在社会运营中的日益融合加剧了它们对经济、法律、教育和医疗等重要领域决策的影响，引发了公众对这些模型涉及歧视安全和可靠性的担忧。然而，先前的歧视测量框架仅评估LLMs的平均歧视行为，往往由于忽视了一个额外的导致歧视的因素，即LLMs在不同上下文中的预测变化而变得不足。在这项工作中，我们提出了Prejudice-Caprice Framework（PCF），通过考虑LLMs的一贯偏见偏好和在多样上

    arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
    
[^4]: 对大型语言模型的多语言置信度评估进行全面研究

    A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models

    [https://arxiv.org/abs/2402.13606](https://arxiv.org/abs/2402.13606)

    该论文介绍了对大型语言模型的多语言置信度评估的全面研究，提出了一个专业多语言问答数据集，并研究了这些置信度分数如何增强模型性能，最终提出了一种跨语言置信度估计方法。

    

    大型语言模型生成幻觉并在预测中表现过于自信的倾向引发了人们对其可靠性的担忧。表明模型响应的可信度或不确定性估计对于开发可靠的人工智能系统至关重要。目前的研究主要集中在英语中LLM的置信度估计上，在其他广泛使用的语言方面仍存在空白，阻碍了可靠AI应用的全球发展。本文介绍了对LLM上的多语言置信度评估（MlingConf）的全面调查。首先，我们引入了一个经过详细检查的专业多语言问答数据集。其次，我们深入研究置信度估计的性能，并研究这些置信度分数如何通过跨不同语言的自我完善来增强LLM的性能。最后，我们提出了一种跨语言置信度估计方法，以实现更精确的估计。

    arXiv:2402.13606v1 Announce Type: new  Abstract: The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more preci
    
[^5]: 一个统一的基于分类学指导的实体集扩展和分类学扩展的指导调整框架

    A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion

    [https://arxiv.org/abs/2402.13405](https://arxiv.org/abs/2402.13405)

    通过统一的基于分类学指导的指导调整框架，本文提出了一种利用现有分类学进行实体关系微调的方法，有效解决实体集扩展、分类学扩展和种子引导分类学构建三个任务。

    

    实体集扩展、分类学扩展和种子引导分类学构建是三个代表性任务，可以用来自动向现有分类学填充新实体。然而，先前的方法通常使用异质技术分别解决这些任务，缺乏统一的视角。为了解决这个问题，在本文中，我们从分类学结构的视角确认了这些任务所需的共同关键技能——找到“兄弟”和找到“父母”，并提出了一个统一的基于分类学指导的指导调整框架来共同解决这三个任务。具体来说，通过利用现有分类学作为丰富的实体关系源，我们利用指导调整来微调大型语言模型，生成父母和兄弟实体。在多个基准数据集上的大量实验证明了TaxoInstruct的有效性，该方法在各项指标上均优于特定任务的基线方法。

    arXiv:2402.13405v1 Announce Type: new  Abstract: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across 
    
[^6]: 大型语言模型能成为良好的情感支持者吗？减轻对情感支持对话中的偏好性偏差

    Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation

    [https://arxiv.org/abs/2402.13211](https://arxiv.org/abs/2402.13211)

    分析了大型语言模型在情感支持对话中的表现，揭示了其存在的偏好性偏差问题，即对特定策略的过高偏好会阻碍有效的情感支持。

    

    情感支持对话（ESC）是一项旨在通过日常对话缓解个体情感困扰的任务。鉴于其固有的复杂性和非直觉性质，ESConv数据集融入了支持策略，以促进生成适当的回应。最近，尽管大型语言模型（LLMs）具有卓越的对话能力，先前的研究表明它们在提供有用的情感支持方面经常遇到困难。因此，本研究首先分析了LLMs在ESConv上的结果，揭示了在选择正确策略和对特定策略的显著偏好方面存在的挑战。在这个基础上，我们探讨了LLMs固有偏好对提供情感支持的影响，因此我们观察到，展现出对特定策略的高偏好会阻碍有效的情感支持，加剧其在预测适当策略方面的鲁棒性。

    arXiv:2402.13211v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover
    
[^7]: 对LLMs的越狱攻击的综合评估

    Comprehensive Assessment of Jailbreak Attacks Against LLMs

    [https://arxiv.org/abs/2402.05668](https://arxiv.org/abs/2402.05668)

    对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。

    

    对大型语言模型（LLMs）的滥用引起了广泛关注。为了解决这个问题，已经采取了安全措施以确保LLMs符合社会伦理。然而，最近的研究发现了一种绕过LLMs安全措施的不稳定漏洞，被称为越狱攻击。通过应用技术，如角色扮演场景、对抗性样本或对安全目标的微妙破坏作为提示，LLMs可以产生不适当甚至有害的回应。虽然研究人员已经研究了几种越狱攻击的类别，但他们都是孤立地进行的。为了填补这个空白，我们提出了对各种越狱攻击方法的首次大规模测量。我们集中在来自四个类别的13种尖端越狱方法、16种违规类别的160个问题以及六种流行的LLMs上。我们广泛的实验结果表明，优化的越狱提示始终能够达到最高的攻击成功率，并表现出...

    Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
    
[^8]: GUARD: 通过角色扮演生成自然语言越狱来测试大型语言模型遵循指南的合规性

    GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

    [https://arxiv.org/abs/2402.03299](https://arxiv.org/abs/2402.03299)

    本论文提出了一个通过角色扮演的系统，可以生成自然语言越狱，用于测试大型语言模型的指南遵循情况。系统通过收集现有越狱并将其组织成知识图来生成新的越狱，证明了其高效性和有效性。

    

    发现绕过大型语言模型（LLM）的安全过滤和有害回应的"越狱"已经鼓励社区采取安全措施。其中一个主要的安全措施是在发布之前用越狱主动测试LLM。因此，这样的测试将需要一种能够大规模且高效地生成越狱的方法。本文在追随一种新颖而直观的策略下，以人类生成的方式来生成越狱。我们提出了一个角色扮演系统，将四种不同角色分配给用户LLM，以便协作生成新的越狱。此外，我们收集现有的越狱，并通过句子逐句进行聚类频率和语义模式的划分，将它们分成不同的独立特征。我们将这些特征组织成一个知识图，使其更易于访问和检索。我们的角色系统将利用这个知识图来生成新的越狱，证明了其有效性。

    The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
    
[^9]: CC查询：从公开文献中发现大规模领域特定知识

    Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])

    [http://arxiv.org/abs/2401.14624](http://arxiv.org/abs/2401.14624)

    本论文提出了一种通过大型语言模型来收集特定领域知识的高效方法，通过该方法构建了一个高质量的名为“Knowledge Pile”的数据集，实验证明其显著改善了特定领域的数据稀缺问题。

    

    大型语言模型在各种任务中展示了显著的潜力，然而特定领域的开源模型和数据仍然非常稀缺。之前的研究主要集中在手动指定资源和收集特定领域的高质量数据，这消耗了大量时间和精力。为了解决这个问题，我们提出了一种基于大型语言模型的高效数据收集方法“CC查询”。该方法通过大型语言模型引导种子信息，并从公开文献中检索相关数据。它不仅收集了特定领域的知识相关数据，还揭示了潜在的推理过程数据。通过应用这种方法，我们构建了一个名为“Knowledge Pile”的高质量数据集，涵盖了包括STEM科学和人文科学在内的四个主要领域。实验结果表明，“Knowledge Pile”显著改善了

    Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improve
    
[^10]: 探索语料库多样性对金融预训练语言模型的影响

    Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models. (arXiv:2310.13312v1 [cs.CL])

    [http://arxiv.org/abs/2310.13312](http://arxiv.org/abs/2310.13312)

    本研究探索了金融预训练语言模型的语料库多样性对其性能的影响，并通过在多样化的金融语料库上训练的新模型FiLM，取得了比现有模型更好的结果。

    

    在过去的几年里，已经提出了各种特定领域的预训练语言模型（PLMs），在生物医学、科学和临床等专业领域表现出色。此外，由于金融数据分析的重大经济影响，金融PLM也得到了研究。然而，我们发现金融PLMs在预训练中没有充分多样化的金融数据。这种缺乏多样性的训练数据导致了较差的泛化性能，在许多下游任务上，通用的PLMs，包括BERT，往往优于金融PLMs。为了解决这个问题，我们收集了各种广泛的金融语料库，并在这些不同的数据集上训练金融语言模型（FiLM）。我们的实验证据证实，FiLM不仅优于现有的金融PLMs，而且优于通用领域的PLMs。此外，我们还提供了实证证据，即这种改进即使对于未见过的语料组也可以实现。

    Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.
    
[^11]: 在双人对话中解码情感：通过句子嵌入利用语义相似性

    Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])

    [http://arxiv.org/abs/2309.12646](http://arxiv.org/abs/2309.12646)

    本研究通过利用句子嵌入和语义相似性，解码了双人对话中的情感，并发现在冲突对话中，妻子的情感与语义相似性呈正相关。

    

    自然语言处理的最新进展突显了句子嵌入在测量语义相似性方面的潜力。然而，其在分析现实中的双人互动并预测对话参与者情感方面的应用仍然很少。为了弥补这一空白，本研究利用50对夫妻之间关于冲突和愉快活动的口头对话。采用基于Transformer的模型all-MiniLM-L6-v2来获得每个发言者话语的嵌入。然后，通过嵌入相邻话语之间的余弦相似性的平均值对对话的整体相似性进行量化。结果显示，语义相似性与妻子在冲突对话中的情感呈正相关（但在愉快对话中不相关）。此外，无论对话类型如何，都未观察到这种相关性与丈夫的情感之间。两个验证检验进一步支持了t

    Recent advancements in Natural Language Processing (NLP) have highlighted the potential of sentence embeddings in measuring semantic similarity. Yet, its application in analyzing real-world dyadic interactions and predicting the affect of conversational participants remains largely uncharted. To bridge this gap, the present study utilizes verbal conversations within 50 married couples talking about conflicts and pleasant activities. Transformer-based model all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from each speaker. The overall similarity of the conversation was then quantified by the average cosine similarity between the embeddings of adjacent utterances. Results showed that semantic similarity had a positive association with wives' affect during conflict (but not pleasant) conversations. Moreover, this association was not observed with husbands' affect regardless of conversation types. Two validation checks further provided support for the validity of t
    
[^12]: 在小型数据集上改进ResNet-9的泛化性能

    Improving Resnet-9 Generalization Trained on Small Datasets. (arXiv:2309.03965v1 [cs.LG])

    [http://arxiv.org/abs/2309.03965](http://arxiv.org/abs/2309.03965)

    本文提出了一种在小型数据集上改进ResNet-9的方法，通过应用一系列技术来提高其泛化性能，在不到10分钟的时间内，在CIFAR-10数据集的10%子集上达到了88%的准确率。

    

    本文提出了我们提出的方法，该方法在ICLR硬件感知高效训练竞赛中获得了第一名。挑战是在不到10分钟的时间内，在一个小数据集上实现尽可能高的图像分类准确率。训练使用的小数据集是从CIFAR-10数据集中随机挑选的5000幅图像。评估由竞赛组织者在一个包含1000幅相同大小图像的秘密数据集上进行。我们的方法包括应用一系列技术来提高ResNet-9的泛化性能，包括：锐度感知优化、标签平滑、梯度居中化、输入图像补丁白化以及基于元学习的训练。我们的实验结果表明，在不到10分钟的时间内，ResNet-9可以在仅训练CIFAR-10数据集的10%子集上达到88%的准确率。

    This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets
    
[^13]: 低秩分解网络的训练加速：顺序冻结和秩量化

    Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])

    [http://arxiv.org/abs/2309.03824](http://arxiv.org/abs/2309.03824)

    本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。

    

    低秩分解（LRD）是一种应用于深度学习模型权重张量的模型压缩技术，以减少可训练参数和计算复杂性。然而，由于在应用LRD后在架构中添加了大量新层，如果分解秩不够小，则可能导致训练/推理加速性不高。问题在于，使用较小的秩会增加分解后的显著准确率下降的风险。本文中，我们提出了两种加速低秩分解模型的技术，而不需要使用较小的秩进行分解。这些方法包括秩优化和顺序冻结分解层。我们在卷积和基于transformer的模型上进行了实验证明，这些技术在保持接近原始模型准确性的同时，可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%。

    Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the o
    
[^14]: 基于注意力机制和子词分割的混合语言仇恨言论检测方法

    AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.11479](http://arxiv.org/abs/2112.11479)

    AtteSTNet是一种基于注意力机制和子词分割的检测混合语言仇恨言论的方法，它不仅与复杂网络相当，而且在各种数据集上性能更好，其极大的简单性和易于维护性是其优点。

    

    技术的最新进展导致社交媒体的使用量增加，也导致大量用户生成的数据，其中包括令人讨厌和冒犯的言论。社交媒体上使用的语言通常是英语和育地方语言的组合。在印度，印地语是主要使用的语言，并经常与英语切换，形成印地英语（Hinglish）语言。过去已经采用了不同的机器学习和深度学习技术来对混合时的印地英语仇恨言论进行分类。然而，这些技术使用的循环或卷积机制计算成本高，内存需求大。过去的技术还使用复杂的数据处理方法，使现有技术非常复杂且难以改变数据。提出了一种更简单的方法，不仅与这些复杂网络一样，并且在如HASOC（印欧语言的仇恨言论和冒犯内容识别）此类混合印地英语文本的数据集上超过了性能基准。所提出的方法名为AtteSTNet，它利用注意力机制和子词分割来识别混合语言中的仇恨言论。所提出的方法比以前的技术表现更好，更简单易于维护。

    Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds perfor
    

