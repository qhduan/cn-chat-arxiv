# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs](https://arxiv.org/abs/2402.11633) | 本论文提出了SOLID模型，利用自我播种和多意图自我指导方案来实现LLMs生成意图感知的信息检索对话。 |

# 详细

[^1]: 利用自我播种和多意图自我指导的LLM生成意图感知的信息检索对话

    Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs

    [https://arxiv.org/abs/2402.11633](https://arxiv.org/abs/2402.11633)

    本论文提出了SOLID模型，利用自我播种和多意图自我指导方案来实现LLMs生成意图感知的信息检索对话。

    

    识别信息检索对话中用户意图对于系统满足用户信息需求至关重要。意图预测（IP）具有挑战性，并需要充分的与人工标注意图对话用于训练。然而，手动注释意图资源密集。虽然大型语言模型（LLMs）已被证明在生成合成数据方面非常有效，但尚无研究使用LLMs生成意图感知的信息检索对话。本文的研究重点是利用LLMs进行零-shot生成大规模、开放领域和意图感知的信息检索对话。我们提出了SOLID，其中包括新颖的自我播种和多意图自我指导方案。前者通过利用LLM自身的知识范围来启动对话生成来提高生成质量；后者促使LLM按顺序生成话语，并通过要求LLM自动完成话题设计来减轻手动话题设计的需要。

    arXiv:2402.11633v1 Announce Type: new  Abstract: Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to auton
    

