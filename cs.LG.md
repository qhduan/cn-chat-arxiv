# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions](https://rss.arxiv.org/abs/2402.01116) | 我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。 |
| [^2] | [Accelerated Smoothing: A Scalable Approach to Randomized Smoothing](https://arxiv.org/abs/2402.07498) | 本文提出了一种加速随机平滑的方法，通过训练替代了蒙特卡洛抽样，显著提高了计算效率，并在各种设置下展示了其在近似平滑分类器方面的精确性。 |
| [^3] | [Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain](https://arxiv.org/abs/2402.06190) | 本文介绍了一种名为LoGoNet的新型神经网络架构，采用自监督学习方法来应对医学图像分析中的挑战。LoGoNet通过采用大内核注意力和双重编码策略，灵活捕捉长、短距离特征相关性。这种创新的组合技术在医学图像分割中特别有益。 |
| [^4] | [Implicit Bias and Fast Convergence Rates for Self-attention](https://arxiv.org/abs/2402.05738) | 该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。 |
| [^5] | [Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity](https://arxiv.org/abs/2402.03167) | 本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。 |
| [^6] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^7] | [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929) | 本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。 |
| [^8] | [Online Reinforcement Learning in Non-Stationary Context-Driven Environments](https://arxiv.org/abs/2302.02182) | 提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。 |
| [^9] | [A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models.](http://arxiv.org/abs/2401.06740) | 这份论文介绍了一种用于定价跳跃扩散模型下欧式篮式期权的深度学习方法，采用了隐式-显式最小移动方法以及残差型人工神经网络逼近，并通过稀疏网格高斯-埃尔米特逼近和基于ANN的高维专用求积规则来离散化积分运算符。 |
| [^10] | [MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks.](http://arxiv.org/abs/2312.15960) | MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。 |
| [^11] | [ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training.](http://arxiv.org/abs/2310.15334) | 本论文设计了一系列序列和并行的ADMM算法解决残差网络训练问题，并通过理论分析证明了算法的收敛性和收敛速度，同时分析了并行实现的时间复杂性和内存消耗的优势。 |
| [^12] | [Conditional Density Estimations from Privacy-Protected Data.](http://arxiv.org/abs/2310.12781) | 本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。 |
| [^13] | [Optimal vintage factor analysis with deflation varimax.](http://arxiv.org/abs/2310.10545) | 本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。 |
| [^14] | [Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences.](http://arxiv.org/abs/2309.03791) | 本论文介绍了一种新的方法ARMOR_D来加强深度学习模型的对抗鲁棒性，该方法基于最优传输正则化差异，通过在分布的邻域上进行最大化期望损失来实现。实验证明，ARMOR_D方法在恶意软件检测和图像识别应用中能够优于现有方法，在对抗攻击下的鲁棒性方面具有较好的效果。 |
| [^15] | [Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions.](http://arxiv.org/abs/2307.14906) | 本文介绍了一种使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统，该系统在大规模电商数据集上通过集成负采样和列表损失函数实现了较高的推荐准确性，并在实践中表现出潜力。 |
| [^16] | [Emergent representations in networks trained with the Forward-Forward algorithm.](http://arxiv.org/abs/2305.18353) | 研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。 |
| [^17] | [Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions.](http://arxiv.org/abs/2304.02868) | 本文探究大型语言模型在玩文字游戏的能力，并发现其表现有竞争力，但仍然缺乏智能，有待提升。 |
| [^18] | [Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks.](http://arxiv.org/abs/2303.17523) | 本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。 |
| [^19] | [Interpretable Few-shot Learning with Online Attribute Selection.](http://arxiv.org/abs/2211.09107) | 本文提出了一种在线属性选择机制的天然可解释模型来处理小样本学习，通过减少每个episode中涉及的属性数量提高准确性和可解释性，同时自动检测并补偿人工智能属性池不足的episode。 |
| [^20] | [Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning.](http://arxiv.org/abs/2207.05442) | 本文提出了一种新的自回归模型，用于分析多元分布时间序列。并且在Wasserstein空间中建模了随机对象，提供了该模型的解的存在性和一致估计器。此方法可以应用于年龄分布和自行车共享网络的观察数据。 |

# 详细

[^1]: 可扩展多模型MPC的基于对偶交互预测的层级架构

    Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions

    [https://rss.arxiv.org/abs/2402.01116](https://rss.arxiv.org/abs/2402.01116)

    我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。

    

    我们提出了一个层级架构，用于在复杂的多模态交通场景中实现可扩展的实时模型预测控制(MPC)。该架构由两个关键组件组成：1) RAID-Net，一种基于注意力机制的新颖循环神经网络，使用拉格朗日对偶性预测自动驾驶车辆与周围车辆之间在MPC预测范围内的相关交互；2) 一个简化的随机MPC问题，消除不相关的避碰约束，提高计算效率。我们的方法在一个模拟交通路口中演示，展示了解决运动规划问题的12倍速提升。您可以在这里找到展示该架构在多个复杂交通场景中的视频：https://youtu.be/-TcMeolCLWc

    We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
    
[^2]: 加速平滑：一种可扩展的随机平滑方法

    Accelerated Smoothing: A Scalable Approach to Randomized Smoothing

    [https://arxiv.org/abs/2402.07498](https://arxiv.org/abs/2402.07498)

    本文提出了一种加速随机平滑的方法，通过训练替代了蒙特卡洛抽样，显著提高了计算效率，并在各种设置下展示了其在近似平滑分类器方面的精确性。

    

    随机平滑以其使用特定分布的平滑噪声确保平滑分类器的鲁棒性而成为一种有效的可证明的对抗性攻击防御方法。然而，这个过程中蒙特卡洛抽样的使用引入了一个计算密集型的因素，限制了在较大规模上实践随机平滑的可行性。为了解决这个局限，我们提出了一种新的方法，用训练替代了蒙特卡洛抽样。通过在各种设置下的广泛实验，我们展示了我们的方法在近似平滑分类器方面具有显著的精确性。此外，我们还证明了我们的方法显著加速了鲁棒半径认证过程，在计算时间上提供了近600倍的改进，克服了传统随机平滑中的计算瓶颈。

    Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.
    
[^3]: Masked LoGoNet：用于医学领域的快速准确3D图像分析

    Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain

    [https://arxiv.org/abs/2402.06190](https://arxiv.org/abs/2402.06190)

    本文介绍了一种名为LoGoNet的新型神经网络架构，采用自监督学习方法来应对医学图像分析中的挑战。LoGoNet通过采用大内核注意力和双重编码策略，灵活捕捉长、短距离特征相关性。这种创新的组合技术在医学图像分割中特别有益。

    

    标准的现代机器学习图像方法在医学应用中面临挑战，因为数据集构建的高成本和有限的标记训练数据。此外，这些方法在部署时通常用于每天处理大量数据，给医疗设施带来高维护成本。在本文中，我们引入了一种新的神经网络架构LoGoNet，采用定制的自监督学习（SSL）方法来缓解这些挑战。LoGoNet在U形架构内整合了一种新颖的特征提取器，利用大内核注意力（LKA）和双重编码策略，灵活地捕捉长、短距离特征相关性。这与现有方法依赖增加网络容量以增强特征提取的方式形成对比。我们模型中这些新技术的组合在医学图像分割中特别有益，考虑到其困难性。

    Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of le
    
[^4]: 隐性偏差与自注意力的快速收敛速率

    Implicit Bias and Fast Convergence Rates for Self-attention

    [https://arxiv.org/abs/2402.05738](https://arxiv.org/abs/2402.05738)

    该论文研究了在自注意力网络中使用梯度下降训练的隐性偏差以及其收敛速率，通过证明在特定数据设置下收敛性是全局的，并提供了W_t到W_mm的有限时间收敛率。

    

    自注意力是transformer的核心机制，它使其与传统神经网络有所区别，并驱动其出色的性能。为了开发自注意力的基本优化原则，我们研究了用梯度下降（GD）训练具有固定线性解码器的自注意力层在二元分类中的隐性偏差。受到在可分离数据上线性逻辑回归中GD的研究启发，最近的工作表明，随着迭代次数t无限接近于无穷大，键-查询矩阵W_t在局部上（相对于初始化方向）收敛到一个硬边界支持向量机解W_mm。我们的工作在四个方面增强了这个结果。首先，我们确定了非平凡的数据设置，对于这些设置，收敛性是全局的，并揭示了优化空间的特性。其次，我们首次提供了W_t到W_mm的有限时间收敛率，并量化了稀疏化的速率。

    Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in t
    
[^5]: 图上的去中心化双级优化: 无环算法更新和瞬态迭代复杂性

    Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity

    [https://arxiv.org/abs/2402.03167](https://arxiv.org/abs/2402.03167)

    本文提出了一种单循环的去中心化双级优化算法（D-SOBA），首次阐明了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA在渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性方面达到了最先进水平。

    

    随机双级优化（SBO）在处理嵌套结构方面的多样性使其在机器学习中变得越来越重要。为了解决大规模SBO，去中心化方法作为有效的范例出现，其中节点与直接相邻节点进行通信，无需中央服务器，从而提高通信效率和增强算法的稳健性。然而，当前的去中心化SBO算法面临挑战，包括昂贵的内部循环更新和对网络拓扑、数据异构性和嵌套双级算法结构的影响不明确。在本文中，我们引入了一种单循环的去中心化SBO（D-SOBA）算法，并建立了其瞬态迭代复杂性，首次澄清了网络拓扑和数据异构性对去中心化双级算法的共同影响。D-SOBA实现了最先进的渐近速率、渐近梯度/海森复杂性和瞬态梯度/海森复杂性。

    Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
    
[^6]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^7]: 样本、估计、聚合：因果发现基础模型的一种方法

    Sample, estimate, aggregate: A recipe for causal discovery foundation models

    [https://arxiv.org/abs/2402.01929](https://arxiv.org/abs/2402.01929)

    本文提出一种因果发现框架，通过深度学习模型预训练与经典发现算法的结合，实现了快速、准确地推断因果结构，并在实验中展示了与现有方法相比更好的表现和推理速度。

    

    因果发现是从数据中推断因果结构的任务，它可以加速科学研究、指导决策等。然而，现有因果发现算法的每个数据集的特性使它们变得缓慢、需要大量数据并且脆弱。受基础模型的启发，我们提出了一种因果发现框架，其中深度学习模型预训练用于处理在较小的变量子集上运行的经典发现算法的预测。这种方法可以利用以下观察结果：经典算法的输出在小问题上计算速度快，对（边际）数据结构具有信息量，且它们的输出结构作为对象在数据集之间可以进行比较。我们的方法在合成和实际数据集上实现了最先进的性能，可以推广到训练期间未见过的数据生成机制，并且提供比现有模型快几个数量级的推理速度。

    Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
    
[^8]: 在非静态上下文驱动环境中的在线强化学习

    Online Reinforcement Learning in Non-Stationary Context-Driven Environments

    [https://arxiv.org/abs/2302.02182](https://arxiv.org/abs/2302.02182)

    提出了一种名为LCPO的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧经验进行锚定来解决强化学习中的灾难性遗忘问题。

    

    我们研究了在非静态环境中的在线强化学习，其中一个随时间变化的外生上下文过程影响着环境动态。在线强化学习在这样的环境中具有挑战性，因为存在“灾难性遗忘”现象。随着训练过程中的新经验增加，代理 tend to forget 先前的知识。以往的方法通常假设任务标签（这在实践中往往是不存在的）或者使用脱机策略学习方法，但这些方法存在不稳定性和性能差的问题。我们提出了一种名为 Locally Constrained Policy Optimization (LCPO) 的在线强化学习方法，通过在优化当前经验回报的同时将策略对旧的经验进行锚定来解决灾难性遗忘问题。为了实现这种锚定，LCPO使用来自当前上下文分布之外的经验样本来局部约束策略优化。我们在Mujoco、经典控制和计算机系统环境中使用多种合成和真实上下文跟踪，评估了LCPO的性能，并发现它能够取得令人满意的结果。

    We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
    
[^9]: 一种用于跳跃扩散模型期权定价的深度隐式-显式最小移动方法

    A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models. (arXiv:2401.06740v1 [q-fin.CP])

    [http://arxiv.org/abs/2401.06740](http://arxiv.org/abs/2401.06740)

    这份论文介绍了一种用于定价跳跃扩散模型下欧式篮式期权的深度学习方法，采用了隐式-显式最小移动方法以及残差型人工神经网络逼近，并通过稀疏网格高斯-埃尔米特逼近和基于ANN的高维专用求积规则来离散化积分运算符。

    

    我们提出了一种新颖的深度学习方法，用于定价跳跃扩散动态下的欧式篮式期权。将期权定价问题表述为一个偏积分微分方程，并通过一种新的隐式-显式最小移动时间步法进行近似，该方法使用深度残差型人工神经网络（ANNs）逐步逼近。积分运算符通过两种不同的方法离散化：a）通过稀疏网格高斯-埃尔米特逼近，采用奇异值分解产生的局部坐标轴，并且b）通过基于ANN的高维专用求积规则。关键是，所提出的ANN的构造确保了解决方案在标的资产较大值时的渐近行为，并且与解决方案先验已知的定性特性相一致输出。对方法维度的性能和鲁棒性进行了评估。

    We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assesse
    
[^10]: MoTCoder: 使用思维模块提升大型语言模型在具有挑战性的编程任务中的能力。

    MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15960](http://arxiv.org/abs/2312.15960)

    MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。

    

    大型语言模型(LLMs)在处理简单的编程任务方面展示出了令人印象深刻的能力。然而，当面对更具挑战性的编程问题时，它们的性能往往表现不佳。我们观察到传统模型往往生成作为单一代码块的解决方案，限制了它们在解决复杂问题上的有效性。为了克服这个限制，我们提出了Modular-of-Thought Coder (MoTCoder)。我们引入了一种创新的MoT指令调整框架，旨在促进将任务分解为逻辑子任务和子模块。我们的研究发现，通过培养和利用子模块，MoTCoder显著提高了生成解决方案的模块化和正确性，导致在APPS上相对pass@1改进了12.9%，在CodeContests上相对pass@1改进了9.43%。我们的代码可在https://github.com/dvlab-research/MoTCoder获得。

    Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
    
[^11]: ADMM训练算法用于残差网络：收敛性，复杂度和并行训练

    ADMM Training Algorithms for Residual Networks: Convergence, Complexity and Parallel Training. (arXiv:2310.15334v1 [cs.LG])

    [http://arxiv.org/abs/2310.15334](http://arxiv.org/abs/2310.15334)

    本论文设计了一系列序列和并行的ADMM算法解决残差网络训练问题，并通过理论分析证明了算法的收敛性和收敛速度，同时分析了并行实现的时间复杂性和内存消耗的优势。

    

    我们通过引入辅助变量，设计了一系列序列和并行的近端点（梯度）ADMM算法来解决完全连接的残差网络（FCResNets）训练问题。通过基于Kurdyka-Lojasiewicz（KL）属性分析框架的证明，我们证明了近端点版本的收敛性，并且可以确保在不同的Kurdyka-Lojasiewicz（KL）指数范围内，实现局部R-线性或亚线性收敛速度。此外，我们从理论上分析了并行实现在时间复杂性和（每个节点的）内存消耗方面的优势。据我们所知，这是第一个从理论上分析应用于FCResNets训练问题的ADMM算法的收敛性，收敛速度，时间复杂性和（每个节点的）内存需求的工作。我们报告了实验结果，展示了高速度，更好的性能，鲁棒性和潜力。

    We design a series of serial and parallel proximal point (gradient) ADMMs for the fully connected residual networks (FCResNets) training problem by introducing auxiliary variables. Convergence of the proximal point version is proven based on a Kurdyka-Lojasiewicz (KL) property analysis framework, and we can ensure a locally R-linear or sublinear convergence rate depending on the different ranges of the Kurdyka-Lojasiewicz (KL) exponent, in which a necessary auxiliary function is constructed to realize our goal. Moreover, the advantages of the parallel implementation in terms of lower time complexity and less (per-node) memory consumption are analyzed theoretically. To the best of our knowledge, this is the first work analyzing the convergence, convergence rate, time complexity and (per-node) runtime memory requirement of the ADMM applied in the FCResNets training problem theoretically. Experiments are reported to show the high speed, better performance, robustness and potential in the 
    
[^12]: 从隐私保护数据中进行条件密度估计

    Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])

    [http://arxiv.org/abs/2310.12781](http://arxiv.org/abs/2310.12781)

    本文提出了一种从隐私保护数据中进行条件密度估计的方法，使用神经条件密度估计器来近似模型参数的后验分布，从而解决了在统计分析过程中只能访问私有化数据导致的计算复杂度增加的问题。

    

    许多现代统计分析和机器学习应用需要在敏感用户数据上进行模型训练。差分隐私提供了一种正式的保证，即个体用户信息不会泄露。在这个框架下，随机算法向保密数据注入校准的噪声，从而产生隐私保护的数据集或查询。然而，在统计分析过程中只能访问私有化数据会导致计算复杂度增加，难以对基础机密数据的参数进行有效的推理。在本工作中，我们提出了基于隐私保护数据集的基于模拟的推理方法。具体而言，我们使用神经条件密度估计器作为一组灵活的分布来近似给定观测到的私有查询结果的模型参数的后验分布。我们在传染病模型下的离散时间序列数据以及普通线性回归模型上说明了我们的方法。

    Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
    
[^13]: 优化拟合因子分析与通货紧缩变量旋转

    Optimal vintage factor analysis with deflation varimax. (arXiv:2310.10545v1 [stat.ML])

    [http://arxiv.org/abs/2310.10545](http://arxiv.org/abs/2310.10545)

    本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。

    

    通货紧缩变量旋转是一种重要的因子分析方法，旨在首先找到原始数据的低维表示，然后寻求旋转，使旋转后的低维表示具有科学意义。尽管Principal Component Analysis (PCA) followed by the varimax rotation被广泛应用于拟合因子分析，但由于varimax rotation需要在正交矩阵集合上解非凸优化问题，因此很难提供理论保证。本文提出了一种逐行求解正交矩阵的通货紧缩变量旋转过程。除了在计算上的优势和灵活性之外，我们还能在广泛的背景下对所提出的过程进行完全的理论保证。在PCA之后采用这种新的varimax方法作为第二步，我们进一步分析了这个两步过程在一个更一般的因子模型的情况下。

    Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.  In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.  Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our resul
    
[^14]: 使用最优传输正则化差异来提高对抗性鲁棒深度学习

    Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])

    [http://arxiv.org/abs/2309.03791](http://arxiv.org/abs/2309.03791)

    本论文介绍了一种新的方法ARMOR_D来加强深度学习模型的对抗鲁棒性，该方法基于最优传输正则化差异，通过在分布的邻域上进行最大化期望损失来实现。实验证明，ARMOR_D方法在恶意软件检测和图像识别应用中能够优于现有方法，在对抗攻击下的鲁棒性方面具有较好的效果。

    

    我们引入了ARMOR_D方法作为增强深度学习模型对抗性鲁棒性的创新方法。这些方法基于一种新的最优传输正则化差异类，通过信息差异和最优传输成本之间的infimal卷积构建。我们使用这些方法来增强对抗性鲁棒性，通过在分布的邻域上最大化期望损失，这被称为分布鲁棒优化技术。作为构建对抗样本的工具，我们的方法允许样本根据最优传输成本进行传输，并根据信息差异进行重新加权。我们在恶意软件检测和图像识别应用上证明了我们方法的有效性，并发现在增强对抗攻击鲁棒性方面，据我们所知，它优于现有方法。ARMOR_D在FGSM攻击下的robustified准确率达到98.29%，在其他攻击下达到98.18%。

    We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ aga
    
[^15]: 使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统

    Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])

    [http://arxiv.org/abs/2307.14906](http://arxiv.org/abs/2307.14906)

    本文介绍了一种使用优化的负采样和损失函数扩展基于会话的Transformer推荐系统，该系统在大规模电商数据集上通过集成负采样和列表损失函数实现了较高的推荐准确性，并在实践中表现出潜力。

    

    本文介绍了TRON，一种使用优化的负采样的可扩展的基于会话的Transformer推荐系统。受到SASRec和GRU4Rec+等现有模型在可扩展性和性能方面的限制，TRON集成了top-k负采样和列表损失函数，以提高其推荐准确性。在相关的大规模电子商务数据集上的评估结果表明，TRON在保持与SASRec类似的训练速度的同时，改进了当前方法的推荐质量。一项实时的A/B测试显示，相对于SASRec，TRON的点击率增加了18.14%，突显了其在实际环境中的潜力。

    This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
    
[^16]: Forward-Forward算法训练的网络中的突现表征

    Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])

    [http://arxiv.org/abs/2305.18353](http://arxiv.org/abs/2305.18353)

    研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。

    

    Backpropagation算法被广泛用于训练神经网络，但其缺乏生物学上的现实性。为了寻找一种更具生物学可行性的替代方案，并避免反向传播梯度，而是使用本地学习规则，最近介绍的Forward-Forward算法将Backpropagation的传递替换为两个前向传递。本研究表明，使用Forward-Forward算法获得的内部表征组织为稳健的，类别特定的集合，由极少量的有效单元(高稀疏度)组成。这与感觉处理过程中观察到的皮层表征非常相似。虽然在使用标准Backpropagation进行训练的模型中没有发现，但是在使用与Forward-Forward相同的训练目标进行优化的网络中也出现了稀疏性。这些结果表明，Forward-Forward提议的学习过程可能更接近生物学学习的现实情况。

    The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
    
[^17]: 大型语言模型能否能够很好地玩文字游戏？现状和未来问题研究

    Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])

    [http://arxiv.org/abs/2304.02868](http://arxiv.org/abs/2304.02868)

    本文探究大型语言模型在玩文字游戏的能力，并发现其表现有竞争力，但仍然缺乏智能，有待提升。

    

    最近，诸如ChatGPT和GPT-4之类的大型语言模型展示了它们与人类用户通信的卓越能力。本技术报告旨在调查它们在玩文字游戏方面的能力，这要求玩家通过与游戏世界的对话来理解环境并对情况做出反应。我们的实验表明，与所有现有系统相比，ChatGPT表现出有竞争力，但仍然表现出较低的智能水平。确切地说，ChatGPT无法通过玩游戏或阅读游戏手册来构建世界模型；它可能无法利用它已经拥有的世界知识；它无法推断出随着游戏进展的每一步的目标。我们的结果在人工智能、机器学习和自然语言处理交叉领域开启了新的研究问题。

    Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
    
[^18]: 利用长短期记忆网络提高量子电路保真度

    Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])

    [http://arxiv.org/abs/2303.17523](http://arxiv.org/abs/2303.17523)

    本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。

    

    量子计算已进入噪声中间规模量子（NISQ）时代，目前我们拥有的量子处理器对辐射和温度等环境变量敏感，因此会产生嘈杂的输出。虽然已经有许多算法和应用程序用于NISQ处理器，但我们仍面临着解释其嘈杂结果的不确定性。具体来说，我们对所选择的量子态有多少信心？这种信心很重要，因为NISQ计算机将输出其量子位测量的概率分布，有时很难区分分布是否表示有意义的计算或只是随机噪声。本文提出了一种新方法来解决这个问题，将量子电路保真度预测框架为时间序列预测问题，因此可以利用长短期记忆（LSTM）神经网络的强大能力。一个完整的工作流程来构建训练电路

    Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
    
[^19]: 在线属性选择的可解释的小样本学习

    Interpretable Few-shot Learning with Online Attribute Selection. (arXiv:2211.09107v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09107](http://arxiv.org/abs/2211.09107)

    本文提出了一种在线属性选择机制的天然可解释模型来处理小样本学习，通过减少每个episode中涉及的属性数量提高准确性和可解释性，同时自动检测并补偿人工智能属性池不足的episode。

    

    小样本学习(few-shot learning, FSL)是一种挑战性的学习问题，每个类别只有很少的样本可用。在FSL中决策的解释比传统分类更加重要，因为错误的几率更大。然而，大多数以前的FSL方法都是黑匣子模型。本文提出了一种基于易于理解的属性的天然可解释模型来处理FSL。此外，我们提出了一种在线属性选择机制，以有效过滤每个episode中不相关的属性。该属性选择机制通过减少每个episode中涉及的属性数量来提高准确性和可解释性。我们提出了一种机制，自动检测人工智能属性池不足的episode，并通过涉及学习的未知属性来补偿。我们证明了所提出的方法可以实现与黑匣子小样本学习模型相当的结果。

    Few-shot learning (FSL) is a challenging learning problem in which only a few samples are available for each class. Decision interpretation is more important in few-shot classification since there is a greater chance of error than in traditional classification. However, most of the previous FSL methods are black-box models. In this paper, we propose an inherently interpretable model for FSL based on human-friendly attributes. Moreover, we propose an online attribute selection mechanism that can effectively filter out irrelevant attributes in each episode. The attribute selection mechanism improves the accuracy and helps with interpretability by reducing the number of participated attributes in each episode. We propose a mechanism that automatically detects the episodes where the pool of human-friendly attributes are not adequate, and compensates by engaging learned unknown attributes. We demonstrate that the proposed method achieves results on par with black-box few-shot-learning model
    
[^20]: Wasserstein多元自回归模型用于建模分布时间序列及其在图形学习中的应用

    Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.05442](http://arxiv.org/abs/2207.05442)

    本文提出了一种新的自回归模型，用于分析多元分布时间序列。并且在Wasserstein空间中建模了随机对象，提供了该模型的解的存在性和一致估计器。此方法可以应用于年龄分布和自行车共享网络的观察数据。

    

    我们提出了一种新的自回归模型，用于统计分析多元分布时间序列。感兴趣的数据包括一组在实线有界间隔上支持的概率测度的多个系列，并且被不同时间瞬间所索引。概率测度被建模为Wasserstein空间中的随机对象。我们通过在Lebesgue测度的切空间中建立自回归模型，首先对所有原始测度进行居中处理，以便它们的Fréchet平均值成为Lebesgue测度。利用迭代随机函数系统的理论，提供了这样一个模型的解的存在性、唯一性和平稳性的结果。我们还提出了模型系数的一致估计器。除了对模拟数据的分析，我们还使用两个实际数据集进行了模型演示：一个是不同国家年龄分布的观察数据集，另一个是巴黎自行车共享网络的观察数据集。

    We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
    

