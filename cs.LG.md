# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling](https://arxiv.org/abs/2404.01685) | 通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学在实验中表现出更高的准确性。 |
| [^2] | [Functional Bilevel Optimization for Machine Learning](https://arxiv.org/abs/2403.20233) | 介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。 |
| [^3] | [Croissant: A Metadata Format for ML-Ready Datasets](https://arxiv.org/abs/2403.19546) | Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。 |
| [^4] | [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103) | PRISM是一种算法，可以自动识别人类可解释且易传递的提示，从而有效生成所需概念，仅使用黑盒访问T2I模型。 |
| [^5] | [Can ChatGPT predict article retraction based on Twitter mentions?](https://arxiv.org/abs/2403.16851) | 本研究探讨了ChatGPT是否能够基于Twitter提及来预测文章的撤回，研究发现在预测未来被撤回的有问题文章方面是具有一定潜力的。 |
| [^6] | [Auditing Fairness under Unobserved Confounding](https://arxiv.org/abs/2403.14713) | 在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。 |
| [^7] | [Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices](https://arxiv.org/abs/2403.12278) | 随机舍入技术能有效隐式正则化高瘦矩阵，确保舍入后的矩阵具有完整的列秩。 |
| [^8] | [Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations](https://arxiv.org/abs/2403.08121) | 本文研究了训练深度齐次神经网络时梯度流动力学的动态性，发现在足够小的初始化下，神经网络的权重在训练早期阶段保持较小规范，并且沿着神经相关函数的KKT点方向近似收敛。 |
| [^9] | [Speech Robust Bench: A Robustness Benchmark For Speech Recognition](https://arxiv.org/abs/2403.07937) | 提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。 |
| [^10] | [A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries](https://arxiv.org/abs/2403.05720) | 介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略 |
| [^11] | [SPEAR:Exact Gradient Inversion of Batches in Federated Learning](https://arxiv.org/abs/2403.03945) | 该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。 |
| [^12] | [Non-Convex Stochastic Composite Optimization with Polyak Momentum](https://arxiv.org/abs/2403.02967) | 本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。 |
| [^13] | [Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad](https://arxiv.org/abs/2403.02648) | KATE是一种新的优化算法，提出了一种与AdaGrad标度不变的适应方法，并在广义线性模型和一般的非凸问题中证明了其标度不变性。数值实验结果表明，KATE在各种场景中均优于AdaGrad并与Adam性能匹配/超越。 |
| [^14] | [RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval](https://arxiv.org/abs/2402.18510) | 本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。 |
| [^15] | [Fusion Encoder Networks](https://arxiv.org/abs/2402.15883) | FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。 |
| [^16] | [On the Stability of Gradient Descent for Large Learning Rate](https://arxiv.org/abs/2402.13108) | 本文研究了线性神经网络在二次损失函数下的优化问题，证明了梯度下降映射的非奇异性以及全局最小值点集的光滑流形特性，为理解大学习率下梯度下降的稳定性提供了重要线索。 |
| [^17] | [Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329) | 该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。 |
| [^18] | [Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting](https://arxiv.org/abs/2402.12220) | 这项研究展示了如何利用贝叶斯学习技术应用于参数高效微调，以防止灾难性遗忘，实现了预训练知识的保留，并在语言建模和语音合成任务中取得成功。 |
| [^19] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^20] | [Momentum Approximation in Asynchronous Private Federated Learning](https://arxiv.org/abs/2402.09247) | 本文提出了动量近似方法，在异步私有联邦学习（FL）中有效结合了动量和异步协议的技术，通过最小化动量更新的偏差来改进模型性能。实证研究证明了动量近似在基准FL数据集上的有效性。 |
| [^21] | [Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models](https://arxiv.org/abs/2402.09236) | 本研究将因果表示学习和基础模型相结合，研究了如何从数据中学习人类可解释的概念。实验证明了这一统一方法的实用性。 |
| [^22] | [Personalized Language Modeling from Personalized Human Feedback](https://arxiv.org/abs/2402.05133) | 该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。 |
| [^23] | [ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection](https://arxiv.org/abs/2402.03235) | 这项工作提出了一种用于多模态3D物体检测的主动学习框架ActiveAnno3D。通过选择最具信息量的训练数据样本进行标注，我们能够在使用一半的训练数据时实现与传统方法相近的检测性能。 |
| [^24] | [FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion](https://arxiv.org/abs/2402.03226) | 本论文提出了一种名为FuseMoE的专家混合Transformer框架，通过创新的门控函数实现灵活融合多模态数据，能够有效地处理缺失模态和不规则采样数据，同时改善模型的预测性能，在临床风险预测任务中具有实际应用价值。 |
| [^25] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^26] | [Dynamic Incremental Optimization for Best Subset Selection](https://arxiv.org/abs/2402.02322) | 本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。 |
| [^27] | [GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting](https://arxiv.org/abs/2401.07958) | GD-CAF提出了一种新颖的方法，将降水预报作为一个时空图序列预报问题，利用图形双流卷积注意力融合来学习历史降水图并在不同空间位置上预测未来的降水。 |
| [^28] | [Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound](https://arxiv.org/abs/2202.05560) | 该研究提出了一种PAC-Bayes界限，能够同时控制多个错误，并提供丰富的信息，适用于回归中测试损失分布或分类中不同错误分类的概率。 |
| [^29] | [Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders.](http://arxiv.org/abs/2401.13009) | 对于循环模型中含有隐藏因变量的因果发现，已经出现了能够处理这种情况的多种技术方法。 |
| [^30] | [Binary Feature Mask Optimization for Feature Selection.](http://arxiv.org/abs/2401.12644) | 这个论文提出了一种新颖的特征选择框架，通过使用特征屏蔽方法来消除特征，而不是从数据集中移除它们。这种方法不需要重新训练机器学习模型，可以综合考虑特征子集的重要性，为通用机器学习模型的特征选择问题提供了一种新的解决方案。 |
| [^31] | [xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein.](http://arxiv.org/abs/2401.06199) | xTrimoPGLM是一个统一的100亿规模预训练蛋白质语言模型，能够同时处理蛋白质理解和生成任务，通过创新的预训练框架和大规模的参数训练，显著优于其他先进模型，在18个蛋白理解基准测试中取得了成功，并能够实现对蛋白质结构的原子分辨率观察。 |
| [^32] | [Transportation Market Rate Forecast Using Signature Transform.](http://arxiv.org/abs/2401.04857) | 本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。 |
| [^33] | [IoT in the Era of Generative AI: Vision and Challenges.](http://arxiv.org/abs/2401.01923) | 在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。 |
| [^34] | [Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment.](http://arxiv.org/abs/2311.01059) | 本研究提出了一种名为ROAM的方法，通过利用先前学习到的行为来实时调节机器人在部署过程中应对未曾见过的情况。在测试中，ROAM可以在单个阶段内实现快速适应，并且在模拟环境和真实场景中取得了成功，具有较高的效率和适应性。 |
| [^35] | [Learning State-Augmented Policies for Information Routing in Communication Networks.](http://arxiv.org/abs/2310.00248) | 本论文研究了在通信网络中的信息路由问题，提出了一种新颖的状态增强策略，通过部署图神经网络架构，利用图卷积来最大化源节点的聚合信息，从而有效地将所需信息路由到目标节点。 |
| [^36] | [Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness.](http://arxiv.org/abs/2308.04137) | 通过综合评估深度学习分类器的性能，发现它们缺乏稳定性和可靠性，并建议采用广泛的数据类型和统一的评估指标进行性能基准测试。 |
| [^37] | [Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning.](http://arxiv.org/abs/2306.09273) | 这篇论文提出了一种针对值函数算法和梯度算法的攻击方法，利用梯度反转重建状态、动作和监督信号，以解决嵌入式人工智能中的隐私泄露问题。 |
| [^38] | [Data Augmentation for Seizure Prediction with Generative Diffusion Model.](http://arxiv.org/abs/2306.08256) | 该论文提出了一种基于扩散模型的数据增强方法DiffEEG，可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。 |
| [^39] | [Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation.](http://arxiv.org/abs/2306.01423) | 本文提出了一种新的深度优化器FAME，使用三重指数移动平均值（TEMA）来估计梯度矩，提供更丰富和准确的数据变化和趋势信息，可以提高计算机视觉等领域中模型的性能表现。 |
| [^40] | [Optimal partition of feature using Bayesian classifier.](http://arxiv.org/abs/2304.14537) | 本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战，并且证明该技术在不同数据集上具有更高的准确率和更低的错误率。 |
| [^41] | [Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments.](http://arxiv.org/abs/2304.09825) | 本研究旨在提高程序生成环境中强化学习的样本效率。研究证明，使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提高效率。 |
| [^42] | [Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs.](http://arxiv.org/abs/2303.13763) | 本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。 |
| [^43] | [Revisiting DeepFool: generalization and improvement.](http://arxiv.org/abs/2303.12481) | 本文提出了一种新的对抗性攻击，该攻击是广义了DeepFool攻击，既有效又计算效率高，适用于评估大型深度神经网络的鲁棒性。 |
| [^44] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |

# 详细

[^1]: 通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学

    A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling

    [https://arxiv.org/abs/2404.01685](https://arxiv.org/abs/2404.01685)

    通过核大小缩放提高嵌入式脉冲神经网络准确性的方法学在实验中表现出更高的准确性。

    

    脉冲神经网络（SNNs）由于其稀疏的基于脉冲的操作而能为基于机器学习的应用提供超低功耗/能耗。目前，大多数SNN架构需要更大的模型大小才能实现更高的准确性，这对资源受限的嵌入式应用不太适合。因此，迫切需要开发能够以可接受的内存占用实现高准确性的SNNs。为此，我们提出了一种通过核大小缩放提高SNNs准确性的新方法学。其关键步骤包括调查不同核大小对准确性的影响，设计新的核大小集合，基于选定的核大小生成SNN架构，并分析SNN模型选择的准确性-内存折衷。实验结果表明，我们的方法学在准确性方面优于最先进的方法（对于CIFAR10有93.24%的准确度）

    arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
    
[^2]: 机器学习中的函数双层优化

    Functional Bilevel Optimization for Machine Learning

    [https://arxiv.org/abs/2403.20233](https://arxiv.org/abs/2403.20233)

    介绍了机器学习中的函数双层优化问题，提出了不依赖于强凸假设的方法，并展示了在仪表回归和强化学习任务中使用神经网络的优势。

    

    在本文中，我们介绍了针对机器学习中的双层优化问题的一种新的函数视角，其中内部目标在函数空间上被最小化。这些类型的问题通常通过在参数设置下开发的方法来解决，其中内部目标对于预测函数的参数强凸。函数视角不依赖于此假设，特别允许使用超参数化的神经网络作为内部预测函数。我们提出了可扩展和高效的算法来解决函数双层优化问题，并展示了我们方法在适合自然函数双层结构的仪表回归和强化学习任务上的优势。

    arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
    
[^3]: Croissant：一种面向机器学习数据集的元数据格式

    Croissant: A Metadata Format for ML-Ready Datasets

    [https://arxiv.org/abs/2403.19546](https://arxiv.org/abs/2403.19546)

    Croissant是一种面向机器学习数据集的元数据格式，使数据集更易发现、可移植和互操作，有助于解决ML数据管理和负责任AI中的重要挑战。

    

    数据是机器学习（ML）的关键资源，但处理数据仍然是一个主要的摩擦点。本文介绍了Croissant，一种用于数据集的元数据格式，简化了数据被ML工具和框架使用的方式。Croissant使数据集更易发现、可移植和互操作，从而解决了ML数据管理和负责任AI中的重要挑战。Croissant已得到几个流行数据集库的支持，涵盖数十万个数据集，可以加载到最流行的ML框架中。

    arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
    
[^4]: 用于个性化文本到图像生成的自动化黑盒提示工程

    Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation

    [https://arxiv.org/abs/2403.19103](https://arxiv.org/abs/2403.19103)

    PRISM是一种算法，可以自动识别人类可解释且易传递的提示，从而有效生成所需概念，仅使用黑盒访问T2I模型。

    

    提示工程对于控制文本到图像（T2I）生成模型的输出是有效的，但由于需要手动制作提示而导致工作繁重。这一挑战促使了自动提示生成算法的发展。然而，这些方法通常在T2I模型之间的可传递性方面遇到困难，需要对基础模型进行白盒访问，并产生非直观的提示。在这项工作中，我们介绍了PRISM，这是一种算法，可以仅使用黑盒访问T2I模型就自动识别人类可解释且易传递的提示，从而有效生成所需概念。受大型语言模型（LLM）越狱的启发，PRISM利用LLM的上下文学习能力来迭代地改进给定参考图像的候选提示分布。我们的实验展示了PRISM在为对象、样式等生成准确提示方面的多样性和有效性。

    arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
    
[^5]: ChatGPT是否能够基于Twitter提及来预测文章的撤回？

    Can ChatGPT predict article retraction based on Twitter mentions?

    [https://arxiv.org/abs/2403.16851](https://arxiv.org/abs/2403.16851)

    本研究探讨了ChatGPT是否能够基于Twitter提及来预测文章的撤回，研究发现在预测未来被撤回的有问题文章方面是具有一定潜力的。

    

    检测有问题的研究文章具有重要意义，本研究探讨了根据被撤回文章在Twitter上的提及是否能够在文章被撤回前发出信号，从而在预测未来被撤回的有问题文章方面发挥作用。分析了包括3,505篇已撤回文章及其相关Twitter提及在内的数据集，以及使用粗糙精确匹配方法获取的具有类似特征的3,505篇未撤回文章。通过四种预测方法评估了Twitter提及在预测文章撤回方面的有效性，包括手动标注、关键词识别、机器学习模型和ChatGPT。手动标注的结果表明，的确有被撤回的文章，其Twitter提及包含在撤回前发出信号的可识别证据，尽管它们只占所有被撤回文章的一小部分。

    arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
    
[^6]: 在未观测混杂因素下审计公平性

    Auditing Fairness under Unobserved Confounding

    [https://arxiv.org/abs/2403.14713](https://arxiv.org/abs/2403.14713)

    在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。

    

    决策系统中的一个基本问题是跨越人口统计线存在不公平性。然而，不公平性可能难以量化，特别是如果我们对公平性的理解依赖于难以衡量的风险等观念（例如，对于那些没有其治疗就会死亡的人平等获得治疗）。审计这种不公平性需要准确测量个体风险，而在未观测混杂的现实环境中，难以估计。在这些未观测到的因素“解释”明显差异的情况下，我们可能低估或高估不公平性。在本文中，我们展示了即使在放宽或（令人惊讶地）甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以对高风险个体的分配率给出信息丰富的界限。我们利用了在许多实际环境中（例如引入新型治疗）我们拥有在任何分配之前的数据的事实。

    arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
    
[^7]: 随机舍入隐式正则化高瘦矩阵

    Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices

    [https://arxiv.org/abs/2403.12278](https://arxiv.org/abs/2403.12278)

    随机舍入技术能有效隐式正则化高瘦矩阵，确保舍入后的矩阵具有完整的列秩。

    

    受到随机舍入在机器学习和大规模深度神经网络模型训练中的流行，我们考虑实矩阵$\mathbf{A}$的随机近似舍入，其中行数远远多于列数。我们提供了新颖的理论证据，并通过大量实验评估支持，高概率下，随机舍入矩阵的最小奇异值远离零--无论$\mathbf{A}$接近奇异还是$\mathbf{A}$奇异。换句话说，随机舍入\textit{隐式正则化}高瘦矩阵$\mathbf{A}$，使得舍入后的版本具有完整的列秩。我们的证明利用了随机矩阵理论中的有力结果，以及随机舍入误差不集中在低维列空间的思想。

    arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
    
[^8]: 早期方向性收敛在深度齐次神经网络中进行小初始化时的分析

    Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations

    [https://arxiv.org/abs/2403.08121](https://arxiv.org/abs/2403.08121)

    本文研究了训练深度齐次神经网络时梯度流动力学的动态性，发现在足够小的初始化下，神经网络的权重在训练早期阶段保持较小规范，并且沿着神经相关函数的KKT点方向近似收敛。

    

    本文研究了训练深度齐次神经网络时梯度流动力学的动态性，这些网络从小初始化开始。本文考虑到具有局部Lipschitz梯度和阶数严格大于两的神经网络。文章证明了对于足够小的初始化，在训练的早期阶段，神经网络的权重保持规范较小，并且在Karush-Kuhn-Tucker (KKT)点处近似沿着神经相关函数的方向收敛。此外，对于平方损失并在神经网络权重上进行可分离假设的情况下，还展示了在损失函数的某些鞍点附近梯度流动动态的类似方向性收敛。

    arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
    
[^9]: 语音鲁棒基准：用于语音识别的鲁棒性基准

    Speech Robust Bench: A Robustness Benchmark For Speech Recognition

    [https://arxiv.org/abs/2403.07937](https://arxiv.org/abs/2403.07937)

    提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。

    

    随着自动语音识别（ASR）模型变得越来越普遍，确保它们在物理世界和数字世界中的各种破坏下进行可靠预测变得愈发重要。我们提出了语音鲁棒基准（SRB），这是一个用于评估ASR模型对各种破坏的鲁棒性的全面基准。SRB由69个输入扰动组成，旨在模拟ASR模型可能在物理世界和数字世界中遇到的各种破坏。我们使用SRB来评估几种最先进的ASR模型的鲁棒性，并观察到模型大小和某些建模选择（如离散表示和自我训练）似乎有助于提高鲁棒性。我们将此分析扩展到衡量ASR模型在来自各种人口亚组的数据上的鲁棒性，即英语和西班牙语使用者以及男性和女性，并观察到模型的鲁棒性在不同亚组之间存在明显差异。

    arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
    
[^10]: 用于生成简要住院病程摘要的领域自适应大语言模型的基准测试

    A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries

    [https://arxiv.org/abs/2403.05720](https://arxiv.org/abs/2403.05720)

    介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略

    

    简要住院病程（BHC）摘要是通过总结临床记录而生成的常见临床文件。虽然大型语言模型（LLMs）在自动化实际任务方面展现出显著能力，但它们在医疗应用（如BHC合成）中的能力尚未得到展示。为了使LLMs能够适应BHC合成，我们引入了一个新颖的基准测试，其中包含从MIMIC-IV记录中提取的经过预处理的数据集，封装了临床记录和简要住院病程（BHC）对。我们评估了两个通用LLMs和三个医疗领域适应的LLMs的性能，以改进从临床记录生成BHC。我们使用临床记录作为输入来生成BHC，采用基于提示的（使用上下文学习）和基于微调的自适应策略来应用于三个开源LLMs（Clinical-T5-Large，Llama2-13B，FLAN-UL2）和两个专有LLMs（GPT-3.5，GPT-4）。我们定量评估了性能。

    arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
    
[^11]: SPEAR：联邦学习中批量精确梯度反演

    SPEAR:Exact Gradient Inversion of Batches in Federated Learning

    [https://arxiv.org/abs/2403.03945](https://arxiv.org/abs/2403.03945)

    该论文提出了第一个能够精确重构批量$b >1$的算法，在联邦学习中解决了梯度反演攻击的问题。

    

    联邦学习是一种流行的协作机器学习框架，在这个框架中，多个客户端仅与服务器共享他们本地数据的梯度更新，而不是实际数据。不幸的是，最近发现梯度反演攻击可以从这些共享的梯度中重构出数据。现有的攻击只能在重要的诚实但好奇设置中对批量大小为$b=1$的数据进行精确重构，对于更大的批量只能进行近似重构。在这项工作中，我们提出了\emph{第一个准确重建批量$b >1$的算法}。这种方法结合了对梯度显式低秩结构的数学见解和基于采样的算法。关键的是，我们利用ReLU诱导的梯度稀疏性，精确地过滤掉大量错误的样本，使最终的重建步骤可行。我们为全连接提供了高效的GPU实现

    arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
    
[^12]: 具有Polyak动量的非凸随机复合优化

    Non-Convex Stochastic Composite Optimization with Polyak Momentum

    [https://arxiv.org/abs/2403.02967](https://arxiv.org/abs/2403.02967)

    本文研究了具有Polyak动量的随机近端梯度方法，在非凸复合优化问题中实现了最佳收敛速度，无论批量大小如何。

    

    随机近端梯度法是广泛使用的随机梯度下降（SGD）方法的一个强大泛化，在机器学习中已经被广泛应用。然而，众所周知，当随机噪声显著时（即仅使用小型或有界批量大小时），该方法在非凸环境中无法收敛。本文关注具有Polyak动量的随机近端梯度方法。我们证明了该方法对于非凸复合优化问题实现了最佳收敛速度，而批量大小大小无关。此外，我们对Polyak动量在复合优化环境中的方差减少效应进行了严格分析，并且我们证明了当近端步骤只能通过近似解来求解时，该方法也会收敛。最后，我们提供了数值实验来验证我们的理论结果。

    arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
    
[^13]: 移除平方根：一种新的高效标度不变版本的AdaGrad

    Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad

    [https://arxiv.org/abs/2403.02648](https://arxiv.org/abs/2403.02648)

    KATE是一种新的优化算法，提出了一种与AdaGrad标度不变的适应方法，并在广义线性模型和一般的非凸问题中证明了其标度不变性。数值实验结果表明，KATE在各种场景中均优于AdaGrad并与Adam性能匹配/超越。

    

    自适应方法在机器学习中非常流行，因为它们可以降低学习速率调整的成本。本文引入了一种名为KATE的新型优化算法，它提出了一个著名的AdaGrad算法的标度不变适应。我们证明了KATE在广义线性模型案例中的标度不变性。此外，对于一般的光滑非凸问题，我们为KATE建立了一个收敛速率为$O \left(\frac{\log T}{\sqrt{T}} \right)$，与AdaGrad和Adam的最佳收敛速率相匹配。我们还通过不同问题的数值实验将KATE与其他最先进的自适应算法Adam和AdaGrad进行了比较，包括在真实数据上进行图像分类和文本分类等复杂机器学习任务。结果表明，在所有考虑到的场景中，KATE始终胜过AdaGrad，并且在性能上匹配/超越Adam。

    arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
    
[^14]: RNNs还不是Transformer：在上下文检索中的关键瓶颈

    RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval

    [https://arxiv.org/abs/2402.18510](https://arxiv.org/abs/2402.18510)

    本文研究了RNNs和Transformer在处理算法问题时的表现能力差距，发现RNNs存在关键瓶颈，即无法完美地从上下文中检索信息，导致无法像Transformer那样轻松解决需要这种能力的任务。

    

    本文探讨循环神经网络（RNNs）和Transformer在解决算法问题时的表示能力差距。我们重点关注RNNs是否能在处理长序列时，通过Chain-of-Thought (CoT)提示，与Transformer的性能相匹配。我们的理论分析显示CoT可以改进RNNs，但无法弥补与Transformer之间的差距。关键瓶颈在于RNNs无法完全从上下文中检索信息，即使经过CoT的增强：对于几个明确或隐式需要这种能力的任务，如联想召回和确定图是否为树，我们证明RNNs表达能力不足以解决这些任务，而Transformer可以轻松解决。相反，我们证明采用增强RNNs上下文检索能力的技术，包括

    arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
    
[^15]: 融合编码器网络

    Fusion Encoder Networks

    [https://arxiv.org/abs/2402.15883](https://arxiv.org/abs/2402.15883)

    FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。

    

    在本文中，我们提出了一种名为融合编码器网络（FENs）的算法类：用于创建将固定长度序列映射到输出的神经网络。生成的神经网络仅具有对数深度（减轻数据在网络中传播时的退化），可以在线性时间内处理序列（或者在具有线性处理器数量的对数时间内）。FENs的关键属性是它们通过训练大致线性数量的常深度神经网络并行学习。这些网络具有常深度意味着反向传播效果良好。需要注意的是，目前FENs的性能仅仅是推测，因为我们尚未实现它们。

    arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
    
[^16]: 关于大学习率下梯度下降的稳定性

    On the Stability of Gradient Descent for Large Learning Rate

    [https://arxiv.org/abs/2402.13108](https://arxiv.org/abs/2402.13108)

    本文研究了线性神经网络在二次损失函数下的优化问题，证明了梯度下降映射的非奇异性以及全局最小值点集的光滑流形特性，为理解大学习率下梯度下降的稳定性提供了重要线索。

    

    目前对理解“稳定性边缘（EoS）”现象存在着相当大的兴趣，这一现象在神经网络训练中被观察到，其特点是损失函数在不同纪元间的非单调下降，而损失的陡峭度（Hessian的谱范数）逐渐接近并稳定在2/(学习率)附近。最近有人提出了使用梯度下降训练时出现EoS的原因——沿梯度下降轨迹附近缺乏平坦的极小值点，同时存在紧致的正向不变集。在本文中，我们证明了在二次损失函数下优化的线性神经网络满足第一个假设以及第二个假设的一个必要条件。更具体地，我们证明了梯度下降映射是非奇异的，损失函数的全局最小值点集构成一个光滑流形，并且稳定的极小值构成有界子集。

    arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i
    
[^17]: 基于查询的对抗性提示生成

    Query-Based Adversarial Prompt Generation

    [https://arxiv.org/abs/2402.12329](https://arxiv.org/abs/2402.12329)

    该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。

    

    最近的研究表明，可以构造对抗性示例，导致一个对其进行了调整的语言模型产生有害字符串或执行有害行为。现有的攻击要么在白盒设置中（完全访问模型权重），要么通过可转移性：一种现象，即在一个模型上精心设计的对抗性示例通常在其他模型上仍然有效。我们通过基于查询的攻击改进以前的工作，利用 API 访问远程语言模型来构造对抗性示例，使模型以（明显）更高的概率发出有害字符串，而不能仅仅使用转移攻击。我们在 GPT-3.5 和 OpenAI 的安全分类器上验证了我们的攻击；我们能够让 GPT-3.5 发出有害字符串，而目前的转移攻击失败了，并且我们几乎以 100% 的概率规避了安全分类器。

    arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
    
[^18]: 贝叶斯参数高效微调以克服灾难性遗忘

    Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting

    [https://arxiv.org/abs/2402.12220](https://arxiv.org/abs/2402.12220)

    这项研究展示了如何利用贝叶斯学习技术应用于参数高效微调，以防止灾难性遗忘，实现了预训练知识的保留，并在语言建模和语音合成任务中取得成功。

    

    虽然最初是被文本转语音合成模型的自适应所激发，但我们认为更通用的参数高效微调（PEFT）是进行这种自适应的适当框架。然而，灾难性遗忘仍然是PEFT面临的问题，它损害了预训练模型固有的能力。我们证明现有的贝叶斯学习技术可以应用于PEFT，以防止灾难性遗忘，只要能够可微地计算微调层的参数转换。在一系列关于语言建模和语音合成任务的基础性实验中，我们利用建立的拉普拉斯近似，包括对角线和Kronecker分解方法，来正则化PEFT与低秩适应（LoRA）并比较它们在保留预训练知识方面的性能。我们的结果表明，我们的方法可以克服灾难性遗忘，而不会降低微调性能。

    arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo
    
[^19]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^20]: 异步私有联邦学习中的动量近似

    Momentum Approximation in Asynchronous Private Federated Learning

    [https://arxiv.org/abs/2402.09247](https://arxiv.org/abs/2402.09247)

    本文提出了动量近似方法，在异步私有联邦学习（FL）中有效结合了动量和异步协议的技术，通过最小化动量更新的偏差来改进模型性能。实证研究证明了动量近似在基准FL数据集上的有效性。

    

    异步协议已被证明能够提高大规模客户端联邦学习（FL）的可扩展性。同时，基于动量的方法可以在同步FL中实现最佳模型质量。然而，在异步FL算法中简单地应用动量会导致收敛速度变慢和模型性能下降。如何有效地结合这两种技术以实现双赢目前尚不清楚。在本文中，我们发现异步性引入了对动量更新的隐含偏差。为了解决这个问题，我们提出了动量近似，通过找到所有历史模型更新的最佳加权平均值来最小化偏差。动量近似与安全聚合和差分隐私是兼容的，并且可以在生产的FL系统中很容易地集成，只需较小的通信和存储成本。我们在基准FL数据集上进行了实证研究，证明了动量近似在性能上的改进效果。

    arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro
    
[^21]: 学习可解释概念：统一因果表示学习与基础模型

    Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models

    [https://arxiv.org/abs/2402.09236](https://arxiv.org/abs/2402.09236)

    本研究将因果表示学习和基础模型相结合，研究了如何从数据中学习人类可解释的概念。实验证明了这一统一方法的实用性。

    

    构建智能机器学习系统有两种广泛的方法。一种方法是构建天生可解释的模型，这是因果表示学习领域的努力方向。另一种方法是构建高性能的基础模型，然后投入努力去理解它们的工作原理。本研究将这两种方法联系起来，研究如何从数据中学习人类可解释的概念。通过结合这两个领域的思想，我们正式定义了概念的概念，并展示了它们可以从多样的数据中被可靠地恢复出来。对于合成数据和大型语言模型的实验证明了我们统一方法的实用性。

    arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
    
[^22]: 个性化语言模型基于个性化人类反馈

    Personalized Language Modeling from Personalized Human Feedback

    [https://arxiv.org/abs/2402.05133](https://arxiv.org/abs/2402.05133)

    该论文提出了一个个性化语言模型的方法，通过在于用户的反馈数据中引入个性化特征来解决强化学习框架在多样化用户偏好下存在的问题。

    

    从个性化人类反馈中进行强化学习（RLHF）是目前主流的框架，用于调整大型语言模型以更好地符合人类偏好。然而，在这个框架下开发的算法的基本前提在用户偏好多样化的情况下可能会出现问题。在本文中，我们旨在通过开发个性化语言模型的方法来解决这个问题。我们首先正式介绍了从个性化人类反馈中学习的任务，并解释了为什么在这种情况下普通的RLHF可能会存在问题。然后，我们提出了一个通用的个性化-RLHF（P-RLHF）框架，需要同时学习用户模型和语言（或奖励）模型。用户模型接收用户信息并输出用户表示。其结构编码了我们对反馈数据中用户偏好的假设。我们为个性化奖励建模和个性化直接偏好优化开发了新的学习目标。

    Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
    
[^23]: ActiveAnno3D - 一种用于多模态3D物体检测的主动学习框架

    ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection

    [https://arxiv.org/abs/2402.03235](https://arxiv.org/abs/2402.03235)

    这项工作提出了一种用于多模态3D物体检测的主动学习框架ActiveAnno3D。通过选择最具信息量的训练数据样本进行标注，我们能够在使用一半的训练数据时实现与传统方法相近的检测性能。

    

    大规模数据集的筛选仍然需要大量的时间和资源，数据通常需要人工标注，创建高质量数据集的难题依然存在。在这项工作中，我们使用主动学习的方法来解决多模态3D物体检测中的研究空白。我们提出了ActiveAnno3D，一个用于选择最具信息量的训练数据样本进行标注的主动学习框架。我们探索了各种连续训练方法，并集成了在计算需求和检测性能方面最高效的方法。此外，我们对nuScenes和TUM Traffic Intersection数据集进行了大量实验和消融研究，使用BEVFusion和PV-RCNN进行了测试。我们展示了当仅使用TUM Traffic Intersection数据集的一半训练数据（77.25 mAP相比于83.50 mAP）时，使用PV-RCNN和基于熵的查询策略几乎可以达到相同的性能，而BEVFusion则在使用一半的训练数据时获得了64.31的mAP。

    The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the trai
    
[^24]: FuseMoE：用于灵活多模态融合的专家混合Transformer

    FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion

    [https://arxiv.org/abs/2402.03226](https://arxiv.org/abs/2402.03226)

    本论文提出了一种名为FuseMoE的专家混合Transformer框架，通过创新的门控函数实现灵活融合多模态数据，能够有效地处理缺失模态和不规则采样数据，同时改善模型的预测性能，在临床风险预测任务中具有实际应用价值。

    

    随着机器学习模型在关键领域越来越多地处理多模态数据，它们面临处理多种模态的双重挑战，这些模态经常因缺失元素而不完整，以及收集样本的时间不规则性和稀疏性。成功利用这种复杂数据，同时克服高质量训练样本的稀缺性，是提高这些模型预测性能的关键。我们引入了``FuseMoE''，这是一个集成创新门控函数的专家混合框架。FuseMoE旨在整合多种模态，并且在处理缺失模态和不规则采样数据轨迹的情况下非常有效。在理论上，我们独特的门控函数有助于提高收敛速度，在多个下游任务中表现更好。FuseMoE的实际实用性通过一系列具有挑战性的临床风险预测任务得到验证。

    As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
    
[^25]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^26]: 动态增量优化用于最佳子集选择

    Dynamic Incremental Optimization for Best Subset Selection

    [https://arxiv.org/abs/2402.02322](https://arxiv.org/abs/2402.02322)

    本文研究了一类$\ell_0$正则化问题的对偶形式，并提出了一种高效的原对偶算法，通过充分利用对偶范围估计和增量策略，提高了最佳子集选择问题的解决方案的效率和统计性质。

    

    最佳子集选择被认为是稀疏学习问题的“黄金标准”。已经提出了各种优化技术来攻击这个非光滑非凸问题。本文研究了一类$\ell_0$正则化问题的对偶形式。基于原始问题和对偶问题的结构，我们提出了一种高效的原对偶算法。通过充分利用对偶范围估计和增量策略，我们的算法潜在地减少了冗余计算并改进了最佳子集选择的解决方案。理论分析和对合成和真实数据集的实验验证了所提出解决方案的效率和统计性质。

    Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
    
[^27]: GD-CAF：用于降水预报的图形双流卷积注意力融合

    GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting

    [https://arxiv.org/abs/2401.07958](https://arxiv.org/abs/2401.07958)

    GD-CAF提出了一种新颖的方法，将降水预报作为一个时空图序列预报问题，利用图形双流卷积注意力融合来学习历史降水图并在不同空间位置上预测未来的降水。

    

    精确的降水预报对于各种应用至关重要，包括洪水预测、灾害管理、优化农业活动、管理交通路线和可再生能源。本文将降水预报形式化为时空图序列预报问题，提出了一种名为图形双流卷积注意力融合（GD-CAF）的新方法，旨在从历史降水图的时空图中学习，并预测未来不同空间位置的降水。

    arXiv:2401.07958v2 Announce Type: replace  Abstract: Accurate precipitation nowcasting is essential for various applications, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolut
    
[^28]: 使用PAC-Bayes界限同时控制多个错误

    Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound

    [https://arxiv.org/abs/2202.05560](https://arxiv.org/abs/2202.05560)

    该研究提出了一种PAC-Bayes界限，能够同时控制多个错误，并提供丰富的信息，适用于回归中测试损失分布或分类中不同错误分类的概率。

    

    当前的PAC-Bayes泛化界限仅限于性能的标量度量，如损失或错误率。我们提供了第一个能够提供丰富信息的PAC-Bayes界限，通过界定一组M种错误类型的经验概率与真实概率之间的Kullback-Leibler差异来控制可能结果的整个分布。

    arXiv:2202.05560v2 Announce Type: replace-cross  Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of M error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided w
    
[^29]: 循环模型中含有隐藏因变量的因果发现方法的比较研究

    Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders. (arXiv:2401.13009v1 [cs.LG])

    [http://arxiv.org/abs/2401.13009](http://arxiv.org/abs/2401.13009)

    对于循环模型中含有隐藏因变量的因果发现，已经出现了能够处理这种情况的多种技术方法。

    

    如今，对因果发现的需求无处不在。理解系统中部分之间的随机依赖性以及实际的因果关系对科学的各个部分都至关重要。因此，寻找可靠的方法来检测因果方向的需求不断增长。在过去的50年里，出现了许多因果发现算法，但大多数仅适用于系统没有反馈环路并且具有因果充分性的假设，即没有未测量的子系统能够影响多个已测量变量。这是不幸的，因为这些限制在实践中往往不能假定。反馈是许多过程的一个重要特性，现实世界的系统很少是完全隔离和完全测量的。幸运的是，在最近几年中，已经发展了几种能够处理循环的、因果不充分的系统的技术。随着多种方法的出现，一种实际的应用方法开始变得可能。

    Nowadays, the need for causal discovery is ubiquitous. A better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. Thus, the need for reliable methods to detect causal directions is growing constantly. In the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. This is unfortunate since those restrictions can often not be presumed in practice. Feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. Fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. And with multiple methods available, a practical ap
    
[^30]: 二进制特征屏蔽优化用于特征选择

    Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])

    [http://arxiv.org/abs/2401.12644](http://arxiv.org/abs/2401.12644)

    这个论文提出了一种新颖的特征选择框架，通过使用特征屏蔽方法来消除特征，而不是从数据集中移除它们。这种方法不需要重新训练机器学习模型，可以综合考虑特征子集的重要性，为通用机器学习模型的特征选择问题提供了一种新的解决方案。

    

    我们研究了通用机器学习模型的特征选择问题。我们引入了一种新颖的框架，该框架考虑了模型的预测结果来选择特征。我们的框架通过使用一种新颖的特征屏蔽方法，在特征选择过程中消除特征，而不是从数据集中完全移除它们。这使我们能够在特征选择过程中使用相同的机器学习模型，而不像其他特征选择方法那样需要在每次迭代中重新训练机器学习模型，因为数据集的维度不同。我们使用机器学习模型的预测结果来获取屏蔽操作符，这为模型的预测性能提供了对特征子集的全面观察。特征选择文献中存在各种方法。然而，没有研究引入一个针对通用机器学习模型的无需训练的框架，以整体考虑特征子集的重要性，而不是只关注单个特征的重要性。

    We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusi
    
[^31]: xTrimoPGLM: 统一的百亿规模预训练蛋白质语言模型，用于解析蛋白质的语言

    xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.06199](http://arxiv.org/abs/2401.06199)

    xTrimoPGLM是一个统一的100亿规模预训练蛋白质语言模型，能够同时处理蛋白质理解和生成任务，通过创新的预训练框架和大规模的参数训练，显著优于其他先进模型，在18个蛋白理解基准测试中取得了成功，并能够实现对蛋白质结构的原子分辨率观察。

    

    蛋白质语言模型在学习蛋白质序列中的生物信息方面显示出显著的成功。然而，大多数现有模型局限于自编码或自回归的预训练目标，这使得它们在处理蛋白质理解和生成任务时很难同时进行。我们提出了一个统一的蛋白质语言模型，xTrimoPGLM，通过创新的预训练框架同时解决这两类任务。我们的关键技术贡献是探索这两类目标的兼容性和联合优化的潜力，从而导致了一个以前所未有的规模，使用1000亿参数和1万亿训练标记来训练xTrimoPGLM的策略。我们广泛的实验证明，1）xTrimoPGLM在四个类别的18个蛋白理解基准测试中明显优于其他先进基线。该模型还有助于对蛋白质结构进行原子分辨率的观察，从而实现了对蛋白质结构的理解和生成。

    Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
    
[^32]: 使用特征变换进行交通市场利率预测

    Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])

    [http://arxiv.org/abs/2401.04857](http://arxiv.org/abs/2401.04857)

    本论文提出了一种基于特征变换的新型统计方法，用于解决交通市场利率的预测挑战。该方法具有通用的非线性属性和特征变换核函数，能够高效生成特征，并在预测过程中准确识别季节性和制度转换。

    

    目前，亚马逊在交通市场利率预测上依赖第三方，尽管这些预测质量差且缺乏可解释性。虽然交通市场利率通常很难准确预测，但我们开发了一种基于特征变换的新型统计技术来解决这些挑战，并构建了一个预测和自适应模型来预测市场利率。这种新技术基于特征变换的两个关键属性。第一个是其通用的非线性，它线性化特征空间，从而将预测问题转化为线性回归分析；第二个是特征变换核函数，它允许在时间序列数据之间进行计算有效的相似性比较。结合起来，这些属性允许进行高效的特征生成，并在预测过程中更精确地识别季节性和制度转换。模型的初步结果显示，这种新方法可以改善市场利率的预测性能。

    Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
    
[^33]: 在生成式人工智能时代的物联网: 视野与挑战

    IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])

    [http://arxiv.org/abs/2401.01923](http://arxiv.org/abs/2401.01923)

    在生成式人工智能时代的物联网，Generative AI的进展带来了巨大的希望，同时也面临着高资源需求、及时工程、设备端推理、安全等关键挑战。

    

    带有感知、网络和计算能力的物联网设备，如智能手机、可穿戴设备、智能音箱和家庭机器人，已经无缝地融入到我们的日常生活中。最近生成式人工智能（Generative AI）的进展，如GPT、LLaMA、DALL-E和稳定扩散等，给物联网的发展带来了巨大的希望。本文分享了我们对Generative AI在物联网中带来的好处的看法和愿景，并讨论了Generative AI在物联网相关领域的一些重要应用。充分利用Generative AI在物联网中是一个复杂的挑战。我们确定了一些最关键的挑战，包括Generative AI模型的高资源需求、及时工程、设备端推理、卸载、设备端微调、联邦学习、安全以及开发工具和基准，并讨论了当前存在的差距以及使Generative AI在物联网中实现的有希望的机会。我们希望这篇文章能够激发新的研究和创新。

    Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
    
[^34]: 在部署时进行实时调节：用于单机器人部署的行为调控

    Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment. (arXiv:2311.01059v1 [cs.RO])

    [http://arxiv.org/abs/2311.01059](http://arxiv.org/abs/2311.01059)

    本研究提出了一种名为ROAM的方法，通过利用先前学习到的行为来实时调节机器人在部署过程中应对未曾见过的情况。在测试中，ROAM可以在单个阶段内实现快速适应，并且在模拟环境和真实场景中取得了成功，具有较高的效率和适应性。

    

    为了在现实世界中取得成功，机器人必须应对训练过程中未曾见过的情况。本研究探讨了在部署过程中针对这些新场景的实时调节问题，通过利用先前学习到的多样化行为库。我们的方法，RObust Autonomous Modulation（ROAM），引入了基于预训练行为的感知价值的机制，以在特定情况下选择和调整预训练行为。关键是，这种调节过程在测试时的单个阶段内完成，无需任何人类监督。我们对选择机制进行了理论分析，并证明了ROAM使得机器人能够在模拟环境和真实的四足动物Go1上快速适应动态变化，甚至在脚上套着滚轮滑鞋的情况下成功前进。与现有方法相比，我们的方法在面对各种分布情况的部署时能够以超过2倍的效率进行调节，通过有效选择来实现适应。

    To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing
    
[^35]: 在通信网络中学习增强状态策略进行信息路由

    Learning State-Augmented Policies for Information Routing in Communication Networks. (arXiv:2310.00248v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2310.00248](http://arxiv.org/abs/2310.00248)

    本论文研究了在通信网络中的信息路由问题，提出了一种新颖的状态增强策略，通过部署图神经网络架构，利用图卷积来最大化源节点的聚合信息，从而有效地将所需信息路由到目标节点。

    

    本文研究了在大规模通信网络中的信息路由问题，该问题可以被形式化为一个只能访问局部信息的约束统计学习问题。我们提出了一种新颖的状态增强（SA）策略，通过在通信网络的拓扑链路上部署图神经网络（GNN）架构，利用图卷积来最大化源节点的聚合信息。所提出的技术仅利用每个节点上的局部信息，并有效地将所需的信息路由到目标节点。我们利用无监督学习过程将GNN架构的输出转换为最优的信息路由策略。实验中，我们对实时网络拓扑进行评估，以验证我们的算法。数值仿真结果显示出与基线算法相比，所提出的方法在训练GNN参数化方面的性能有所提高。

    This paper examines the problem of information routing in a large-scale communication network, which can be formulated as a constrained statistical learning problem having access to only local information. We delineate a novel State Augmentation (SA) strategy to maximize the aggregate information at source nodes using graph neural network (GNN) architectures, by deploying graph convolutions over the topological links of the communication network. The proposed technique leverages only the local information available at each node and efficiently routes desired information to the destination nodes. We leverage an unsupervised learning procedure to convert the output of the GNN architecture to optimal information routing strategies. In the experiments, we perform the evaluation on real-time network topologies to validate our algorithms. Numerical simulations depict the improved performance of the proposed method in training a GNN parameterization as compared to baseline algorithms.
    
[^36]: 深度学习分类器性能的综合评估揭示出惊人的缺乏稳定性

    Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])

    [http://arxiv.org/abs/2308.04137](http://arxiv.org/abs/2308.04137)

    通过综合评估深度学习分类器的性能，发现它们缺乏稳定性和可靠性，并建议采用广泛的数据类型和统一的评估指标进行性能基准测试。

    

    可靠而稳健的评估方法是开发本身稳健可靠的机器学习模型的必要第一步。然而，目前用于评估分类器的常规评估协议在综合评估性能方面存在不足，因为它们往往依赖于有限类型的测试数据，忽视其他类型的数据。例如，使用标准测试数据无法评估分类器对于未经训练的类别样本的预测。另一方面，使用包含未知类别样本的数据进行测试无法评估分类器对于已知类别标签的预测能力。本文提倡使用各种不同类型的数据进行性能基准测试，并使用一种可应用于所有这些数据类型的单一指标，以产生一致的性能评估结果。通过这样的基准测试发现，目前的深度神经网络，包括使用认为是全面的方法进行训练的网络，也存在缺乏稳定性的问题。

    Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to pro
    
[^37]: 你的房间不是私密的：关于强化学习的梯度反转攻击

    Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.09273](http://arxiv.org/abs/2306.09273)

    这篇论文提出了一种针对值函数算法和梯度算法的攻击方法，利用梯度反转重建状态、动作和监督信号，以解决嵌入式人工智能中的隐私泄露问题。

    

    嵌入式人工智能的显著发展吸引了人们的极大关注，该技术使得机器人可以在虚拟环境中导航、感知和互动。由于计算机视觉和大型语言模型方面的显著进展，隐私问题在嵌入式人工智能领域变得至关重要，因为机器人可以访问大量个人信息。然而，关于强化学习算法中的隐私泄露问题，尤其是关于值函数算法和梯度算法的问题，在研究中尚未得到充分考虑。本文旨在通过提出一种攻击值函数算法和梯度算法的方法，利用梯度反转重建状态、动作和监督信号，来解决这一问题。选择使用梯度进行攻击是因为常用的联邦学习技术仅利用基于私人用户数据计算的梯度来优化模型，而不存储或传输用户数据。

    The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
    
[^38]: 基于生成扩散模型的癫痫预测数据增强方法

    Data Augmentation for Seizure Prediction with Generative Diffusion Model. (arXiv:2306.08256v1 [eess.SP])

    [http://arxiv.org/abs/2306.08256](http://arxiv.org/abs/2306.08256)

    该论文提出了一种基于扩散模型的数据增强方法DiffEEG，可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。

    

    目标：癫痫预测对于改善患者生活质量具有重要意义，重点在于区分发作前状态与发作后状态。随着机器学习的发展，癫痫预测方法取得了显著进展。然而，发作前与发作后状态数据之间的严重不平衡仍然是一个巨大的挑战，限制了分类器的性能。数据扩增是解决这个问题的一个直观方法。现有的数据扩增方法通过重叠或重新组合数据来生成样本。由于这些转换无法完全探索特征空间并提供新信息，所以生成的样本分布受到原始数据的限制。由于癫痫脑电图表示在不同发作之间具有差异性，这些生成的样本不能提供足够的多样性以在新的癫痫发作中实现高性能。因此，我们提出了一种使用扩散模型的新型数据增强方法DiffEEG。方法：扩散模型是一种建模数据分布的强大工具，我们使用此模型来对原始脑电图数据进行转换以生成多样性的样本，进而提高分类器的性能。结果：DiffEEG在神经网络和SVM模型上进行的实验表明，它可以有效地提高癫痫预测的性能，超过了现有的数据扩增方法。

    Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusi
    
[^39]: 利用三重指数移动平均值实现快速自适应矩估计

    Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])

    [http://arxiv.org/abs/2306.01423](http://arxiv.org/abs/2306.01423)

    本文提出了一种新的深度优化器FAME，使用三重指数移动平均值（TEMA）来估计梯度矩，提供更丰富和准确的数据变化和趋势信息，可以提高计算机视觉等领域中模型的性能表现。

    

    网络优化是深度学习领域中的一个关键步骤，直接影响计算机视觉等多种领域中模型的性能。虽然多种优化器已经被开发出来，但目前的方法在准确快速地识别梯度趋势方面仍然有限，这可能会导致网络性能不佳。本文提出了一种新的深度优化器，称为快速自适应矩估计（FAME），它首次使用三重指数移动平均值（TEMA）来估计梯度矩。将TEMA纳入优化过程中，可以提供更丰富和准确的数据变化和趋势信息，与目前所有主要自适应优化方法中使用的标准指数移动平均值相比。我们提出的FAME优化器已经在广泛的基准测试中得到了验证，包括CIFAR-10，CIFAR-100，PASCAL-VOC，MS-COCO和Cityscapes。

    Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
    
[^40]: 基于贝叶斯分类器的特征最优分区研究

    Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])

    [http://arxiv.org/abs/2304.14537](http://arxiv.org/abs/2304.14537)

    本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战，并且证明该技术在不同数据集上具有更高的准确率和更低的错误率。

    

    朴素贝叶斯分类器是一种应用贝叶斯原理的流行分类方法，尽管输入变量之间的条件依赖关系听起来很好，但实际上会导致大多数投票风格的行为。朴素贝叶斯算法中的某些特征被称为独立特征，因为在预测分类时它们没有条件相关性或依赖性。本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战。在不同的数据集上，我们明确证明了我们的技术的有效性，在错误率更低、准确率更高或相当的情况下，与随机森林和XGBoost等模型相比。

    The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
    
[^41]: 利用离线数据加速程序生成环境中的强化学习

    Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])

    [http://arxiv.org/abs/2304.09825](http://arxiv.org/abs/2304.09825)

    本研究旨在提高程序生成环境中强化学习的样本效率。研究证明，使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提高效率。

    

    强化学习面临的主要挑战之一是代理能够将其学习策略推广到未见过的环境中。此外，训练强化学习代理需要与环境进行大量交互。受离线强化学习和模仿学习的最近成功启发，我们进行了一项研究，以调查代理是否可以利用轨迹的离线数据来提高程序生成环境中的样本效率。我们考虑了两种使用离线数据的模仿学习方法：（1）在在线强化学习训练之前预训练策略和（2）同时训练在线强化学习和来自离线数据的模仿学习。我们分析了可用的离线轨迹的质量（轨迹的最佳性）和多样性（轨迹数量和覆盖级别）对两种方法有效性的影响。在MiniGrid环境中的四个知名稀疏奖励任务中，我们发现使用模仿学习进行预训练和同时进行模仿学习和在线强化学习的方法可以提供更高的样本效率。

    One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
    
[^42]: 无需边缘但具有结构感知性：从GNN到MLP的原型引导知识蒸馏。

    Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])

    [http://arxiv.org/abs/2303.13763](http://arxiv.org/abs/2303.13763)

    本文提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。

    

    将高精度的图神经网络（GNN）在图任务中压缩成低延迟的多层感知器（MLP）已成为热门研究课题。以前的方法会将图的边缘处理成额外的输入给MLP，但这样的图结构对于各种场景可能无法获得。因此，我们提出了一种原型引导知识蒸馏（PGKD）方法，它不需要图形边缘，但可以在不考虑边缘的情况下学习结构感知的MLP。具体而言，我们分析了GNN教师中的图形结构信息，并通过原型在无边缘设置中从GNN到MLP进行了知识蒸馏。在流行的图形基准实验中的实验结果表明了所提出的PGKD方法的有效性和鲁棒性。

    Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
    
[^43]: 重新审视DeepFool：泛化和改进

    Revisiting DeepFool: generalization and improvement. (arXiv:2303.12481v1 [cs.LG])

    [http://arxiv.org/abs/2303.12481](http://arxiv.org/abs/2303.12481)

    本文提出了一种新的对抗性攻击，该攻击是广义了DeepFool攻击，既有效又计算效率高，适用于评估大型深度神经网络的鲁棒性。

    

    深度神经网络被已知容易受到对抗样本的攻击，这些输入稍加修改便会导致网络做出错误的预测。这导致了大量研究，以评估这些网络对此类扰动的鲁棒性度量。最小l2对抗扰动的鲁棒性，是一种特别重要的鲁棒性度量。然而，现有的用于评估此类鲁棒性度量的方法，要么计算成本高，要么不太准确。在本文中，我们引入了一种新的对抗性攻击方法，它在效果和计算效率之间保持平衡。我们提出的攻击是广义了深度欺骗（DeepFool）攻击，但它们仍然易于理解和实现。我们展示了我们的攻击在效果和计算效率方面均优于现有方法。我们提出的攻击也适用于评估大型深度神经网络的鲁棒性。

    Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal l2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large
    
[^44]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    

