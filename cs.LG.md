# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization](https://arxiv.org/abs/2403.18915) | æå‡ºäº†ä½¿ç”¨æœ€ä¼˜ä¼ è¾“è¿›è¡Œå°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¤šæç¤ºå­¦ä¹ æ¡†æ¶å’Œæœ€ä¼˜ä¼ è¾“ç†è®ºçš„ç»“åˆï¼Œæœ‰æ•ˆåœ°æ•æ‰é€šç”¨ç‰¹å¾å’Œå‡è½»è¿‡æ‹Ÿåˆé£é™© |
| [^2] | [From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima](https://arxiv.org/abs/2403.02418) | å±€éƒ¨æ›²ç‡å˜åŒ–å¯¼è‡´ç³»ç»Ÿä»è‰¯æ€§ä¸”å¯Œæœ‰ä¿¡æ¯çš„å±€éƒ¨æ™¯è§‚é€æ¸é™·å…¥æ— ä¿¡æ¯çš„è¿·å®«ï¼Œå…³é”®è½¬å˜ä¸æ—¶é—´ç›¸å…³çš„Hessiançš„é˜ˆå€¼æœ‰å…³ã€‚ |
| [^3] | [DualView: Data Attribution from the Dual Perspective](https://arxiv.org/abs/2402.12118) | æå‡ºäº†DualViewï¼Œä¸€ç§åŸºäºæ›¿ä»£å»ºæ¨¡çš„åæœŸæ•°æ®å½’å› æ–¹æ³•ï¼Œå…·æœ‰é«˜æ•ˆè®¡ç®—å’Œä¼˜è´¨è¯„ä¼°ç»“æœã€‚ |
| [^4] | [Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://arxiv.org/abs/2402.04379) | ç»†è°ƒè¯­è¨€æ¨¡å‹ç”¨äºç”Ÿæˆç¨³å®šææ–™ï¼Œå…·æœ‰å¯é æ€§é«˜å’Œçµæ´»æ€§å¼ºçš„ä¼˜åŠ¿ï¼Œèƒ½ä»¥è¾ƒé«˜çš„é€Ÿç‡ç”Ÿæˆè¢«é¢„æµ‹ä¸ºäºšç¨³æ€çš„ææ–™ã€‚ |
| [^5] | [Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled Learning and Conditional Generation with Extra Data](https://arxiv.org/abs/2006.07841) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠé¢å¤–æ— æ ‡ç­¾æ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ |
| [^6] | [Learning Concepts Definable in First-Order Logic with Counting](https://arxiv.org/abs/1909.03820) | è¯¥ç ”ç©¶å°†ä¸€é˜¶é€»è¾‘ä¸è®¡æ•°ç¬¦å·ç›¸ç»“åˆï¼Œè¯æ˜äº†å¯ä»¥åœ¨å¤šå¯¹æ•°åº¦ç»“æ„ä¸‹ä»¥æ¬¡çº¿æ€§æ—¶é—´ä¸€è‡´å­¦ä¹ å¯å®šä¹‰çš„åˆ†ç±»å™¨ï¼Œä¸ºåŒ…å«æ•°å€¼æ–¹é¢çš„æœºå™¨å­¦ä¹ æ‰©å±•å­¦ä¹ æ¡†æ¶è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ |
| [^7] | [I-CEE: Tailoring Explanations of Image Classification Models to User Expertise.](http://arxiv.org/abs/2312.12102) | I-CEEæ˜¯ä¸€ä¸ªäººä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œä¸ºç”¨æˆ·ä¸“ä¸šçŸ¥è¯†å®šåˆ¶äº†å›¾åƒåˆ†ç±»æ¨¡å‹çš„è§£é‡Šï¼Œé€šè¿‡æä¾›ä¿¡æ¯ä¸°å¯Œçš„ç¤ºä¾‹å›¾åƒã€å±€éƒ¨è§£é‡Šå’Œæ¨¡å‹å†³ç­–æ¥å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„å†³ç­–ã€‚ |
| [^8] | [Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences.](http://arxiv.org/abs/2309.03791) | æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ARMOR_Dæ¥åŠ å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•åŸºäºæœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šè¿›è¡Œæœ€å¤§åŒ–æœŸæœ›æŸå¤±æ¥å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒARMOR_Dæ–¹æ³•åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸­èƒ½å¤Ÿä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§æ–¹é¢å…·æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚ |
| [^9] | [Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning.](http://arxiv.org/abs/2308.01358) | æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚ |
| [^10] | [Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift.](http://arxiv.org/abs/2302.10160) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…³äºæ ¸å²­å›å½’çš„åå˜é‡è½¬ç§»ç­–ç•¥ï¼Œé€šè¿‡ä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç‰¹å¾åˆ†å¸ƒä¸‹çš„å­¦ä¹ ï¼Œå®ç°å‡æ–¹è¯¯å·®æœ€å°åŒ–ã€‚ |

# è¯¦ç»†

[^1]: ä½¿ç”¨æœ€ä¼˜ä¼ è¾“è¿›è¡Œå°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½çš„æç¤ºå­¦ä¹ 

    PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization

    [https://arxiv.org/abs/2403.18915](https://arxiv.org/abs/2403.18915)

    æå‡ºäº†ä½¿ç”¨æœ€ä¼˜ä¼ è¾“è¿›è¡Œå°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¤šæç¤ºå­¦ä¹ æ¡†æ¶å’Œæœ€ä¼˜ä¼ è¾“ç†è®ºçš„ç»“åˆï¼Œæœ‰æ•ˆåœ°æ•æ‰é€šç”¨ç‰¹å¾å’Œå‡è½»è¿‡æ‹Ÿåˆé£é™©

    

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥å¤„ç†å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ—¶åºåŠ¨ä½œå®šä½ï¼ˆTALï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£å†³äº†ä¼ ç»Ÿå•æ ·æœ¬å­¦ä¹ æ–¹æ³•çš„å›ºæœ‰å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ç”±äºæ— æ³•åœ¨çœŸå®ä¸–ç•Œçš„è§†é¢‘ä¸­è·¨ä¸åŒä¸Šä¸‹æ–‡è¿›è¡Œæ³›åŒ–è€Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚é‰´äºè§†é¢‘ä¸­æ‘„åƒæœºè§†è§’ã€èƒŒæ™¯å’Œç‰©ä½“çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢å¼ºäº†æœ€ä¼˜ä¼ è¾“çš„å¤šæç¤ºå­¦ä¹ æ¡†æ¶ã€‚è¿™ä¸ªè®¾è®¡å…è®¸æ¨¡å‹ä¸ºæ¯ä¸ªåŠ¨ä½œå­¦ä¹ ä¸€ç»„å¤šæ ·çš„æç¤ºï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰é€šç”¨ç‰¹å¾å¹¶åˆ†å¸ƒè¡¨ç¤ºä»¥å‡è½»è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨æœ€ä¼˜ä¼ è¾“ç†è®ºï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°å°†è¿™äº›æç¤ºä¸åŠ¨ä½œç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä¼˜åŒ–ä»¥è·å¾—é€‚åº”è§†é¢‘æ•°æ®å¤šé¢æ€§çš„ç»¼åˆè¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†åŠ¨ä½œå®šä½æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ã€‚

    arXiv:2403.18915v1 Announce Type: cross  Abstract: This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization a
    
[^2]: ä»é›¶åˆ°è‹±é›„ï¼šæ— çŸ¥åˆå€¼å¤„çš„å±€éƒ¨æ›²ç‡å¦‚ä½•è¿œç¦»ç³Ÿç³•çš„æå°å€¼

    From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima

    [https://arxiv.org/abs/2403.02418](https://arxiv.org/abs/2403.02418)

    å±€éƒ¨æ›²ç‡å˜åŒ–å¯¼è‡´ç³»ç»Ÿä»è‰¯æ€§ä¸”å¯Œæœ‰ä¿¡æ¯çš„å±€éƒ¨æ™¯è§‚é€æ¸é™·å…¥æ— ä¿¡æ¯çš„è¿·å®«ï¼Œå…³é”®è½¬å˜ä¸æ—¶é—´ç›¸å…³çš„Hessiançš„é˜ˆå€¼æœ‰å…³ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†æ¢¯åº¦ä¸‹é™åœ¨éå‡¸å’Œé«˜ç»´è®¾ç½®ä¸­çš„ä¼˜åŒ–åŠ¨åŠ›å­¦ï¼Œé‡ç‚¹å…³æ³¨ç›¸ä½æ¢å¤é—®é¢˜ä½œä¸ºå¤æ‚æŸå¤±æ™¯è§‚çš„æ¡ˆä¾‹ç ”ç©¶ã€‚é€šè¿‡åˆ†æå±€éƒ¨æ›²ç‡åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å˜åŒ–ï¼Œæˆ‘ä»¬å‘ç°åœ¨ä¸­é—´ä¿¡å™ªæ¯”ä¸‹ï¼ŒHessianåœ¨ä¸‹é™çš„ç¬¬ä¸€ä¸ªé˜¶æ®µæ˜¾ç¤ºå‡ºæŒ‡å‘å¥½æå°å€¼çš„ä¸‹é™æ–¹å‘ï¼Œç„¶ååœ¨ç»“æŸæ—¶è¢«å›°åœ¨ç³Ÿç³•çš„æå°å€¼ä¸­ã€‚å› æ­¤ï¼Œå±€éƒ¨æ™¯è§‚èµ·åˆæ˜¯è‰¯æ€§ä¸”å¯Œæœ‰ä¿¡æ¯çš„ï¼Œç„¶åæ¢¯åº¦ä¸‹é™å°†ç³»ç»Ÿå¸¦å…¥æ— ä¿¡æ¯çš„è¿·å®«ã€‚ä¸¤ä¸ªé˜¶æ®µä¹‹é—´çš„è½¬å˜ä¸æ—¶é—´ç›¸å…³çš„Hessiançš„BBPç±»å‹é˜ˆå€¼ç›¸å…³è”ã€‚

    arXiv:2403.02418v1 Announce Type: new  Abstract: We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in prac
    
[^3]: DualViewï¼šåŒé‡è§†è§’ä¸‹çš„æ•°æ®å½’å› 

    DualView: Data Attribution from the Dual Perspective

    [https://arxiv.org/abs/2402.12118](https://arxiv.org/abs/2402.12118)

    æå‡ºäº†DualViewï¼Œä¸€ç§åŸºäºæ›¿ä»£å»ºæ¨¡çš„åæœŸæ•°æ®å½’å› æ–¹æ³•ï¼Œå…·æœ‰é«˜æ•ˆè®¡ç®—å’Œä¼˜è´¨è¯„ä¼°ç»“æœã€‚

    

    æœ¬æ–‡æå‡ºäº†DualViewï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ›¿ä»£å»ºæ¨¡çš„åæœŸæ•°æ®å½’å› æ–¹æ³•ï¼Œå±•ç¤ºäº†é«˜è®¡ç®—æ•ˆç‡å’Œè‰¯å¥½çš„è¯„ä¼°ç»“æœã€‚æˆ‘ä»¬ä¸“æ³¨äºç¥ç»ç½‘ç»œï¼Œåœ¨ä¸æ–‡çŒ®ç›¸å…³çš„é€‚å½“å®šé‡è¯„ä¼°ç­–ç•¥ä¸‹è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æŠ€æœ¯ï¼Œæ¯”è¾ƒäº†ä¸ç›¸å…³ä¸»è¦æœ¬åœ°æ•°æ®å½’å› æ–¹æ³•çš„æ€§èƒ½ã€‚

    arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
    
[^4]: ç»†è°ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆç¨³å®šçš„æ— æœºææ–™æ–‡æœ¬

    Fine-Tuned Language Models Generate Stable Inorganic Materials as Text

    [https://arxiv.org/abs/2402.04379](https://arxiv.org/abs/2402.04379)

    ç»†è°ƒè¯­è¨€æ¨¡å‹ç”¨äºç”Ÿæˆç¨³å®šææ–™ï¼Œå…·æœ‰å¯é æ€§é«˜å’Œçµæ´»æ€§å¼ºçš„ä¼˜åŠ¿ï¼Œèƒ½ä»¥è¾ƒé«˜çš„é€Ÿç‡ç”Ÿæˆè¢«é¢„æµ‹ä¸ºäºšç¨³æ€çš„ææ–™ã€‚

    

    æˆ‘ä»¬æå‡ºäº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç»†è°ƒï¼Œä»¥ç”Ÿæˆç¨³å®šææ–™ã€‚è™½ç„¶éä¼ ç»Ÿï¼Œä½†åœ¨æ–‡æœ¬ç¼–ç çš„åŸå­æ•°æ®ä¸Šç»†è°ƒå¤§å‹è¯­è¨€æ¨¡å‹éå¸¸ç®€å•æ˜“è¡Œï¼ŒåŒæ—¶å¯é æ€§é«˜ï¼Œçº¦90%çš„é‡‡æ ·ç»“æ„éµå®ˆåŸå­ä½ç½®å’Œç”µè·çš„ç‰©ç†çº¦æŸã€‚é€šè¿‡æ¥è‡ªå­¦ä¹ çš„æœºå™¨å­¦ä¹ åŠ¿å’Œé‡‘æ ‡å‡†DFTè®¡ç®—çš„èƒ½é‡ä»¥ä¸Šçš„è®¡ç®—ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æœ€å¼ºæ¨¡å‹ï¼ˆç»†è°ƒLLaMA-2 70Bï¼‰å¯ä»¥ä»¥CDVAEç«äº‰æ‰©æ•£æ¨¡å‹çš„çº¦ä¸¤å€é€Ÿç‡ï¼ˆ49% vs 28%ï¼‰ç”Ÿæˆè¢«é¢„æµ‹ä¸ºäºšç¨³æ€çš„ææ–™ã€‚ç”±äºæ–‡æœ¬æç¤ºçš„å›ºæœ‰çµæ´»æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åŒæ—¶ç”¨äºç¨³å®šææ–™çš„æ— æ¡ä»¶ç”Ÿæˆã€éƒ¨åˆ†ç»“æ„çš„å¡«å……å’Œæ–‡æœ¬æ¡ä»¶ç”Ÿæˆã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜è¯­è¨€æ¨¡å‹æ•æ‰æ™¶ä½“ç»“æ„çš„å…³é”®å¯¹ç§°æ€§çš„èƒ½åŠ›éšæ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œæ”¹å–„ï¼Œè¿™è¡¨æ˜é¢„è®­ç»ƒçš„LLMçš„åå·®å‡ºå¥‡åœ°é€‚åˆåŸå­æ€§çš„åº”ç”¨ã€‚

    We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic
    
[^5]: åŒæ—¶è¿›è¡Œæ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆï¼Œåˆ©ç”¨é¢å¤–æ•°æ®æ¥åˆ†ç±»å’Œç”Ÿæˆ

    Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled Learning and Conditional Generation with Extra Data

    [https://arxiv.org/abs/2006.07841](https://arxiv.org/abs/2006.07841)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠé¢å¤–æ— æ ‡ç­¾æ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

    

    åœ¨è®¸å¤šæœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œæ ‡è®°ç±»åˆ«æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„ç“¶é¢ˆã€‚è™½ç„¶å­˜åœ¨ä¸°å¯Œçš„æ— æ ‡ç­¾æ•°æ®å¹¶æä¾›æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†åˆ©ç”¨å®ƒä»¬æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬æ–‡é€šè¿‡åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾ï¼ˆPositive-Unlabeledï¼ŒPUï¼‰åˆ†ç±»å’Œæœ‰æ¡ä»¶ç”Ÿæˆï¼Œä»¥åŠé¢å¤–çš„æ— æ ‡ç­¾æ•°æ®ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œä½¿å¾—åœ¨é¢å¯¹é¢å¤–æ•°æ®ï¼ˆå°¤å…¶æ˜¯åˆ†å¸ƒå¤–çš„æ— æ ‡ç­¾æ•°æ®ï¼‰æ—¶ï¼ŒåŒæ—¶è¿›è¡ŒPUåˆ†ç±»å’Œæœ‰æ¡ä»¶ç”Ÿæˆæˆä¸ºå¯èƒ½ï¼Œé€šè¿‡æ¢ç´¢å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼š1ï¼‰é€šè¿‡ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„æ–°å‹åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆClassifier-Noise-Invariant Conditional GANï¼ŒCNI-CGANï¼‰æ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œ2ï¼‰åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚ä»ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†CNI-CGANçš„æœ€ä¼˜æ¡ä»¶ï¼Œå¹¶åœ¨å®éªŒä¸­é€šè¿‡å¹¿æ³›çš„è¯„ä¼°æ¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚

    The scarcity of class-labeled data is a ubiquitous bottleneck in many machine learning problems. While abundant unlabeled data typically exist and provide a potential solution, it is highly challenging to exploit them. In this paper, we address this problem by leveraging Positive-Unlabeled~(PU) classification and the conditional generation with extra unlabeled data \emph{simultaneously}. In particular, we present a novel training framework to jointly target both PU classification and conditional generation when exposed to extra data, especially out-of-distribution unlabeled data, by exploring the interplay between them: 1) enhancing the performance of PU classifiers with the assistance of a novel Classifier-Noise-Invariant Conditional GAN~(CNI-CGAN) that is robust to noisy labels, 2) leveraging extra data with predicted labels from a PU classifier to help the generation. Theoretically, we prove the optimal condition of CNI-CGAN, and experimentally, we conducted extensive evaluations on
    
[^6]: ç”¨è®¡æ•°ç¬¦å·çš„ä¸€é˜¶é€»è¾‘å®šä¹‰çš„æ¦‚å¿µçš„å­¦ä¹ 

    Learning Concepts Definable in First-Order Logic with Counting

    [https://arxiv.org/abs/1909.03820](https://arxiv.org/abs/1909.03820)

    è¯¥ç ”ç©¶å°†ä¸€é˜¶é€»è¾‘ä¸è®¡æ•°ç¬¦å·ç›¸ç»“åˆï¼Œè¯æ˜äº†å¯ä»¥åœ¨å¤šå¯¹æ•°åº¦ç»“æ„ä¸‹ä»¥æ¬¡çº¿æ€§æ—¶é—´ä¸€è‡´å­¦ä¹ å¯å®šä¹‰çš„åˆ†ç±»å™¨ï¼Œä¸ºåŒ…å«æ•°å€¼æ–¹é¢çš„æœºå™¨å­¦ä¹ æ‰©å±•å­¦ä¹ æ¡†æ¶è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†åœ¨Groheå’ŒTur\'anå¼•å…¥çš„é€»è¾‘æ¡†æ¶ä¸‹çš„å…³ç³»èƒŒæ™¯ç»“æ„ä¸Šçš„å¸ƒå°”åˆ†ç±»é—®é¢˜ã€‚ä¼—æ‰€å‘¨çŸ¥(Groheå’ŒRitzert, LICS 2017)ï¼Œåœ¨å¤šå¯¹æ•°åº¦ç»“æ„ä¸Šçš„ä¸€é˜¶é€»è¾‘å¯å®šä¹‰çš„åˆ†ç±»å™¨å¯ä»¥åœ¨æ¬¡çº¿æ€§æ—¶é—´å†…å­¦ä¹ ï¼Œå…¶ä¸­ç»“æ„çš„åº¦å’Œè¿è¡Œæ—¶é—´æ˜¯ä»¥ç»“æ„çš„å¤§å°ä¸ºå•ä½æ¥è¡¡é‡çš„ã€‚æˆ‘ä»¬å°†ç»“æœæ¨å¹¿åˆ°äº†ç”±Kuskeå’ŒSchweikardt(LICS 2017)å¼•å…¥çš„å¸¦è®¡æ•°çš„ä¸€é˜¶é€»è¾‘FOCNï¼Œå®ƒä½œä¸ºä¸€ä¸ªå¹¿æ³›æ¨å¹¿å„ç§è®¡æ•°é€»è¾‘çš„è¡¨ç°é€»è¾‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯ä»¥åœ¨å¤šå¯¹æ•°åº¦ç»“æ„ç±»ä¸Šå®šä¹‰çš„FOCNä¸­çš„åˆ†ç±»å™¨å¯ä»¥åœ¨æ¬¡çº¿æ€§æ—¶é—´å†…ä¸€è‡´åœ°å­¦ä¹ ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯å°†å­¦ä¹ æ¡†æ¶æ‰©å±•ä»¥åŒ…å«æœºå™¨å­¦ä¹ çš„æ•°å€¼æ–¹é¢çš„ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬å°†è¿™ä¸€ç»“æœæ‰©å±•åˆ°äº†æ— è§†çš„æ¦‚ç‡

    arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
    
[^7]: I-CEE: å°†å›¾åƒåˆ†ç±»æ¨¡å‹çš„è§£é‡Šå®šåˆ¶ä¸ºç”¨æˆ·ä¸“ä¸šçŸ¥è¯†

    I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.12102](http://arxiv.org/abs/2312.12102)

    I-CEEæ˜¯ä¸€ä¸ªäººä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œä¸ºç”¨æˆ·ä¸“ä¸šçŸ¥è¯†å®šåˆ¶äº†å›¾åƒåˆ†ç±»æ¨¡å‹çš„è§£é‡Šï¼Œé€šè¿‡æä¾›ä¿¡æ¯ä¸°å¯Œçš„ç¤ºä¾‹å›¾åƒã€å±€éƒ¨è§£é‡Šå’Œæ¨¡å‹å†³ç­–æ¥å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„å†³ç­–ã€‚

    

    æœ‰æ•ˆè§£é‡Šé»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹çš„å†³ç­–å¯¹äºä¾èµ–å®ƒä»¬çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è´Ÿè´£ä»»éƒ¨ç½²è‡³å…³é‡è¦ã€‚è¯†åˆ«åˆ°å…¶é‡è¦æ€§ï¼Œå¯ä»¥ç”Ÿæˆè¿™äº›è§£é‡Šçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰é¢†åŸŸæä¾›äº†å‡ ç§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸€ä¸æ–­å‘å±•çš„å·¥ä½œä¸­ï¼Œå¯¹ç”¨æˆ·ï¼ˆè§£é‡Šå¯¹è±¡ï¼‰çš„å…³æ³¨ç›¸å¯¹è¾ƒå°‘ï¼Œå¤§å¤šæ•°XAIæŠ€æœ¯äº§ç”Ÿçš„æ˜¯â€œä¸€åˆ€åˆ‡â€çš„è§£é‡Šã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œå®ç°æ›´åŠ ä»¥äººä¸ºä¸­å¿ƒçš„XAIï¼Œæˆ‘ä»¬æå‡ºäº†I-CEEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºç”¨æˆ·ä¸“ä¸šçŸ¥è¯†å®šåˆ¶å›¾åƒåˆ†ç±»è§£é‡Šçš„æ¡†æ¶ã€‚å—åˆ°ç°æœ‰å·¥ä½œçš„å¯å‘ï¼ŒI-CEEé€šè¿‡ä¸ºç”¨æˆ·æä¾›ä¿¡æ¯ä¸°å¯Œçš„è®­ç»ƒæ•°æ®å­é›†ï¼ˆå³ç¤ºä¾‹å›¾åƒï¼‰ã€ç›¸åº”çš„å±€éƒ¨è§£é‡Šå’Œæ¨¡å‹å†³ç­–æ¥è§£é‡Šå›¾åƒåˆ†ç±»æ¨¡å‹çš„å†³ç­–ã€‚ç„¶è€Œï¼Œä¸æ­¤å‰çš„å·¥ä½œä¸åŒçš„æ˜¯ï¼ŒI-CEEæ¨¡æ‹Ÿäº†ç¤ºä¾‹å›¾åƒçš„ä¿¡æ¯é‡ä¾èµ–äºç”¨æˆ·ä¸“ä¸šçŸ¥è¯†çš„æƒ…å†µï¼Œä»è€Œä¸ºä¸åŒçš„ç”¨æˆ·æä¾›ä¸åŒçš„ç¤ºä¾‹ã€‚

    Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
    
[^8]: ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚æ¥æé«˜å¯¹æŠ—æ€§é²æ£’æ·±åº¦å­¦ä¹ 

    Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])

    [http://arxiv.org/abs/2309.03791](http://arxiv.org/abs/2309.03791)

    æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ARMOR_Dæ¥åŠ å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•åŸºäºæœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šè¿›è¡Œæœ€å¤§åŒ–æœŸæœ›æŸå¤±æ¥å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒARMOR_Dæ–¹æ³•åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸­èƒ½å¤Ÿä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§æ–¹é¢å…·æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚

    

    æˆ‘ä»¬å¼•å…¥äº†ARMOR_Dæ–¹æ³•ä½œä¸ºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹æŠ—æ€§é²æ£’æ€§çš„åˆ›æ–°æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åŸºäºä¸€ç§æ–°çš„æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ç±»ï¼Œé€šè¿‡ä¿¡æ¯å·®å¼‚å’Œæœ€ä¼˜ä¼ è¾“æˆæœ¬ä¹‹é—´çš„infimalå·ç§¯æ„å»ºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ–¹æ³•æ¥å¢å¼ºå¯¹æŠ—æ€§é²æ£’æ€§ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šæœ€å¤§åŒ–æœŸæœ›æŸå¤±ï¼Œè¿™è¢«ç§°ä¸ºåˆ†å¸ƒé²æ£’ä¼˜åŒ–æŠ€æœ¯ã€‚ä½œä¸ºæ„å»ºå¯¹æŠ—æ ·æœ¬çš„å·¥å…·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ ·æœ¬æ ¹æ®æœ€ä¼˜ä¼ è¾“æˆæœ¬è¿›è¡Œä¼ è¾“ï¼Œå¹¶æ ¹æ®ä¿¡æ¯å·®å¼‚è¿›è¡Œé‡æ–°åŠ æƒã€‚æˆ‘ä»¬åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸Šè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°åœ¨å¢å¼ºå¯¹æŠ—æ”»å‡»é²æ£’æ€§æ–¹é¢ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä¼˜äºç°æœ‰æ–¹æ³•ã€‚ARMOR_Dåœ¨FGSMæ”»å‡»ä¸‹çš„robustifiedå‡†ç¡®ç‡è¾¾åˆ°98.29%ï¼Œåœ¨å…¶ä»–æ”»å‡»ä¸‹è¾¾åˆ°98.18%ã€‚

    We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ aga
    
[^9]: å‹ç¼©å’Œåˆ†å¸ƒå¼æœ€å°äºŒä¹˜å›å½’ï¼šæ”¶æ•›é€Ÿåº¦åŠå…¶åœ¨è”é‚¦å­¦ä¹ ä¸­çš„åº”ç”¨

    Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])

    [http://arxiv.org/abs/2308.01358](http://arxiv.org/abs/2308.01358)

    æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨æœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­ï¼Œå‹ç¼©å¯¹éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å‡ ç§æ— åå‹ç¼©æ“ä½œç¬¦ä¹‹é—´çš„æ”¶æ•›é€Ÿåº¦å·®å¼‚ï¼Œè¿™äº›æ“ä½œç¬¦éƒ½æ»¡è¶³ç›¸åŒçš„æ–¹å·®æ¡ä»¶ï¼Œä»è€Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ€å°äºŒä¹˜å›å½’ï¼ˆLSRï¼‰çš„æƒ…å†µï¼Œå¹¶åˆ†æäº†ä¸€ä¸ªä¾èµ–äºéšæœºåœºçš„æœ€å°äºŒä¹˜å›å½’çš„éšæœºé€¼è¿‘ç®—æ³•ã€‚æˆ‘ä»¬å¯¹éšæœºåœºçš„ä¸€èˆ¬æ€§å‡è®¾è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼ˆç‰¹åˆ«æ˜¯æœŸæœ›çš„HÃ¶lderæ­£åˆ™æ€§ï¼‰å¹¶å¯¹å™ªå£°åæ–¹å·®è¿›è¡Œäº†é™åˆ¶ï¼Œä»¥ä¾¿åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬å‹ç¼©ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç»“æœæ‰©å±•åˆ°è”é‚¦å­¦ä¹ çš„æƒ…å†µä¸‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¯¹åŠ æ€§å™ªå£°çš„åæ–¹å·®ğ–¢ğ– ğ–­ğ–¨ğ– å¯¹æ”¶æ•›æ€§çš„å½±å“ã€‚

    In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced 
    
[^10]: æ ¸å²­å›å½’ä¸‹ä¼ªæ ‡ç­¾çš„åå˜é‡è½¬ç§»ç­–ç•¥

    Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift. (arXiv:2302.10160v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10160](http://arxiv.org/abs/2302.10160)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…³äºæ ¸å²­å›å½’çš„åå˜é‡è½¬ç§»ç­–ç•¥ï¼Œé€šè¿‡ä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç‰¹å¾åˆ†å¸ƒä¸‹çš„å­¦ä¹ ï¼Œå®ç°å‡æ–¹è¯¯å·®æœ€å°åŒ–ã€‚

    

    æˆ‘ä»¬æå‡ºå¹¶åˆ†æäº†ä¸€ç§åŸºäºåå˜é‡è½¬ç§»çš„æ ¸å²­å›å½’æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ç›®æ ‡åˆ†å¸ƒä¸Šå­¦ä¹ ä¸€ä¸ªå‡æ–¹è¯¯å·®æœ€å°çš„å›å½’å‡½æ•°ï¼ŒåŸºäºä»ç›®æ ‡åˆ†å¸ƒé‡‡æ ·çš„æœªæ ‡è®°æ•°æ®å’Œå¯èƒ½å…·æœ‰ä¸åŒç‰¹å¾åˆ†å¸ƒçš„å·²æ ‡è®°æ•°æ®ã€‚æˆ‘ä»¬å°†å·²æ ‡è®°æ•°æ®åˆ†æˆä¸¤ä¸ªå­é›†ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œæ ¸å²­å›å½’ï¼Œä»¥è·å¾—å€™é€‰æ¨¡å‹é›†åˆå’Œä¸€ä¸ªå¡«å……æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åè€…å¡«å……ç¼ºå¤±çš„æ ‡ç­¾ï¼Œç„¶åç›¸åº”åœ°é€‰æ‹©æœ€ä½³çš„å€™é€‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„éæ¸è¿‘æ€§è¿‡é‡é£é™©ç•Œè¡¨æ˜ï¼Œåœ¨ç›¸å½“ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¼°è®¡å™¨èƒ½å¤Ÿé€‚åº”ç›®æ ‡åˆ†å¸ƒä»¥åŠåå˜é‡è½¬ç§»çš„ç»“æ„ã€‚å®ƒèƒ½å¤Ÿå®ç°æ¸è¿‘æ­£æ€è¯¯å·®ç‡ç›´åˆ°å¯¹æ•°å› å­çš„æœ€å°æé™ä¼˜åŒ–ã€‚åœ¨æ¨¡å‹é€‰æ‹©ä¸­ä½¿ç”¨ä¼ªæ ‡ç­¾ä¸ä¼šäº§ç”Ÿä¸»è¦è´Ÿé¢å½±å“ã€‚

    We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate model accordingly. Our non-asymptotic excess risk bounds show that in quite general scenarios, our estimator adapts to the structure of the target distribution as well as the covariate shift. It achieves the minimax optimal error rate up to a logarithmic factor. The use of pseudo-labels in model selection does not have major negative impacts.
    

