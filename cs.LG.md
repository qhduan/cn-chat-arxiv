# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type](https://rss.arxiv.org/abs/2402.01632) | 这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。 |
| [^2] | [Adaptive Optimization for Prediction with Missing Data](https://rss.arxiv.org/abs/2402.01543) | 本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。 |
| [^3] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^4] | [One-Shot Domain Incremental Learning](https://arxiv.org/abs/2403.16707) | 提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。 |
| [^5] | [Adaptive LPD Radar Waveform Design with Generative Deep Learning](https://arxiv.org/abs/2403.12254) | 使用生成式深度学习设计自适应LPD雷达波形，以在不被发现的情况下有效地进行测距和感知 |
| [^6] | [KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101) | KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。 |
| [^7] | [Hamiltonian Property Testing](https://arxiv.org/abs/2403.02968) | 本文研究了Hamiltonian的本地性测试作为属性测试问题，重点在于确定未知的$n$比特Hamiltonian是否是$k$局部的，通过对$H$的时间演化进行访问来解决问题。 |
| [^8] | [A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications](https://arxiv.org/abs/2403.00485) | 该论文综述了几何图神经网络的数据结构、模型和应用，通过提出具备不变性/等变性属性的几何GNN来更好地表征几何图的几何形状和拓扑，并提供了现有模型的统一视角。 |
| [^9] | [Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process](https://arxiv.org/abs/2402.16324) | 该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。 |
| [^10] | [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148) | 介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。 |
| [^11] | [Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules](https://arxiv.org/abs/2402.12714) | 提出了具有等变性的预训练Transformer(EPT)框架，能够统一多领域分子的几何学习，通过块增强表示和E(3)等变性实现更准确的3D结构表示。 |
| [^12] | [On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions](https://arxiv.org/abs/2402.03982) | 本文研究了在宽松假设下的随机优化中Adam算法的收敛性。我们引入了一个全面的噪声模型，并证明了在这个模型下，Adam算法可以以较高的概率高效地寻找到一个稳定点。与其他随机一阶算法相比，Adam算法具有更好的自适应性能，无需调整步长和问题参数。 |
| [^13] | [Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments](https://arxiv.org/abs/2402.02593) | 在量化噪声环境中，利用连续可微激活函数进行学习可以减轻模拟量化误差，为计算机视觉、信号处理等多个机器学习领域的硬件实现提供了指导。 |
| [^14] | [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316) | 这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。 |
| [^15] | [Score-based Causal Representation Learning: Linear and General Transformations](https://arxiv.org/abs/2402.00849) | 这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。 |
| [^16] | [Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction](https://arxiv.org/abs/2401.15603) | 该论文提出了一种特征值修正策略，可以提升谱图神经网络的表达能力，使多项式滤波器摆脱重复特征值输入的限制，并增强了特征值的均匀分布。 |
| [^17] | [Cross-domain Random Pre-training with Prototypes for Reinforcement Learning](https://arxiv.org/abs/2302.05614) | 提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。 |
| [^18] | [Learning to Manipulate under Limited Information.](http://arxiv.org/abs/2401.16412) | 本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。 |
| [^19] | [SoK: Facial Deepfake Detectors.](http://arxiv.org/abs/2401.04364) | 本文对最新的面部深度伪造检测器进行了全面回顾和分析，提供了对其有效性影响因素的深入见解，并在各种攻击场景中进行了评估。 |
| [^20] | [Communication-Efficient Federated Optimization over Semi-Decentralized Networks.](http://arxiv.org/abs/2311.18787) | 本文提出了一种基于半分散网络的通信高效算法PISCO, 通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。 |
| [^21] | [From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces.](http://arxiv.org/abs/2310.19786) | 通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。 |
| [^22] | [Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning.](http://arxiv.org/abs/2310.18209) | 提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。 |
| [^23] | [Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules.](http://arxiv.org/abs/2310.06746) | 通过因果规则学习，我们可以利用加权因果规则来估计和加强对异质治疗效应的理解。 |
| [^24] | [ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery.](http://arxiv.org/abs/2309.17203) | ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。 |
| [^25] | [MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems.](http://arxiv.org/abs/2309.08421) | 本研究开发了一种新颖的机器学习框架MIML，该框架通过将无标记细胞图像与生物力学属性数据相结合，实现了高精度细胞分类。该方法利用了形态信息，将细胞属性理解得更全面，相较于仅考虑单一数据类型的模型，实现了98.3％的分类精度。该方法已在白细胞和肿瘤细胞分类中得到证明，并具有更广泛的应用潜力。 |
| [^26] | [Reinforcement Learning for Generative AI: A Survey.](http://arxiv.org/abs/2308.14328) | 该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。 |
| [^27] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^28] | [Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning.](http://arxiv.org/abs/2307.04996) | 本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。 |
| [^29] | [FedNoisy: Federated Noisy Label Learning Benchmark.](http://arxiv.org/abs/2306.11650) | FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。 |
| [^30] | [Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach.](http://arxiv.org/abs/2306.07566) | 本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。 |
| [^31] | [Fast and Effective GNN Training with Linearized Random Spanning Trees.](http://arxiv.org/abs/2306.04828) | 本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。 |
| [^32] | [How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?.](http://arxiv.org/abs/2305.06587) | 该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。 |
| [^33] | [Quantitative Trading using Deep Q Learning.](http://arxiv.org/abs/2304.06037) | 本文探讨了强化学习在量化交易中的运用，揭示了基于 RL 的交易算法的案例研究结果表明，它有潜力优于传统的交易算法。这一研究代表了一个有希望的研究领域，可以潜在地引导更为复杂和有效的交易系统的开发。 |

# 详细

[^1]: 超越尺度：具有任意类型未知超参数的无遗憾贝叶斯优化

    Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type

    [https://rss.arxiv.org/abs/2402.01632](https://rss.arxiv.org/abs/2402.01632)

    这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。

    

    贝叶斯优化需要拟合高斯过程模型，而拟合高斯过程模型需要指定超参数 - 大部分理论文献假设这些超参数是已知的。之前的理论研究通常假设数据在空间中均匀填充，而常用的高斯过程超参数的最大似然估计器只有在这种情况下才是一致的。然而，在贝叶斯优化中，数据不一定满足这种均匀填充的条件。由于无法保证超参数估计的正确性，并且这些超参数可以显著影响高斯过程拟合，因此对具有未知超参数的贝叶斯优化进行理论分析非常具有挑战性。之前提出的具有无遗憾特性的算法仅能处理特殊情况下的未知长度尺度、再生核希尔伯特空间范数，并且仅适用于频率派的情况。我们提出了一种新的算法，命名为HE-GP-UCB，它是第一个具有无遗憾特性的算法，在具有未知超参数的情况下实现了贝叶斯优化。

    Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
    
[^2]: 针对缺失数据预测的自适应优化方法

    Adaptive Optimization for Prediction with Missing Data

    [https://rss.arxiv.org/abs/2402.01543](https://rss.arxiv.org/abs/2402.01543)

    本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。

    

    在训练具有缺失条目的预测模型时，最常用和多功能的方法是一种流水线技术，首先填充缺失条目，然后计算预测结果。本文将缺失数据预测视为一个两阶段的自适应优化问题，并提出了一种新的模型类别，自适应线性回归模型，其中回归系数能够适应观测特征集。我们表明一些自适应线性回归模型等同于同时学习填充规则和下游线性回归模型而不是顺序学习。我们利用这种联合填充-回归的解释将我们的框架推广到非线性模型。在数据非完全随机缺失的情况下，我们的方法在样外准确性方面实现了2-10%的改进。

    When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
    
[^3]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^4]: 单次领域增量学习

    One-Shot Domain Incremental Learning

    [https://arxiv.org/abs/2403.16707](https://arxiv.org/abs/2403.16707)

    提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。

    

    在以前关于用于分类的深度神经网络模型的研究中已经讨论了领域增量学习（DIL）。在DIL中，我们假设随着时间的推移观察新领域上的样本。模型必须对所有领域上的输入进行分类。然而，在实践中，我们可能会遇到这样一种情况，即我们需要在新领域的样本仅间歇性地被观察的约束下执行DIL。因此，在本研究中，我们考虑了一个极端情况，即我们只有一份来自新领域的样本，我们称之为单次DIL。我们首先经验性地表明现有的DIL方法在单次DIL中表现不佳。通过各种调查，我们分析了这种失败的原因。根据我们的分析，我们明确了单次DIL的困难是由批归一化层中的统计数据引起的。因此，我们提出了一种关于这些统计数据的技术，并展示了我们技术的有效性。

    arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
    
[^5]: 使用生成式深度学习设计自适应LPD雷达波形

    Adaptive LPD Radar Waveform Design with Generative Deep Learning

    [https://arxiv.org/abs/2403.12254](https://arxiv.org/abs/2403.12254)

    使用生成式深度学习设计自适应LPD雷达波形，以在不被发现的情况下有效地进行测距和感知

    

    我们提出了一种新颖的基于学习的方法，用于自适应生成低概率检测（LPD）雷达波形，使其与其操作环境融为一体。我们设计的波形旨在遵循一个与环境中的无线电频率（RF）背景无法区分的分布，同时仍然有效用于测距和感知。为此，我们使用了一种无监督的对抗性学习框架；我们的生成器网络生成旨在困惑评论家网络的波形，评论家网络被优化为区分生成的波形与背景。为了确保我们生成的波形对于感知仍然有效，我们引入并最小化了一个基于模糊函数的损失函数到生成的波形上。我们通过将我们生成的波形的单脉冲可检测性与使用单独训练的检测神经网络来比较传统LPD波形的性能来评估我们方法的性能。我们发现我们的方法能够ge

    arXiv:2403.12254v1 Announce Type: cross  Abstract: We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can ge
    
[^6]: KnowAgent: 知识增强规划用于基于LLM的Agent

    KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents

    [https://arxiv.org/abs/2403.03101](https://arxiv.org/abs/2403.03101)

    KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。

    

    大型语言模型(LLMs)在复杂推理任务中表现出巨大潜力，但在处理更复杂的挑战时仍有所不足，特别是与环境互动通过生成可执行动作时。这种不足主要来自于语言Agent中缺乏内置动作知识，导致在任务求解过程中无法有效引导规划轨迹，从而导致规划幻觉。为了解决这个问题，我们引入了KnowAgent，一种旨在通过整合显式动作知识来增强LLM规划能力的新方法。具体而言，KnowAgent采用了一个动作知识库和一个知识型自学习策略来限制规划过程中的行动路径，实现更合理的轨迹合成，进而提高语言Agent的计划性能。基于HotpotQA和ALFWorld的实验结果基于不同的主干模型。

    arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
    
[^7]: Hamiltonian性质测试

    Hamiltonian Property Testing

    [https://arxiv.org/abs/2403.02968](https://arxiv.org/abs/2403.02968)

    本文研究了Hamiltonian的本地性测试作为属性测试问题，重点在于确定未知的$n$比特Hamiltonian是否是$k$局部的，通过对$H$的时间演化进行访问来解决问题。

    

    本文研究了Hamiltonian本地性测试作为一个属性测试问题，即确定一个未知的$n$比特Hamiltonian $H$是否是$k$局部的，或者与所有$k$局部Hamiltonian都相距$\varepsilon$，并通过对$H$的时间演化进行访问来解决问题。

    arXiv:2403.02968v1 Announce Type: cross  Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2
    
[^8]: 几何图神经网络综述：数据结构、模型和应用

    A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications

    [https://arxiv.org/abs/2403.00485](https://arxiv.org/abs/2403.00485)

    该论文综述了几何图神经网络的数据结构、模型和应用，通过提出具备不变性/等变性属性的几何GNN来更好地表征几何图的几何形状和拓扑，并提供了现有模型的统一视角。

    

    几何图是一种具有几何特征的特殊图形，对于建模许多科学问题至关重要。与一般图不同，几何图通常具有平移、旋转和反射等物理对称性，这使它们难以被当前的图神经网络（GNN）有效处理。为了解决这个问题，研究人员提出了各种具备不变性/等变性属性的几何图神经网络，以更好地表征几何图的几何形状和拓扑。鉴于该领域的当前进展，有必要对与几何GNN相关的数据结构、模型和应用进行全面调查。在本文中，基于必要但简洁的数学基础知识，我们从几何消息传递的角度提供了现有模型的统一视角。此外，我们总结了应用程序以及相关数据集，以促进以后的研究。

    arXiv:2403.00485v1 Announce Type: new  Abstract: Geometric graph is a special kind of graph with geometric features, which is vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To tackle this issue, researchers proposed a variety of Geometric Graph Neural Networks equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later 
    
[^9]: 实现约$O(1/\epsilon)$的样本复杂度用于约束马尔可夫决策过程

    Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process

    [https://arxiv.org/abs/2402.16324](https://arxiv.org/abs/2402.16324)

    该论文提出了一种算法，在约束马尔可夫决策过程中实现了约$O(1/\epsilon)$的样本复杂度，相比先前文献中已有的$O(1/\epsilon^2)$样本复杂度有所提升。

    

    我们考虑约束马尔可夫决策过程（CMDP）的强化学习问题，在顺序学习和决策中满足安全性或资源约束方面起着关键作用。在这个问题中，我们拥有有限资源和未知转移概率的MDP。在每个阶段，我们采取一个行动，收集奖励并消耗一些资源，所有假设都是未知的，并且需要随着时间学习。在这项工作中，我们迈出了为CMDP问题推导出最优的问题相关保证的第一步。我们得出了一个对数遗憾界限，这转化为$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$的样本复杂度界限，其中$\kappa$是一个与问题相关的参数，但与$\epsilon$无关。我们的样本复杂度界限改进了先前文献中针对CMDP问题建立的$O(1/\epsilon^2)$样本复杂度。

    arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
    
[^10]: 通过上下文对抗游戏防御越狱提示

    Defending Jailbreak Prompts via In-Context Adversarial Game

    [https://arxiv.org/abs/2402.13148](https://arxiv.org/abs/2402.13148)

    介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。

    

    大语言模型(LLMs)展现出在不同应用领域中的显著能力。然而，对其安全性的担忧，特别是对越狱攻击的脆弱性，仍然存在。受到深度学习中对抗训练和LLM代理学习过程的启发，我们引入了上下文对抗游戏(ICAG)来防御越狱攻击，无需进行微调。ICAG利用代理学习进行对抗游戏，旨在动态扩展知识以防御越狱攻击。与依赖静态数据集的传统方法不同，ICAG采用迭代过程来增强防御和攻击代理。这一持续改进过程加强了对新生成的越狱提示的防御。我们的实证研究证实了ICAG的有效性，经由ICAG保护的LLMs在各种攻击场景中显著降低了越狱成功率。

    arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
    
[^11]: 具有等变性的预训练Transformer用于多域3D分子的统一几何学习

    Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules

    [https://arxiv.org/abs/2402.12714](https://arxiv.org/abs/2402.12714)

    提出了具有等变性的预训练Transformer(EPT)框架，能够统一多领域分子的几何学习，通过块增强表示和E(3)等变性实现更准确的3D结构表示。

    

    通过在大量未标记的3D分子上进行预训练已经展示出在各种科学应用中具有优势。然而，先前的努力通常集中在特定领域（蛋白质或小分子）的模型预训练上，错失了利用跨领域知识的机会。为了弥补这一差距，我们引入了等变预训练Transformer（EPT），这是一个新颖的预训练框架，旨在协调小分子和蛋白质的几何学习。具体来说，EPT通过块增强表示统一了多领域分子的几何建模，能够关注每个原子更广泛的上下文。在Transformer框架上，EPT进一步通过E(3)等变性进行增强，以促进准确表示3D结构。EPT的另一个关键创新是其块级预训练任务，这允许在包含小分子和蛋白质的数据集上进行联合预训练。

    arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim
    
[^12]: 在宽松假设下关于随机优化中Adam收敛性的研究

    On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions

    [https://arxiv.org/abs/2402.03982](https://arxiv.org/abs/2402.03982)

    本文研究了在宽松假设下的随机优化中Adam算法的收敛性。我们引入了一个全面的噪声模型，并证明了在这个模型下，Adam算法可以以较高的概率高效地寻找到一个稳定点。与其他随机一阶算法相比，Adam算法具有更好的自适应性能，无需调整步长和问题参数。

    

    适应性动量评估（Adam）算法在训练各种深度学习任务中非常有效。尽管如此，在非凸光滑场景下，特别是在可能存在无界梯度和仿射方差噪声的情况下，对于Adam的理论理解仍然有限。在本文中，我们研究了在这些具有挑战性条件下的普通Adam算法。我们引入了一个全面的噪声模型，该模型控制着仿射方差噪声、有界噪声和次高斯噪声。我们证明了在这个通用噪声模型下，Adam算法可以以$\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$的概率高效地寻找到一个稳定点，其中$T$表示总迭代次数，与随机一阶算法的更底效率相匹配。更重要的是，我们揭示了在相同条件下，Adam算法无需调整步长和任何问题参数，具有比随机梯度下降更好的自适应性能。

    The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide 
    
[^13]: 在量化噪声环境中利用连续可微激活函数进行学习的优化

    Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments

    [https://arxiv.org/abs/2402.02593](https://arxiv.org/abs/2402.02593)

    在量化噪声环境中，利用连续可微激活函数进行学习可以减轻模拟量化误差，为计算机视觉、信号处理等多个机器学习领域的硬件实现提供了指导。

    

    实际世界中的模拟系统固有地受到噪声的影响，这可能会阻碍各种深度学习模型的收敛性和准确性。我们证明了像GELU和SiLU这样的可微激活函数可以稳健地传播梯度，有助于减轻普遍存在于所有模拟系统中的模拟量化误差。我们在量化噪声存在的情况下进行了卷积、线性和Transformer网络的分析和训练。我们能够证明，与传统的修正线性激活函数相比，连续可微激活函数在抗噪声方面具有显著优势。与ReLU相比，在接近零时梯度误差高出100倍。我们的研究结果为选择适当的激活函数提供了指导，以实现在计算机视觉、信号处理等多个机器学习领域中具有高性能和可靠性的硬件实现。

    Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.
    
[^14]: 你的扩散模型实际上是一个可证明鲁棒的分类器

    Your Diffusion Model is Secretly a Certifiably Robust Classifier

    [https://arxiv.org/abs/2402.02316](https://arxiv.org/abs/2402.02316)

    这项研究提出了一种新的扩散分类器家族，称为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。通过将扩散分类器推广到分类高斯受损数据，并将其与随机平滑技术相结合，构建了具有非常量Lipschitzness的平滑分类器。这些NDCs显示出卓越的认证鲁棒性。

    

    近期，扩散模型被作为鲁棒分类的生成器分类器所采用。然而，对于扩散分类器鲁棒性的综合理论理解仍然缺乏，这让我们怀疑它们是否会容易受到未来更强攻击的影响。在本研究中，我们提出了一种新的扩散分类器家族，命名为噪声扩散分类器（NDCs），其具有最新的可证明的鲁棒性。具体来说，我们通过推导这些分布的证据下界（ELBOs），利用ELBO近似似然度量，并使用贝叶斯定理计算分类概率，将扩散分类器推广到分类高斯受损数据。我们将这些推广的扩散分类器与随机平滑技术相结合，构建具有非常量Lipschitzness的平滑分类器。实验结果表明我们提出的NDCs在鲁棒性方面具有卓越的认证能力。值得注意的是，我们是第一个达到80%的...

    Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
    
[^15]: 基于得分的因果表示学习：线性和一般的转化

    Score-based Causal Representation Learning: Linear and General Transformations

    [https://arxiv.org/abs/2402.00849](https://arxiv.org/abs/2402.00849)

    这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。

    

    本篇论文针对一般非参数潜在因果模型和将潜在变量映射到观测变量的未知转化，研究了基于干预的因果表示学习（CRL）。研究了线性和一般的转化。这篇论文同时讨论了可识别性和实现性两个方面。可识别性是指确定算法不相关的条件，以确保恢复真实的潜在因果变量和潜在因果图。实现性是指算法方面，解决设计算法来实现可识别保证的问题。通过将得分函数（即密度函数对数的梯度）与CRL之间建立新联系，本文设计了一种得分为基础的算法类，确保了可识别性和实现性。首先，本文专注于线性转化，并展示了每个n个随机硬干预下该转化的因果表示可识别。

    This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
    
[^16]: 用特征值修正提升谱图神经网络的表达能力

    Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction

    [https://arxiv.org/abs/2401.15603](https://arxiv.org/abs/2401.15603)

    该论文提出了一种特征值修正策略，可以提升谱图神经网络的表达能力，使多项式滤波器摆脱重复特征值输入的限制，并增强了特征值的均匀分布。

    

    在最近几年中，特征为多项式滤波器的谱图神经网络越来越受到关注，在节点分类等任务中取得了显著的表现。这些模型通常假设规范化拉普拉斯矩阵的特征值彼此不同，因此期望多项式滤波器具有很高的拟合能力。然而，本文在实证上观察到规范化拉普拉斯矩阵经常具有重复的特征值。此外，我们从理论上建立了可辨认特征值的数量在确定谱图神经网络的表达能力方面起着关键作用。鉴于这一观察结果，我们提出了一种特征值修正策略，可以使多项式滤波器摆脱重复特征值输入的限制。具体而言，所提出的特征值修正策略增强了特征值的均匀分布，从而减轻了谱图神经网络的表达能力受限的问题。

    arXiv:2401.15603v2 Announce Type: replace  Abstract: In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mit
    
[^17]: 具有原型的跨领域随机预训练用于强化学习

    Cross-domain Random Pre-training with Prototypes for Reinforcement Learning

    [https://arxiv.org/abs/2302.05614](https://arxiv.org/abs/2302.05614)

    提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。

    

    此工作已提交给IEEE进行可能的出版。 CRPTpro提出了一种用于基于图像的RL的跨领域自监督随机预训练框架，利用原型。 CRPTpro采用了跨领域随机策略，可以轻松快速地从多个领域中抽样多样化数据，以提高预训练效率。此外，通过提出一种新颖的内在损失进行原型表示学习，以在不同领域中预训练有效且通用的编码器。在没有微调的情况下，跨领域编码器可以高效地应用于不同领域中定义的具有挑战性的下游视觉控制RL任务。 与以前的方法如APT和Proto-RL相比，CRP

    arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
    
[^18]: 学习在有限信息下进行操纵

    Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])

    [http://arxiv.org/abs/2401.16412](http://arxiv.org/abs/2401.16412)

    本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。

    

    根据社会选择理论的经典结果，任何合理的偏好投票方法有时会给个体提供报告不真实偏好的激励。对于比较投票方法来说，不同投票方法在多大程度上更或者更少抵抗这种策略性操纵已成为一个关键考虑因素。在这里，我们通过神经网络在不同规模下对限制信息下学习如何利用给定投票方法进行操纵的成功程度来衡量操纵的抵抗力。我们训练了将近40,000个不同规模的神经网络来对抗8种不同的投票方法，在6种限制信息情况下，进行包含5-21名选民和3-6名候选人的委员会规模选举的操纵。我们发现，一些投票方法，如Borda方法，在有限信息下可以被神经网络高度操纵，而其他方法，如Instant Runoff方法，虽然被一个理想的操纵者利润化操纵，但在有限信息下不会受到操纵。

    By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
    
[^19]: SoK：面部深度伪造检测器

    SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])

    [http://arxiv.org/abs/2401.04364](http://arxiv.org/abs/2401.04364)

    本文对最新的面部深度伪造检测器进行了全面回顾和分析，提供了对其有效性影响因素的深入见解，并在各种攻击场景中进行了评估。

    

    深度伪造技术迅速成为对社会构成深远和严重威胁的原因之一，主要由于其易于制作和传播。这种情况加速了深度伪造检测技术的发展。然而，许多现有的检测器在验证时 heavily 依赖实验室生成的数据集，这可能无法有效地让它们应对新颖、新兴和实际的深度伪造技术。本文对最新的深度伪造检测器进行广泛全面的回顾和分析，根据几个关键标准对它们进行评估。这些标准将这些检测器分为 4 个高级组别和 13 个细粒度子组别，都遵循一个统一的标准概念框架。这种分类和框架提供了对影响检测器功效的因素的深入和实用的见解。我们对 16 个主要的检测器在各种标准的攻击场景中的普适性进行评估，包括黑盒攻击场景。

    Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
    
[^20]: 通信高效的半分散网络联邦优化

    Communication-Efficient Federated Optimization over Semi-Decentralized Networks. (arXiv:2311.18787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.18787](http://arxiv.org/abs/2311.18787)

    本文提出了一种基于半分散网络的通信高效算法PISCO, 通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。

    

    在大规模的联邦和分散式学习中，通信效率是最具挑战性的瓶颈之一。本文提出了一种半分散通信协议下的通信高效算法PISCO，通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。PISCO算法通过梯度追踪和多个本地更新保证了对数据异质性的鲁棒性。我们证明了PISCO算法在非凸问题上的收敛速度，并展示了在数量方面，PISCO算法具有线性加速的优势。

    In large-scale federated and decentralized learning, communication efficiency is one of the most challenging bottlenecks. While gossip communication -- where agents can exchange information with their connected neighbors -- is more cost-effective than communicating with the remote server, it often requires a greater number of communication rounds, especially for large and sparse networks. To tackle the trade-off, we examine the communication efficiency under a semi-decentralized communication protocol, in which agents can perform both agent-to-agent and agent-to-server communication in a probabilistic manner. We design a tailored communication-efficient algorithm over semi-decentralized networks, referred to as PISCO, which inherits the robustness to data heterogeneity thanks to gradient tracking and allows multiple local updates for saving communication. We establish the convergence rate of PISCO for nonconvex problems and show that PISCO enjoys a linear speedup in terms of the number
    
[^21]: 从外部到Swap遗憾2.0：针对大动作空间的高效约化和无知对手

    From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19786](http://arxiv.org/abs/2310.19786)

    通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。

    

    我们提供了一种新颖的从Swap遗憾最小化到外部遗憾最小化的约化方法，改进了Blum-Mansour和Stolz-Lugosi的经典约化方法，不需要行为空间的有限性。我们证明，只要存在某个假设类的无外部遗憾算法，就必然存在相同类别的无Swap遗憾算法。对于学习专家建议问题，我们的结果意味着可以保证在$\log(N)^{O(1/\epsilon)}$轮后，每次迭代复杂度为$O(N)$的情况下，Swap遗憾被限定为$\epsilon$，而Blum-Mansour和Stolz-Lugosi的经典约化方法需要$O(N/\epsilon^2)$轮和至少$\Omega(N^2)$的每次迭代复杂度。我们的结果伴随着一个相关的下界，与[BM07]不同，这个下界适用于无知和$\ell_1$-受限的对手和可以利用这个下界的学习者。

    We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
    
[^22]: 超卷曲图对比学习的对齐和外壳同构性

    Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])

    [http://arxiv.org/abs/2310.18209](http://arxiv.org/abs/2310.18209)

    提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。

    

    学习对下游任务有益的自监督图表示是具有挑战性的。在各种方法中，对比学习具有竞争力的性能。对比学习的嵌入被排列在一个超球面上，从而使得在欧几里德空间中的余弦距离测量成为可能。然而，许多领域的潜在几何结构，如图形，展现了高度非欧几里德的特性。为此，我们提出了一种新颖的对比学习框架，用于学习高质量的图嵌入。具体而言，我们设计了一种有效捕捉层次数据不变性信息的对齐度量，同时我们提出了一种替代均匀度量来防止所谓的维度塌陷。我们证明，在双曲空间中，必须解决与树的属性相关的叶子和高度层面的均匀性，而在双曲流形的环境空间中，这些概念转化为对同构性的施加。

    Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
    
[^23]: 因果规则学习：通过加权因果规则增强对异质治疗效应的理解

    Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules. (arXiv:2310.06746v1 [cs.LG])

    [http://arxiv.org/abs/2310.06746](http://arxiv.org/abs/2310.06746)

    通过因果规则学习，我们可以利用加权因果规则来估计和加强对异质治疗效应的理解。

    

    解释性是利用机器学习方法估计异质治疗效应时的关键问题，特别是对于医疗应用来说，常常需要做出高风险决策。受到解释性的预测性、描述性、相关性框架的启发，我们提出了因果规则学习，该方法通过找到描述潜在子群的精细因果规则集来估计和增强我们对异质治疗效应的理解。因果规则学习包括三个阶段：规则发现、规则选择和规则分析。在规则发现阶段，我们利用因果森林生成一组具有相应子群平均治疗效应的因果规则池。选择阶段使用D-学习方法从这些规则中选择子集，将个体水平的治疗效应作为子群水平效应的线性组合进行解构。这有助于回答之前文献忽视的问题：如果一个个体同时属于多个不同的治疗子群，会怎么样呢？

    Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously bel
    
[^24]: ComSD: 在无监督技能发现中平衡行为质量和多样性

    ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])

    [http://arxiv.org/abs/2309.17203](http://arxiv.org/abs/2309.17203)

    ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。

    

    无监督技能发现的理想方法能够在没有外部奖励的情况下产生多样且合格的技能，同时发现的技能集能够以各种方式高效地适应下游任务。本文提出了Contrastive multi-objectives Skill Discovery (ComSD)，通过更合理的互信息估计和动态加权的内在奖励来减轻发现的行为在质量和多样性之间的冲突。

    Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
    
[^25]: MIML: 通过微流控系统内的机械特性对高精度细胞分类进行多重图像机器学习

    MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v1 [eess.IV])

    [http://arxiv.org/abs/2309.08421](http://arxiv.org/abs/2309.08421)

    本研究开发了一种新颖的机器学习框架MIML，该框架通过将无标记细胞图像与生物力学属性数据相结合，实现了高精度细胞分类。该方法利用了形态信息，将细胞属性理解得更全面，相较于仅考虑单一数据类型的模型，实现了98.3％的分类精度。该方法已在白细胞和肿瘤细胞分类中得到证明，并具有更广泛的应用潜力。

    

    无标记细胞分类有助于为进一步使用或检查提供原始细胞，然而现有技术在特异性和速度方面往往不足。在本研究中，我们通过开发一种新颖的机器学习框架MIML来解决这些局限性。该架构将无标记细胞图像与生物力学属性数据相结合，利用每个细胞固有的广阔且常常被低估的形态信息。通过整合这两种类型的数据，我们的模型提供了对细胞属性更全面的理解，利用了传统机器学习模型中通常被丢弃的形态信息。这种方法使细胞分类精度达到了惊人的98.3％，大大优于仅考虑单一数据类型的模型。MIML已被证明在白细胞和肿瘤细胞分类中有效，并具有更广泛的应用潜力。

    Label-free cell classification is advantageous for supplying pristine cells for further use or examination, yet existing techniques frequently fall short in terms of specificity and speed. In this study, we address these limitations through the development of a novel machine learning framework, Multiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images with biomechanical property data, harnessing the vast, often underutilized morphological information intrinsic to each cell. By integrating both types of data, our model offers a more holistic understanding of the cellular properties, utilizing morphological information typically discarded in traditional machine learning models. This approach has led to a remarkable 98.3\% accuracy in cell classification, a substantial improvement over models that only consider a single data type. MIML has been proven effective in classifying white blood cells and tumor cells, with potential for broader applicatio
    
[^26]: 强化学习在生成型人工智能中的应用：综述

    Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14328](http://arxiv.org/abs/2308.14328)

    该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。

    

    深度生成型人工智能是机器学习界中一个长期存在的重要主题，可以影响到诸多应用领域，如文本生成和计算机视觉。训练生成模型的主要范式是最大似然估计，通过减小模型分布和目标分布之间的差异来推动学习器捕捉并逼近目标数据分布。这种公式成功地建立了生成任务的目标，然而却无法满足使用者对生成模型的所有需求。强化学习作为一种竞争性选择，通过创建新的目标来注入新的训练信号，展示了它的强大和灵活性，可以从多个角度引入人类归纳偏好，如对抗学习、手动设计规则和学习奖励模型，以建立一个性能良好的模型。

    Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
    
[^27]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^28]: 利用自动生成的知识图谱和强化学习增强推荐系统

    Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])

    [http://arxiv.org/abs/2307.04996](http://arxiv.org/abs/2307.04996)

    本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。

    

    个性化推荐在直接营销中越来越重要，激发了通过知识图谱（KG）应用来提升客户体验的研究动机。例如，在金融服务领域，公司可以通过向客户提供相关金融文章来培养关系，促进客户参与和促进知情的金融决策。尽管一些方法专注于基于KG的推荐系统以改进内容，但在本研究中，我们专注于可解释的基于KG的推荐系统来进行决策。为此，我们提出了两种基于知识图谱的个性化文章推荐方法，用于一家大型跨国金融服务公司的一组客户。第一种方法使用强化学习，第二种方法使用XGBoost算法来向客户推荐文章。这两种方法都利用从结构化（表格数据）和非结构化数据（大量文本数据）生成的KG。

    Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
    
[^29]: FedNoisy: 分布式噪声标签学习基准测试

    FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11650](http://arxiv.org/abs/2306.11650)

    FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。

    

    联合学习已经因为无需对来自客户端的敏感数据进行聚合而变得受欢迎。但是，数据隔离的分布式和孤立性可能会受到数据质量的复杂性的影响，使其更容易受到噪声标签的干扰。许多努力都致力于在集中式或联合式环境中防御噪声标签的负面影响。然而，缺乏一个全面考虑各种典型联合学习场景中噪声标签影响的基准测试。在这项工作中，我们提供了第一个标准化的基准测试，可以帮助研究人员充分探索潜在的联合噪声设置。此外，我们进行了全面的实验，探索这些数据设置的特性，并揭示了联合学习中的挑战性场景，这可能指导未来的方法开发。我们强调我们基准测试中提出的20个基本设置，适用于5个以上的数据集，并提供了标准化的仿真流程。

    Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
    
[^30]: 学习选择标签下的异质决策者：一种工具变量方法

    Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])

    [http://arxiv.org/abs/2306.07566](http://arxiv.org/abs/2306.07566)

    本文提出了一种处理选择性标记数据的学习问题的方法。通过利用历史决策由一组异质决策者做出的事实，我们建立了一种有原理的工具变量框架，并提出了一种加权学习方法，用于学习预测规则。

    

    我们研究了在选择性标记数据下的学习问题。这种问题在历史决策导致结果仅部分标记时出现。标记数据分布可能与整体人群有显著差异，特别是当历史决策和目标结果可以同时受某些未观察到的因素影响时。因此，仅基于标记数据进行学习可能会导致在整体人群中的严重偏差。我们的论文通过利用许多应用中历史决策由一组异质决策者做出的事实来解决此挑战。具体而言，我们在一个有原理的工具变量框架下分析了这种设置。我们建立了满足观察到的数据时任何给定预测规则的全体风险的点识别条件，并在点识别失败时提供了尖锐的风险界限。我们进一步提出了一种加权学习方法，用于学习预测规则。

    We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
    
[^31]: 线性化随机生成树与GNN的快速高效训练

    Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])

    [http://arxiv.org/abs/2306.04828](http://arxiv.org/abs/2306.04828)

    本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。

    

    我们提出了一种新的有效和可扩展的框架，用于在给定图形结构数据的监督节点分类任务中训练GNN。我们的方法通过线性化从输入网络中提取的随机生成树得到一系列路径图来逐步精细化权重更新操作。路径图被设计为保留原始图的基本拓扑和节点信息。同时，路径图的稀疏性使得GNN训练更轻便，除了可扩展性外，还有助于缓解过度压缩和过度平滑等经典训练问题。我们在多个真实世界的图形基准测试上进行了广泛的实验研究，并将我们的框架应用于图形卷积网络，与众所周知的基线相比，同时提高了训练速度和测试精度。

    We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
    
[^32]: 面向时间序列预测的谱时图神经网络的表达能力研究

    How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])

    [http://arxiv.org/abs/2305.06587](http://arxiv.org/abs/2305.06587)

    该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。

    

    谱时图神经网络是大多数基于图神经网络(GNN)的时间序列预测模型的一个有前途的抽象。然而，我们需要更多关于这种方法的基础知识。本文建立了一个理论框架，揭示了谱时GNN的表现力。我们的结果表明，具有线性谱时GNN是普适的，在温和的假设下，它们的表现力受到我们的离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。为了使我们的发现在实践中有用，我们详细讨论了相关限制，并概述了在谱域中设计空间和时间模块的理论蓝图。基于这些见解，并为了展示基于我们的框架，谱时GNN有多么强大，我们提出了一个名为 Temporal Graph GegenConv (TGC) 的简单实例，显著优于大多数已有的模型。

    Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
    
[^33]: 使用深度 Q 学习进行量化交易。

    Quantitative Trading using Deep Q Learning. (arXiv:2304.06037v1 [q-fin.TR])

    [http://arxiv.org/abs/2304.06037](http://arxiv.org/abs/2304.06037)

    本文探讨了强化学习在量化交易中的运用，揭示了基于 RL 的交易算法的案例研究结果表明，它有潜力优于传统的交易算法。这一研究代表了一个有希望的研究领域，可以潜在地引导更为复杂和有效的交易系统的开发。

    

    强化学习是机器学习的一个分支，已被用于机器人技术、游戏玩法和自主系统等多种应用。近年来，人们越来越关注强化学习在量化交易领域的应用，旨在在金融市场上进行盈利交易。本论文探讨了 RL 在量化交易中的使用，并提供了一个基于 RL 的交易算法的案例研究。结果显示，RL 可以成为量化交易的强有力工具，并且它有潜力优于传统的交易算法。在量化交易中使用强化学习代表着一个有希望的研究领域，可以潜在地引导更为复杂和有效的交易系统的开发。未来的工作可以探索使用替代的强化学习算法，整合其他数据来源，并在不同的资产类别上测试该系统。总体而言，我们的研究证明了它的潜力。

    Reinforcement learning (RL) is a branch of machine learning that has been used in a variety of applications such as robotics, game playing, and autonomous systems. In recent years, there has been growing interest in applying RL to quantitative trading, where the goal is to make profitable trades in financial markets. This paper explores the use of RL in quantitative trading and presents a case study of a RL-based trading algorithm. The results show that RL can be a powerful tool for quantitative trading, and that it has the potential to outperform traditional trading algorithms. The use of reinforcement learning in quantitative trading represents a promising area of research that can potentially lead to the development of more sophisticated and effective trading systems. Future work could explore the use of alternative reinforcement learning algorithms, incorporate additional data sources, and test the system on different asset classes. Overall, our research demonstrates the potential 
    

