# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning](https://arxiv.org/abs/2404.02127) | 本研究提出了一个名为LawInstruct的大型法律指导数据集，证明了领域特定的预训练和指导调整可以改善在LegalBench上的性能，为在法律领域开发具有更强信息处理和决策能力的模型提供了一个资源。 |
| [^2] | [Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement](https://arxiv.org/abs/2403.20298) | 本文基于评论文本提出了一种双曲CDR方法，以应对推荐系统中的数据稀疏性挑战，避免传统基于距离的领域对齐技术可能引发的问题。 |
| [^3] | [Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design](https://arxiv.org/abs/2403.18044) | 该研究开发了一种用于控制应用的深度多面体自编码器，在大规模系统的计算非线性控制器设计中展现出比标准线性方法更好的性能，其特定架构使得实现更高阶级数展开几乎没有额外计算负担。 |
| [^4] | [TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer](https://arxiv.org/abs/2403.12481) | TT-BLIP模型通过使用BLIP和Tri-Transformer技术，结合文本和图像的多模态信息提取，采用Multimodal Tri-Transformer融合特征，实现了增强的综合表征和改进的多模态数据分析。 |
| [^5] | [Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off](https://arxiv.org/abs/2402.14648) | 重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。 |
| [^6] | [Avoiding Catastrophe in Continuous Spaces by Asking for Help](https://arxiv.org/abs/2402.08062) | 在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。 |
| [^7] | [Return-Aligned Decision Transformer](https://arxiv.org/abs/2402.03923) | 本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。 |
| [^8] | [Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives](https://arxiv.org/abs/2312.03885) | 本论文通过计算高阶导数，将牛顿法应用于神经网络中，提出了一个适用于各种架构的深度神经网络的二阶优化方法。 |
| [^9] | [Explicitly Disentangled Representations in Object-Centric Learning.](http://arxiv.org/abs/2401.10148) | 这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。 |
| [^10] | [S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models.](http://arxiv.org/abs/2310.06715) | 本研究解析了基于深度学习的睡眠阶段分类模型的设计空间，找到了适用于不同输入表示的稳健架构，并在睡眠数据集上实现了显著的性能提升。 |
| [^11] | [Brain-Inspired Computational Intelligence via Predictive Coding.](http://arxiv.org/abs/2308.07870) | 这项研究介绍了一种通过预测编码的脑启发式计算智能方法，它可以解决现有人工智能方法的一些重要限制，并具有在机器学习领域有希望的应用潜力。 |
| [^12] | [Assessing the Use of AutoML for Data-Driven Software Engineering.](http://arxiv.org/abs/2307.10774) | AutoML作为一种自动化构建端到端AI/ML流水线的解决方案被广泛关注，但目前对其在开发AI/ML系统的团队中的采用程度和感知程度缺乏信息。 |
| [^13] | [Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning.](http://arxiv.org/abs/2306.05494) | 本文对于基于机器学习的网络入侵检测系统(NIDS)的对抗性攻击进行了分类，同时探究了持续再训练对NIDS对抗性攻击的影响。实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。 |
| [^14] | [One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data.](http://arxiv.org/abs/2302.06375) | 本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。 |
| [^15] | [Models for information propagation on graphs.](http://arxiv.org/abs/2201.07577) | 本文提出了统一的图上信息传播模型，其中包括三种不同的类别，利用波、路径行程时间和eikonal方程来描述信息的传播，并给出了它们之间的等价性。此外，本文还提出了一种新的混合模型，用于描述波和eikonal模型的结合。作者在随机图形、小世界图和实际网络上进行了数值模拟。 |

# 详细

[^1]: FLawN-T5: 有效指导调整数据混合在法律推理中的实证研究

    FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning

    [https://arxiv.org/abs/2404.02127](https://arxiv.org/abs/2404.02127)

    本研究提出了一个名为LawInstruct的大型法律指导数据集，证明了领域特定的预训练和指导调整可以改善在LegalBench上的性能，为在法律领域开发具有更强信息处理和决策能力的模型提供了一个资源。

    

    arXiv:2404.02127v1  公告类型: 跨领域  摘要: 指导调整是使语言模型对直接用户交互有效的重要步骤。然而，许多法律任务仍然超出了大多数开放式LLMs的范围，而且目前该领域还没有任何大规模的数据集。这严重限制了该应用领域的研究。在这项工作中，我们策划了一个名为LawInstruct的大型法律指导数据集，涵盖了17个司法管辖区、24种语言，总计1200万个示例。我们呈现证据表明，领域特定的预训练和指导调整能够改善在LegalBench上的性能，包括将Flan-T5 XL在基准线上提高8个点或16%。然而，该效应并不适用于所有任务、训练模式、模型大小和其他因素。LawInstruct是一个资源，可以加速在法律领域开发具有更强信息处理和决策能力的模型。

    arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
    
[^2]: 基于双曲嵌入和层次感知域解耦的基于评论的跨领域推荐

    Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement

    [https://arxiv.org/abs/2403.20298](https://arxiv.org/abs/2403.20298)

    本文基于评论文本提出了一种双曲CDR方法，以应对推荐系统中的数据稀疏性挑战，避免传统基于距离的领域对齐技术可能引发的问题。

    

    数据稀疏性问题对推荐系统构成了重要挑战。本文提出了一种基于评论文本的算法，以应对这一问题。此外，跨领域推荐（CDR）吸引了广泛关注，它捕捉可在领域间共享的知识，并将其从更丰富的领域（源领域）转移到更稀疏的领域（目标领域）。然而，现有大多数方法假设欧几里德嵌入空间，在准确表示更丰富的文本信息和处理用户和物品之间的复杂交互方面遇到困难。本文倡导一种基于评论文本的双曲CDR方法来建模用户-物品关系。首先强调了传统的基于距离的领域对齐技术可能会导致问题，因为在双曲几何中对小修改造成的干扰会被放大，最终导致层次性崩溃。

    arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
    
[^3]: 深度多面体自编码器用于低维线性参数变化逼近和非线性反馈设计

    Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design

    [https://arxiv.org/abs/2403.18044](https://arxiv.org/abs/2403.18044)

    该研究开发了一种用于控制应用的深度多面体自编码器，在大规模系统的计算非线性控制器设计中展现出比标准线性方法更好的性能，其特定架构使得实现更高阶级数展开几乎没有额外计算负担。

    

    多面体自编码器提供了多面体中状态的低维参数化。对于非线性PDE，这很容易应用于低维线性参数变化(LPV)逼近，因为它们已被用于通过状态相关Riccati方程的级数展开实现有效的非线性控制器设计。在这项工作中，我们开发了一种用于控制应用的多面体自编码器，并展示了它如何在视图非线性系统的LPV逼近方面优于标准线性方法，以及特定架构如何在几乎没有额外计算的情况下实现更高阶级数展开。我们通过彻底的数值研究展示了该方法在大规模系统的计算非线性控制器设计中的性质和潜力。

    arXiv:2403.18044v1 Announce Type: cross  Abstract: Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.
    
[^4]: TT-BLIP：使用BLIP和Tri-Transformer增强假新闻检测

    TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer

    [https://arxiv.org/abs/2403.12481](https://arxiv.org/abs/2403.12481)

    TT-BLIP模型通过使用BLIP和Tri-Transformer技术，结合文本和图像的多模态信息提取，采用Multimodal Tri-Transformer融合特征，实现了增强的综合表征和改进的多模态数据分析。

    

    arXiv:2403.12481v1 公告类型：新   摘要：检测假新闻受到了极大关注。许多先前的方法将独立编码的单模态数据进行串联，忽略了综合多模态信息的好处。此外，对于文本和图像缺乏专门的特征提取进一步限制了这些方法。本文介绍了一种名为TT-BLIP的端到端模型，该模型对三种类型的信息应用了引导式语言-图像预训练用于统一的视觉-语言理解和生成（BLIP）：BERT 和 BLIP\textsubscript{Txt} 用于文本，ResNet 和 BLIP\textsubscript{Img} 用于图像，以及用于多模态信息的双向 BLIP 编码器。多模态三角变换器使用三种类型的多头注意机制融合三模态特征，确保了增强表示和改进的多模态数据分析。实验使用了两个假新闻数据集，微博和Gossipcop。 结果表明，

    arXiv:2403.12481v1 Announce Type: new  Abstract: Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results in
    
[^5]: 在对抗训练中重新思考不变性正则化以改善鲁棒性-准确性权衡

    Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off

    [https://arxiv.org/abs/2402.14648](https://arxiv.org/abs/2402.14648)

    重新审视了基于表示的不变性正则化方法，提出了Asymmetrically Representation-regularized Adversarial Training (AR-AT)来解决“梯度冲突”和混合分布问题，改善鲁棒性-准确性权衡。

    

    尽管对抗训练一直是抵抗对抗性样本（AEs）的最先进方法，但它们存在鲁棒性-准确性权衡问题。在这项研究中，我们重新审视基于表示的不变性正则化，学习具有辨别性却对抗性不变的表示，旨在缓解这种权衡。我们在经验上确定了妨碍不变性正则化的两个关键问题：（1）不变性损失和分类目标之间的“梯度冲突”，表明存在“崩溃解”，以及（2）由于干净和对抗性输入的分布发散而出现的混合分布问题。为了解决这些问题，我们提出了一种不对称表示正则化的对抗训练（AR-AT），该方法结合了一个停止梯度操作和一个预测器来避免“崩溃解”，灵感来自最近的非对比自监督学习。

    arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
    
[^6]: 避免连续空间中的灾难：通过寻求帮助

    Avoiding Catastrophe in Continuous Spaces by Asking for Help

    [https://arxiv.org/abs/2402.08062](https://arxiv.org/abs/2402.08062)

    在连续空间中，通过寻求帮助来避免灾难。引入了一种上下文多臂赌博问题的变体，目标是最小化灾难发生的概率。提出了一种算法，在连续1D状态空间和相对简单的回报函数下，遗憾和向导师查询率都趋近于0。

    

    大多数具有正式遗憾保证的强化学习算法假设所有错误都是可逆的，并依赖于尝试所有可能的选项。当一些错误是无法修复甚至是灾难性的时，这种方法会导致糟糕的结果。我们提出了一种上下文多臂赌博问题的变体，在这个问题中，目标是最小化发生灾难的概率。具体而言，我们假设每轮的回报代表了在该轮避免灾难的概率，并尝试最大化回报的乘积（总体避免灾难的概率）。为了给 agent 一些成功的机会，我们允许有限次向导师提问，并假设回报函数为 Lipschitz 连续的。我们提出了一种算法，当时间跨度增长时，它的遗憾和向导师查询率都趋近于 0，假设是一个连续的 1D 状态空间和相对"简单"的回报函数。我们还提供了一个匹配的下界：在没有简单性假设的情况下，任何算法要么不断查询异常的行为，要么每次查询完全相同的行为。

    Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
    
[^7]: 返回对齐的决策Transformer

    Return-Aligned Decision Transformer

    [https://arxiv.org/abs/2402.03923](https://arxiv.org/abs/2402.03923)

    本研究提出了返回对齐的决策Transformer（RADT），通过分离回报与传统输入序列，实现有效地将实际回报与目标回报对齐。

    

    传统的离线强化学习方法旨在学习最大化累积奖励（即回报）的最优策略。然而，随着应用范围的扩大，训练能够最大化回报并使实际回报与指定目标回报对齐的智能体变得越来越重要，从而控制智能体的性能。决策Transformer（DT）通过监督学习优化生成以目标回报为条件的动作的策略，并配备了使用目标回报控制智能体的机制。尽管DT旨在对齐实际回报与目标回报，但我们在实验中发现了DT中实际回报与目标回报之间的差异。在本文中，我们提出了返回对齐的决策Transformer（RADT），旨在有效地将实际回报与目标回报对齐。我们的模型将回报从传统的输入序列中分离出来，传统输入序列通常包含回报、状态和动作。

    Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
    
[^8]: 通过高阶导数总结，将牛顿法应用于神经网络的改进

    Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives

    [https://arxiv.org/abs/2312.03885](https://arxiv.org/abs/2312.03885)

    本论文通过计算高阶导数，将牛顿法应用于神经网络中，提出了一个适用于各种架构的深度神经网络的二阶优化方法。

    

    我们考虑了一种应用于向量变量$\boldsymbol{\theta}$上的函数$\mathcal{L}$的基于梯度的优化方法，在这种情况下，$\boldsymbol{\theta}$被表示为元组$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$的张量。该框架包括许多常见的用例，例如通过梯度下降来训练神经网络。首先，我们提出了一种计算成本低廉的技术，通过自动微分和计算技巧，提供关于$\mathcal{L}$及其张量$\mathbf{T}_s$之间相互作用的高阶信息。其次，我们利用这种技术来建立一个二阶优化方法，适用于训练各种架构的深度神经网络。这个二阶方法利用了$\boldsymbol{\theta}$被分割为张量$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$的分区结构，因此不需要计算$\mathcal{L}$的Hessian矩阵。

    We consider a gradient-based optimization method applied to a function $\mathcal{L}$ of a vector of variables $\boldsymbol{\theta}$, in the case where $\boldsymbol{\theta}$ is represented as a tuple of tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\mathcal{L}$, especially about the interactions between the tensors $\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\boldsymbol{\theta}$ into tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\mathcal{
    
[^9]: 在物体中心化学习中明确解开的表示

    Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])

    [http://arxiv.org/abs/2401.10148](http://arxiv.org/abs/2401.10148)

    这篇论文提出了一种在物体中心化学习中明确解开形状和纹理成分的方法，通过将潜在空间划分为两个不重叠的子集，使得模型更加稳定和有效。

    

    从原始视觉数据中提取结构化表示是机器学习中一个重要且长期存在的挑战。最近，无监督学习物体中心化表示的技术引起了越来越多的关注。在这个背景下，增强潜在特征的稳定性可以提高下游任务训练的效率和效果。在这个方向上一个有希望的步骤是解开导致数据变化的因素。先前，不变卡槽注意实现了从其他特征中解开位置、尺度和方向。扩展这一方法，我们着重于分离形状和纹理组成部分。特别地，我们提出了一种新颖的架构，将物体中心化模型中的形状和纹理成分偏置为潜在空间维度的两个不重叠子集。这些子集是先验已知的，因此在训练过程之前。在一系列物体中心化测试中进行的实验揭示了...

    Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
    
[^10]: S4Sleep: 解析基于深度学习的睡眠阶段分类模型的设计空间

    S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models. (arXiv:2310.06715v1 [cs.LG])

    [http://arxiv.org/abs/2310.06715](http://arxiv.org/abs/2310.06715)

    本研究解析了基于深度学习的睡眠阶段分类模型的设计空间，找到了适用于不同输入表示的稳健架构，并在睡眠数据集上实现了显著的性能提升。

    

    对于多通道睡眠脑电图记录进行睡眠阶段打分是一项耗时且存在显著的评分人员之间差异的任务。因此，应用机器学习算法可以带来很大的益处。虽然已经为此提出了许多算法，但某些关键的架构决策并未得到系统性的探索。在本研究中，我们详细调查了广泛的编码器-预测器架构范畴内的这些设计选择。我们找到了适用于时间序列和声谱图输入表示的稳健架构。这些架构将结构化状态空间模型作为组成部分，对广泛的SHHS数据集的性能进行了统计显著的提升。这些改进通过统计和系统误差估计进行了评估。我们预计，从本研究中获得的架构洞察不仅对未来的睡眠分期研究有价值，而且对整体睡眠研究都有价值。

    Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hol
    
[^11]: 通过预测编码实现脑启发式计算智能

    Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])

    [http://arxiv.org/abs/2308.07870](http://arxiv.org/abs/2308.07870)

    这项研究介绍了一种通过预测编码的脑启发式计算智能方法，它可以解决现有人工智能方法的一些重要限制，并具有在机器学习领域有希望的应用潜力。

    

    人工智能（AI）正在迅速成为本世纪的关键技术之一。到目前为止，在AI领域取得的大部分成果都是使用误差反向传播学习算法训练的深度神经网络所实现的。然而，这种方法的普及应用已经凸显出了一些重要的局限性，例如计算成本高、难以量化不确定性、缺乏鲁棒性、不可靠性和生物学上的不合理性。解决这些限制可能需要受到神经科学理论的启发和指导的方案。其中一种理论称为预测编码（PC），在机器智能任务中表现出有希望的性能，具有令人兴奋的特性，使其在机器学习社区中具有潜在的价值：PC可以模拟不同脑区的信息处理，可以用于认知控制和机器人技术，并在变分推理方面具有坚实的数学基础，提供了一个强大的工具。

    Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
    
[^12]: 评估AutoML在数据驱动软件工程中的应用

    Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])

    [http://arxiv.org/abs/2307.10774](http://arxiv.org/abs/2307.10774)

    AutoML作为一种自动化构建端到端AI/ML流水线的解决方案被广泛关注，但目前对其在开发AI/ML系统的团队中的采用程度和感知程度缺乏信息。

    

    背景：由于人工智能（AI）和机器学习（ML）在构建软件应用方面的广泛应用，公司正努力招聘具有深入了解这些技术的员工。在这种情况下，AutoML作为填补AI / ML技能缺口的有希望的解决方案大受欢迎，因为它承诺自动化构建端到端AI / ML流水线，这些流水线通常由专门的团队成员设计。目标：尽管受到越来越多的关注和高期望，但目前对于开发AI / ML系统的团队当前采用AutoML的程度以及从实践者和研究者的视角来看它的感知程度缺乏信息。方法：为了填补这些空白，本文提出了一项混合方法研究，包括对两个软件工程数据集中12个端到端AutoML工具的基准测试以及用户调查和后续访谈，以进一步了解AutoML的采用和感知。结果：我们发现AutoML解决方案可以

    Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can 
    
[^13]: 神经网络中对抗性漏洞攻击的实用性测试：动态学习的影响

    Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])

    [http://arxiv.org/abs/2306.05494](http://arxiv.org/abs/2306.05494)

    本文对于基于机器学习的网络入侵检测系统(NIDS)的对抗性攻击进行了分类，同时探究了持续再训练对NIDS对抗性攻击的影响。实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。

    

    机器学习被广泛应用于网络入侵检测系统(NIDS)中，由于其自动化的特性和在处理和分类大量数据上的高精度。但机器学习存在缺陷，其中最大的问题之一是对抗性攻击，其目的是使机器学习模型产生错误的预测。本文提出了两个独特的贡献：对抗性攻击对基于机器学习的NIDS实用性问题的分类和对持续训练对NIDS对抗性攻击的影响进行了研究。我们的实验表明，即使没有对抗性训练，持续再训练也可以减少对抗性攻击的影响。虽然对抗性攻击可能会危及基于机器学习的NIDS，但持续再训练可带来一定的缓解效果。

    Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
    
[^14]: 一种适用于所有时间序列的Transformer：表示和训练具有时间相关的异构表格数据

    One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06375](http://arxiv.org/abs/2302.06375)

    本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。

    

    近年来，将深度学习技术应用于表格数据的兴趣日益增长，以复制其他人工智能领域在这一结构化领域的成功。特别有趣的是，表格数据具有时间依赖性，例如金融交易。然而，表格值的异质性，其中类别元素与数值项混合，使得这种适应变得困难。在本文中，我们提出了一种Transformer架构来表示异构的时间相关的表格数据，数值特征使用一组频率函数表示，并且整个网络使用唯一的损失函数进行统一训练。

    There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
    
[^15]: 图上信息传播模型

    Models for information propagation on graphs. (arXiv:2201.07577v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2201.07577](http://arxiv.org/abs/2201.07577)

    本文提出了统一的图上信息传播模型，其中包括三种不同的类别，利用波、路径行程时间和eikonal方程来描述信息的传播，并给出了它们之间的等价性。此外，本文还提出了一种新的混合模型，用于描述波和eikonal模型的结合。作者在随机图形、小世界图和实际网络上进行了数值模拟。

    

    我们提出和统一了不同的图上信息传播模型。第一类模型将传播建模为一种波，它从一组已知节点在初始时间开始向所有其他未知节点传播，传播的顺序由信息波前的到达时间确定。第二类模型基于路径上的行程时间的概念。从一组初始已知节点到一个节点的信息传播时间被定义为所有可以到达该节点的路径的子集上的广义旅行时间的最小值。最后一个模型类是通过在每个未知节点上施加一个eikonal形式的局部方程，并在已知节点处施加边界条件来给出的。在一个节点的解的值与具有较低值的相邻节点的解的值耦合。我们提供了模型类的精确公式，并证明了它们之间的等价性。受到第一到达时间模型和eikonal方程之间的联系的启发，我们提出了一种新的混合形式，结合了波和eikonal模型。最后，我们在各种图形上展示了模型的数值模拟，包括随机图形、小世界图和实际网络。

    We propose and unify classes of different models for information propagation over graphs. In a first class, propagation is modelled as a wave which emanates from a set of known nodes at an initial time, to all other unknown nodes at later times with an ordering determined by the arrival time of the information wave front. A second class of models is based on the notion of a travel time along paths between nodes. The time of information propagation from an initial known set of nodes to a node is defined as the minimum of a generalised travel time over subsets of all admissible paths. A final class is given by imposing a local equation of an eikonal form at each unknown node, with boundary conditions at the known nodes. The solution value of the local equation at a node is coupled to those of neighbouring nodes with lower values. We provide precise formulations of the model classes and prove equivalences between them. Motivated by the connection between first arrival time model and the e
    

