# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes](https://arxiv.org/abs/2402.05406) | 本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。 |
| [^2] | [Scalable Multi-view Clustering via Explicit Kernel Features Maps](https://arxiv.org/abs/2402.04794) | 本文介绍了一种可扩展的多视角子空间聚类框架，并提出了一种高效的优化策略，利用内核特征映射来减轻计算负担。该算法可在大规模数据集上应用，并在几分钟内完成。 |
| [^3] | [Multi-event Video-Text Retrieval.](http://arxiv.org/abs/2308.11551) | 本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。 |
| [^4] | [Representation-Driven Reinforcement Learning.](http://arxiv.org/abs/2305.19922) | 该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。 |

# 详细

[^1]: 现在所有人都修剪：仅使用前向传递的LLM结构化修剪

    Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes

    [https://arxiv.org/abs/2402.05406](https://arxiv.org/abs/2402.05406)

    本文提出了一种仅使用前向传递的LLM结构化修剪方法，通过Bonsai生成的修剪模型在性能上优于梯度-based结构化修剪方法，并且速度是半结构化修剪模型的两倍。

    

    鉴于非专业从业者和最富有资源的机构之间的硬件差距，尺寸不断增长的LLM变得越来越难以使用。虽然提出了许多方法来压缩LLM，以使其资源消耗可管理，但这些方法本身往往耗费资源，使其目标用户群无法接触到。在这项工作中，我们探讨了仅使用前向传递的LLM结构化修剪问题。我们希望让从业者能够修剪模型，使其规模大到硬件仅有足够的内存来运行推理。我们开发了Bonsai，这是一种无梯度、扰动修剪方法，能够生成小、快和准确的修剪模型。我们观察到，Bonsai生成的修剪模型（i）优于更昂贵的梯度-based结构化修剪方法生成的模型，并且（ii）与半结构化修剪模型相比，速度快一倍且准确性相当。

    Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
    
[^2]: 通过显式的内核特征映射实现可扩展的多视角聚类

    Scalable Multi-view Clustering via Explicit Kernel Features Maps

    [https://arxiv.org/abs/2402.04794](https://arxiv.org/abs/2402.04794)

    本文介绍了一种可扩展的多视角子空间聚类框架，并提出了一种高效的优化策略，利用内核特征映射来减轻计算负担。该算法可在大规模数据集上应用，并在几分钟内完成。

    

    多视角学习作为数据科学和机器学习中的重要组成部分引起了越来越多的关注，这是由于实际应用中多视角的普遍存在，特别是在网络的上下文中。本文介绍了一种新的可扩展的多视角子空间聚类框架。提出了一种高效的优化策略，利用内核特征映射来减轻计算负担，同时保持良好的聚类性能。算法的可扩展性意味着它可以在标准机器上应用于大规模数据集，包括具有数百万数据点的数据集，并在几分钟内完成。我们在真实世界各种规模的基准网络上进行了大量实验证明我们的算法在多视角子空间聚类方法和属性网络多视角方法方面的性能。

    A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks. In this paper we introduce a new scalability framework for multi-view subspace clustering. An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance. The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes. We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches.
    
[^3]: 多事件视频文本检索

    Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])

    [http://arxiv.org/abs/2308.11551](http://arxiv.org/abs/2308.11551)

    本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。

    

    视频文本检索（VTR）是互联网上海量视频文本数据时代中一项关键的多模态任务。使用双流视觉-语言模型架构学习视频文本对的联合表示成为VTR任务中一种突出的方法。然而，这些模型在假设视频文本对应是双射的情况下运行，并忽视了更实际的情况，即视频内容通常涵盖多个事件，而用户查询或网页元数据等文本往往是具体的，并对应单个事件。这造成了之前的训练目标与实际应用之间的差距，在推理过程中可能导致早期模型的性能下降。本研究引入了多事件视频文本检索（MeVTR）任务，针对每个视频包含多个不同事件的场景，作为传统视频文本检索任务的一个利基场景。

    Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
    
[^4]: 表示驱动的强化学习框架

    Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19922](http://arxiv.org/abs/2305.19922)

    该论文提出了一个表示驱动的强化学习框架，通过在线性特征空间中嵌入策略网络，重新框定探索-利用问题为表示-利用问题，以实现最佳的探索。该框架通过应用进化和策略梯度法取得了显著的性能提升。

    

    我们提出了一个表示驱动的强化学习框架。通过将策略表示为其期望值的估计，我们利用来自情境推断的方法来指导探索和利用。特别地，将策略网络嵌入到线性特征空间中，使我们能够将探索-利用问题重新框定为表示-利用问题，其中良好的策略表示能够实现最佳的探索。我们通过应用进化和策略梯度法来展示该框架的有效性，相比于传统方法，这些方法带来了显著的性能提升。我们的框架提供了一种强化学习的新视角，强调了策略表示在决定最佳探索-利用策略方面的重要性。

    We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
    

