# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction](https://arxiv.org/abs/2404.02360) | FraGNNet是一种用于化合物到质谱预测的深度概率模型，能够高效准确地预测高分辨率谱，在性能上超越了现有的模型 |
| [^2] | [A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules](https://arxiv.org/abs/2404.01245) | 该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。 |
| [^3] | [Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching](https://arxiv.org/abs/2403.18705) | 本文介绍了一种通过一组受限耦合引入的条件Wasserstein距离，它等于后验Wasserstein距离的期望，推导了其性质，并提出了近似速度场的方法。 |
| [^4] | [Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing](https://arxiv.org/abs/2402.02817) | 本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。 |
| [^5] | [Predicting the cardinality and maximum degree of a reduced Gr\"obner basis.](http://arxiv.org/abs/2302.05364) | 该论文利用神经网络回归模型预测了简化Gr\"obner基的基数和最大总度数，结果表明神经网络具有更好的性能统计，相比于朴素猜测或多元回归模型。 |

# 详细

[^1]: FraGNNet：一种用于质谱预测的深度概率模型

    FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction

    [https://arxiv.org/abs/2404.02360](https://arxiv.org/abs/2404.02360)

    FraGNNet是一种用于化合物到质谱预测的深度概率模型，能够高效准确地预测高分辨率谱，在性能上超越了现有的模型

    

    识别复杂混合物中化合物的方法中，质谱的识别是一个关键步骤。传统的质谱到化合物（MS2C）问题的解决方案涉及将未知的质谱与已知的质谱-分子库进行匹配，但这种方法受限于库覆盖不完整。化合物到质谱（C2MS）模型可以通过将真实库与预测谱进行增强来提高检索率。不幸的是，许多现有的C2MS模型在预测分辨率、可扩展性或可解释性方面存在问题。我们开发了一种新的C2MS预测概率方法——FraGNNet，可以高效准确地预测高分辨率谱。FraGNNet使用结构化的潜在空间来揭示定义谱的基本过程。我们的模型在预测误差方面实现了最先进的性能，并超越了现有的C2MS模型。

    arXiv:2404.02360v1 Announce Type: new  Abstract: The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a t
    
[^2]: 大型语言模型水印的统计框架: 枢轴、检测效率和最优规则

    A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules

    [https://arxiv.org/abs/2404.01245](https://arxiv.org/abs/2404.01245)

    该论文提出了一个通用框架，用于设计大型语言模型水印的统计效率和检测规则，通过关键统计量和秘密密钥控制误报率，同时评估水印检测规则的能力。

    

    自ChatGPT于2022年11月推出以来，将几乎不可察觉的统计信号嵌入到大型语言模型（LLMs）生成的文本中，也被称为水印，已被用作从其人类撰写对应物上可证检测LLM生成文本的原则性方法。 本文介绍了一个通用灵活的框架，用于推理水印的统计效率并设计强大的检测规则。受水印检测的假设检验公式启发，我们的框架首先选择文本的枢轴统计量和由LLM提供给验证器的秘密密钥，以实现控制误报率（将人类撰写的文本错误地检测为LLM生成的错误）。 接下来，该框架允许通过获取渐近错误负率（将LLM生成文本错误地检测为人类撰写的错误）的封闭形式表达式来评估水印检测规则的能力。

    arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
    
[^3]: 具有贝叶斯OT流匹配应用的条件Wasserstein距离

    Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching

    [https://arxiv.org/abs/2403.18705](https://arxiv.org/abs/2403.18705)

    本文介绍了一种通过一组受限耦合引入的条件Wasserstein距离，它等于后验Wasserstein距离的期望，推导了其性质，并提出了近似速度场的方法。

    

    在逆问题中，许多条件生成模型通过最小化联合度量与其学习逼近之间的距离来近似后验测度。尽管这种方法在Kullback--Leibler分歧的情况下也控制后验测度之间的距离，但一般来说对于Wasserstein距离并不成立。在本文中，我们通过一组受限耦合引入了一种条件Wasserstein距离，它等于后验Wasserstein距离的期望。有趣的是，条件Wasserstein-1流的对偶形式以一种非常自然的方式类似于条件Wasserstein GAN文献中的损失。我们推导了条件Wasserstein距离的理论性质，表征相应的测地线和速度场以及流ODE。随后，我们建议通过放宽条件Wasserstein距离来近似速度场。

    arXiv:2403.18705v1 Announce Type: new  Abstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein dista
    
[^4]: 基于先处理、中处理和后处理的线性差异约束下的贝叶斯最优公平分类

    Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing

    [https://arxiv.org/abs/2402.02817](https://arxiv.org/abs/2402.02817)

    本文提出了一种基于贝叶斯最优的公平分类方法，通过先处理、中处理和后处理来最小化分类错误，并在给定群体公平性约束的情况下进行优化。该方法引入了线性和双线性差异度量的概念，并找到了贝叶斯最优公平分类器的形式。本方法能够处理多个公平性约束和常见情况。

    

    机器学习算法可能对受保护的群体产生不公平的影响。为解决这个问题，我们开发了基于贝叶斯最优的公平分类方法，旨在在给定群体公平性约束的情况下最小化分类错误。我们引入了线性差异度量的概念，它们是概率分类器的线性函数；以及双线性差异度量，它们在群体回归函数方面也是线性的。我们证明了几种常见的差异度量（如人口平等、机会平等和预测平等）都是双线性的。我们通过揭示与Neyman-Pearson引理的连接，找到了在单一线性差异度量下的贝叶斯最优公平分类器的形式。对于双线性差异度量，贝叶斯最优公平分类器变成了群体阈值规则。我们的方法还可以处理多个公平性约束（如平等的几率）和受保护属性常见的情况。

    Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
    
[^5]: 预测简化Gr\"obner基数和最大度数的神经网络回归模型

    Predicting the cardinality and maximum degree of a reduced Gr\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)

    [http://arxiv.org/abs/2302.05364](http://arxiv.org/abs/2302.05364)

    该论文利用神经网络回归模型预测了简化Gr\"obner基的基数和最大总度数，结果表明神经网络具有更好的性能统计，相比于朴素猜测或多元回归模型。

    

    我们构建了神经网络回归模型，用以预测二项理想的Gr\"obner基复杂度的关键指标。这项工作说明了为什么利用神经网络从Gr\"obner计算中进行预测并不是一个简单的过程。我们使用两个概率模型来生成和提供一个大规模的数据集，能够捕捉到Gr\"obner复杂度的足够变异性。我们利用这些数据来训练神经网络并预测简化Gr\"obner基的基数和其元素的最大总度数。虽然基数预测问题不同于经典的机器学习问题，但我们的模拟结果表明，神经网络具有更好的性能统计，如$r^2 = 0.401$，相比于朴素猜测或多元回归模型的$r^2 = 0.180$。

    We construct neural network regression models to predict key metrics of complexity for Gr\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.
    

