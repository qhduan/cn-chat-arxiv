# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR](https://arxiv.org/abs/2403.18364) | 提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。 |
| [^2] | [Early Period of Training Impacts Out-of-Distribution Generalization](https://arxiv.org/abs/2403.15210) | 神经网络训练早期阶段的影响对超出分布泛化进行了研究，通过探究学习动态和用于调查的渐进解冻方法，揭示了其重要性。 |
| [^3] | [Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt](https://arxiv.org/abs/2403.11780) | 提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。 |
| [^4] | [Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods](https://arxiv.org/abs/2403.08352) | 自动化机器学习的数据增强方法旨在自动化数据增强过程，为改善机器学习模型泛化性能提供了更高效的方式。 |
| [^5] | [Normalising Flow-based Differentiable Particle Filters](https://arxiv.org/abs/2403.01499) | 本文提出了一种基于正规化流的可微粒子滤波器框架，能够实现有效的概率密度估计并在复杂环境中灵活学习这些模块。 |
| [^6] | [Theoretical Foundations of Deep Selective State-Space Models](https://arxiv.org/abs/2402.19047) | 随着GateLoop、Mamba和GLA等具有乘法交互的线性递归驱动下的深度SSM架构的出现，它们在准确性和效率上超越了基于注意力的文本训练的基础模型。 |
| [^7] | [Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior](https://arxiv.org/abs/2402.15402) | 该论文提出了一种具有策略结构先验的高效未知物体重新排列系统，通过内外环的学习，实现了抓取、观察和放置在感知噪声中的优化。 |
| [^8] | [Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer](https://arxiv.org/abs/2402.15173) | 提出了HiZOO，一种对角Hessian信息的零阶优化器，以增强LLMs微调过程中的模型收敛速度和准确性 |
| [^9] | [Scaling Efficient LLMs](https://arxiv.org/abs/2402.14746) | 训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。 |
| [^10] | [A Bound on the Maximal Marginal Degrees of Freedom](https://arxiv.org/abs/2402.12885) | 该论文提出了对于核岭回归的低秩近似和替代方法中，关于低维近似秩的一个下界，从而保证可靠的预测能力，并将有效维度与最大统计杠杆得分联系起来。 |
| [^11] | [Automated Security Response through Online Learning with Adaptive Conjectures](https://arxiv.org/abs/2402.12499) | 该论文通过自适应猜想的在线学习，提出了一种适用于IT基础设施的自动化安全响应方法，其中游戏参与者通过Bayesian学习调整猜想，并通过推演更新策略，最终实现了最佳拟合，提高了推演在猜想模型下的性能。 |
| [^12] | [Rethink Model Re-Basin and the Linear Mode Connectivity](https://arxiv.org/abs/2402.05966) | 本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。 |
| [^13] | [Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation](https://arxiv.org/abs/2402.04031) | Polyp-DDPM是一种基于扩散的方法，利用分割掩码生成逼真的胃肠道息肉图像，提升了分割效果，并在图像质量和分割性能方面优于现有方法，为训练提供了高质量、多样化的合成数据集，使得分割模型达到与真实图像相比可比的效果。 |
| [^14] | [Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models](https://arxiv.org/abs/2402.01749) | 本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。 |
| [^15] | [Testing Stationarity and Change Point Detection in Reinforcement Learning](https://arxiv.org/abs/2203.01707) | 开发了一种能够在非平稳环境中进行策略优化的强化学习方法，通过测试最优Q函数的非平稳性并开发序贯变点检测方法来实现。 |
| [^16] | [SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning.](http://arxiv.org/abs/2401.09949) | SymbolNet是一种神经网络方法，通过动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性，实现了符号回归。通过引入稀疏正则化项，我们的模型可以自适应调整自身的强度，并收敛到目标稀疏度水平。与现有方法相比，SymbolNet能高效处理具有超过10个输入的数据集。 |
| [^17] | [Improving Summarization with Human Edits.](http://arxiv.org/abs/2310.05857) | 本文介绍了一种改进摘要生成的方法，使用人工编辑的反馈数据，并通过序列对齐（不）似然训练(SALT)技术将人工编辑数据与模型生成数据结合起来。实验证明了这种方法在医学领域摘要生成中的有效性。 |
| [^18] | [Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation.](http://arxiv.org/abs/2310.02368) | 本论文提出了一种名为静态质量指标强化学习（RLSQM）的新技术，用于解决大型语言模型（LLM）在自动生成测试用例时可能生成不良代码异味的问题。通过训练特定的奖励模型和利用PPO算法进行优化，我们实现了对单个质量指标和整体质量的优化。 |
| [^19] | [A new solution and concrete implementation steps for Artificial General Intelligence.](http://arxiv.org/abs/2308.09721) | 本研究提出了在人工通用智能领域中解决现有技术缺陷的新方案，并分析了大型模型技术路线的局限性。 |
| [^20] | [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning.](http://arxiv.org/abs/2307.08941) | 该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。 |
| [^21] | [Test-Time Training on Video Streams.](http://arxiv.org/abs/2307.05014) | 该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。 |
| [^22] | [Partial Identifiability for Domain Adaptation.](http://arxiv.org/abs/2306.06510) | 本论文提出了一种针对无监督领域自适应的部分可识别性方法，通过依赖跨域因果机制的最小改变属性，在保持特定组分跨域不变的前提下最小化分布转移的不必要影响。 |
| [^23] | [Machine learning with tree tensor networks, CP rank constraints, and tensor dropout.](http://arxiv.org/abs/2305.19440) | 本文介绍了一种新的机器学习方法，通过基于树状张量网络的CP秩约束和张量丢弃，来构建低秩分类器，并在时尚-MNIST图像分类中展示出了优异的表现。 |
| [^24] | [Rethink Depth Separation with Intra-layer Links.](http://arxiv.org/abs/2305.07037) | 添加内部层连接可以显著提高网络的表示能力，并修改深度分离理论，使得带有内部层连接的浅层网络可以表示深层网络的一些困难函数。 |
| [^25] | [Approximation by non-symmetric networks for cross-domain learning.](http://arxiv.org/abs/2305.03890) | 本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。 |
| [^26] | [Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning.](http://arxiv.org/abs/2304.11834) | 该论文提出了一种新的迁移学习流程，通过绘制具有鲁棒性的子网络，改进了预训练模型在资源受限的边缘设备上的传输能力。 |
| [^27] | [GDP nowcasting with artificial neural networks: How much does long-term memory matter?.](http://arxiv.org/abs/2304.05805) | 通过比较四种人工神经网络和动态因子模型对美国GDP季度增长的预测表现，研究发现在平衡经济增长期间，更长的输入序列能够实现更准确的预测，但是这种效果会在不到两年的时间内消失。在经济动荡时期，长期记忆的效果变得明显。 |
| [^28] | [Block-regularized 5$\times$2 Cross-validated McNemar's Test for Comparing Two Classification Algorithms.](http://arxiv.org/abs/2304.03990) | 本论文提出了一种块正则化5×2交叉验证McNemar检验，该方法通过规范化训练集之间的重叠记录来产生高质量的误差率估计，相较于传统的留置方法有更高的功率和稳定性。 |
| [^29] | [Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data.](http://arxiv.org/abs/2206.00775) | 本文提出了一种基于自适应本地邻域的神经网络技术，用于从稀疏采样数据中高效重建MR图像，该技术具有较强的适应性和鲁棒性。 |
| [^30] | [Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators.](http://arxiv.org/abs/2106.06575) | Auto-NBA是一种能够高效搜索深度神经网络加速器的算法，它可以同时考虑优化网络、比特宽度和加速器三个耦合的方面。 |

# 详细

[^1]: 面向5G-NR的意图感知DRL上行动态调度器

    Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR

    [https://arxiv.org/abs/2403.18364](https://arxiv.org/abs/2403.18364)

    提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。

    

    我们研究了支持工业物联网用户设备（IIoT UEs）具有意图（即所请求的服务质量（QoS））和随机流量到达的问题。提出了一种基于深度强化学习（DRL）的集中动态调度器，用于学习如何在IIoT UEs之间调度可用通信资源的时间频率资源。所提出的调度器利用RL框架来适应无线通信系统和流量到达中的动态变化。此外，提出了一种基于图的简化方案，以减少RL框架的状态和动作空间，以实现快速收敛和更好的学习策略。仿真结果表明，与几种传统调度方案（如轮询、半静态和启发式方法）相比，所提出的智能调度器在保证IIoT UEs所表达的意图方面具有有效性。

    arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
    
[^2]: 训练早期影响超出分布泛化

    Early Period of Training Impacts Out-of-Distribution Generalization

    [https://arxiv.org/abs/2403.15210](https://arxiv.org/abs/2403.15210)

    神经网络训练早期阶段的影响对超出分布泛化进行了研究，通过探究学习动态和用于调查的渐进解冻方法，揭示了其重要性。

    

    先前的研究发现神经网络训练的早期阶段的差异显著影响分布内（ID）任务的表现。然而，神经网络往往对超出分布（OOD）数据敏感，在下游应用中变得不太可靠。然而，由于其复杂性和缺乏有效的分析方法，早期训练阶段对OOD泛化的影响仍未得到充分研究。在这项工作中，我们研究了学习动态和神经网络训练早期阶段的OOD泛化之间的关系。我们利用Fisher信息的痕迹和锐利度，重点关注渐进解冻（即在训练过程中逐渐解冻参数）作为研究方法。通过一系列实证实验，我们展示了1）在训练过程中选择不同时间点的可训练参数数量，即逐渐解冻的实现方式

    arXiv:2403.15210v1 Announce Type: new  Abstract: Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual 
    
[^3]: Prompt-Singer: 带自然语言提示的可控唱歌声音合成

    Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt

    [https://arxiv.org/abs/2403.11780](https://arxiv.org/abs/2403.11780)

    提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。

    

    近期的唱歌声音合成(SVS)方法取得了显著的音频质量和自然度，然而它们缺乏显式控制合成唱歌风格属性的能力。我们提出Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的SVS方法。我们采用基于仅解码器的变压器模型架构，具有多尺度层次结构，并设计了一个分离音高表示的范围旋律解耦的方法，从而实现了基于文本的音域控制同时保持了旋律准确性。此外，我们探索了各种实验设置，包括不同类型的文本表示，文本编码器微调，以及引入语音数据以减轻数据稀缺性，旨在促进进一步研究。实验证明，我们的模型具有良好的控制能力和音频质量。音频示例可访问 http://prompt-singer.

    arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
    
[^4]: 利用自动化机器学习的数据增强方法及与传统数据增强方法性能比较

    Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods

    [https://arxiv.org/abs/2403.08352](https://arxiv.org/abs/2403.08352)

    自动化机器学习的数据增强方法旨在自动化数据增强过程，为改善机器学习模型泛化性能提供了更高效的方式。

    

    数据增强被认为是常用于提高机器学习模型泛化性能的最重要的正则化技术。它主要涉及应用适当的数据转换操作，以创建具有所需属性的新数据样本。尽管其有效性，这一过程通常具有挑战性，因为手动创建和测试不同候选增强及其超参数需耗费大量时间。自动化数据增强方法旨在自动化这一过程。最先进的方法通常依赖于自动化机器学习（AutoML）原则。本研究提供了基于AutoML的数据增强技术的全面调查。我们讨论了使用AutoML实现数据增强的各种方法，包括数据操作、数据集成和数据合成技术。我们详细讨论了技术

    arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
    
[^5]: 基于正规化流的可微粒子滤波器

    Normalising Flow-based Differentiable Particle Filters

    [https://arxiv.org/abs/2403.01499](https://arxiv.org/abs/2403.01499)

    本文提出了一种基于正规化流的可微粒子滤波器框架，能够实现有效的概率密度估计并在复杂环境中灵活学习这些模块。

    

    最近，人们对将神经网络引入粒子滤波器中的兴趣日益增加，例如可微粒子滤波器，以实现在复杂环境中对非线性非高斯状态空间模型进行联合顺序状态估计和模型学习。现有的可微粒子滤波器主要由普通神经网络构建，不允许密度估计。因此，它们要么受限于自举粒子滤波框架，要么采用预定义的分布系列（例如高斯分布），限制了它们在更复杂的现实世界场景中的性能。本文提出了一种使用（条件）正规化流构建动态模型、提议分布和测量模型的可微粒子滤波器框架。这不仅使其能够产生有效的概率密度，还使所提出的方法能够灵活地学习这些模块。

    arXiv:2403.01499v1 Announce Type: new  Abstract: Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments. Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation. As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios. In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model. This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible w
    
[^6]: 深度选择性状态空间模型的理论基础

    Theoretical Foundations of Deep Selective State-Space Models

    [https://arxiv.org/abs/2402.19047](https://arxiv.org/abs/2402.19047)

    随着GateLoop、Mamba和GLA等具有乘法交互的线性递归驱动下的深度SSM架构的出现，它们在准确性和效率上超越了基于注意力的文本训练的基础模型。

    

    结构化状态空间模型（SSM）如S4，源自Gu等人的开创性工作，作为建模序列数据的有效方法而日益受到青睐。深度SSM在各种领域展现出卓越的性能，相较于基于注意力的transformers，训练和推理成本降低。最近的研究表明，如果驱动SSM的线性递归允许输入和隐藏状态之间的乘法交互（如GateLoop，Mamba，GLA），那么所得到的架构可以在准确性和效率上超越基于注意力的文本训练的基础模型，参数规模达到十亿级。在本文中，我们使用Rough Path Theory的工具，为这一最近的发现提供了理论基础：我们表明，当随机线性递归配备简单的输入控制转换（选择性机制）时，隐藏状态可被证明是低维的投影。

    arXiv:2402.19047v1 Announce Type: new  Abstract: Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional proj
    
[^7]: 抓取、观察和放置：具有策略结构先验的高效未知物体重新排列

    Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior

    [https://arxiv.org/abs/2402.15402](https://arxiv.org/abs/2402.15402)

    该论文提出了一种具有策略结构先验的高效未知物体重新排列系统，通过内外环的学习，实现了抓取、观察和放置在感知噪声中的优化。

    

    我们关注未知物体重新排列任务，即机器人应重新配置物体到由RGB-D图像指定的期望目标配置中。最近的研究通过整合基于学习的感知模块来探索未知物体重新排列系统。然而，它们对感知误差敏感，并且较少关注任务级性能。本文旨在开发一个有效的系统，用于在感知噪声中重新排列未知物体。我们在理论上揭示了噪声感知如何以分离的方式影响抓取和放置，并展示这样的分离结构不容易改善任务的最优性。我们提出了具有分离结构作为先验的GSP，一个双环系统。对于内环，我们学习主动观察策略以提高放置的感知。对于外环，我们学习一个抓取策略，意识到物体匹配和抓取能力。

    arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
    
[^8]: 无痛人工大语言模型的二阶微调：一种基于Hessian信息的零阶优化器

    Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer

    [https://arxiv.org/abs/2402.15173](https://arxiv.org/abs/2402.15173)

    提出了HiZOO，一种对角Hessian信息的零阶优化器，以增强LLMs微调过程中的模型收敛速度和准确性

    

    通过背向传播过程对大型语言模型（LLMs）进行微调，通常需要昂贵的GPU内存。最近的研究转向使用零阶优化器进行微调，通过两次前向传递显著节省内存。然而，这些优化器受不同维度之间参数曲率的异质性困扰。在这项工作中，我们提出了HiZOO，一种对角Hessian信息的零阶优化器，这是第一项利用对角Hessian增强零阶优化器进行LLMs微调的工作。HiZOO避免了昂贵的内存成本，并且每步只增加了一个前向传递。对各种模型（350M〜66B参数）进行的大量实验表明，HiZOO提高了模型收敛速度，显著减少了训练步骤，并有效提高了模型准确性。此外，我们可视化了HiZOO在测试函数上的优化轨迹，

    arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
    
[^9]: 扩展高效的LLM模型

    Scaling Efficient LLMs

    [https://arxiv.org/abs/2402.14746](https://arxiv.org/abs/2402.14746)

    训练得到的LLM模型通常是稀疏的，为了提高效率，研究了在训练语料上达到所需准确度的参数最少的高效LLM模型，得出了参数数量与自然训练语料规模之间的关系，并指出扩展可以揭示新技能。

    

    训练得到的LLM模型通常是稀疏的，即大部分参数为零，这引发了关于效率的问题。为此，我们研究了高效的LLM模型，即那些在训练语料上达到所需准确度的参数最少。具体地，我们比较了当前规模下训练损失的理论和实证估计，以获得自然训练语料中独特序列数量上下界的数量。我们的结果暗示：(1)要在训练语料中表示的技能数量翻倍，需要将语料规模大约扩展三到五倍，(2)对于高效的LLM模型，参数数量$N$和自然训练语料规模$D$满足$N \sim D^{0.58}$的关系，(3)如果一个LLM模型的参数数量小于训练语料中的独特序列数量，扩展可以揭示出新的技能。

    arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
    
[^10]: 对最大边际自由度的一个界限

    A Bound on the Maximal Marginal Degrees of Freedom

    [https://arxiv.org/abs/2402.12885](https://arxiv.org/abs/2402.12885)

    该论文提出了对于核岭回归的低秩近似和替代方法中，关于低维近似秩的一个下界，从而保证可靠的预测能力，并将有效维度与最大统计杠杆得分联系起来。

    

    arXiv:2402.12885v1 公告类型: 交叉摘要: 通用核岭回归在内存分配和计算时间上成本高昂。本文研究了核岭回归的低秩近似和替代方法，以应对这些困难。本文的基本贡献在于对低维近似的秩提出了一个下界，要求其保持可靠的预测能力。该界限将有效维度与最大统计杠杆得分联系起来。我们通过涉及核的正则性来表征有效维度及其随正则化参数的增长行为。对于适当选择的核，这种增长被证明是对数渐近的，从而证明了低秩近似作为Nyström方法的合理性。

    arXiv:2402.12885v1 Announce Type: cross  Abstract: Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\"om method.
    
[^11]: 通过自适应猜想的在线学习实现自动化安全响应

    Automated Security Response through Online Learning with Adaptive Conjectures

    [https://arxiv.org/abs/2402.12499](https://arxiv.org/abs/2402.12499)

    该论文通过自适应猜想的在线学习，提出了一种适用于IT基础设施的自动化安全响应方法，其中游戏参与者通过Bayesian学习调整猜想，并通过推演更新策略，最终实现了最佳拟合，提高了推演在猜想模型下的性能。

    

    我们研究了针对IT基础设施的自动化安全响应，并将攻击者和防御者之间的互动形式表述为一个部分观测、非平稳博弈。我们放宽了游戏模型正确规定的标准假设，并考虑每个参与者对模型有一个概率性猜想，可能在某种意义上错误规定，即真实模型的概率为0。这种形式允许我们捕捉关于基础设施和参与者意图的不确定性。为了在线学习有效的游戏策略，我们设计了一种新颖的方法，其中一个参与者通过贝叶斯学习迭代地调整其猜想，并通过推演更新其策略。我们证明了猜想会收敛到最佳拟合，并提供了在具有猜测模型的情况下推演实现性能改进的上限。为了刻画游戏的稳定状态，我们提出了Berk-Nash平衡的一个变种。

    arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
    
[^12]: 重新思考模型重新基底和线性模态连接性

    Rethink Model Re-Basin and the Linear Mode Connectivity

    [https://arxiv.org/abs/2402.05966](https://arxiv.org/abs/2402.05966)

    本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。

    

    最近的研究表明，对于足够宽的模型来说，大部分随机梯度下降（SGD）的解可以收敛到相同的基底，只是顺序可能不同。这种现象被称为模型重新基底的阶段，对于模型平均化有重要影响。然而，当前的重新基底策略在效果上存在局限性，因为对底层机制的理解不够全面。为了填补这一空白，我们的研究重新审视了标准做法，并揭示了现有匹配算法的频繁不足之处，我们通过适当的重归一化来缓解这些问题。通过引入更直接的分析方法，我们揭示了匹配算法与重归一化过程之间的相互作用。这种观点不仅澄清和改进了以前的研究结果，还促进了新的洞见。例如，它将线性模态连接性与剪枝联系起来，从而激发了一种轻量且有效的后剪枝插件，可以直接与任何现有的剪枝技术合并。

    Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
    
[^13]: Polyp-DDPM: 基于扩散的语义息肉合成方法，以增强分割效果

    Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation

    [https://arxiv.org/abs/2402.04031](https://arxiv.org/abs/2402.04031)

    Polyp-DDPM是一种基于扩散的方法，利用分割掩码生成逼真的胃肠道息肉图像，提升了分割效果，并在图像质量和分割性能方面优于现有方法，为训练提供了高质量、多样化的合成数据集，使得分割模型达到与真实图像相比可比的效果。

    

    本研究提出了Polyp-DDPM，一种基于扩散的方法，用于在条件掩码上生成逼真的息肉图像，旨在增强胃肠道息肉的分割效果。我们的方法解决了医学图像数据限制、高昂的注释成本和隐私问题带来的挑战。通过将扩散模型条件化于分割掩码（表示异常区域的二进制掩码），Polyp-DDPM在图像质量和分割性能方面优于现有方法（Frechet Inception Distance (FID) 评分为78.47，而高于83.79的评分；Intersection over Union (IoU) 为0.7156，而基准模型合成图像为0.6694以下，真实数据为0.7067）。我们的方法生成了高质量、多样化的合成数据集用于训练，从而提升了息肉分割模型与真实图像的可比性，并提供更大的数据增强能力以改善分割效果。

    This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve seg
    
[^14]: 迈向城市智能：城市基础模型综述与展望

    Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models

    [https://arxiv.org/abs/2402.01749](https://arxiv.org/abs/2402.01749)

    本文综述了城市基础模型在智能城市发展中的重要性和潜力，并提出了一个以数据为中心的分类方法。这个新兴领域面临着一些挑战，如缺乏清晰的定义和系统性的综述，需要进一步的研究和解决方案。

    

    机器学习技术现已成为智能城市服务进步的核心，对提高城市环境的效率、可持续性和宜居性起到至关重要的作用。最近出现的ChatGPT等基础模型在机器学习和人工智能领域标志着一个革命性的转变。它们在上下文理解、问题解决和适应各种任务方面的无与伦比的能力表明，将这些模型整合到城市领域中可能对智能城市的发展产生变革性影响。尽管对城市基础模型（UFMs）的兴趣日益增长，但这个新兴领域面临着一些挑战，如缺乏清晰的定义、系统性的综述和可普遍化的解决方案。为此，本文首先介绍了UFM的概念，并讨论了构建它们所面临的独特挑战。然后，我们提出了一个以数据为中心的分类方法，对当前与UFM相关的工作进行了分类。

    Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
    
[^15]: 在强化学习中测试平稳性和变点检测

    Testing Stationarity and Change Point Detection in Reinforcement Learning

    [https://arxiv.org/abs/2203.01707](https://arxiv.org/abs/2203.01707)

    开发了一种能够在非平稳环境中进行策略优化的强化学习方法，通过测试最优Q函数的非平稳性并开发序贯变点检测方法来实现。

    

    我们考虑可能非平稳环境下的离线强化学习（RL）方法。许多文献中现有的RL算法依赖于需要系统转换和奖励函数随时间保持恒定的平稳性假设。然而，实践中平稳性假设是有限制的，并且在许多应用中很可能被违反，包括交通信号控制、机器人技术和移动健康。在本文中，我们开发了一种一致的程序，基于预先收集的历史数据测试最优Q函数的非平稳性，无需额外的在线数据收集。基于所提出的检验，我们进一步开发了一种顺序变点检测方法，可以自然地与现有最先进的RL方法相结合，在非平稳环境中进行策略优化。我们的方法的有效性通过理论结果、仿真研究和实践中的案例得到了展示。

    arXiv:2203.01707v3 Announce Type: replace-cross  Abstract: We consider offline reinforcement learning (RL) methods in possibly nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal Q-function based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimization in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, an
    
[^16]: SymbolNet: 自适应动态修剪的神经符号回归

    SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])

    [http://arxiv.org/abs/2401.09949](http://arxiv.org/abs/2401.09949)

    SymbolNet是一种神经网络方法，通过动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性，实现了符号回归。通过引入稀疏正则化项，我们的模型可以自适应调整自身的强度，并收敛到目标稀疏度水平。与现有方法相比，SymbolNet能高效处理具有超过10个输入的数据集。

    

    与遗传编程的使用相反，神经网络方法可在高输入维度下有效扩展，并利用梯度方法加速方程搜索。常见的表达式复杂性约束方法依赖于多阶段修剪方法进行微调，但这往往会导致显著的性能损失。在本文中，我们提出了一种神经网络方法，即SymbolNet，以一种新颖的框架实现符号回归，该框架可以在单个训练中动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性。我们引入了每个修剪类型的稀疏正则化项，该项可以自适应调整自身的强度，并导致收敛到目标稀疏度水平。与大多数现有的符号回归方法无法高效处理具有超过10个输入的数据集不同，我们证明了我们的模型的有效性。

    Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the 
    
[^17]: 使用人工编辑改进摘要生成

    Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05857](http://arxiv.org/abs/2310.05857)

    本文介绍了一种改进摘要生成的方法，使用人工编辑的反馈数据，并通过序列对齐（不）似然训练(SALT)技术将人工编辑数据与模型生成数据结合起来。实验证明了这种方法在医学领域摘要生成中的有效性。

    

    最近的研究表明，通过人类反馈范式学习可以产生高质量的文本。现有的工作在通用领域抽象化摘要生成中使用人类反馈来训练大型语言模型(LLMs)，并获得了超越传统似然训练的摘要质量。在本文中，我们关注一种较少探索的人类反馈形式——人工编辑。我们提出了一种新颖的技术——序列对齐（不）似然训练(SALT)，在训练循环中同时使用人工编辑和模型生成的数据。此外，我们还展示了使用现有训练数据中的基准摘要来模拟人工编辑，以及在训练后获取的模型生成摘要，以减少对昂贵的人工编辑数据的需求。在实验中，我们将人类反馈的探索从通用领域摘要生成扩展到医学领域摘要生成。我们的结果表明SALT在改进摘要生成方面的有效性。

    Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
    
[^18]: 从自动反馈中进行强化学习以生成高质量的单元测试

    Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])

    [http://arxiv.org/abs/2310.02368](http://arxiv.org/abs/2310.02368)

    本论文提出了一种名为静态质量指标强化学习（RLSQM）的新技术，用于解决大型语言模型（LLM）在自动生成测试用例时可能生成不良代码异味的问题。通过训练特定的奖励模型和利用PPO算法进行优化，我们实现了对单个质量指标和整体质量的优化。

    

    软件测试是软件开发的关键方面，创建符合最佳实践的高质量测试对于有效的维护至关重要。最近，大型语言模型（LLM）在代码生成方面越来越受欢迎，包括自动创建测试用例。然而，这些LLM通常在大量公开可用的代码上进行训练，其中可能包含不符合最佳实践甚至包含测试代码异味（反模式）的测试用例。为了解决这个问题，我们提出了一种称为静态质量指标强化学习（RLSQM）的新技术。首先，我们分析了LLM生成的反模式，并展示了LLM可以生成不良的测试代码异味。因此，我们为每个静态质量指标训练了专门的奖励模型，然后利用Proximal Policy Optimization （PPO）来训练逐个优化单个质量指标的模型。此外，我们将这些奖励融合到一个统一的奖励模型中，以实现对整体质量的优化。

    Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca
    
[^19]: 人工通用智能的新解决方案和具体实施步骤

    A new solution and concrete implementation steps for Artificial General Intelligence. (arXiv:2308.09721v1 [cs.LG])

    [http://arxiv.org/abs/2308.09721](http://arxiv.org/abs/2308.09721)

    本研究提出了在人工通用智能领域中解决现有技术缺陷的新方案，并分析了大型模型技术路线的局限性。

    

    目前，主流人工智能通常采用“注意机制+深度学习”+“强化学习”的技术路径。在人工智能生成内容（AIGC）领域取得了重大进展，掀起了大模型的技术浪潮。但在涉及与实际环境交互的领域，如养老护理、家庭保姆、农业生产和车辆驾驶等，试错成本很高，需要大量试错的强化学习过程很难实现。因此，为了实现可应用于任何领域的人工通用智能（AGI），我们需要同时利用现有技术并解决现有技术的缺陷，从而进一步发展人工智能的技术浪潮。本文分析了大型模型技术路线的局限性，并通过解决这些局限性，提出解决方案，从而解决了这一问题。

    At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the
    
[^20]: NTK-近似MLP融合用于高效的语言模型微调

    NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])

    [http://arxiv.org/abs/2307.08941](http://arxiv.org/abs/2307.08941)

    该论文通过使用神经切向核近似MLP融合，提出了一种高效的语言模型微调方法。实验证明，这种方法能够在降低计算和存储开销的同时保持较好的模型性能。

    

    在许多自然语言处理应用中，微调预训练语言模型(PLM)已成为主要策略。然而，即使是微调PLM和进行推理也是昂贵的，特别是在计算能力较低的边缘设备上。已经广泛研究了一些通用的方法（例如量化和蒸馏）来减少PLM微调的计算/存储开销，但很少有一次性压缩技术被探索。在本文中，我们研究了多层感知器(MLP)模块中预训练语言模型(PLM)的神经切向核(NTK)，并提出通过NTK近似MLP融合来创建一个轻量级的PLM。为实现这一目标，我们将MLP重新视为一束子MLP，并将它们聚类为给定数量的质心，然后将其恢复为压缩的MLP，并意外地显示出对原始PLM的NTK进行良好近似的效果。在自然语言处理数据集上进行了大量实验以验证PLM微调的效果。

    Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
    
[^21]: 视频流上的测试时培训

    Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])

    [http://arxiv.org/abs/2307.05014](http://arxiv.org/abs/2307.05014)

    该论文扩展了测试时培训（TTT）到视频流的设置中，提出了在线TTT方法，相对于固定模型基线和离线TTT，在多个任务上都有显著的性能优势，包括实例和全景分割。

    

    先前的研究已经将测试时培训（TTT）确定为一种在测试时进一步改进训练模型的通用框架。在对每个测试实例进行预测之前，模型会使用自监督任务（例如使用掩蔽自动编码器进行图像重建）在同一实例上进行训练。我们将TTT扩展到流式设置中，其中多个测试实例（在我们的情况下为视频帧）按时间顺序到达。我们的扩展是在线TTT：当前模型从上个模型初始化，然后在当前帧和前几个帧的小窗口上进行训练。在线TTT在四个任务上明显优于固定模型基线，在三个实际数据集上的相对改进分别为45%和66%。令人惊讶的是，在线TTT也优于其离线版本，后者访问更多信息，可以训练所有帧而不考虑时间顺序。这与先前的研究结果不同。

    Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
    
[^22]: 针对领域自适应的部分可识别性

    Partial Identifiability for Domain Adaptation. (arXiv:2306.06510v1 [cs.LG])

    [http://arxiv.org/abs/2306.06510](http://arxiv.org/abs/2306.06510)

    本论文提出了一种针对无监督领域自适应的部分可识别性方法，通过依赖跨域因果机制的最小改变属性，在保持特定组分跨域不变的前提下最小化分布转移的不必要影响。

    

    无监督领域适应对于许多没有目标域标签信息的实际应用至关重要。通常情况下，如果没有进一步的假设，特征和标签的联合分布在目标域中是不可识别的。为了解决这个问题，我们依赖跨域因果机制的最小改变属性，以最小化分布转移的不必要影响。为了编码这个属性，我们首先使用一个带有两个分区潜变量子空间的潜变量模型来制定数据生成过程：不变部分的分布在跨域时保持不变，而稀疏的可变部分会在不同的域中发生变化。我们进一步限制了域移位对可变部分的影响。在温和的条件下，我们展示了部分可识别的潜变量，从而证明了目标域中数据和标签的联合分布也是可识别的。

    Unsupervised domain adaptation is critical to many real-world applications where label information is unavailable in the target domain. In general, without further assumptions, the joint distribution of the features and the label is not identifiable in the target domain. To address this issue, we rely on the property of minimal changes of causal mechanisms across domains to minimize unnecessary influences of distribution shifts. To encode this property, we first formulate the data-generating process using a latent variable model with two partitioned latent subspaces: invariant components whose distributions stay the same across domains and sparse changing components that vary across domains. We further constrain the domain shift to have a restrictive influence on the changing components. Under mild conditions, we show that the latent variables are partially identifiable, from which it follows that the joint distribution of data and labels in the target domain is also identifiable. Give
    
[^23]: 基于树张量网络、CP秩约束和张量丢弃的机器学习方法。

    Machine learning with tree tensor networks, CP rank constraints, and tensor dropout. (arXiv:2305.19440v1 [cs.LG])

    [http://arxiv.org/abs/2305.19440](http://arxiv.org/abs/2305.19440)

    本文介绍了一种新的机器学习方法，通过基于树状张量网络的CP秩约束和张量丢弃，来构建低秩分类器，并在时尚-MNIST图像分类中展示出了优异的表现。

    

    张量网络可以通过降低自由度来近似表示$N$阶张量，并构成一系列压缩的小张量网络。在[arXiv:2205.15296]文章中，作者提出可以通过对张量网络中的张量的CP秩附加约束，进一步降低计算成本。本文旨在展示如何利用基于树状张量网络(TTN)的CP秩约束和张量丢弃的方法来进行机器学习，并表明该方法在时尚-MNIST图像分类中优于其他基于张量网络的方法。当分支系数$b=4$时，低秩TTN分类器达到了测试集准确率90.3\%，同时拥有较低的计算成本。基于线性元素构成的张量网络分类器避免了深度神经网络的梯度消失问题。CP秩约束还有其他优点：可以减少和调整模型参数数量。

    Tensor networks approximate order-$N$ tensors with a reduced number of degrees of freedom that is only polynomial in $N$ and arranged as a network of partially contracted smaller tensors. As suggested in [arXiv:2205.15296] in the context of quantum many-body physics, computation costs can be further substantially reduced by imposing constraints on the canonical polyadic (CP) rank of the tensors in such networks. Here we demonstrate how tree tensor networks (TTN) with CP rank constraints and tensor dropout can be used in machine learning. The approach is found to outperform other tensor-network based methods in Fashion-MNIST image classification. A low-rank TTN classifier with branching ratio $b=4$ reaches test set accuracy 90.3\% with low computation costs. Consisting of mostly linear elements, tensor network classifiers avoid the vanishing gradient problem of deep neural networks. The CP rank constraints have additional advantages: The number of parameters can be decreased and tuned m
    
[^24]: 通过内部层连接重新思考深度分离

    Rethink Depth Separation with Intra-layer Links. (arXiv:2305.07037v1 [cs.LG])

    [http://arxiv.org/abs/2305.07037](http://arxiv.org/abs/2305.07037)

    添加内部层连接可以显著提高网络的表示能力，并修改深度分离理论，使得带有内部层连接的浅层网络可以表示深层网络的一些困难函数。

    

    深度分离理论现在被广泛认为是深度神经网络优越性的一个有效解释，它由两部分组成：i）存在一种可以由深度网络表示的函数；ii）这样的函数不能由宽度低于某一阈值的浅层网络表示。然而，这个理论是建立在前馈网络上的。很少有研究在向解决现实问题的最常见的网络类型——快捷网络中考虑深度分离理论。本文发现，添加内部层连接可以修改深度分离理论。首先，我们报告了通过界限估计、显式构造和功能空间分析可以通过添加内部层连接显著提高网络的表示能力。然后，我们通过展示一个带有内部层连接的浅层网络不需要像之前一样变得宽来表示由深层网络构造的一些困难函数来修改深度分离理论。

    The depth separation theory is nowadays widely accepted as an effective explanation for the power of depth, which consists of two parts: i) there exists a function representable by a deep network; ii) such a function cannot be represented by a shallow network whose width is lower than a threshold. However, this theory is established for feedforward networks. Few studies, if not none, considered the depth separation theory in the context of shortcuts which are the most common network types in solving real-world problems. Here, we find that adding intra-layer links can modify the depth separation theory. First, we report that adding intra-layer links can greatly improve a network's representation capability through bound estimation, explicit construction, and functional space analysis. Then, we modify the depth separation theory by showing that a shallow network with intra-layer links does not need to go as wide as before to express some hard functions constructed by a deep network. Such
    
[^25]: 非对称网络逼近用于跨域学习

    Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])

    [http://arxiv.org/abs/2305.03890](http://arxiv.org/abs/2305.03890)

    本文研究使用非对称内核进行基于内核网络逼近的通用方法，结果表明它可以在跨域学习中显著提高基于内核网络的逼近能力。

    

    在过去的30年中，机器学习在众多过程（如：浅层或深度神经网络逼近、径向基函数网络和各种内核方法）的逼近能力（表达能力）研究中促进了大量的研究。本文针对不变学习、传递学习和合成孔径雷达成像等应用，引入了一种使用非对称内核来研究基于内核网络逼近能力的通用方法。我们考虑使用一组内核的更一般方法，如广义平移网络（其中包括神经网络和平移不变核作为特殊情况）和旋转区函数核。与传统的基于内核的逼近方法不同，我们不能要求内核是正定的。研究结果表明，使用非对称内核可以显著提高内核网络的逼近能力，特别是对于源域和目标域可能在分布上不同的跨域学习。

    For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
    
[^26]: 鲁棒门票能够更好地传输: 在迁移学习中绘制更具传输性的子网络

    Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (arXiv:2304.11834v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11834](http://arxiv.org/abs/2304.11834)

    该论文提出了一种新的迁移学习流程，通过绘制具有鲁棒性的子网络，改进了预训练模型在资源受限的边缘设备上的传输能力。

    

    迁移学习利用在丰富数据的源任务上预训练的深度神经网络(DNN)的特征表示，以赋予下游任务的有效微调。然而，预训练模型往往过于庞大，无法提供可推广的表示，这限制了它们在资源受限的边缘设备上的部署。为了弥补这一差距，我们提出了一种新的迁移学习流程，利用我们的发现：鲁棒门票能够更好地传输，即通过适当引入对抗鲁棒性的方式绘制的子网络可以在传统的彩票门票子网络上取得更好的传输能力。大量实验和消融研究验证了我们提出的迁移学习流程在各种下游任务和稀疏模式中都能实现增强的准确度-稀疏度权衡，进一步丰富了彩票门票假设。

    Transfer learning leverages feature representations of deep neural networks (DNNs) pretrained on source tasks with rich data to empower effective finetuning on downstream tasks. However, the pretrained models are often prohibitively large for delivering generalizable representations, which limits their deployment on edge devices with constrained resources. To close this gap, we propose a new transfer learning pipeline, which leverages our finding that robust tickets can transfer better, i.e., subnetworks drawn with properly induced adversarial robustness can win better transferability over vanilla lottery ticket subnetworks. Extensive experiments and ablation studies validate that our proposed transfer learning pipeline can achieve enhanced accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity patterns, further enriching the lottery ticket hypothesis.
    
[^27]: 用人工神经网络预测国内生产总值：长期记忆有多大的作用？

    GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])

    [http://arxiv.org/abs/2304.05805](http://arxiv.org/abs/2304.05805)

    通过比较四种人工神经网络和动态因子模型对美国GDP季度增长的预测表现，研究发现在平衡经济增长期间，更长的输入序列能够实现更准确的预测，但是这种效果会在不到两年的时间内消失。在经济动荡时期，长期记忆的效果变得明显。

    

    在本研究中，我们将不同的统计模型应用于美国经济季度国内生产总值（GDP）增长预测。使用每月的FRED-MD数据库，我们比较了动态因子模型（DFM）和四个人工神经网络（ANNs）的预测表现：多层感知机（MLP）、一维卷积神经网络（1D CNN）、长短期记忆网络（LSTM）和门控循环单元（GRU）。实证分析呈现了两个不同评估周期的结果。第一个周期（2010年第1季度至2019年第4季度）具有平衡的经济增长，而第二个周期（2010年第1季度至2022年第3季度）还包括COVID-19衰退期间的时间。根据我们的结果，更长的输入序列在平衡经济增长期间能够实现更准确的预测。然而，在一个相对较低的阈值值（约六个季度或十八个月）以后，这种效应会消失。在经济动荡期（如COVID-19衰退期间），长期记忆的效果会变得较为明显。

    In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
    
[^28]: 比较两种分类算法的块正则化5×2交叉验证McNemar检验

    Block-regularized 5$\times$2 Cross-validated McNemar's Test for Comparing Two Classification Algorithms. (arXiv:2304.03990v1 [cs.LG])

    [http://arxiv.org/abs/2304.03990](http://arxiv.org/abs/2304.03990)

    本论文提出了一种块正则化5×2交叉验证McNemar检验，该方法通过规范化训练集之间的重叠记录来产生高质量的误差率估计，相较于传统的留置方法有更高的功率和稳定性。

    

    在比较两种分类算法的任务中，广泛使用的McNemar检验旨在推断出两种分类算法的错误率之间存在重大差异。然而，传统的McNemar检验的功率通常不太理想，因为测试中的留置（HO）方法仅使用一次训练验证拆分，这通常会产生高度变化的错误率估计。相反，交叉验证（CV）方法在多次重复HO方法的基础上产生稳定的估计，因此CV方法在提高McNemar检验功率方面具有巨大优势。在所有类型的CV方法中，块正则化5×2 CV（BCV）在许多先前的研究中已经显示出比其他CV方法在算法比较任务中更为优越，因为5×2 BCV可以通过使所有训练集之间的重叠记录数规范化来产生高质量的误差率估计。

    In the task of comparing two classification algorithms, the widely-used McNemar's test aims to infer the presence of a significant difference between the error rates of the two classification algorithms. However, the power of the conventional McNemar's test is usually unpromising because the hold-out (HO) method in the test merely uses a single train-validation split that usually produces a highly varied estimation of the error rates. In contrast, a cross-validation (CV) method repeats the HO method in multiple times and produces a stable estimation. Therefore, a CV method has a great advantage to improve the power of McNemar's test. Among all types of CV methods, a block-regularized 5$\times$2 CV (BCV) has been shown in many previous studies to be superior to the other CV methods in the comparison task of algorithms because the 5$\times$2 BCV can produce a high-quality estimator of the error rate by regularizing the numbers of overlapping records between all training sets. In this stu
    
[^29]: 基于自适应本地邻域的神经网络用于从稀疏采样数据中重建MR图像

    Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.00775](http://arxiv.org/abs/2206.00775)

    本文提出了一种基于自适应本地邻域的神经网络技术，用于从稀疏采样数据中高效重建MR图像，该技术具有较强的适应性和鲁棒性。

    

    最近的医学图像重建技术致力于以尽可能低的成本和对患者产生最小不良影响的方式生成适用于临床使用的高质量医学图像。最近的研究表明，使用深度学习可以从稀疏采样的k空间数据中重建MR图像具有显著的潜力。在本文中，我们提出了一种在重建时通过对训练集的小区域进行适应性估计来快速估计深度神经网络的技术。简而言之，我们的算法在搜索与测试重建相似的数据集邻居和训练这些邻居上的局部网络，然后更新测试重建之间进行交替。由于我们的重建模型是在某种程度上与正在重建的图像相似的数据集上学习而不是在大规模多样的训练集上拟合的，因此它对新的扫描更具适应性。它还可以处理训练集中的变化。

    Recent medical image reconstruction techniques focus on generating high-quality medical images suitable for clinical use at the lowest possible cost and with the fewest possible adverse effects on patients. Recent works have shown significant promise for reconstructing MR images from sparsely sampled k-space data using deep learning. In this work, we propose a technique that rapidly estimates deep neural networks directly at reconstruction time by fitting them on small adaptively estimated neighborhoods of a training set. In brief, our algorithm alternates between searching for neighbors in a data set that are similar to the test reconstruction, and training a local network on these neighbors followed by updating the test reconstruction. Because our reconstruction model is learned on a dataset that is in some sense similar to the image being reconstructed rather than being fit on a large, diverse training set, it is more adaptive to new scans. It can also handle changes in training set
    
[^30]: Auto-NBA: 针对网络、比特宽度和加速器三个联合空间进行高效搜索的算法

    Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06575](http://arxiv.org/abs/2106.06575)

    Auto-NBA是一种能够高效搜索深度神经网络加速器的算法，它可以同时考虑优化网络、比特宽度和加速器三个耦合的方面。

    

    要同时优化和加速深度神经网络，需要联合考虑三个不同却高度耦合的方面：网络、比特宽度和加速器。然而，联合搜索面临的挑战尚未被完全理解和解决。这些关键挑战包括（1）是否扩大由于巨大联合空间而导致的内存消耗，还是采用次优设计，（2）加速器设计空间的离散性与网络和比特宽度设计空间相耦合且不同，以及（3）网络-加速器联合搜索中的“鸡生蛋蛋生鸡”问题：即联合搜索需要计算操作的硬件成本，然而在搜索期间，尚不知道整个网络的最佳加速器，因此无法计算这些成本。为了解决这些问题，我们提出了一个名为Auto-NBA的框架，以实现针对网络、比特宽度和加速器三个联合空间的高效搜索。

    While maximizing deep neural networks' (DNNs') acceleration efficiency requires a joint search/design of three different yet highly coupled aspects, including the networks, bitwidths, and accelerators, the challenges associated with such a joint search have not yet been fully understood and addressed. The key challenges include (1) the dilemma of whether to explode the memory consumption due to the huge joint space or achieve sub-optimal designs, (2) the discrete nature of the accelerator design space that is coupled yet different from that of the networks and bitwidths, and (3) the chicken and egg problem associated with network-accelerator co-search, i.e., co-search requires operation-wise hardware cost, which is lacking during search as the optimal accelerator depending on the whole network is still unknown during search. To tackle these daunting challenges towards optimal and fast development of DNN accelerators, we propose a framework dubbed Auto-NBA to enable jointly searching fo
    

