# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Unconditional Latent Diffusion Models Memorize Patient Imaging Data](https://rss.arxiv.org/abs/2402.01054) | 本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。 |
| [^2] | [Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning](https://arxiv.org/abs/2404.01714) | 提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性 |
| [^3] | [Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization](https://arxiv.org/abs/2404.00530) | 本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。 |
| [^4] | [Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/abs/2403.18103) | 该教程讨论了图像和视觉领域中扩散模型的基本理念，适合对扩散模型研究或应用感兴趣的本科生和研究生。 |
| [^5] | [A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2403.17218) | 本研究调查了十一种领先的大型语言模型在漏洞检测中的能力，并评估了它们的性能，为探索LLMs推理能力的极限提供了重要案例研究。 |
| [^6] | [Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators](https://arxiv.org/abs/2403.16950) | 在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。 |
| [^7] | [A Survey on Consumer IoT Traffic: Security and Privacy](https://arxiv.org/abs/2403.16149) | 本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。 |
| [^8] | [Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems](https://arxiv.org/abs/2403.12335) | 引入了时间一致Koopman自编码器（tcKAE）来生成准确的长期预测，在有限嘈杂的训练数据下通过一致性正则化项增强了模型的稳健性和泛化能力。 |
| [^9] | [Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search](https://arxiv.org/abs/2403.09570) | 本文引入了一种新颖的信息理论获取函数，用于平衡在连续的优化任务中获得最优值或解信息的需求。 |
| [^10] | [Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations](https://arxiv.org/abs/2402.17853) | 提出了一种名为潜在神经PDE求解器（LNS）的框架，通过在潜在空间学习系统动态并使用较粗糙的离散化，可以大大简化神经PDE求解器的训练过程，降低计算成本。 |
| [^11] | [Parallelized Midpoint Randomization for Langevin Monte Carlo](https://arxiv.org/abs/2402.14434) | 探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。 |
| [^12] | [Forecasting high-impact research topics via machine learning on evolving knowledge graphs](https://arxiv.org/abs/2402.08640) | 通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。 |
| [^13] | [Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees](https://arxiv.org/abs/2402.08090) | 本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。 |
| [^14] | [Rethinking the Capacity of Graph Neural Networks for Branching Strategy](https://arxiv.org/abs/2402.07099) | 本文研究了图神经网络（GNNs）在分支策略中的容量，并发现了消息传递GNN (MP-GNN) 的表达能力的局限性以及另一种GNN结构 second-order folklore GNN (2-FGNN) 的通用逼近性质。 |
| [^15] | [Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning](https://arxiv.org/abs/2402.04080) | 本文介绍了一种熵正则化的扩散策略与Q-集合相结合的离线强化学习方法，该方法通过将一个复杂的动作分布转化为标准高斯分布，然后使用逆时间SDE采样动作，以改善离线数据集的探索能力，并通过学习Q-集合的下信心界实现更强健的策略改进。在D4RL基准任务的大多数任务上达到了最先进的性能。 |
| [^16] | [A Two-Scale Complexity Measure for Deep Learning Models.](http://arxiv.org/abs/2401.09184) | 这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。 |
| [^17] | [On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration.](http://arxiv.org/abs/2401.04729) | 本研究探讨了在人工智能协作中提供上下文信息对人类委托行为的影响，发现提供上下文信息显著提高了人工智能与人类团队的表现，并且委托行为在不同上下文信息下发生显著变化。这项研究推进了对人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了见解。 |
| [^18] | [Prosody Analysis of Audiobooks.](http://arxiv.org/abs/2310.06930) | 本研究通过使用一个含有93个书籍和对应有声书的数据集，提出了改进的模型来预测有声书文本中的韵律属性。结果显示，我们的预测韵律与人类朗读比商业级TTS系统更相关，并且人们更喜欢韵律增强的有声书朗读。 |
| [^19] | [Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI.](http://arxiv.org/abs/2308.05764) | 该论文提出了一种通过从心脏MRI中的知识转移解锁心电图的诊断潜力的方法。通过将CMR图像中的领域特定信息转移到ECG嵌入中，该方法实现了仅根据ECG数据进行全面的心脏筛查，并能预测心血管疾病的个体风险和确定心脏表型。 |
| [^20] | [Manifold Filter-Combine Networks.](http://arxiv.org/abs/2307.04056) | 这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。 |
| [^21] | [Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions.](http://arxiv.org/abs/2306.10189) | 本论文提出了一种线性方法，通过多元占位核函数在高维状态空间中学习非参数ODE系统，可以解决显式公式按二次方缩放的问题。这种方法在高度非线性的数据和图像数据中都具有通用性。 |
| [^22] | [Offline Prioritized Experience Replay.](http://arxiv.org/abs/2306.05412) | 本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。 |
| [^23] | [Asymptotic Inference for Multi-Stage Stationary Treatment Policy with High Dimensional Features.](http://arxiv.org/abs/2301.12553) | 本研究填补了在高维特征变量存在的情况下，对于多阶段静态治疗策略本身进行推断的工作空白，提出了一种增强的估计器以提高价值函数的准确性。 |
| [^24] | [On Sequential Bayesian Inference for Continual Learning.](http://arxiv.org/abs/2301.01828) | 这篇论文研究了顺序贝叶斯推断在连续学习中的应用。研究表明，尽管使用了真实后验，这种方法仍无法防止神经网络中的灾难性遗忘。同时，模型误差和任务数据不平衡也会导致连续学习性能的下降。 |
| [^25] | [ENCODE: Encoding NetFlows for Network Anomaly Detection.](http://arxiv.org/abs/2207.03890) | 本文提出了一种编码算法，通过考虑特征值的频率和上下文，对NetFlow数据进行预处理，以提高网络异常检测的效果。 |

# 详细

[^1]: 无条件的隐式扩散模型记忆患者影像数据

    Unconditional Latent Diffusion Models Memorize Patient Imaging Data

    [https://rss.arxiv.org/abs/2402.01054](https://rss.arxiv.org/abs/2402.01054)

    本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。

    

    生成式的隐式扩散模型在医学影像领域具有广泛的应用。一个值得注意的应用是通过提出合成数据作为真实患者数据的替代品来实现隐私保护的开放数据共享。尽管有这个应用的前景，但这些模型容易出现患者数据的记忆问题，即模型生成患者数据的副本而不是新的合成样本。这破坏了保护患者数据的整个目的，甚至可能导致患者被重新识别。考虑到这个问题的重要性，令人惊讶的是，在医学影像界中这个问题并没有得到太多关注。为此，我们评估了医学图像合成中隐式扩散模型的记忆问题。我们训练了2D和3D的隐式扩散模型，使用CT、MR和X光数据集进行合成数据的生成。之后，我们利用自监督模型来评估训练数据被记忆的程度，并进一步研究可能导致记忆的各种因素。

    Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
    
[^2]: 基于共轭梯度的自适应矩估计优化算法用于深度学习

    Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning

    [https://arxiv.org/abs/2404.01714](https://arxiv.org/abs/2404.01714)

    提出一种基于共轭梯度样式的新优化算法CG-like-Adam，用于深度学习，并在收敛分析和数值实验中展示了其优越性

    

    训练深度神经网络是一项具有挑战性的任务。为加快培训速度并增强深度神经网络的性能，我们将传统的共轭梯度修正为共轭梯度样式，并将其并入通用Adam中，因此提出了一种名为CG-like-Adam的新优化算法用于深度学习。具体而言，通用Adam的一阶和二阶矩估计均由共轭梯度样式替换。收敛分析处理了一阶矩估计的指数移动平均系数为常数且一阶矩估计无偏的情况。数值实验显示了基于CIFAR10/100数据集的所提算法的优越性。

    arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
    
[^3]: 将坏苹果与好橘子进行比较：通过联合优化偏好对齐大型语言模型

    Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization

    [https://arxiv.org/abs/2404.00530](https://arxiv.org/abs/2404.00530)

    本研究提出了一种新的偏好获取方法，通过DOVE协议对指令-响应对的联合概率进行优化，以对齐大型语言模型。

    

    一种常见的对齐大型语言模型（LLMs）的技术依赖于通过比较在固定上下文中条件生成的多个生成的人类偏好。然而，当这些生成放置在相同的上下文中时，这仅利用了成对比较。然而，这种条件排名通常无法捕获人类偏好的复杂和多维方面。在这项工作中，我们重新审视偏好获取的传统范式，并提出了一个基于在指令-响应对上联合引发偏好的新轴。虽然先前的偏好优化是针对条件排名协议（例如，DPO）设计的，但我们提出的偏好获取协议引入了DOVE，这是一个新的偏好优化目标，通过提升所选指令-响应对的联合概率来降低所拒绝指令-响应对的概率。

    arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
    
[^4]: 关于图像和视觉扩散模型的教程

    Tutorial on Diffusion Models for Imaging and Vision

    [https://arxiv.org/abs/2403.18103](https://arxiv.org/abs/2403.18103)

    该教程讨论了图像和视觉领域中扩散模型的基本理念，适合对扩散模型研究或应用感兴趣的本科生和研究生。

    

    近年来生成工具的惊人增长使得文本到图像生成和文本到视频生成等许多令人兴奋的应用成为可能。这些生成工具背后的基本原理是扩散概念，一种特殊的采样机制，克服了以前方法中被认为困难的一些缺点。本教程的目标是讨论扩散模型的基本理念。本教程的目标受众包括对研究扩散模型或将这些模型应用于解决其他问题感兴趣的本科生和研究生。

    arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
    
[^5]: 大型语言模型在漏洞检测方面的能力综合研究

    A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection

    [https://arxiv.org/abs/2403.17218](https://arxiv.org/abs/2403.17218)

    本研究调查了十一种领先的大型语言模型在漏洞检测中的能力，并评估了它们的性能，为探索LLMs推理能力的极限提供了重要案例研究。

    

    大型语言模型（LLMs）已经展现出在代码生成和其他软件工程任务方面具有巨大潜力。漏洞检测对于维护软件系统的安全、完整性和可信度至关重要。精确的漏洞检测需要对代码进行推理，这使得它成为探索LLMs推理能力极限的良好案例研究。尽管最近的研究已经利用通用提示技术将LLMs应用于漏洞检测，但它们在这一任务中的完整能力以及在解释确定的漏洞时所犯的错误类型仍不清楚。在本文中，我们调查了十一种领先的在代码生成方面处于最前沿且通常用作编码助手的LLMs，并评估了它们在漏洞检测方面的能力。我们系统地搜索了效果最佳的提示，结合了诸如上下文学习和链式学习等技术。

    arXiv:2403.17218v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.   In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of
    
[^6]: 与人类判断相一致：大型语言模型评估中成对偏好的作用

    Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators

    [https://arxiv.org/abs/2403.16950](https://arxiv.org/abs/2403.16950)

    在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。

    

    大型语言模型（LLMs）作为自动评估器在评估生成的自然语言质量方面表现出有希望的能力。然而，LLMs在评估中仍存在偏见，常常难以生成与人类评估一致的连贯评估。在这项工作中，我们首先对LLM评估器与人类判断之间的不一致进行系统研究，揭示现有旨在减轻偏见的校准方法不足以有效将LLM评估器对齐。受到RLHF中对偏好数据的使用的启发，我们将评估形式化为一个排序问题，并引入Pairwise-preference Search（PAIRS），这是一种以LLMs进行成对比较并有效对候选文本进行排序的基于不确定性引导的搜索方法。PAIRS在代表性评估任务上实现了最先进的性能，并且显示出比直接打分有显著改进。

    arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
    
[^7]: 消费者物联网流量的调查：安全与隐私

    A Survey on Consumer IoT Traffic: Security and Privacy

    [https://arxiv.org/abs/2403.16149](https://arxiv.org/abs/2403.16149)

    本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。

    

    在过去几年里，消费者物联网（CIoT）已经进入了公众生活。尽管CIoT提高了人们日常生活的便利性，但也带来了新的安全和隐私问题。我们尝试通过流量分析这一安全领域中的流行方法，找出研究人员可以从流量分析中了解CIoT安全和隐私方面的内容。本调查从安全和隐私角度探讨了CIoT流量分析中的新特征、CIoT流量分析的最新进展以及尚未解决的挑战。我们从2018年1月至2023年12月收集了310篇与CIoT流量分析有关的安全和隐私角度的论文，总结了识别了CIoT新特征的CIoT流量分析过程。然后，我们根据五个应用目标详细介绍了现有的研究工作：设备指纹识别、用户活动推断、恶意行为检测、隐私泄露以及通信模式识别。

    arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
    
[^8]: 一种用于预测动态系统的时间一致的Koopman自编码器

    Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems

    [https://arxiv.org/abs/2403.12335](https://arxiv.org/abs/2403.12335)

    引入了时间一致Koopman自编码器（tcKAE）来生成准确的长期预测，在有限嘈杂的训练数据下通过一致性正则化项增强了模型的稳健性和泛化能力。

    

    缺乏足够高质量的数据经常是高维时空动态系统数据驱动建模中关键挑战。Koopman自编码器（KAEs）利用深度神经网络（DNNs）的表达能力、自编码器的降维能力以及Koopman算子的谱特性，学习具有更简单线性动态的降阶特征空间。然而，KAEs的有效性受限于有限而嘈杂的训练数据集，导致泛化能力较差。为解决这一问题，我们引入了一种称为时间一致Koopman自编码器（tcKAE）的模型，旨在即使在受限且嘈杂的训练数据情况下生成准确的长期预测。这是通过强制在不同时间步上保持预测一致性的一致性正则化项实现的，从而增强了tcKAE相对于现有模型的稳健性和泛化能力。

    arXiv:2403.12335v1 Announce Type: new  Abstract: Absence of sufficiently high-quality data often poses a key challenge in data-driven modeling of high-dimensional spatio-temporal dynamical systems. Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks (DNNs), the dimension reduction capabilities of autoencoders, and the spectral properties of the Koopman operator to learn a reduced-order feature space with simpler, linear dynamics. However, the effectiveness of KAEs is hindered by limited and noisy training datasets, leading to poor generalizability. To address this, we introduce the Temporally-Consistent Koopman Autoencoder (tcKAE), designed to generate accurate long-term predictions even with constrained and noisy training data. This is achieved through a consistency regularization term that enforces prediction coherence across different time steps, thus enhancing the robustness and generalizability of tcKAE over existing models. We provide analytical justifica
    
[^9]: 基于多保真度的贝叶斯优化方法及跨任务可转移的最大值熵搜索

    Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search

    [https://arxiv.org/abs/2403.09570](https://arxiv.org/abs/2403.09570)

    本文引入了一种新颖的信息理论获取函数，用于平衡在连续的优化任务中获得最优值或解信息的需求。

    

    在许多应用中，设计者面临一系列优化任务，任务的目标是昂贵评估的黑盒函数形式。本文介绍了一种新的信息理论获取函数，用于平衡需要获取不同任务的最优值或解的信息和通过参数的转移传递。

    arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
    
[^10]: 潜在神经PDE求解器：用于偏微分方程的降阶建模框架

    Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations

    [https://arxiv.org/abs/2402.17853](https://arxiv.org/abs/2402.17853)

    提出了一种名为潜在神经PDE求解器（LNS）的框架，通过在潜在空间学习系统动态并使用较粗糙的离散化，可以大大简化神经PDE求解器的训练过程，降低计算成本。

    

    神经网络在加速由偏微分方程（PDEs）控制的系统的数值模拟方面显示出了巨大潜力。与许多现有的在高维离散化场上操作的神经网络代理不同，我们提议在潜在空间学习系统的动态，使用更粗糙的离散化。在我们提出的框架 - 潜在神经PDE求解器（LNS）中，首先训练一个非线性自动编码器，将系统的全阶表示投影到网格减少的空间中，接着训练一个时间模型来预测这个网格减少的空间中的未来状态。这种降阶过程通过大大减少伴随细粒度离散化的计算成本，简化了时间模型的训练。我们研究了提出的框架以及几种其他流行的神经PDE求解器在各种类型的系统上的能力，包括单相和多相流体系统。

    arXiv:2402.17853v1 Announce Type: cross  Abstract: Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows a
    
[^11]: 并行中点随机化的 Langevin Monte Carlo

    Parallelized Midpoint Randomization for Langevin Monte Carlo

    [https://arxiv.org/abs/2402.14434](https://arxiv.org/abs/2402.14434)

    探索在能够进行梯度平行评估的框架中的抽样问题，提出了并行化的随机中点方法，并通过新技术导出了对抽样和目标密度之间Wasserstein距离的上界，量化了并行处理单元带来的运行时改进。

    

    我们探讨了在可以进行梯度的平行评估的框架中的抽样问题。我们的研究重点放在由平滑和强log-凹密度表征的目标分布上。我们重新审视了并行化的随机中点方法，并运用最近开发用于分析其纯顺序版本的证明技术。利用这些技术，我们得出了抽样和目标密度之间的Wasserstein距离的上界。这些界限量化了通过利用并行处理单元所实现的运行时改进，这可能是相当可观的。

    arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
    
[^12]: 通过机器学习在不断演化的知识图谱上预测高影响力的研究主题

    Forecasting high-impact research topics via machine learning on evolving knowledge graphs

    [https://arxiv.org/abs/2402.08640](https://arxiv.org/abs/2402.08640)

    通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。

    

    科学出版物的指数增长对人类研究者构成了严峻挑战。它迫使研究者将注意力集中在更狭窄的子领域上，使得发现其他领域的新颖且有影响力的研究想法和合作变得困难。虽然有办法预测科学论文未来的引用次数，但通常需要等到研究完成并且论文写成后才能进行评估，这样就错过了想法构思的早期阶段。在本文中，我们展示了如何预测从未被研究者发布的想法的影响力。为此，我们开发了一个大型的演化知识图谱，其中包含超过2100万篇科学论文。它结合了从论文内容中创建的语义网络和从历史引用中创建的影响网络。利用机器学习，我们可以高准确度地预测演化网络的动态情况，从而预测新的研究方向的影响力。我们预期这种能力将有助于研究者发现具有高影响力的研究主题。

    The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
    
[^13]: 学习神经收缩动力学：扩展线性化和全局保证

    Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees

    [https://arxiv.org/abs/2402.08090](https://arxiv.org/abs/2402.08090)

    本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。

    

    在学习的动态系统中，全局稳定性和鲁棒性保证对于确保系统在面对不确定性时的良好行为至关重要。我们提出了扩展线性化收缩动力学（ELCD），该系统是第一个具有任意度量下全局收缩性保证的基于神经网络的动力系统。ELCD的关键特性是非线性向量场扩展线性化的参数化。在其最基本形式下，ELCD保证全局指数稳定、平衡收缩以及在某些度量下全局收缩。为了实现在数据空间中相对于更一般度量的收缩，我们训练数据空间和潜在空间之间的微分同胚，并在潜在空间中强制收缩性，从而确保数据空间的全局收缩性。我们在2D、4D和8D的LASA数据集上展示了ELCD的性能。

    Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
    
[^14]: 重新思考图神经网络在分支策略中的容量

    Rethinking the Capacity of Graph Neural Networks for Branching Strategy

    [https://arxiv.org/abs/2402.07099](https://arxiv.org/abs/2402.07099)

    本文研究了图神经网络（GNNs）在分支策略中的容量，并发现了消息传递GNN (MP-GNN) 的表达能力的局限性以及另一种GNN结构 second-order folklore GNN (2-FGNN) 的通用逼近性质。

    

    图神经网络（GNNs）被广泛应用于预测混合整数线性规划（MILPs）的属性和启发式，并加速MILP求解器。本文研究了GNNs在表示提供分支限界算法中高效策略的强分支（SB）得分方面的能力。尽管现有文献中经常使用最简单的消息传递GNN（MP-GNN）来学习SB得分，但我们证明了其表达能力的一个根本局限性--存在两个不同SB得分的MILP实例，无论参数的数量如何，都无法通过任何MP-GNN区分。此外，我们建立了一个通用逼近定理，用于另一种GNN结构称为second-order folklore GNN（2-FGNN）。我们证明了对于任何MILP数据分布，总是存在一个可以以任意高精度和任意高概率逼近SB得分的2-FGNN。一个小规模的数值实验

    Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical 
    
[^15]: 熵正则化扩散策略与Q-集合用于离线强化学习

    Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning

    [https://arxiv.org/abs/2402.04080](https://arxiv.org/abs/2402.04080)

    本文介绍了一种熵正则化的扩散策略与Q-集合相结合的离线强化学习方法，该方法通过将一个复杂的动作分布转化为标准高斯分布，然后使用逆时间SDE采样动作，以改善离线数据集的探索能力，并通过学习Q-集合的下信心界实现更强健的策略改进。在D4RL基准任务的大多数任务上达到了最先进的性能。

    

    本文提出了训练用于离线强化学习的扩散策略的先进技术。核心是一个均值回归随机微分方程（SDE），它将复杂的动作分布转化为标准高斯分布，然后在环境状态条件下使用相应的逆时间SDE采样动作，类似于典型的扩散策略。我们展示了这样一个SDE有解，我们可以用它来计算策略的对数概率，从而得到一个熵正则项，改进了离线数据集的探索能力。为了减轻来自分布外数据点的不准确值函数的影响，我们进一步提出了学习Q-集合的下信心界以实现更强健的策略改进。通过将熵正则化扩散策略与Q-集合结合应用于离线强化学习，我们的方法在D4RL基准任务的大多数任务上达到了最先进的性能。代码可在\href{https://github.com/ruoqizzz/Entro}{https://github.com/ruoqizzz/Entro}找到。

    This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entro
    
[^16]: 深度学习模型的两尺度复杂度测量

    A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])

    [http://arxiv.org/abs/2401.09184](http://arxiv.org/abs/2401.09184)

    这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。

    

    我们引入了一种基于有效维度的统计模型新容量测量2sED。这个新的数量在对模型进行温和假设的情况下，可以可靠地限制泛化误差。此外，对于标准数据集和流行的模型架构的模拟结果表明，2sED与训练误差具有很好的相关性。对于马尔可夫模型，我们展示了如何通过逐层迭代的方法有效地从下方近似2sED，从而解决具有大量参数的深度学习模型。模拟结果表明，这种近似对不同的突出模型和数据集都很好。

    We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
    
[^17]: 关于上下文信息对人类在人工智能协作中的委托行为的影响

    On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration. (arXiv:2401.04729v1 [cs.HC])

    [http://arxiv.org/abs/2401.04729](http://arxiv.org/abs/2401.04729)

    本研究探讨了在人工智能协作中提供上下文信息对人类委托行为的影响，发现提供上下文信息显著提高了人工智能与人类团队的表现，并且委托行为在不同上下文信息下发生显著变化。这项研究推进了对人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了见解。

    

    人工智能的不断增强能力为人工智能与人类的协作带来了新的可能性。利用现有的互补能力，让人们将个别实例委托给人工智能是一种有前景的方法。然而，使人们有效地委托实例需要他们评估自己和人工智能在给定任务的背景下的能力。在这项工作中，我们探讨了在人类决定将实例委托给人工智能时提供上下文信息的效果。我们发现，提供上下文信息显著提高了人工智能与人类团队的表现。此外，我们还表明，当参与者接收到不同类型的上下文信息时，委托行为会发生显著变化。总体而言，这项研究推进了人工智能委托中人工智能与人类互动的理解，并为设计更有效的协作系统提供了可行的见解。

    The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems.
    
[^18]: 《有声书的韵律分析》

    Prosody Analysis of Audiobooks. (arXiv:2310.06930v1 [cs.SD])

    [http://arxiv.org/abs/2310.06930](http://arxiv.org/abs/2310.06930)

    本研究通过使用一个含有93个书籍和对应有声书的数据集，提出了改进的模型来预测有声书文本中的韵律属性。结果显示，我们的预测韵律与人类朗读比商业级TTS系统更相关，并且人们更喜欢韵律增强的有声书朗读。

    

    最近在文本转语音方面取得了一些进展，使得从文本中生成自然音效的音频成为可能。然而，有声书朗读涉及到读者的戏剧性声音和语调，更多地依赖情感、对话和叙述。使用我们的数据集，包括93本书与其对应的有声书，我们提出了改进的模型，用于从叙述文本中预测韵律属性（音高、音量和语速），并使用语言建模。我们预测的韵律属性与人类朗读的相关性要远高于商业级TTS系统的结果：在24本书中，我们预测的音高对22本书的人类阅读更具相关性，而我们预测的音量属性对23本书的人类阅读更加相似。最后，我们进行了一项人类评估研究，以量化人们更喜欢韵律增强的有声书朗读还是商业级文本转语音系统。

    Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
    
[^19]: 通过从心脏MRI中的知识转移解锁心电图的诊断潜力

    Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])

    [http://arxiv.org/abs/2308.05764](http://arxiv.org/abs/2308.05764)

    该论文提出了一种通过从心脏MRI中的知识转移解锁心电图的诊断潜力的方法。通过将CMR图像中的领域特定信息转移到ECG嵌入中，该方法实现了仅根据ECG数据进行全面的心脏筛查，并能预测心血管疾病的个体风险和确定心脏表型。

    

    心电图 (ECG) 是一种广泛可用的诊断工具，可以快速和经济高效地评估心血管健康状况。然而，在心血管疾病的诊断中，通常更喜欢使用昂贵的心脏磁共振 (CMR) 成像进行更详细的检查。虽然 CMR 成像可以提供详细的心脏解剖可视化，但由于长时间扫描和高昂的费用，它并不广泛可用。为了解决这个问题，我们提出了一种第一种自监督对比方法，将CMR图像中的领域特定信息转移到ECG嵌入中。我们的方法将多模态对比学习与屏蔽数据建模相结合，实现了仅根据ECG数据进行全面的心脏筛查。在使用来自40044名UK Biobank受试者的数据进行的广泛实验证明了我们方法的实用性和可推广性。我们预测了各种心血管疾病的个体风险，并仅根据ECG数据确定了不同的心脏表型。

    The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
    
[^20]: 流形滤波-组合网络

    Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])

    [http://arxiv.org/abs/2307.04056](http://arxiv.org/abs/2307.04056)

    这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。

    

    我们介绍了一类大型流形神经网络(MNNs)，我们称之为流形滤波-组合网络。这个类别包括了Wang、Ruiz和Ribeiro之前的研究中考虑的MNNs，流形散射变换(一种基于小波的神经网络模型)，以及其他有趣的之前在文献中未考虑的示例，如Kipf和Welling的图卷积网络的流形等效。然后，我们考虑了一种基于构建数据驱动图的方法，用于在没有对流形有全局知识的情况下实现这样的网络，而只能访问有限数量的样本点。我们提供了网络在样本点数趋于无穷大时能够保证收敛到其连续极限的充分条件。与之前的工作(主要关注特定的MNN结构和图构建)不同，我们的收敛速度并不依赖于使用的滤波器数量。而且，它表现出线性的收敛速度。

    We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
    
[^21]: 通过多元占位核函数学习高维非参数微分方程

    Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions. (arXiv:2306.10189v1 [stat.ML])

    [http://arxiv.org/abs/2306.10189](http://arxiv.org/abs/2306.10189)

    本论文提出了一种线性方法，通过多元占位核函数在高维状态空间中学习非参数ODE系统，可以解决显式公式按二次方缩放的问题。这种方法在高度非线性的数据和图像数据中都具有通用性。

    

    从$d$维状态空间中$n$个轨迹快照中学习非参数的常微分方程（ODE）系统需要学习$d$个函数。除非具有额外的系统属性知识，例如稀疏性和对称性，否则显式的公式按二次方缩放。在这项工作中，我们提出了一种使用向量值再生核希尔伯特空间提供的隐式公式学习的线性方法。通过将ODE重写为更弱的积分形式，我们随后进行最小化并推导出我们的学习算法。最小化问题的解向量场依赖于与解轨迹相关的多元占位核函数。我们通过对高度非线性的模拟和真实数据进行实验证实了我们的方法，其中$d$可能超过100。我们进一步通过从图像数据学习非参数一阶拟线性偏微分方程展示了所提出的方法的多样性。

    Learning a nonparametric system of ordinary differential equations (ODEs) from $n$ trajectory snapshots in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach to learning using the implicit formulation provided by vector-valued Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral form, which we subsequently minimize, we derive our learning algorithm. The minimization problem's solution for the vector field relies on multivariate occupation kernel functions associated with the solution trajectories. We validate our approach through experiments on highly nonlinear simulated and real data, where $d$ may exceed 100. We further demonstrate the versatility of the proposed method by learning a nonparametric first order quasilinear partial differential 
    
[^22]: 离线优先经验重放

    Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05412](http://arxiv.org/abs/2306.05412)

    本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。

    

    离线强化学习面临着分布偏移问题。为了解决这个问题，现有的工作主要集中在设计学习策略和行为策略之间的复杂约束。然而，这些约束通过均匀采样等方式被应用到表现良好和表现差的行动上，这可能会对学习策略产生负面影响。为了缓解这个问题，我们提出了离线优先经验重放（OPER），其中包括一类优先级函数，用于将高回报的转换置于更频繁的访问中。通过理论分析，我们证明了这类优先级函数能够引起行为策略的改善，当策略约束到这个改进的策略上时，离线强化学习算法很可能得到更好的解决方案。我们开发了两种实用策略来获得基于拟合值网络的优先权重（OPER-A）或者u

    Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
    
[^23]: 多阶段静态治疗策略的高维特征渐近推断

    Asymptotic Inference for Multi-Stage Stationary Treatment Policy with High Dimensional Features. (arXiv:2301.12553v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12553](http://arxiv.org/abs/2301.12553)

    本研究填补了在高维特征变量存在的情况下，对于多阶段静态治疗策略本身进行推断的工作空白，提出了一种增强的估计器以提高价值函数的准确性。

    

    动态治疗规则是一系列针对个体特征量身定制的多阶段决策函数。在实践中，一类重要的治疗策略是多阶段静态治疗策略，其使用相同的决策函数来指定治疗分配概率，在决策时基于同时包括基线变量（例如人口统计学）和时变变量（例如常规检测到的疾病生物标志物）的一组特征。虽然已经有大量文献对与动态治疗策略相关的价值函数进行有效推断，但在高维特征变量存在的情况下，对于治疗策略本身的工作还很少。我们旨在填补这项工作的空白。具体而言，我们首先基于增强的倒数权重估计器估计多阶段静态治疗策略，以提高价值函数的准确性。

    Dynamic treatment rules or policies are a sequence of decision functions over multiple stages that are tailored to individual features. One important class of treatment policies for practice, namely multi-stage stationary treatment policies, prescribe treatment assignment probabilities using the same decision function over stages, where the decision is based on the same set of features consisting of both baseline variables (e.g., demographics) and time-evolving variables (e.g., routinely collected disease biomarkers). Although there has been extensive literature to construct valid inference for the value function associated with the dynamic treatment policies, little work has been done for the policies themselves, especially in the presence of high dimensional feature variables. We aim to fill in the gap in this work. Specifically, we first estimate the multistage stationary treatment policy based on an augmented inverse probability weighted estimator for the value function to increase
    
[^24]: 关于连续学习的顺序贝叶斯推断

    On Sequential Bayesian Inference for Continual Learning. (arXiv:2301.01828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01828](http://arxiv.org/abs/2301.01828)

    这篇论文研究了顺序贝叶斯推断在连续学习中的应用。研究表明，尽管使用了真实后验，这种方法仍无法防止神经网络中的灾难性遗忘。同时，模型误差和任务数据不平衡也会导致连续学习性能的下降。

    

    顺序贝叶斯推断可用于连续学习，以防止过去任务的灾难性遗忘，并在学习新任务时提供信息丰富的先验。我们重新审视顺序贝叶斯推断，并测试是否有访问真实后验的保证可以防止贝叶斯神经网络中的灾难性遗忘。为了做到这一点，我们使用哈密顿蒙特卡洛执行顺序贝叶斯推断。我们通过对哈密顿蒙特卡洛样本拟合密度估计器，将后验传播为新任务的先验。我们发现这种方法无法防止灾难性遗忘，证明了在神经网络中执行顺序贝叶斯推断的困难。然后，我们研究了顺序贝叶斯推断和连续学习的简单分析示例，并强调了模型错误说明问题，即使进行了准确的推断，也可能导致次优的连续学习性能。此外，我们讨论了任务数据不平衡可能导致遗忘的问题。

    Sequential Bayesian inference can be used for continual learning to prevent catastrophic forgetting of past tasks and provide an informative prior when learning new tasks. We revisit sequential Bayesian inference and test whether having access to the true posterior is guaranteed to prevent catastrophic forgetting in Bayesian neural networks. To do this we perform sequential Bayesian inference using Hamiltonian Monte Carlo. We propagate the posterior as a prior for new tasks by fitting a density estimator on Hamiltonian Monte Carlo samples. We find that this approach fails to prevent catastrophic forgetting demonstrating the difficulty in performing sequential Bayesian inference in neural networks. From there we study simple analytical examples of sequential Bayesian inference and CL and highlight the issue of model misspecification which can lead to sub-optimal continual learning performance despite exact inference. Furthermore, we discuss how task data imbalances can cause forgetting.
    
[^25]: ENCODE：用于网络异常检测的编码NetFlows

    ENCODE: Encoding NetFlows for Network Anomaly Detection. (arXiv:2207.03890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03890](http://arxiv.org/abs/2207.03890)

    本文提出了一种编码算法，通过考虑特征值的频率和上下文，对NetFlow数据进行预处理，以提高网络异常检测的效果。

    

    NetFlow数据是许多网络分析师和研究人员使用的流行网络日志格式。与深度包检查相比，使用NetFlow的优势在于更易于收集和处理，对隐私性的侵入性更小。许多研究使用机器学习来使用NetFlow数据检测网络攻击。这些机器学习流水线的第一步是在将数据提供给机器学习算法之前对数据进行预处理。存在许多方法来预处理NetFlow数据；然而，这些方法只是简单地将现有方法应用于数据，而不考虑网络数据的特定属性。我们认为，对于源自软件系统（如NetFlow或软件日志）的数据，特征值的频率和上下文的相似性比值本身的相似性更重要。在这项工作中，我们提出了一种编码算法，在处理数据时直接考虑特征值的频率和上下文。

    NetFlow data is a popular network log format used by many network analysts and researchers. The advantages of using NetFlow over deep packet inspection are that it is easier to collect and process, and it is less privacy intrusive. Many works have used machine learning to detect network attacks using NetFlow data. The first step for these machine learning pipelines is to pre-process the data before it is given to the machine learning algorithm. Many approaches exist to pre-process NetFlow data; however, these simply apply existing methods to the data, not considering the specific properties of network data. We argue that for data originating from software systems, such as NetFlow or software logs, similarities in frequency and contexts of feature values are more important than similarities in the value itself. In this work, we propose an encoding algorithm that directly takes the frequency and the context of the feature values into account when the data is being processed. Different ty
    

