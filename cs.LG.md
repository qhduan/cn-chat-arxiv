# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [INPC: Implicit Neural Point Clouds for Radiance Field Rendering](https://arxiv.org/abs/2403.16862) | 提出了一种新颖的隐式点云表示方法，结合了连续八叉树概率场和多分辨率哈希网格，实现了快速渲染和保留细致几何细节的优势，并且在几个常见基准数据集上实现了最先进的图像质量。 |
| [^2] | [M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling](https://arxiv.org/abs/2403.13728) | 提出了一种新的方法，通过多目标分层输出反馈优化的方式，利用乘子诱导的损失景观调度解决神经网络参数化的复杂损失函数优化问题。 |
| [^3] | [Low-Cost Privacy-Aware Decentralized Learning](https://arxiv.org/abs/2403.11795) | ZIP-DL是一种低成本的隐私感知去中心化学习算法，通过向每个模型更新添加相关噪声，在保护隐私的同时实现了较高的模型准确性，具有较好的收敛速度和隐私保证。 |
| [^4] | [Improving generalisation via anchor multivariate analysis](https://arxiv.org/abs/2403.01865) | 引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。 |
| [^5] | [Structure Preserving Diffusion Models](https://arxiv.org/abs/2402.19369) | 提出了一种结构保持的扩散过程，可以学习具有群对称性等额外结构的分布，并开发了一系列对称等变扩散模型来实现这一点。 |
| [^6] | [Random Geometric Graph Alignment with Graph Neural Networks](https://arxiv.org/abs/2402.07340) | 本文研究了在图对齐问题中，通过图神经网络可以高概率恢复正确的顶点对齐。通过特定的特征稀疏性和噪声水平条件，我们证明了图神经网络的有效性，并与直接匹配方法进行了比较。 |
| [^7] | [The VampPrior Mixture Model](https://arxiv.org/abs/2402.04412) | 本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。 |
| [^8] | [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2402.02746) | 标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。 |
| [^9] | [Hypergraph-MLP: Learning on Hypergraphs without Message Passing](https://arxiv.org/abs/2312.09778) | 提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。 |
| [^10] | [The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning.](http://arxiv.org/abs/2401.05710) | 这项研究展示了一种适应性分布式奖励评论家架构，能够在未知扰动的情况下恢复真实奖励，并在多个控制任务中取得较高的回报。 |
| [^11] | [Transformers are Efficient In-Context Estimators for Wireless Communication.](http://arxiv.org/abs/2311.00226) | 这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。 |
| [^12] | [Associative Transformer Is A Sparse Representation Learner.](http://arxiv.org/abs/2309.12862) | 关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。 |
| [^13] | [Hypergraph Structure Inference From Data Under Smoothness Prior.](http://arxiv.org/abs/2308.14172) | 本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。 |
| [^14] | [AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).](http://arxiv.org/abs/2308.05239) | 这篇论文提出了适应现代应用和组织要求的AI-enabled软件和系统架构框架，重点关注机器学习驱动的智能物联网系统(CPS)。作者提出了用于评估和基准化ML-enabled CPS的优点标准。 |
| [^15] | [SketchOGD: Memory-Efficient Continual Learning.](http://arxiv.org/abs/2305.16424) | SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。 |
| [^16] | [Statistical Guarantees of Group-Invariant GANs.](http://arxiv.org/abs/2305.13517) | 本研究提出了群不变GAN的统计保证，发现当学习群不变分布时，群不变GAN所需样本数会按群体大小的幂比例减少。 |
| [^17] | [Q-malizing flow and infinitesimal density ratio estimation.](http://arxiv.org/abs/2305.11857) | 研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。 |
| [^18] | [Learning Hypergraphs From Signals With Dual Smoothness Prior.](http://arxiv.org/abs/2211.01717) | 本研究提出了一种基于双重平滑先验的超图结构学习框架，可从观察到的信号中学习超图结构以捕获实体间的内在高阶关系。 |

# 详细

[^1]: INPC：用于辐射场渲染的隐式神经点云

    INPC: Implicit Neural Point Clouds for Radiance Field Rendering

    [https://arxiv.org/abs/2403.16862](https://arxiv.org/abs/2403.16862)

    提出了一种新颖的隐式点云表示方法，结合了连续八叉树概率场和多分辨率哈希网格，实现了快速渲染和保留细致几何细节的优势，并且在几个常见基准数据集上实现了最先进的图像质量。

    

    我们引入了一种新的方法，用于重建和合成无边界的现实世界场景。与以往使用体积场、基于网格的模型或离散点云代理的方法相比，我们提出了一种混合场景表示，它在连续八叉树概率场和多分辨率哈希网格中隐含地编码点云。通过这样做，我们结合了两个世界的优势，保留了在优化过程中有利的行为：我们的新颖隐式点云表示和可微的双线性光栅化器实现了快速渲染，同时保留了细微的几何细节，而无需依赖于像结构运动点云这样的初始先验。我们的方法在几个常见基准数据集上实现了最先进的图像质量。此外，我们实现了快速推理，可交互帧速率，并且可以提取显式点云以进一步提高性能。

    arXiv:2403.16862v1 Announce Type: cross  Abstract: We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.
    
[^2]: M-HOF-Opt: 多目标分层输出反馈优化：基于乘子诱导损失景观调度的方法

    M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling

    [https://arxiv.org/abs/2403.13728](https://arxiv.org/abs/2403.13728)

    提出了一种新的方法，通过多目标分层输出反馈优化的方式，利用乘子诱导的损失景观调度解决神经网络参数化的复杂损失函数优化问题。

    

    当一个神经网络参数化的损失函数由许多项组成时，在优化过程中对权重乘子的组合选择形成了一个具有挑战性的问题。为了解决这个问题，我们提出了一个概率图模型（PGM），用于联合模型参数和乘子演化过程，具有基于超体积的似然，促进每个损失项的多目标下降。相应的参数和乘子估计作为一个顺序决策过程被转化为一个最优控制问题，其中多目标下降目标被分层地分派到一系列约束优化子问题中。子问题约束根据帕累托支配自动适应并作为低层乘子控制器调度损失景观的设定点，通过每个损失项的输出反馈来运行。我们的方法是无乘子的，并且在时代尺度上运行。

    arXiv:2403.13728v1 Announce Type: new  Abstract: When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs,
    
[^3]: 低成本隐私感知去中心化学习

    Low-Cost Privacy-Aware Decentralized Learning

    [https://arxiv.org/abs/2403.11795](https://arxiv.org/abs/2403.11795)

    ZIP-DL是一种低成本的隐私感知去中心化学习算法，通过向每个模型更新添加相关噪声，在保护隐私的同时实现了较高的模型准确性，具有较好的收敛速度和隐私保证。

    

    本文介绍了一种新颖的隐私感知去中心化学习（DL）算法ZIP-DL，该算法依赖于在模型训练过程中向每个模型更新添加相关噪声。这种技术确保了由于其相关性，在聚合过程中添加的噪声几乎相互抵消，从而最小化对模型准确性的影响。此外，ZIP-DL不需要多次通信轮进行噪声抵消，解决了隐私保护与通信开销之间的常见权衡。我们为收敛速度和隐私保证提供了理论保证，从而使ZIP-DL可应用于实际场景。我们的广泛实验研究表明，ZIP-DL在易受攻击性和准确性之间取得了最佳权衡。特别是，与基线DL相比，ZIP-DL（i）将可追踪攻击的有效性降低了多达52个点，（ii）准确性提高了高达37个百分点。

    arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
    
[^4]: 通过锚多元分析改善泛化能力

    Improving generalisation via anchor multivariate analysis

    [https://arxiv.org/abs/2403.01865](https://arxiv.org/abs/2403.01865)

    引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。

    

    我们在锚回归（AR）中引入因果正则化扩展，以改善超出分布（OOD）的泛化能力。我们提出了与锚框架相匹配的损失函数，以确保对分布转移的稳健性。各种多元分析（MVA）算法，如（正交化）PLS、RRR和MLR，均在锚框架内。我们观察到简单的正则化增强了OOD设置中的稳健性。在合成和真实的气候科学问题中，为所选算法提供了估计器，展示了其一致性和有效性。经验验证突显了锚正则化的多功能性，强调其与MVA方法的兼容性，并强调其在增强可复制性的同时抵御分布转移中的作用。扩展的AR框架推进了因果推断方法论，解决了可靠OOD泛化的需求。

    arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
    
[^5]: 结构保持的扩散模型

    Structure Preserving Diffusion Models

    [https://arxiv.org/abs/2402.19369](https://arxiv.org/abs/2402.19369)

    提出了一种结构保持的扩散过程，可以学习具有群对称性等额外结构的分布，并开发了一系列对称等变扩散模型来实现这一点。

    

    近年来，扩散模型已成为主要的分布学习方法。在本文中，我们介绍了结构保持的扩散过程，这是一类用于学习具有额外结构（如群对称性）的分布的扩散过程，通过制定扩散转换步骤保持对称性的理论条件。除了实现等变数据采样轨迹外，我们通过开发一系列不同的对称等变扩散模型来说明这些结果，这些模型能够学习固有对称的分布。我们使用实证研究验证所开发的模型符合提出的理论，并在样本均等性方面能够胜过现有方法。我们还展示了如何利用提出的模型实现理论上保证的等变图像噪声。

    arXiv:2402.19369v1 Announce Type: new  Abstract: Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise r
    
[^6]: 用图神经网络对随机几何图进行对齐

    Random Geometric Graph Alignment with Graph Neural Networks

    [https://arxiv.org/abs/2402.07340](https://arxiv.org/abs/2402.07340)

    本文研究了在图对齐问题中，通过图神经网络可以高概率恢复正确的顶点对齐。通过特定的特征稀疏性和噪声水平条件，我们证明了图神经网络的有效性，并与直接匹配方法进行了比较。

    

    我们研究了在顶点特征信息存在的情况下，图神经网络在图对齐问题中的性能。具体而言，给定两个独立扰动的单个随机几何图以及噪声稀疏特征的情况下，任务是恢复两个图的顶点之间的未知一对一映射关系。我们证明在特征向量的稀疏性和噪声水平满足一定条件的情况下，经过精心设计的单层图神经网络可以在很高的概率下通过图结构来恢复正确的顶点对齐。我们还证明了噪声水平的条件上界，仅存在对数因子差距。最后，我们将图神经网络的性能与直接在噪声顶点特征上求解分配问题进行了比较。我们证明了当噪声水平至少为常数时，这种直接匹配会导致恢复不完全，而图神经网络可以容忍n

    We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. We also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate n
    
[^7]: VampPrior混合模型

    The VampPrior Mixture Model

    [https://arxiv.org/abs/2402.04412](https://arxiv.org/abs/2402.04412)

    本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。

    

    当前用于深度潜变量模型（DLVMs）的聚类先验需要预先定义聚类的数量，并且容易受到较差的初始化的影响。解决这些问题可以通过同时执行集成和聚类的方式极大地改进基于深度学习的scRNA-seq分析。我们将VampPrior（Tomczak和Welling，2018）调整为Dirichlet过程高斯混合模型，得到VampPrior混合模型（VMM），这是一种新颖的DLVM先验。我们提出了一个推理过程，交替使用变分推理和经验贝叶斯，以清楚地区分变分和先验参数。在基准数据集上使用VMM的变分自动编码器获得了极具竞争力的聚类性能。将VMM与广受欢迎的scRNA-seq集成方法scVI（Lopez等，2018）相结合，显著改善了其性能，并自动将细胞分组为具有生物意义的聚类。

    Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
    
[^8]: 标准 Gaussian 过程在高维贝叶斯优化中足以应对

    Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization

    [https://arxiv.org/abs/2402.02746](https://arxiv.org/abs/2402.02746)

    标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。

    

    长期以来，人们普遍认为使用标准 Gaussian 过程（GP）进行贝叶斯优化（BO），即标准 BO，在高维优化问题中效果不佳。这种观念可以部分归因于 Gaussian 过程在协方差建模和函数估计中对高维输入的困难。虽然这些担忧看起来合理，但缺乏支持这种观点的经验证据。本文系统地研究了在各种合成和真实世界基准问题上，使用标准 GP 回归进行高维优化的贝叶斯优化。令人惊讶的是，标准 GP 的表现始终位于最佳范围内，往往比专门为高维优化设计的现有 BO 方法表现更好。与刻板印象相反，我们发现标准 GP 可以作为学习高维目标函数的能力强大的代理。在没有强结构假设的情况下，使用标准 GP 进行 BO 可以获得非常好的性能。

    There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
    
[^9]: 超图-MLP：在无需消息传递的超图上学习

    Hypergraph-MLP: Learning on Hypergraphs without Message Passing

    [https://arxiv.org/abs/2312.09778](https://arxiv.org/abs/2312.09778)

    提出了一种名为Hypergraph-MLP的新型学习框架，用于处理超图结构数据，可以在训练监督中集成超图结构信息而无需消息传递，从而在推理时减少过度平滑和结构扰动引起的挑战。

    

    超图在建模包含两个以上实体的高阶关系数据中至关重要，在机器学习和信号处理中越来越受重视。许多超图神经网络利用在超图结构上的消息传递来增强节点表征学习，从而在超图节点分类等任务中取得了令人印象深刻的表现。然而，这些基于消息传递的模型面临着过度平滑以及在推理时对结构扰动的高延迟和敏感性等挑战。为了应对这些挑战，我们提出了一种另类方法，即将关于超图结构的信息集成到训练监督中，而无需明确的消息传递，从而在推理时也消除了对其的依赖。具体而言，我们引入了Hypergraph-MLP，一种新颖的用于超图结构数据的学习框架，其中学习模型是一个简单的多层感知机。

    arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
    
[^10]: 对扰动奖励强化学习的分布式奖励评论家架构的研究

    The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])

    [http://arxiv.org/abs/2401.05710](http://arxiv.org/abs/2401.05710)

    这项研究展示了一种适应性分布式奖励评论家架构，能够在未知扰动的情况下恢复真实奖励，并在多个控制任务中取得较高的回报。

    

    我们研究了在未知奖励扰动的情况下的强化学习。现有的方法对这个问题做出了强大的假设，包括奖励平滑性、已知扰动和/或不会改变最优策略的扰动。我们研究了未知任意扰动的情况，这些扰动对奖励空间进行了离散化和洗牌，但在扰动后，真实奖励属于最频繁观察到的类别。这类扰动泛化了现有的类别（并在极限情况下泛化了所有连续有界扰动），并战胜了现有的方法。我们引入了一种自适应的分布式奖励评论家，并在理论上证明了在技术条件下它可以恢复真实奖励。在离散和连续控制任务中的目标扰动下，我们在40/57个环境中赢利/平局（相对于最佳基线的16/57）。即使在非目标扰动下，我们仍然胜过设计为带有目标扰动的基线。

    We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed
    
[^11]: Transformers是无线通信中高效的上下文估计器

    Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])

    [http://arxiv.org/abs/2311.00226](http://arxiv.org/abs/2311.00226)

    这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。

    

    预训练的Transformers可以进行上下文学习，在只有少量提示的情况下，适应新的任务，而不需要任何显式的模型优化。受到这个属性的启发，我们提出了一种新的方法，称为上下文估计，用于估计从接收到的符号中的传输符号的经典通信问题。通信信道本质上是一个将传输符号映射到接收符号的噪声函数，这个函数可以由一个未知参数表示，其统计数据依赖于一个（也是未知的）潜在上下文。传统方法忽略了这种层次结构，只是试图使用已知的传输信号进行最小二乘估计，然后用于估计连续的未知传输符号。我们建立了基本联系，即Transformers在少量提示下展示出出色的上下文序列完成能力，因此它们应该能够隐式确定...

    Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
    
[^12]: 关联变换器是一种稀疏表示学习器

    Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])

    [http://arxiv.org/abs/2309.12862](http://arxiv.org/abs/2309.12862)

    关联变换器（AiT）是一种采用低秩显式记忆和关联记忆的稀疏表示学习器，通过联合端到端训练实现模块特化和注意力瓶颈的形成。

    

    在传统的Transformer模型中，出现了一种新兴的基于稀疏交互的注意力机制，这种机制与生物原理更为接近。包括Set Transformer和Perceiver在内的方法采用了与有限能力的潜在空间相结合的交叉注意力机制。基于最近对全局工作空间理论和关联记忆的神经科学研究，我们提出了关联变换器（AiT）。AiT引入了低秩显式记忆，既可以作为先验来指导共享工作空间的瓶颈注意力，又可以作为关联记忆的吸引子。通过联合端到端训练，这些先验自然地发展出模块的特化，每个模块对形成注意力瓶颈的归纳偏好有所贡献。瓶颈可以促进输入之间为将信息写入内存而进行竞争。我们展示了AiT是一种稀疏表示学习器。

    Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
    
[^13]: 从数据中基于光滑性先验推断超图结构

    Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])

    [http://arxiv.org/abs/2308.14172](http://arxiv.org/abs/2308.14172)

    本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。

    

    超图在处理涉及多个实体的高阶关系数据中非常重要。在没有明确超图可用的情况下，希望能够从节点特征中推断出有意义的超图结构，以捕捉数据内在的关系。然而，现有的方法要么采用简单预定义的规则，不能精确捕捉潜在超图结构的分布，要么学习超图结构和节点特征之间的映射，但需要大量标记数据（即预先存在的超图结构）进行训练。这两种方法都局限于实际情景中的应用。为了填补这一空白，我们提出了一种新的光滑性先验，使我们能够设计一种方法，在没有标记数据作为监督的情况下推断出每个潜在超边的概率。所提出的先验表示超边中的节点特征与包含该超边的超边的特征高度相关。

    Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
    
[^14]: AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).

    AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])

    [http://arxiv.org/abs/2308.05239](http://arxiv.org/abs/2308.05239)

    这篇论文提出了适应现代应用和组织要求的AI-enabled软件和系统架构框架，重点关注机器学习驱动的智能物联网系统(CPS)。作者提出了用于评估和基准化ML-enabled CPS的优点标准。

    

    在文献中提出了几种软件、系统和企业的架构框架。它们识别了各种利益相关者，并定义了架构的观点和视图，以框架和解决利益相关者的关注点。然而，在现有的架构框架中，尚未包括与数据科学和机器学习相关的利益相关者，如数据科学家和数据工程师。因此，它们未能解决响应数据科学社区关注的架构视点和视图。本文通过建立适用于现代应用和组织的架构框架来填补这一空白，其中机器学习工件普遍存在且至关重要。具体而言，我们专注于机器学习驱动的智能物联网系统（CPS），并提出了两组适应CPS高效开发和性能评估的优点标准，即用于评估和基准化机器学习驱动CPS的标准，

    Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
    
[^15]: SketchOGD：内存高效的持续学习

    SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])

    [http://arxiv.org/abs/2305.16424](http://arxiv.org/abs/2305.16424)

    SketchOGD提出了一种内存高效的解决灾难性遗忘的方法，通过采用在线草图算法，将模型梯度压缩为固定大小的矩阵，从而改进了现有的算法——正交梯度下降（OGD）。

    

    当机器学习模型在一系列任务上持续训练时，它们容易忘记先前任务上学习到的知识，这种现象称为灾难性遗忘。现有的解决灾难性遗忘的方法往往涉及存储过去任务的信息，这意味着内存使用是确定实用性的主要因素。本文提出了一种内存高效的解决灾难性遗忘的方法，改进了一种已有的算法——正交梯度下降（OGD）。OGD利用先前模型梯度来找到维持先前数据点性能的权重更新。然而，由于存储先前模型梯度的内存成本随算法运行时间增长而增加，因此OGD不适用于任意长时间跨度的连续学习。针对这个问题，本文提出了SketchOGD。SketchOGD采用在线草图算法，将模型梯度压缩为固定大小的矩阵。

    When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
    
[^16]: Group-Invariant GAN的统计保证

    Statistical Guarantees of Group-Invariant GANs. (arXiv:2305.13517v1 [stat.ML])

    [http://arxiv.org/abs/2305.13517](http://arxiv.org/abs/2305.13517)

    本研究提出了群不变GAN的统计保证，发现当学习群不变分布时，群不变GAN所需样本数会按群体大小的幂比例减少。

    

    Group-Invariant生成对抗网络(GAN)是一种GAN，其中生成器和判别器具有硬性集团对称性。实证研究表明，这些网络能够学习具有显着改进数据效率的集团不变分布。在本研究中，我们旨在通过分析群不变GAN的样本复杂度减少来严格量化这种改进。我们的研究发现，在学习群不变分布时，群不变GAN所需样本数按照群体大小的幂比例减少，这个幂取决于分布支持的本质维度。据我们所知，这项工作是首个为群不变生成模型，特别是GAN提供统计估计的工作，并可以为其他群不变生成模型的研究提供借鉴。

    Group-invariant generative adversarial networks (GANs) are a type of GANs in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally with a power of the group size, and this power depends on the intrinsic dimension of the distribution's support. To our knowledge, this work presents the first statistical estimation for group-invariant generative models, specifically for GANs, and it may shed light on the study of other group-invariant generative models.
    
[^17]: Q-malizing流和无穷小密度比估计

    Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])

    [http://arxiv.org/abs/2305.11857](http://arxiv.org/abs/2305.11857)

    研究提出了一种可以从一个数据分布P传输到任意访问通过有限样本的Q的流模型。这个模型通过神经ODE模型进行，可以进行无穷小DRE。

    

    连续的正则化流在生成任务中被广泛使用，其中流网络从数据分布P传输到正态分布。一种能够从P传输到任意Q的流模型，其中P和Q都可通过有限样本访问，将在各种应用兴趣中使用，特别是在最近开发的望远镜密度比估计中（DRE），它需要构建中间密度以在P和Q之间建立桥梁。在这项工作中，我们提出了这样的“Q-malizing流”，通过神经ODE模型进行，该模型通过经验样本的可逆传输从P到Q（反之亦然），并通过最小化传输成本进行正则化。训练好的流模型使我们能够沿与时间参数化的log密度进行无穷小DRE，通过训练附加的连续时间流网络使用分类损失来估计log密度的时间偏导数。通过积分时间得分网络

    Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
    
[^18]: 基于双重平滑先验学习信号的超图结构

    Learning Hypergraphs From Signals With Dual Smoothness Prior. (arXiv:2211.01717v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01717](http://arxiv.org/abs/2211.01717)

    本研究提出了一种基于双重平滑先验的超图结构学习框架，可从观察到的信号中学习超图结构以捕获实体间的内在高阶关系。

    

    超图结构学习是从观察到的信号中学习超图结构，以捕捉实体之间内在的高阶关系，当数据集中没有可用的超图拓扑结构时，这变得非常关键。本文提出了一种新的双重平滑先验的超图结构学习框架HGSL，通过把每个超边与具有节点信号平滑性和边连接性的子图对应起来，揭示了观察到的节点信号和超图结构之间的映射。实验结果表明了该方法的有效性。

    Hypergraph structure learning, which aims to learn the hypergraph structures from the observed signals to capture the intrinsic high-order relationships among the entities, becomes crucial when a hypergraph topology is not readily available in the datasets. There are two challenges that lie at the heart of this problem: 1) how to handle the huge search space of potential hyperedges, and 2) how to define meaningful criteria to measure the relationship between the signals observed on nodes and the hypergraph structure. In this paper, for the first challenge, we adopt the assumption that the ideal hypergraph structure can be derived from a learnable graph structure that captures the pairwise relations within signals. Further, we propose a hypergraph structure learning framework HGSL with a novel dual smoothness prior that reveals a mapping between the observed node signals and the hypergraph structure, whereby each hyperedge corresponds to a subgraph with both node signal smoothness and e
    

