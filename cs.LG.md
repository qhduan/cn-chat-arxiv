# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [XpertAI: uncovering model strategies for sub-manifolds](https://arxiv.org/abs/2403.07486) | XpertAI是一个框架，可以将预测策略解开为多个特定范围的子策略，并允许将模型的查询制定为这些子策略的线性组合。 |
| [^2] | [Ant Colony Sampling with GFlowNets for Combinatorial Optimization](https://arxiv.org/abs/2403.07041) | 本文提出了生成流蚁群采样器（GFACS），一种结合生成流网络与蚁群优化方法的神经引导元启发式算法，在组合优化任务中表现优于基线ACO算法并与特定问题启发式方法具有竞争力。 |
| [^3] | [Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL](https://arxiv.org/abs/2403.06323) | 本研究通过将其降低为标准强化学习提出了两个通用的元算法，一个基于乐观算法，另一个基于策略优化，概括了以往的风险敏感强化学习理论并证实了新的理论在具有有界可覆盖性的MDP中的有效性。 |
| [^4] | [LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression](https://arxiv.org/abs/2403.04348) | LoCoDL是一种通信高效的分布式学习算法，结合了本地训练和压缩技术，具有双倍加速的通信复杂度优势，特别适用于一般异构条件下的强凸函数。 |
| [^5] | [SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction](https://arxiv.org/abs/2403.01570) | 提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。 |
| [^6] | [Fine-tuning with Very Large Dropout](https://arxiv.org/abs/2403.00946) | 通过使用非常高的dropout率进行微调，可以实现超出分布性能，这超出了集成和权重平均方法。 |
| [^7] | [DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation](https://arxiv.org/abs/2402.17812) | DropBP提出了一种新颖的方式来加速大型语言模型的微调，通过在反向传播过程中随机丢弃层以减少计算成本同时保持准确性。 |
| [^8] | [Foundational Inference Models for Dynamical Systems](https://arxiv.org/abs/2402.07594) | 本研究提出了一种基于监督学习的框架，用于从噪声数据中零样本推理动态系统的普通微分方程（ODE）。通过生成大型ODE数据集，并利用神经网络将噪声观察和初始条件以及向量场进行映射，得到称为基础推理模型（FIM）的结果模型。这些模型可以复制、匹配和组合，用于构建任何维度的推理模型。 |
| [^9] | [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://arxiv.org/abs/2402.07033) | 本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。 |
| [^10] | [Non-Stationary Latent Auto-Regressive Bandits](https://arxiv.org/abs/2402.03110) | 本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。 |
| [^11] | [Are Generative AI systems Capable of Supporting Information Needs of Patients?](https://arxiv.org/abs/2402.00234) | 生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。 |
| [^12] | [On Rademacher Complexity-based Generalization Bounds for Deep Learning](https://arxiv.org/abs/2208.04284) | 该论文研究了基于Rademacher复杂度的方法在对卷积神经网络进行少类别图像分类时生成非空泛化界限。其中的关键技术贡献是发展了针对函数空间和具有一般Lipschitz激活函数的CNNs的新的Talagrand压缩引理。 |
| [^13] | [AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis.](http://arxiv.org/abs/2401.10895) | 本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。 |
| [^14] | [FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering.](http://arxiv.org/abs/2310.16152) | 本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。 |
| [^15] | [MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale.](http://arxiv.org/abs/2310.12457) | MuseGNN提出了一种可解释和可收敛的大规模图神经网络层，通过迭代地减少基于采样的能量函数，同时作为预测特征和能量函数最小化者，具有竞争力的准确性和可扩展性。 |
| [^16] | [Direct Gradient Temporal Difference Learning.](http://arxiv.org/abs/2308.01170) | 本文提出了直接梯度时间差分学习的方法，通过使用马尔可夫数据流中的两个样本来解决双重取样问题，去除了传统方法中的额外权重，保证了计算效率，并提供了收敛性的分析。 |
| [^17] | [Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence.](http://arxiv.org/abs/2305.15557) | 提出了一种新的非参数方法，用于识别随机微分方程中的漂移和扩散系数，该方法具有快速的收敛率，使得学习速率随着未知系数的光滑度增加而变得更加紧密。 |

# 详细

[^1]: XpertAI：揭示子流形的模型策略

    XpertAI: uncovering model strategies for sub-manifolds

    [https://arxiv.org/abs/2403.07486](https://arxiv.org/abs/2403.07486)

    XpertAI是一个框架，可以将预测策略解开为多个特定范围的子策略，并允许将模型的查询制定为这些子策略的线性组合。

    

    近年来，可解释人工智能（XAI）方法已经促进了深入验证和知识提取机器学习模型。尽管针对分类进行了广泛研究，但很少有XAI解决方案解决了特定于回归模型的挑战。在回归中，解释需要精确制定以应对特定用户查询（例如区分“为什么输出大于0？”和“为什么输出大于50？”）。此外，它们应反映模型在相关数据子流形上的行为。在本文中，我们介绍了XpertAI，这是一个将预测策略解开为多个范围特定的子策略，并允许将对模型的精准查询（“被解释物”）的制定为这些子策略的线性组合的框架。XpertAI通常制定可以与基于遮挡、梯度集成或反向传播的流行XAI归因技术一起使用。

    arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
    
[^2]: 使用GFlowNets的蚁群采样用于组合优化

    Ant Colony Sampling with GFlowNets for Combinatorial Optimization

    [https://arxiv.org/abs/2403.07041](https://arxiv.org/abs/2403.07041)

    本文提出了生成流蚁群采样器（GFACS），一种结合生成流网络与蚁群优化方法的神经引导元启发式算法，在组合优化任务中表现优于基线ACO算法并与特定问题启发式方法具有竞争力。

    

    本文介绍了生成流蚁群采样器（GFACS），这是一种新颖的用于组合优化的神经引导元启发式算法。GFACS 将生成流网络（GFlowNets）与蚁群优化（ACO）方法相结合。GFlowNets 是一种生成模型，它在组合空间中学习构造性策略，通过在输入图实例上提供决策变量的知情先验分布来增强 ACO。此外，我们引入了一种新颖的训练技巧组合，包括搜索引导的局部探索、能量归一化和能量塑形，以提高 GFACS 的性能。我们的实验结果表明，GFACS 在七个组合优化任务中优于基线 ACO 算法，并且在车辆路径问题的问题特定启发式方法中具有竞争力。源代码可在 \url{https://github.com/ai4co/gfacs} 获取。

    arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
    
[^3]: 使用优化等价证明降低到标准强化学习中的风险敏感RL

    Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL

    [https://arxiv.org/abs/2403.06323](https://arxiv.org/abs/2403.06323)

    本研究通过将其降低为标准强化学习提出了两个通用的元算法，一个基于乐观算法，另一个基于策略优化，概括了以往的风险敏感强化学习理论并证实了新的理论在具有有界可覆盖性的MDP中的有效性。

    

    我们研究了具有优化等价证明（OCE）风险的风险敏感强化学习（RSRL），该风险概括了条件值风险（CVaR）、熵风险和马科维茨的均值-方差。通过增强马尔可夫决策过程（MDP），我们提出了两个通用的元算法，通过将其降低为标准RL：一个基于乐观算法，另一个基于策略优化。我们的乐观元算法概括了几乎所有之前RSRL理论，该理论使用熵风险或CVaR。在离散奖励下，我们的乐观理论还证明了具有有界可覆盖性的MDP（例如外生块MDP）的第一个RSRL遗憾上界。在离散奖励下，我们的策略优化元算法在一个新颖的度量中享有全局收敛性和局部改进保证，该度量下界为真实的OCE风险。最后，我们使用PPO实例化我们的框架，构建一个MDP，并展示它学习了最优的风险敏感。

    arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive
    
[^4]: LoCoDL: 具有本地训练和压缩的通信高效分布式学习

    LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression

    [https://arxiv.org/abs/2403.04348](https://arxiv.org/abs/2403.04348)

    LoCoDL是一种通信高效的分布式学习算法，结合了本地训练和压缩技术，具有双倍加速的通信复杂度优势，特别适用于一般异构条件下的强凸函数。

    

    在分布式优化和学习中，甚至在现代联邦学习框架中，由于通信速度慢且成本高，通信至关重要。我们介绍了LoCoDL，这是一种通信高效的算法，它利用了本地训练和压缩这两种流行且有效的技术，本地训练降低了通信频率，压缩则是发送短的比特流而不是完整的浮点数向量。LoCoDL适用于大类别的无偏压缩器，其中包括广泛使用的稀疏化和量化方法。LoCoDL在一般异构条件下具有双倍加速的通信复杂度优势，这取决于函数的条件数和模型维度，特别是在强凸函数的情况下。在实践中得到了验证，LoCoDL胜过了现有的算法。

    arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.
    
[^5]: SERVAL：垂直模型和LLM之间的协同学习，实现零-shot级别的医学预测

    SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction

    [https://arxiv.org/abs/2403.01570](https://arxiv.org/abs/2403.01570)

    提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。

    

    近期大型语言模型（LLMs）的发展展示出对通用和常识问题卓越的零-shot能力。然而，LLMs在领域特定垂直问题上的应用仍然落后，主要是由于垂直知识方面的问题和不足。此外，垂直数据注释过程通常需要劳动密集型的专家参与，因此增加了增强模型垂直能力的额外挑战。在本文中，我们提出了SERVAL，一个协同学习流水线，旨在通过相互增强，对LLMs和小模型的垂直能力进行无监督开发。具体来说，SERVAL利用LLMs的零-shot输出作为注释，利用其置信度来从头开始教授一个强大的垂直模型。反过来，训练有素的垂直模型引导LLM微调，以增强其零-shot能力，逐步改进两者。

    arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
    
[^6]: 使用非常大的Dropout进行微调

    Fine-tuning with Very Large Dropout

    [https://arxiv.org/abs/2403.00946](https://arxiv.org/abs/2403.00946)

    通过使用非常高的dropout率进行微调，可以实现超出分布性能，这超出了集成和权重平均方法。

    

    今天不可能假装机器学习实践与训练和测试数据遵循相同分布的观念是兼容的。该论文调查了使用非常高的丢弃率来获得这种丰富表示，尽管使用这样的丢弃率从头开始训练深度网络几乎是不可能的，但在这些条件下对大型预训练模型进行微调不仅是可能的，而且实现了超越集成和权重平均方法的超出分布性能。

    arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
    
[^7]: DropBP：通过丢弃反向传播加速大型语言模型的微调

    DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation

    [https://arxiv.org/abs/2402.17812](https://arxiv.org/abs/2402.17812)

    DropBP提出了一种新颖的方式来加速大型语言模型的微调，通过在反向传播过程中随机丢弃层以减少计算成本同时保持准确性。

    

    训练深度神经网络通常涉及正向和反向传播过程中的大量计算成本。传统的层次丢弃技术在训练过程中丢弃某些层以减少计算负担。然而，在正向传播过程中丢弃层会对训练过程产生不利影响，降低准确性。本文提出了DropBP，这是一种旨在减少计算成本同时保持准确性的新方法。DropBP在反向传播过程中随机丢弃层，不影响正向传播。此外，DropBP计算每个层的敏感性以分配适当的丢失率，从而稳定训练过程。DropBP旨在通过反向传播增强训练过程的效率，从而加速使用反向传播进行完全微调和参数高效微调。

    arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
    
[^8]: 动态系统的基础推理模型

    Foundational Inference Models for Dynamical Systems

    [https://arxiv.org/abs/2402.07594](https://arxiv.org/abs/2402.07594)

    本研究提出了一种基于监督学习的框架，用于从噪声数据中零样本推理动态系统的普通微分方程（ODE）。通过生成大型ODE数据集，并利用神经网络将噪声观察和初始条件以及向量场进行映射，得到称为基础推理模型（FIM）的结果模型。这些模型可以复制、匹配和组合，用于构建任何维度的推理模型。

    

    普通微分方程（ODE）构成了作为自然和社会现象模型的动态系统的基础。然而，推断出最佳描述给定现象的一组噪声观察的ODE可能非常具有挑战性，现有的模型往往也非常专业化和复杂。在这项工作中，我们提出了一种新颖的监督式学习框架，用于从噪声数据中零样本推理ODE。我们首先通过对初始条件空间和定义它们的向量场空间的分布进行采样，生成大型一维ODE数据集。然后，我们学习将这些方程的解的噪声观察与其相应的初始条件和向量场之间的神经映射。我们将结果模型称为基础推理模型（FIM），它们可以（i）沿时间维复制和匹配以增加分辨率；（ii）复制和组合以构建任何维度的推理模型。

    Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
    
[^9]: Fiddler：用于Mixture-of-Experts模型快速推断的CPU-GPU编排

    Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models

    [https://arxiv.org/abs/2402.07033](https://arxiv.org/abs/2402.07033)

    本文介绍了Fiddler，一种用于Mixture-of-Experts模型的资源高效推断引擎，通过CPU-GPU编排实现最小化数据传输，相比现有方法提高了一个数量级的推断速度。

    

    基于Mixture-of-Experts（MoE）架构的大型语言模型（LLM）在各种任务上表现出了很好的性能。然而，在资源受限的环境下运行这些模型，即GPU内存资源不丰富的情况下，由于模型规模庞大，存在挑战。现有的将模型权重卸载到CPU内存的系统，由于频繁地在CPU和GPU之间移动数据而导致显著的开销。在本文中，我们提出了Fiddler，一种用于MoE模型的资源高效推断引擎，实现了CPU-GPU编排。Fiddler的核心思想是利用CPU的计算能力来最小化CPU和GPU之间的数据传输。我们的评估结果表明，Fiddler能够在单个具有24GB内存的GPU上运行未压缩的Mixtral-8x7B模型（参数超过90GB），每秒生成超过3个token，相比现有方法提高一个数量级。Fiddler的代码可以公开访问，网址为\url{https://github.com/efeslab/fiddler}

    Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
    
[^10]: 非平稳潜在自回归赌博机

    Non-Stationary Latent Auto-Regressive Bandits

    [https://arxiv.org/abs/2402.03110](https://arxiv.org/abs/2402.03110)

    本文提出了非平稳潜在自回归赌博机问题，并提出了一个算法，在这种环境下可以达到较低的遗憾率。

    

    本文考虑具有非平稳奖励的随机多臂赌博机问题。我们提出了一个新颖的非平稳环境的公式，其中臂的平均奖励随时间变化是由一些未知的潜在自回归(AR)状态的顺序k决定的。我们将这个新的环境称为潜在AR赌博机。潜在AR赌博机的不同形式在许多现实世界的场景中都出现，特别是在行为健康或教育等新兴科学领域中，这里缺乏对环境的机制建模。如果AR顺序k已知，我们提出了一个算法，在这种情况下，算法表现出O(k√T)的遗憾率。实证结果显示，即使k被错误地估计，我们的算法在多个非平稳环境中也胜过标准的UCB算法。

    We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
    
[^11]: 生成式AI系统能否支持患者的信息需求？

    Are Generative AI systems Capable of Supporting Information Needs of Patients?

    [https://arxiv.org/abs/2402.00234](https://arxiv.org/abs/2402.00234)

    生成式AI系统被应用于支持患者信息需求的研究中，以提高患者对放射学数据的理解和管理能力。通过与患者和医疗专家的对话分析，我们确定了常见的医学信息需求和问题。

    

    患有复杂疾病如癌症的患者面临复杂的信息挑战，他们不仅需要了解他们的疾病，还需要学会如何管理它。与医疗专家（放射科医师、肿瘤科医师）密切互动可以提高患者的学习能力，从而改善疾病预后。然而，这种方法资源密集且占用了专家的时间，使他们无法完成其他关键任务。鉴于生成式AI模型在改进医疗系统方面的最新进展，我们的工作研究了生成式视觉问答系统在放射学成像数据背景下如何负责任地支持患者的信息需求。我们进行了一项形成性需求发现研究，参与者讨论了一个虚构近亲的胸部计算机断层扫描（CT）图像和相关的放射学报告。通过对参与者和医疗专家之间的对话的主题分析，我们确定常见的医学信息需求和问题。

    Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
    
[^12]: 基于Rademacher复杂度的深度学习一般化界限研究

    On Rademacher Complexity-based Generalization Bounds for Deep Learning

    [https://arxiv.org/abs/2208.04284](https://arxiv.org/abs/2208.04284)

    该论文研究了基于Rademacher复杂度的方法在对卷积神经网络进行少类别图像分类时生成非空泛化界限。其中的关键技术贡献是发展了针对函数空间和具有一般Lipschitz激活函数的CNNs的新的Talagrand压缩引理。

    

    我们展示了基于Rademacher复杂度的方法可以生成对卷积神经网络（CNNs）进行分类少量类别图像非空泛化界限。新的Talagrand压缩引理的发展对于高维映射函数空间和具有一般Lipschitz激活函数的CNNs是一个关键技术贡献。我们的结果表明，Rademacher复杂度不依赖于CNNs的网络长度，特别是对于诸如ReLU，Leaky ReLU，Parametric Rectifier Linear Unit，Sigmoid和Tanh等特定类型的激活函数。

    We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
    
[^13]: 供应链风险评估中的人工智能：一项系统文献综述和文献计量分析

    AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])

    [http://arxiv.org/abs/2401.10895](http://arxiv.org/abs/2401.10895)

    本文通过系统文献综述和文献计量分析，填补了供应链风险评估中新兴人工智能/机器学习技术的研究空白，为了解这些技术在实践中的实际影响提供了关键见解。

    

    通过整合人工智能和机器学习技术，供应链风险评估(SCRA)经历了深刻的演变，革新了预测能力和风险缓解策略。这种演变的重要性在于在现代供应链中确保运营的韧性和连续性，需要稳健的风险管理策略。以往的综述已经概述了已建立的方法，但忽视了新兴的人工智能/机器学习技术，在理解其在SCRA中的实际影响方面存在明显的研究空白。本文进行了系统的文献综述，并结合了全面的文献计量分析。我们仔细研究了1717篇论文，并从2014年至2023年之间发表的48篇文章中获得了关键见解。该综述填补了这一研究空白，通过回答关键研究问题，探究了现有的人工智能/机器学习技术、方法论、研究结果和未来发展方向。

    Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
    
[^14]: FLTrojan: 通过选择性权重篡改对联邦语言模型进行隐私泄露攻击

    FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])

    [http://arxiv.org/abs/2310.16152](http://arxiv.org/abs/2310.16152)

    本文提出了一种FLTrojan攻击方法，通过选择性权重篡改，从联邦语言模型中泄露隐私敏感用户数据。通过观察到FL中中间轮次的模型快照可以引起更大的隐私泄露，并发现隐私泄露可以通过篡改模型的选择性权重来加剧。

    

    联邦学习(Federated learning, FL)正成为许多技术应用中的关键组件，包括语言建模领域，其中个体FL参与者在其本地数据集中往往具有敏感的文本数据。然而，确定联邦语言模型中的隐私泄露程度并不简单，现有的攻击只是试图提取数据，而不考虑数据的敏感性或天真性。为了填补这一空白，在本文中，我们介绍了关于从联邦语言模型中泄露隐私敏感用户数据的两个新发现。首先，我们观察到FL中中间轮次的模型快照比最终训练模型能够造成更大的隐私泄露。其次，我们确定隐私泄露可以通过篡改模型的选择性权重来加剧，这些权重特别负责记忆敏感训练数据。我们展示了恶意客户端如何在FL中泄露其他用户的隐私敏感数据。

    Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
    
[^15]: MuseGNN: 可解释和可收敛的大规模图神经网络层

    MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])

    [http://arxiv.org/abs/2310.12457](http://arxiv.org/abs/2310.12457)

    MuseGNN提出了一种可解释和可收敛的大规模图神经网络层，通过迭代地减少基于采样的能量函数，同时作为预测特征和能量函数最小化者，具有竞争力的准确性和可扩展性。

    

    在能够建模具有跨实例关系的数据的许多图神经网络（GNN）架构中，一类重要的子类涉及设计层，其正向传递迭代地减少感兴趣的图正则化能量函数。通过这种方式，输出层产生的节点嵌入既可作为解决下游任务（如节点分类）的预测特征，又可作为能量函数最小化者，继承了可靠的归纳偏置和可解释性。然而，构建以这种方式构建的GNN架构的扩展仍然具有挑战性，部分原因是正向传递的收敛可能涉及具有相当深度的模型。为了解决这个限制，我们提出了一种基于采样的能量函数和可扩展的GNN层，通过在某些设置中具有收敛保证的指导，迭代地减少它。我们还基于这些设计实例化了一个完整的GNN架构，该模型在准确性和可扩展性方面均具有竞争力。

    Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe
    
[^16]: 直接梯度时间差分学习

    Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])

    [http://arxiv.org/abs/2308.01170](http://arxiv.org/abs/2308.01170)

    本文提出了直接梯度时间差分学习的方法，通过使用马尔可夫数据流中的两个样本来解决双重取样问题，去除了传统方法中的额外权重，保证了计算效率，并提供了收敛性的分析。

    

    脱机学习使强化学习（RL）代理能够反事实地推理未执行的策略，是强化学习中最重要的思想之一。然而，当与函数逼近和自举这两个在大规模强化学习中不可或缺的因素结合时，会导致不稳定性。这就是臭名昭著的致命三元组。梯度时间差分（GTD）是解决这个致命三元组的一种强大工具。它的成功是通过使用权重复制或Fenchel对偶间接解决双重取样问题而实现的。在这篇论文中，我们提出了一种直接方法来解决双重取样问题，只需在逐渐增加的马尔可夫数据流中使用两个样本。所得到的算法与GTD一样计算效率高，但摒弃了GTD的额外权重。我们所付出的唯一代价是随着时间的推移，内存呈对数增长。我们提供了渐近和有限样本分析，其中收敛性可以得到保证。

    Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver
    
[^17]: 非参数学习具有快速收敛率的随机微分方程

    Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])

    [http://arxiv.org/abs/2305.15557](http://arxiv.org/abs/2305.15557)

    提出了一种新的非参数方法，用于识别随机微分方程中的漂移和扩散系数，该方法具有快速的收敛率，使得学习速率随着未知系数的光滑度增加而变得更加紧密。

    

    我们提出了一种新颖的非参数学习范式来识别非线性随机微分方程的漂移和扩散系数，该范式依赖于状态的离散时间观测。其关键思想是将相应的Fokker-Planck方程的基于RKHS的近似拟合到这些观测值，从而得出理论学习速率的估计值，这与以往的工作不同，当未知漂移和扩散系数的光滑度越高时，理论估计值越来越紧。由于我们的方法是基于内核的，因此离线预处理可以在原则上得到有效的数值实现。

    We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
    

