# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generative-Contrastive Heterogeneous Graph Neural Network](https://arxiv.org/abs/2404.02810) | 本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。 |
| [^2] | [Backpropagation through space, time, and the brain](https://arxiv.org/abs/2403.16933) | 提出了 Generalized Latent Equilibrium (GLE)，它是一种针对神经元网络的物理动态局部时空信用分配的计算框架。 |
| [^3] | [DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers](https://arxiv.org/abs/2403.10266) | 动态序列并行性（DSP）为多维Transformer模型引入了一种高效的序列并行方法，通过动态切换并行维度实现对多维注意力模型的优化。 |
| [^4] | [Exploring Challenges in Deep Learning of Single-Station Ground Motion Records](https://arxiv.org/abs/2403.07569) | 本研究旨在评估深度学习模型从场地运动记录中学习的效果，并探讨辅助信息对此过程的影响。 |
| [^5] | [Learning with Noisy Foundation Models](https://arxiv.org/abs/2403.06869) | 本文首次全面了解和分析了预训练数据集中的噪声性质，有效减轻其对下游任务影响。 |
| [^6] | [Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain](https://arxiv.org/abs/2403.06432) | 提出了一种受到计算机视觉中 JEPA 架构启发的 Spatio-Temporal Joint Embedding Masked Autoencoder（ST-JEMA）用于动态功能连接的自监督学习。 |
| [^7] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^8] | [Linear Dynamics-embedded Neural Network for Long-Sequence Modeling](https://arxiv.org/abs/2402.15290) | 提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。 |
| [^9] | [Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation](https://arxiv.org/abs/2402.14264) | 采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性 |
| [^10] | [Conformal Predictive Programming for Chance Constrained Optimization](https://arxiv.org/abs/2402.07407) | 可容许预测规划（CPP）是一种解决受任意随机参数影响的优化问题的方法，通过利用样本和量子引理将机遇受限优化（CCO）问题转化为确定性优化问题，并具备边际概率可行性保证。 |
| [^11] | [Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off](https://arxiv.org/abs/2402.07002) | 本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。 |
| [^12] | [Joint Problems in Learning Multiple Dynamical Systems](https://arxiv.org/abs/2311.02181) | 聚类时间序列的新问题，提出联合划分轨迹集并学习每个部分的线性动态系统模型，以最小化所有模型的最大误差 |
| [^13] | [Advective Diffusion Transformers for Topological Generalization in Graph Learning.](http://arxiv.org/abs/2310.06417) | 本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。 |
| [^14] | [Robust Transfer Learning with Unreliable Source Data.](http://arxiv.org/abs/2310.04606) | 本文提出了一个新的鲁棒性迁移学习方法TAB模型，通过衡量目标与源回归函数之间的模糊度水平来改善分类任务，并避免负迁移。通过实验验证，TAB模型在非参数分类和逻辑回归任务上表现出了优越的性能。 |
| [^15] | [Learning Quantum Processes with Quantum Statistical Queries.](http://arxiv.org/abs/2310.02075) | 本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。 |
| [^16] | [A simple connection from loss flatness to compressed representations in neural networks.](http://arxiv.org/abs/2310.01770) | 该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。 |
| [^17] | [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection.](http://arxiv.org/abs/2309.15670) | 这个研究构建了一个基于孟加拉语的注释语料库，用于多标签情感检测。通过使用基于上下文的方法以及BERT模型，填补了这一学科领域的空白。 |
| [^18] | [When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions.](http://arxiv.org/abs/2306.15546) | 基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。 |
| [^19] | [Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review.](http://arxiv.org/abs/2303.07546) | 本综述研究了受限对抗学习方法和自动化软件测试中受限数据生成方法的最新技术应用，探讨将这些方法整合至测试工具中以提高数字系统的鲁棒性和弹性。 |
| [^20] | [One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data.](http://arxiv.org/abs/2302.06375) | 本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。 |

# 详细

[^1]: 生成-对比异构图神经网络

    Generative-Contrastive Heterogeneous Graph Neural Network

    [https://arxiv.org/abs/2404.02810](https://arxiv.org/abs/2404.02810)

    本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。

    

    异构图表达了现实世界中复杂关系，包括多种类型的节点和边。受自监督学习启发，对比异构图神经网络(HGNNs)利用数据增强和辨别器展现了巨大潜力用于下游任务。然而，由于图的离散和抽象特性，数据增强仍然存在限制。为了解决上述限制，我们提出了一种新颖的\textit{生成-对比异构图神经网络(GC-HGNN)}。

    arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
    
[^2]: 通过空间、时间和大脑进行反向传播

    Backpropagation through space, time, and the brain

    [https://arxiv.org/abs/2403.16933](https://arxiv.org/abs/2403.16933)

    提出了 Generalized Latent Equilibrium (GLE)，它是一种针对神经元网络的物理动态局部时空信用分配的计算框架。

    

    有效的神经网络学习需要根据它们对解决任务的相对贡献来调整单个突触。然而，无论是生物还是人工的物理神经系统都受到时空局限。这样的网络如何执行高效的信用分配，在很大程度上仍是一个悬而未决的问题。在机器学习中，错误的反向传播算法几乎普遍被空间（BP）和时间（BPTT）两种方式给出答案。然而，BP(TT)被广泛认为依赖于不具生物学意义的假设，特别是关于时空局限性，而正向传播模型，如实时递归学习（RTRL），则受到内存约束的限制。我们引入了广义潜在平衡（GLE），这是一个针对神经元物理动态网络完全局部时空信用分配的计算框架。我们从

    arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
    
[^3]: DSP：多维Transformer的动态序列并行性

    DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers

    [https://arxiv.org/abs/2403.10266](https://arxiv.org/abs/2403.10266)

    动态序列并行性（DSP）为多维Transformer模型引入了一种高效的序列并行方法，通过动态切换并行维度实现对多维注意力模型的优化。

    

    通过本文介绍的动态序列并行性（DSP）方法，可以为多维Transformer模型实现高效的序列并行性。其关键思想是根据当前计算阶段动态切换并行性维度，利用多维注意力的潜在特性。这种动态维度切换使得序列并行性在多维模型中具有最小的通信开销。

    arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
    
[^4]: 探究单台场地运动记录的深度学习挑战

    Exploring Challenges in Deep Learning of Single-Station Ground Motion Records

    [https://arxiv.org/abs/2403.07569](https://arxiv.org/abs/2403.07569)

    本研究旨在评估深度学习模型从场地运动记录中学习的效果，并探讨辅助信息对此过程的影响。

    

    当代的深度学习模型在地震学和地震工程的各种应用中展现出了令人期待的结果。这些模型主要依赖利用场地运动记录进行地震事件分类、定位、地震早期预警系统和结构健康监测等任务。然而，这些模型从这些复杂的时间序列信号中有效学习的程度尚未得到彻底分析。本研究的目标是评估辅助信息（如地震相到达时间或网络内地震台站分布）在从场地运动记录中进行深度学习过程中的主导程度，可能会影响其有效性。我们对两种深度学习模型进行超参数搜索，评估它们在从场地运动记录中进行深度学习的有效性，同时检查辅助信息的影响。

    arXiv:2403.07569v1 Announce Type: cross  Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information o
    
[^5]: 在有噪声基础模型中学习

    Learning with Noisy Foundation Models

    [https://arxiv.org/abs/2403.06869](https://arxiv.org/abs/2403.06869)

    本文首次全面了解和分析了预训练数据集中的噪声性质，有效减轻其对下游任务影响。

    

    基础模型通常是在大规模数据集上进行预训练，然后通过调整来适应下游任务。然而，大规模预训练数据集往往无法获取或成本过高，可能包含标签噪声，这可能会对模型的泛化能力造成不利影响，并带来意想不到的风险。本文是首个全面了解和分析预训练数据集中噪声性质，并有效减轻其对下游任务影响的工作。具体而言，通过在合成有噪声的ImageNet-1K、YFCC15M和CC12M数据集上进行完全监督和图像-文本对比预训练的广泛实验，我们证明了，尽管预训练中的轻微噪声可以使同领域（ID）性能受益，即训练和测试数据共享类似分布，但它总是会破坏跨领域（OOD）性能，在那里训练和测试分布明显不同。

    arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
    
[^6]: 人类大脑动态功能连接的自监督学习中的联合嵌入掩蔽自编码器

    Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain

    [https://arxiv.org/abs/2403.06432](https://arxiv.org/abs/2403.06432)

    提出了一种受到计算机视觉中 JEPA 架构启发的 Spatio-Temporal Joint Embedding Masked Autoencoder（ST-JEMA）用于动态功能连接的自监督学习。

    

    arXiv:2403.06432v1 通告类型: 新的 摘要: 图神经网络（GNNs）在学习动态功能连接方面表现出潜力，可以区分人脑网络中的表现型。然而，获得用于训练的大量标记临床数据通常具有资源密集性，这使得实际应用变得困难。因此，在标签稀缺设置中，利用未标记数据对于表示学习变得至关重要。尽管生成式自监督学习技术，特别是掩蔽自编码器，在各个领域的表示学习中展现出了有希望的结果，但它们在动态图形上的应用以及动态功能连接方面仍未得到充分探讨，面临着捕捉高级语义表示方面的挑战。在这里，我们介绍了时空联合嵌入掩蔽自编码器（ST-JEMA），受到计算机视觉中联合嵌入预测架构（JEPA）的启发。ST-JEMA采用了一种受JEPA启发的策略来重构

    arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin
    
[^7]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^8]: 嵌入线性动力学的神经网络用于长序列建模

    Linear Dynamics-embedded Neural Network for Long-Sequence Modeling

    [https://arxiv.org/abs/2402.15290](https://arxiv.org/abs/2402.15290)

    提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。

    

    由于现有模型在长序列建模中性能和计算效率之间的权衡成为瓶颈，受到控制理论中具有多输入多输出的连续状态空间模型（SSMs）启发，我们提出了一种名为嵌入线性动力学的神经网络（LDNN）的新型神经网络。 SSM的连续、离散和卷积属性使LDNN具有少量参数、灵活的推断和在长序列任务中高效训练的特点。 我们开发了两种有效策略，对角化和“解耦然后快速傅立叶变换（FFT）”，以将卷积的时间复杂度从$O(LNH\max\{L, N\})$降低到$O(LN\max\{H, \log L\})$。 我们通过双向非因果和多头设置进一步改进了LDNN，以适应更广泛的应用范围。 对长距离竞技场（LRA）的大量实验表明了LDNN的有效性和最先进的性能。

    arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
    
[^9]: 双稳健学习在处理效应估计中的结构不可知性最优性

    Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation

    [https://arxiv.org/abs/2402.14264](https://arxiv.org/abs/2402.14264)

    采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性

    

    平均处理效应估计是因果推断中最核心的问题，应用广泛。虽然文献中提出了许多估计策略，最近还纳入了通用的机器学习估计器，但这些方法的统计最优性仍然是一个开放的研究领域。本文采用最近引入的统计下界结构不可知框架，该框架对干扰函数没有结构性质假设，除了访问黑盒估计器以达到小误差；当只愿意考虑使用非参数回归和分类神谕作为黑盒子过程的估计策略时，这一点尤其吸引人。在这个框架内，我们证明了双稳健估计器对于平均处理效应（ATE）和平均处理效应的统计最优性。

    arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
    
[^10]: 可容许预测规划用于机遇受限优化

    Conformal Predictive Programming for Chance Constrained Optimization

    [https://arxiv.org/abs/2402.07407](https://arxiv.org/abs/2402.07407)

    可容许预测规划（CPP）是一种解决受任意随机参数影响的优化问题的方法，通过利用样本和量子引理将机遇受限优化（CCO）问题转化为确定性优化问题，并具备边际概率可行性保证。

    

    在对预测规划（CP）的进展的激励下，我们提出了可容许预测规划（CPP），一种解决机遇受限优化（CCO）问题的方法，即受任意随机参数影响的非线性约束函数的优化问题。CPP利用这些随机参数的样本以及量子引理（CP的核心）将CCO问题转化为确定性优化问题。然后，我们通过：（1）将量子表示为线性规划以及其KKT条件（CPP-KKT）；（2）使用混合整数规划（CPP-MIP）来呈现CPP的两种易于处理的改进。CPP具备对CCO问题进行边际概率可行性保证，这与现有方法（例如样本逼近和场景方法）在概念上有所不同。尽管我们探讨了与样本逼近方法的算法相似之处，但我们强调CPP的优势在于易于扩展。

    Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be ext
    
[^11]: 客户端协作：具有保证隐私-效用权衡改进的灵活差分隐私联邦学习

    Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off

    [https://arxiv.org/abs/2402.07002](https://arxiv.org/abs/2402.07002)

    本论文提出了一种名为FedCEO的新型联邦学习框架，在保护用户隐私的同时，通过让客户端相互协作，实现了对模型效用和隐私之间的权衡。通过高效的张量低秩近端优化，该框架能够恢复被打断的语义信息，并在效用-隐私权衡方面取得了显著的改进。

    

    为了防止用户数据的隐私泄漏，在联邦学习中广泛使用差分隐私，但它并不是免费的。噪声的添加会随机干扰模型的语义完整性，并且这种干扰会随着通信轮次的增加而累积。在本文中，我们引入了一种具有严格隐私保证的新型联邦学习框架，名为FedCEO，通过让客户端"相互协作"，旨在在模型效用和用户隐私之间找到一种权衡。具体而言，我们在服务器上对堆叠的本地模型参数进行了高效的张量低秩近端优化，展示了它在光谱空间中灵活截断高频组分的能力。这意味着我们的FedCEO能够通过平滑全局语义空间来有效恢复被打断的语义信息，以适应不同隐私设置和持续的训练过程。此外，我们将SOTA的效用-隐私权衡边界提高了一个数量级。

    To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
    
[^12]: 学习多个动态系统中的联合问题

    Joint Problems in Learning Multiple Dynamical Systems

    [https://arxiv.org/abs/2311.02181](https://arxiv.org/abs/2311.02181)

    聚类时间序列的新问题，提出联合划分轨迹集并学习每个部分的线性动态系统模型，以最小化所有模型的最大误差

    

    时间序列的聚类是一个经过充分研究的问题，其应用范围从通过代谢产物浓度获得的定量个性化代谢模型到量子信息理论中的状态判别。我们考虑了一个变种，即给定一组轨迹和一些部分，我们联合划分轨迹集并学习每个部分的线性动态系统（LDS）模型，以使得所有模型的最大误差最小化。我们提出了全局收敛的方法和EM启发式算法，并附上了有前景的计算结果。

    arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
    
[^13]: 用于图学习中的拓扑概括的流动扩散变压器

    Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])

    [http://arxiv.org/abs/2310.06417](http://arxiv.org/abs/2310.06417)

    本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。

    

    图扩散方程与图神经网络（GNN）密切相关，并且最近引起了人们的关注，作为分析GNN动力学、形式化其表达能力和证明架构选择的有原则的框架。图学习中的一个关键问题是GNN的概括能力。当前方法的一个主要限制在于假设训练集和测试集中的图拓扑来自相同的分布。本文通过探索图扩散方程在不同图拓扑存在下的外推和概括能力，迈出了解析GNN概括性的一步。我们首先展示了基于图上局部扩散的现有模型在概括能力上的不足，这是由于对拓扑变化的指数敏感性引起的。随后的分析揭示了非局部扩散的潜力，它倡导对完全连接的潜在图进行特征传播。

    Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
    
[^14]: 具有不可靠源数据的鲁棒性迁移学习

    Robust Transfer Learning with Unreliable Source Data. (arXiv:2310.04606v1 [stat.ML])

    [http://arxiv.org/abs/2310.04606](http://arxiv.org/abs/2310.04606)

    本文提出了一个新的鲁棒性迁移学习方法TAB模型，通过衡量目标与源回归函数之间的模糊度水平来改善分类任务，并避免负迁移。通过实验验证，TAB模型在非参数分类和逻辑回归任务上表现出了优越的性能。

    

    本文针对鲁棒性迁移学习中的贝叶斯分类器的模糊性和目标与源分布之间的弱可转移信号所带来的挑战进行了研究。我们引入了一种新的量，称为“模糊度水平”，用于衡量目标与源回归函数之间的差异，并提出了一种简单的迁移学习方法，并建立了一个一般定理，说明了这个新量与学习的迁移性在风险改善方面的关系。我们提出的“边界周围转移”(Transfer Around Boundary, TAB)模型通过在目标数据和源数据性能之间进行平衡的阈值，既高效又鲁棒，能够改善分类并避免负迁移。此外，我们还展示了TAB模型在非参数分类和逻辑回归任务上的有效性，达到了最优的上界，只有对数因子的差距。通过仿真研究进一步支持了TAB的有效性。

    This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We 
    
[^15]: 使用量子统计查询学习量子过程

    Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)

    [http://arxiv.org/abs/2310.02075](http://arxiv.org/abs/2310.02075)

    本文提出了第一个在量子统计查询模型内学习量子过程的框架，并提供了一个高效的学习器和可证明的性能保证。通过在密码分析中的应用，揭示了经典读出量子物理不可克隆函数的脆弱性，这是量子硬件安全领域一个重要的开放问题的解决方法。

    

    学习复杂的量子过程是量子计算和量子机器学习的许多领域的一个核心挑战，应用于量子基准测试、密码分析和变分量子算法。本文引入了第一个学习框架，用于在量子统计查询（QSQ）模型内研究量子过程学习，提供了对量子过程（QPSQs）进行统计查询的第一个正式定义。该框架使我们能够提出一种高效的QPSQ学习器，适用于任意量子过程，并附带可证明的性能保证。我们还提供了数值模拟来展示该算法的有效性。通过在密码分析中应用该框架，突出了经典读出量子物理不可克隆函数（CR-QPUFs）的脆弱性，解决了量子硬件安全领域中的一个重要开放问题。这项工作是朝着深入理解量子过程学习迈出的重要一步。

    Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
    
[^16]: 损失平坦性与神经网络中压缩表示的简单联系

    A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])

    [http://arxiv.org/abs/2310.01770](http://arxiv.org/abs/2310.01770)

    该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。

    

    对深度神经网络的泛化能力进行研究的方法有很多种，包括至少两种不同的方法：一种基于参数空间中损失景观的形状，另一种基于特征空间中表示流形的结构（即单位活动的空间）。这两种方法相关但很少同时进行研究和明确关联。在这里，我们提出了一种简单的分析方法来建立这种联系。我们展示了在深度神经网络学习的最后阶段，神经表示流形的体积压缩与正在进行的参数优化所探索的最小值周围的损失平坦性相关。我们证明了这可以由一个相对简单的数学关系来预测：损失平坦性意味着神经表示的压缩。我们的结果与\citet{ma_linear_2021}的先前研究密切相关，该研究展示了平坦性（即小特征值）与表示流形的体积压缩之间的关系。

    Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
    
[^17]: MONOVAB: 用于孟加拉语多标签情感检测的注释语料库

    MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])

    [http://arxiv.org/abs/2309.15670](http://arxiv.org/abs/2309.15670)

    这个研究构建了一个基于孟加拉语的注释语料库，用于多标签情感检测。通过使用基于上下文的方法以及BERT模型，填补了这一学科领域的空白。

    

    近年来，情感分析(SA)和情感识别(ER)在孟加拉语中越来越流行，孟加拉语是世界上第七大使用人数最多的语言。然而，孟加拉语的结构复杂，这使得准确提取情绪变得困难。在这个研究领域中，已经采用了一些不同的方法，例如提取积极和消极情感以及多类情绪。然而，在这种语言中提取多种情绪几乎是未开发的领域，它涉及基于一段文本识别出多种情感。因此，本研究展示了一种基于从Facebook上抓取的数据构建注释语料库的详细方法，以填补这个学科领域的空白，克服挑战。为了使这种注释更有成果，采用了基于上下文的方法。转换器中的双向编码器表示(BERT)。

    In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
    
[^18]: 当基础模型遇到联邦学习：动机、挑战和未来方向

    When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])

    [http://arxiv.org/abs/2306.15546](http://arxiv.org/abs/2306.15546)

    基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。

    

    基础模型（FM）与联邦学习（FL）的交叉提供了相互的好处，在AI研究中提供了解锁新可能性的独特机会，解决了AI和现实世界应用中的关键挑战。FL扩展了FM的数据可用性，并实现了计算共享，分散了训练过程，并减轻了FL参与者的负担。它促进了协作式FM发展，民主化了这一过程，促进了包容性和创新。另一方面，FM以其庞大的规模、预训练的知识和出色的性能，为FL提供了一个强大的起点，促进了在非独立同分布数据下更快的收敛和更好的性能。此外，利用FM生成合成数据可以丰富数据多样性，减少过拟合，保护隐私。通过研究FL和FM之间的相互作用，本文旨在加深对它们协同关系的理解，强调动机和挑战。

    The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
    
[^19]: 受限对抗学习及其在自动化软件测试中的应用：系统综述（arXiv:2303.07546v1 [cs.SE]）

    Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (arXiv:2303.07546v1 [cs.SE])

    [http://arxiv.org/abs/2303.07546](http://arxiv.org/abs/2303.07546)

    本综述研究了受限对抗学习方法和自动化软件测试中受限数据生成方法的最新技术应用，探讨将这些方法整合至测试工具中以提高数字系统的鲁棒性和弹性。

    

    每种新技术都会增加隐含的漏洞，让越来越多的网络攻击者利用。自动化软件测试可以成为快速分析数千行代码的有前途的解决方案，通过生成和略微修改功能特定的测试数据来遇到多个漏洞和攻击向量。这个过程与受限对抗学习方法生成的受限性对抗性示例相似，因此将这些方法整合到自动化测试工具中可能会有显着的好处。因此，本系统综述侧重于限制数据生成方法在对抗学习和软件测试中的应用的当前最新技术，旨在指导研究人员和开发人员使用对抗学习方法增强测试工具，提高数字系统的弹性和鲁棒性。对于对抗机器学习的发现受限制的数据生成应用是系统化的。

    Every novel technology adds hidden vulnerabilities ready to be exploited by a growing number of cyber-attacks. Automated software testing can be a promising solution to quickly analyze thousands of lines of code by generating and slightly modifying function-specific testing data to encounter a multitude of vulnerabilities and attack vectors. This process draws similarities to the constrained adversarial examples generated by adversarial learning methods, so there could be significant benefits to the integration of these methods in automated testing tools. Therefore, this systematic review is focused on the current state-of-the-art of constrained data generation methods applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance testing tools with adversarial learning methods and improve the resilience and robustness of their digital systems. The found constrained data generation applications for adversarial machine learning were systemat
    
[^20]: 一种适用于所有时间序列的Transformer：表示和训练具有时间相关的异构表格数据

    One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06375](http://arxiv.org/abs/2302.06375)

    本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。

    

    近年来，将深度学习技术应用于表格数据的兴趣日益增长，以复制其他人工智能领域在这一结构化领域的成功。特别有趣的是，表格数据具有时间依赖性，例如金融交易。然而，表格值的异质性，其中类别元素与数值项混合，使得这种适应变得困难。在本文中，我们提出了一种Transformer架构来表示异构的时间相关的表格数据，数值特征使用一组频率函数表示，并且整个网络使用唯一的损失函数进行统一训练。

    There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
    

