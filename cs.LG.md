# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning Progress Driven Multi-Agent Curriculum](https://arxiv.org/abs/2205.10016) | 提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。 |
| [^2] | [Learning-Augmented B-Trees.](http://arxiv.org/abs/2211.09251) | 这是一个学习增强的B树，通过使用具有复合优先级的Treaps，每个项目的深度由其预测权重确定，推广了最近的学习增强BST，并且是第一个可以利用访问序列中的局部性的B树数据结构。 |

# 详细

[^1]: 学习进度驱动的多智能体课程

    Learning Progress Driven Multi-Agent Curriculum

    [https://arxiv.org/abs/2205.10016](https://arxiv.org/abs/2205.10016)

    提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。

    

    课程强化学习（CRL）旨在通过逐渐增加任务的难度（通常由可实现的预期回报量化）来加快学习速度。受CRL在单智能体环境中的成功启发，一些研究尝试将CRL应用于多智能体强化学习（MARL），使用智能体数量来控制任务难度。然而，现有的工作通常使用手动定义的课程，如线性方案。本文首先将最先进的单智能体自主式CRL应用于稀疏奖励MARL。虽然表现令人满意，但我们确定了现有基于奖励的CRL方法生成的课程存在两个潜在缺陷：（1）高回报的任务可能不提供信息量大的学习信号，（2）在多智能体产生更高回报的任务中，加剧了学分分配困难。因此，我们进一步提出了自主式MARL（SPMARL），以基于任务的优先级进行安排。

    arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
    
[^2]: 学习增强的B树

    Learning-Augmented B-Trees. (arXiv:2211.09251v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2211.09251](http://arxiv.org/abs/2211.09251)

    这是一个学习增强的B树，通过使用具有复合优先级的Treaps，每个项目的深度由其预测权重确定，推广了最近的学习增强BST，并且是第一个可以利用访问序列中的局部性的B树数据结构。

    

    本研究通过使用具有复合优先级的Treaps来研究学习增强的二叉搜索树（BST）和B树。结果是一个简单的搜索树，其中每个项目的深度由其预测权重$w_x$确定。为了实现这个结果，每个项目$x$都有其复合优先级$-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$，其中$U(0, 1)$是均匀分布的随机变量。这将最近的学习增强BST（Lin-Luo-Woodruff ICML`22）推广到任意输入和预测，而不仅仅适用于Zipfian分布。它还提供了第一个可以根据访问序列中的局部性进行在线自我重组的B树数据结构。该数据结构对于预测错误是健壮的，可以处理插入、删除以及预测更新。

    We study learning-augmented binary search trees (BSTs) and B-Trees via Treaps with composite priorities. The result is a simple search tree where the depth of each item is determined by its predicted weight $w_x$. To achieve the result, each item $x$ has its composite priority $-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. This generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, to arbitrary inputs and predictions. It also gives the first B-Tree data structure that can provably take advantage of localities in the access sequence via online self-reorganization. The data structure is robust to prediction errors and handles insertions, deletions, as well as prediction updates.
    

