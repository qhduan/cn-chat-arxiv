# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks](https://arxiv.org/abs/2404.02058) | fastprop是一种DeepQSPR框架，通过使用分子级描述符，在极大缩短时间内，在多样数据集上达到并超越了学习表示的性能。 |
| [^2] | [FlexCap: Generating Rich, Localized, and Flexible Captions in Images](https://arxiv.org/abs/2403.12026) | FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。 |
| [^3] | [Algorithmic syntactic causal identification](https://arxiv.org/abs/2403.09580) | 通过替换传统概率论为对称单调范畴的替代基础，可以扩展因果识别技术到更多因果设置中。 |
| [^4] | [What is different between these datasets?](https://arxiv.org/abs/2403.05652) | 这里是中文总结出的一句话要点 |
| [^5] | [Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis](https://arxiv.org/abs/2402.14184) | 基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。 |
| [^6] | [API Pack: A Massive Multilingual Dataset for API Call Generation](https://arxiv.org/abs/2402.09615) | 这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成 |
| [^7] | [On the Universality of Coupling-based Normalizing Flows](https://arxiv.org/abs/2402.06578) | 我们提出了一个新颖的理论框架，用于理解基于耦合的标准化流的表达能力，并提出了一个新的分布普适性定理来克服以前工作的限制。这些结果支持耦合架构的表达能力，并弥补了实证结果和理论理解之间的差距。 |
| [^8] | [A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions](https://arxiv.org/abs/2402.01968) | 这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。 |
| [^9] | [SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes](https://arxiv.org/abs/2402.01855) | SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。 |
| [^10] | [Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach](https://arxiv.org/abs/2308.10699) | 本论文提供了一种基于组合多臂赌博机的成本有效的在线决策框架，并利用后验抽样或BayesUCB进行探索。实验结果证明了该框架在实际问题中的适用性。 |
| [^11] | [Learning Concepts Definable in First-Order Logic with Counting](https://arxiv.org/abs/1909.03820) | 该研究将一阶逻辑与计数符号相结合，证明了可以在多对数度结构下以次线性时间一致学习可定义的分类器，为包含数值方面的机器学习扩展学习框架迈出了第一步。 |
| [^12] | [Provably Stable Feature Rankings with SHAP and LIME.](http://arxiv.org/abs/2401.15800) | 这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。 |
| [^13] | [Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification.](http://arxiv.org/abs/2401.09953) | 通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。 |
| [^14] | [Continuously Learning New Words in Automatic Speech Recognition.](http://arxiv.org/abs/2401.04482) | 该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。 |
| [^15] | [Imputation using training labels and classification via label imputation.](http://arxiv.org/abs/2311.16877) | 本论文提出一种在填充缺失数据时将标签与输入堆叠的方法，能够显著提高填充效果，并同时填充标签和输入。该方法适用于各种类型的数据，且在实验证明具有有希望的准确性结果。 |
| [^16] | [Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations.](http://arxiv.org/abs/2310.18897) | 本研究提出了一种方法，通过在不连续Galerkin方法中加入神经常微分方程，学习子网格尺度模型的效果，从而提高模拟的准确性和加速计算过程。 |
| [^17] | [Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems.](http://arxiv.org/abs/2310.16123) | 提出了一种锚定空间优化传输（ASOT）问题，通过将分布映射到共享的锚点空间，学习其潜在的共同特征，从而加速了多个传输问题的批处理。 |
| [^18] | [Out-of-distribution detection using normalizing flows on the data manifold.](http://arxiv.org/abs/2308.13792) | 利用正则化流在低维数据流形上进行外域检测，通过估计密度和测量与流形的距离来判断外域数据，有效提高外域检测的准确性。 |
| [^19] | [Computing the gradients with respect to all parameters of a quantum neural network using a single circuit.](http://arxiv.org/abs/2307.08167) | 该论文提出了一种使用单个电路计算量子神经网络所有参数梯度的方法，相比传统方法，它具有较低的电路深度和较少的编译时间，从而加速了总体运行时间。 |

# 详细

[^1]: 具有快速prop的可推广、快速和准确的DeepQSPR Part 1: 框架和基准测试

    Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks

    [https://arxiv.org/abs/2404.02058](https://arxiv.org/abs/2404.02058)

    fastprop是一种DeepQSPR框架，通过使用分子级描述符，在极大缩短时间内，在多样数据集上达到并超越了学习表示的性能。

    

    量化结构-性质关系研究旨在定义分子结构与任意感兴趣的数量之间的映射。历史上，这是通过开发描述符来实现的，这需要显著的领域专业知识，并且难以泛化。因此，该领域已经演变为分子属性预测，并转为使用高度可推广的学习表示。该论文介绍了fastprop，一种DeepQSPR框架，使用一组明智的分子级描述符，在极大缩短的时间内满足并超越了多样数据集上学习表示的性能。fastprop可以在github上免费获取，网址为github.com/JacksonBurns/fastprop。

    arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
    
[^2]: FlexCap：在图像中生成丰富、本地化和灵活的标题

    FlexCap: Generating Rich, Localized, and Flexible Captions in Images

    [https://arxiv.org/abs/2403.12026](https://arxiv.org/abs/2403.12026)

    FlexCap模型能够生成图像中具有不同长度的区域描述，在密集字幕任务和视觉问答系统中表现出优越性能。

    

    我们介绍了一种多功能的$\textit{灵活字幕}$视觉-语言模型（VLM），能够生成长度不同的特定区域描述。该模型FlexCap经过训练，可为输入的边界框生成长度条件的字幕，从而可以控制其输出的信息密度，描述范围从简洁的对象标签到详细的字幕。为了实现这一点，我们从带字幕的图像开始创建了大规模的图像区域描述训练数据集。这种灵活的字幕功能有几个宝贵的应用。首先，FlexCap在Visual Genome数据集上的密集字幕任务中表现出优越性能。其次，可以通过采用FlexCap生成本地化描述作为大型语言模型的输入来构建视觉问答（VQA）系统。由此产生的系统在许多VQ上实现了最新技术的零样本性能。

    arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
    
[^3]: 算法句法因果识别

    Algorithmic syntactic causal identification

    [https://arxiv.org/abs/2403.09580](https://arxiv.org/abs/2403.09580)

    通过替换传统概率论为对称单调范畴的替代基础，可以扩展因果识别技术到更多因果设置中。

    

    在因果贝叶斯网络（CBN）中进行因果识别是因果推断中的一项重要工具，允许从理论上可能的情况下的观测分布推导干预分布。然而，大多数现有的因果识别形式，如使用d分离和do-演算的技术都是在CBN上利用经典概率论的数学语言表达的。然而，在许多因果设置中，概率论和因此目前的因果识别技术不适用，如关系数据库、数据流程序（例如硬件描述语言）、分布式系统和大多数现代机器学习算法。我们表明，可以通过用对称单调范畴的替代公理基础来消除这种限制。在这种替代公理化中，我们展示了如何获得一个明确且清晰的

    arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
    
[^4]: 这里是翻译过的论文标题

    What is different between these datasets?

    [https://arxiv.org/abs/2403.05652](https://arxiv.org/abs/2403.05652)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
    
[^5]: 基于拓扑数据分析的语言模型多样性集成

    Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis

    [https://arxiv.org/abs/2402.14184](https://arxiv.org/abs/2402.14184)

    基于拓扑数据分析的方法，通过估计NLP模型集成的权重，提高了集成模型的质量，提高了文本分类准确性和相关不确定性估计。

    

    集成是提高机器学习模型性能的重要工具。在与自然语言处理相关的情况下，由于开源中存在多个大型模型，集成有助于提升方法的性能。然而，现有方法主要依赖于对集成中每个模型的预测进行简单平均，对每个模型赋予相同权重，忽略了模型质量和一致性的差异。我们提出利用不仅单个模型表现知识，还使用它们之间的相似性来估计NLP模型集成的权重。通过采用基于拓扑数据分析（TDA）的距离度量，我们改进了我们的集成。文本分类准确性和相关不确定性估计的质量得到提高。

    arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
    
[^6]: API Pack：一个用于API调用生成的大规模多语言数据集

    API Pack: A Massive Multilingual Dataset for API Call Generation

    [https://arxiv.org/abs/2402.09615](https://arxiv.org/abs/2402.09615)

    这个论文介绍了一个名为API Pack的大规模多语言数据集，旨在提高大型语言模型的API调用生成能力，通过实验证明了其在生成未见过的API调用方面的高准确率，并实现了跨语言的API调用生成

    

    我们介绍了API Pack，一个包含超过一百万个指令-API调用对的多语言数据集，旨在提高大型语言模型的API调用生成能力。通过实验，我们证明了API Pack在提升模型在这一特定任务上的效果的同时，保持其在一般编码方面的整体熟练程度。仅在20,000个Python实例上对CodeLlama-13B进行微调，其生成未见过的API调用的准确率比GPT-3.5和GPT-4分别高出10%和5%。扩展到100k个例子可以提高对训练期间未见过的新API的泛化能力。此外，实现了跨语言的API调用生成，而无需大量语言特定的数据。数据集、经过微调的模型和整体代码库可在https://github.com/anonymous_url上公开获取。

    arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
    
[^7]: 关于基于耦合的标准化流的普适性

    On the Universality of Coupling-based Normalizing Flows

    [https://arxiv.org/abs/2402.06578](https://arxiv.org/abs/2402.06578)

    我们提出了一个新颖的理论框架，用于理解基于耦合的标准化流的表达能力，并提出了一个新的分布普适性定理来克服以前工作的限制。这些结果支持耦合架构的表达能力，并弥补了实证结果和理论理解之间的差距。

    

    我们提出了一个新颖的理论框架，用于理解基于耦合的标准化流（如RealNVP）的表达能力。尽管耦合流在科学应用中很普遍，但由于其受限的架构，对于耦合流的全面理解仍然困难。现有的定理在实际应用中存在限制，因为它们需要使用任意病态的神经网络。此外，我们还证明了这些结构本质上导致体积保持流，这是一个限制表达能力的基本约束。我们提出了一种新的基于分布的耦合标准化流普适性定理，克服了以前工作的几个限制。我们的结果支持耦合架构具有表达能力的普遍经验，并为选择耦合函数的表达能力提供了细致入微的观点，填补了实证结果和理论理解之间的差距。

    We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.
    
[^8]: 对上下文感知多agent系统的调查：技术、挑战和未来发展方向

    A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions

    [https://arxiv.org/abs/2402.01968](https://arxiv.org/abs/2402.01968)

    这篇论文调查了上下文感知多代理系统的技术、挑战和未来发展方向，并提供了一个综合的概述。它介绍了上下文感知系统和多代理系统的特性，以及集成这些系统的通用过程。

    

    随着新兴主题的兴起，自主代理的研究兴趣正在增加。大型语言模型的显著成就已经展示了在自主代理中达到人类智能的巨大潜力。然而，挑战在于使这些代理能够在动态环境中学习、推理和导航不确定性。当处理动态情况时，上下文意识成为强化多agent系统的关键因素。尽管现有的研究专注于上下文感知系统和多agent系统，但缺乏全面概述如何将上下文感知系统与多agent系统集成的综合调查。为了填补这个空白，本调查提供了对最先进的上下文感知多agent系统的全面概述。首先，我们概述了促进这些系统之间集成的上下文感知系统和多 agent 系统的特性。随后，我们提出了一个通用的过程来建模上下文感知和多agent系统的集成。

    Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
    
[^9]: SPDE先验在端到端神经数据同化方案的不确定性量化中的应用

    SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes

    [https://arxiv.org/abs/2402.01855](https://arxiv.org/abs/2402.01855)

    SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。

    

    大规模地球物理数据集的时空插值通常通过最优插值(Optimal Interpolation，OI)和更复杂的基于模型或数据驱动的数据同化技术来处理。在过去的十年中，随机偏微分方程(Spatio-temporal Partial Differential Equations，SPDE)和高斯马尔科夫随机场(Gaussian Markov Random Fields，GMRF)之间的联系开辟了一条新的途径，用于处理最优插值中的大数据集和物理诱导协方差矩阵。深度学习社区的最新进展也使得可以将这个问题视为嵌入数据同化变分框架的神经网络体系结构的联合学习问题。重建任务被视为一个包含在变分内部成本中的先验学习问题和后者的基于梯度的最小化：先验模型和求解器都被表示为具有自动微分的神经网络，可以通过最小化损失函数来训练，该损失函数通常被表示为一些真实值和重建值之间的均方误差。

    The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
    
[^10]: 成本有效的在线决策：一种组合多臂赌博机方法

    Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach

    [https://arxiv.org/abs/2308.10699](https://arxiv.org/abs/2308.10699)

    本论文提供了一种基于组合多臂赌博机的成本有效的在线决策框架，并利用后验抽样或BayesUCB进行探索。实验结果证明了该框架在实际问题中的适用性。

    

    在线决策在许多实际应用中起关键作用。在许多情况下，决策是基于对传入数据点进行一系列测试来进行的。然而，执行所有测试可能是昂贵的，并且并非总是可行的。在本文中，我们提出了一种基于组合多臂赌博机的在线决策问题的新形式，并考虑了执行测试的（可能是随机的）成本。基于这种形式化，我们提供了一种新的成本有效的在线决策框架，可以利用后验抽样或BayesUCB进行探索。我们对用于成本有效在线决策的Thompson抽样进行了理论分析，并提供了各种实验结果，证明我们的框架适用于实际问题。

    Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the (possibly stochastic) cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a theoretical analysis of Thompson Sampling for cost-efficient online decision making, and present various experimental results that demonstrate the applicability of our framework to real-world problems.
    
[^11]: 用计数符号的一阶逻辑定义的概念的学习

    Learning Concepts Definable in First-Order Logic with Counting

    [https://arxiv.org/abs/1909.03820](https://arxiv.org/abs/1909.03820)

    该研究将一阶逻辑与计数符号相结合，证明了可以在多对数度结构下以次线性时间一致学习可定义的分类器，为包含数值方面的机器学习扩展学习框架迈出了第一步。

    

    我们研究了在Grohe和Tur\'an引入的逻辑框架下的关系背景结构上的布尔分类问题。众所周知(Grohe和Ritzert, LICS 2017)，在多对数度结构上的一阶逻辑可定义的分类器可以在次线性时间内学习，其中结构的度和运行时间是以结构的大小为单位来衡量的。我们将结果推广到了由Kuske和Schweikardt(LICS 2017)引入的带计数的一阶逻辑FOCN，它作为一个广泛推广各种计数逻辑的表现逻辑。具体来说，我们证明了可以在多对数度结构类上定义的FOCN中的分类器可以在次线性时间内一致地学习。这可以看作是将学习框架扩展以包含机器学习的数值方面的第一步。我们将这一结果扩展到了无视的概率

    arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
    
[^12]: 使用SHAP和LIME进行可证明稳定的特征排名

    Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])

    [http://arxiv.org/abs/2401.15800](http://arxiv.org/abs/2401.15800)

    这项研究提出了一种通过利用多重假设检验的思想，来设计可靠地排名机器学习模型中最重要特征的特征归因方法，旨在解决SHAP和LIME等常用方法由于随机采样导致的高度不稳定性问题。实验证明了该方法的有效性和计算效率。

    

    特征归因是了解机器学习模型预测的普遍工具。然而，用于评分输入变量的常用方法，如SHAP和LIME，由于随机采样而具有高度不稳定性。借鉴多重假设检验的思想，我们设计了能够以高概率正确排名最重要特征的归因方法。我们的算法RankSHAP保证$K$个最高Shapley值具有超过$1-\alpha$的正确排序概率。实证结果证明了其有效性和令人印象深刻的计算效率。我们还在之前的工作基础上为LIME提供了类似的结果，确保以正确顺序选择最重要的特征。

    Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
    
[^13]: 通过双棱镜: 光谱视角下的图数据增强用于图分类

    Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])

    [http://arxiv.org/abs/2401.09953](http://arxiv.org/abs/2401.09953)

    通过光谱视角，研究了图数据增强方法中的图属性和结构变化的关系，发现保持低频特征值不变可以保留关键属性，提出了双棱镜（DP）增强方法，该方法灵活地保留关键的图属性同时增加图的多样性。

    

    图神经网络（GNNs）已成为处理图数据的首选工具，其通过图数据增强技术的提高加强了其有效性。尽管增强方法的发展，但图属性扭曲和受限结构变化等问题仍然存在。这引发了一个问题：是否可能开发更加保留属性并具有结构敏感性的增强方法？通过光谱镜头，我们研究了图属性、它们的增强和它们的光谱行为之间的相互作用，并发现保持低频特征值不变可以保持生成的增强图的关键属性。这些观察结果启发我们引入了双棱镜（DP）增强方法，包括DP-Noise和DP-Mask，它们灵活地保留了关键的图属性并丰富了增强图。大量实验证实了我们方法的有效性，为一种新的、有前景的直接方法提供了支持。

    Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
    
[^14]: 在自动语音识别中持续学习新词

    Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])

    [http://arxiv.org/abs/2401.04482](http://arxiv.org/abs/2401.04482)

    该论文提出了一种自我监督的持续学习方法，用于解决自动语音识别中识别新词的问题。通过对讲座录音进行推理和收集包含新词的话语，然后在自适应数据集上进行持续学习，可以在新词出现频率较高时提高性能，同时保持整体性能。

    

    尽管最近取得了进展，但自动语音识别（ASR）系统仍然远未完美。典型的错误包括缩写词、命名实体和领域特定的专用词，这些词几乎没有或没有数据可用来训练。为了解决识别这些词的问题，我们提出了一种自我监督的持续学习方法。给定带有对应幻灯片的讲座录音，我们通过使用先前工作中的记忆增强型ASR模型来将模型偏向于从幻灯片中解码新词。然后，我们对讲座进行推理，将包含检测到的新词的话语收集到自适应数据集中。接着，对这个集合进行持续学习，通过调整添加到模型的每个权重矩阵的低秩矩阵权重。整个过程对多个讲座进行迭代。我们展示了通过这种方法，我们在新词出现频率较高时获得了性能的提升（超过80%的召回率），同时保持了模型的整体性能。

    Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
    
[^15]: 使用训练标签进行填充和通过标签填充进行分类

    Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16877](http://arxiv.org/abs/2311.16877)

    本论文提出一种在填充缺失数据时将标签与输入堆叠的方法，能够显著提高填充效果，并同时填充标签和输入。该方法适用于各种类型的数据，且在实验证明具有有希望的准确性结果。

    

    在实际应用中，缺失数据是一个常见的问题。已经开发了各种填充方法来处理缺失数据。然而，尽管训练数据通常都有标签，但常见的填充方法通常只依赖于输入而忽略标签。在这项工作中，我们阐述了将标签堆叠到输入中可以显着提高输入的填充效果。此外，我们提出了一种分类策略，该策略将预测的测试标签初始化为缺失值，并将标签与输入堆叠在一起进行填充。这样可以同时填充标签和输入。而且，该技术能够处理具有缺失标签的训练数据，无需任何先前的填充，并且适用于连续型、分类型或混合型数据。实验证明在准确性方面取得了有希望的结果。

    Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
    
[^16]: 使用神经常微分方程增强低阶不连续Galerkin方法在可压Navier-Stokes方程中的应用

    Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations. (arXiv:2310.18897v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2310.18897](http://arxiv.org/abs/2310.18897)

    本研究提出了一种方法，通过在不连续Galerkin方法中加入神经常微分方程，学习子网格尺度模型的效果，从而提高模拟的准确性和加速计算过程。

    

    随着计算能力的增长，模拟变得更加复杂和准确。然而，高保真度的模拟需要巨大的计算资源。为了降低计算成本，通常会运行一个低保真度模型并采用子网格尺度模型，但选择适当的子网格尺度模型并对其进行调节是具有挑战性的。我们在不连续Galerkin（DG）空间离散化的背景下提出了一种新颖的方法，通过在偏微分方程模拟中引入神经常微分算子来学习子网格尺度模型的效果。我们的方法在连续级别上学习低阶DG求解器中缺失的尺度，从而提高低阶DG近似的准确性，同时以一定程度的精度加速滤波高阶DG模拟。我们通过实验证明了我们方法的性能。

    The growing computing power over the years has enabled simulations to become more complex and accurate. While immensely valuable for scientific discovery and problem-solving, however, high-fidelity simulations come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations augmented by neural ordinary differential operators in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach throug
    
[^17]: 锚定空间优化传输：加速多个传输问题的批处理

    Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems. (arXiv:2310.16123v1 [cs.LG])

    [http://arxiv.org/abs/2310.16123](http://arxiv.org/abs/2310.16123)

    提出了一种锚定空间优化传输（ASOT）问题，通过将分布映射到共享的锚点空间，学习其潜在的共同特征，从而加速了多个传输问题的批处理。

    

    最优传输（OT）理论提供了一种有效的比较定义在指定度量空间上的概率分布的方法，但它的计算复杂度为立方级。尽管Sinkhorn算法极大地降低了OT解的计算复杂度，但在实践中，多个OT问题的解仍然耗时和占用内存。然而，关于OT计算加速的许多工作通常基于单个OT问题的前提，忽视了一个小批量中分布的潜在共同特征。因此，我们提出了一种翻译的OT问题，称为锚定空间优化传输（ASOT）问题，专门用于批处理多个OT问题的解。对于提出的ASOT问题，分布将被映射到一个共享的锚点空间，该空间学习潜在的共同特征，从而帮助加速OT批处理。基于所提出的ASOT，Wasserstei

    The optimal transport (OT) theory provides an effective way to compare probability distributions on a defined metric space, but it suffers from cubic computational complexity. Although the Sinkhorn's algorithm greatly reduces the computational complexity of OT solutions, the solutions of multiple OT problems are still time-consuming and memory-comsuming in practice. However, many works on the computational acceleration of OT are usually based on the premise of a single OT problem, ignoring the potential common characteristics of the distributions in a mini-batch. Therefore, we propose a translated OT problem designated as the anchor space optimal transport (ASOT) problem, which is specially designed for batch processing of multiple OT problem solutions. For the proposed ASOT problem, the distributions will be mapped into a shared anchor point space, which learns the potential common characteristics and thus help accelerate OT batch processing. Based on the proposed ASOT, the Wasserstei
    
[^18]: 利用数据流形上的正则化流进行外域检测

    Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])

    [http://arxiv.org/abs/2308.13792](http://arxiv.org/abs/2308.13792)

    利用正则化流在低维数据流形上进行外域检测，通过估计密度和测量与流形的距离来判断外域数据，有效提高外域检测的准确性。

    

    外域检测的一种常见方法是估计基础数据分布，为外域数据分配较低的可能性值。正则化流是基于可能性的生成模型，通过保持维度的可逆变换提供可计算的密度估计。传统的正则化流在外域检测中容易失败，因为基于可能性的模型面临着维度诅咒的问题。根据流形假设，现实世界的数据通常位于低维流形上。本研究调查了使用正则化流进行外域检测时的流形学习效果。我们通过在低维流形上估计密度，并结合测量与流形的距离作为外域检测的标准。然而，单独使用它们对于这个任务是不足够的。广泛的实验证明了流形学习对外域检测的有效性。

    A common approach for out-of-distribution detection involves estimating an underlying data distribution, which assigns a lower likelihood value to out-of-distribution data. Normalizing flows are likelihood-based generative models providing a tractable density estimation via dimension-preserving invertible transformations. Conventional normalizing flows are prone to fail in out-of-distribution detection, because of the well-known curse of dimensionality problem of the likelihood-based models. According to the manifold hypothesis, real-world data often lie on a low-dimensional manifold. This study investigates the effect of manifold learning using normalizing flows on out-of-distribution detection. We proceed by estimating the density on a low-dimensional manifold, coupled with measuring the distance from the manifold, as criteria for out-of-distribution detection. However, individually, each of them is insufficient for this task. The extensive experimental results show that manifold lea
    
[^19]: 使用单个电路计算量子神经网络所有参数的梯度

    Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2307.08167](http://arxiv.org/abs/2307.08167)

    该论文提出了一种使用单个电路计算量子神经网络所有参数梯度的方法，相比传统方法，它具有较低的电路深度和较少的编译时间，从而加速了总体运行时间。

    

    在使用参数平移规则计算量子神经网络的梯度时，需要对网络的单个可调参数计算两次代价函数。当参数总数较高时，需要调整和运行多次用于计算的量子电路。在这里，我们提出了一种仅使用一个电路计算所有梯度的方法，它具有较低的电路深度和较少的经典寄存器。我们还在真实量子硬件和模拟器上进行了实验证明，我们的方法具有电路编译时间明显缩短的优势，从而加速了总体运行时间。

    When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
    

