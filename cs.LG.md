# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics](https://arxiv.org/abs/2404.02175) | 使用统计物理学和市场营销动态的理论框架，本研究提出了一个创新的方程，准确捕捉了广告支出与消费者反应之间的复杂关系，并验证了其有效性。 |
| [^2] | [CMP: Cooperative Motion Prediction with Multi-Agent Communication](https://arxiv.org/abs/2403.17916) | 该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。 |
| [^3] | [Physics-Informed Diffusion Models](https://arxiv.org/abs/2403.14404) | 提出了一个信息化去噪扩散模型框架，可在模型训练期间对生成样本施加约束，以改善样本与约束的对齐程度并提供自然的正则化，适用性广泛。 |
| [^4] | [Adaptive Split Learning over Energy-Constrained Wireless Edge Networks](https://arxiv.org/abs/2403.05158) | 设计了一种在无线边缘网络中为设备动态选择分裂点并为服务器分配计算资源的自适应分裂学习方案，以最小化平均训练延迟为目标，并提出了一种名为OPEN的在线算法解决此问题。 |
| [^5] | [Preventing Reward Hacking with Occupancy Measure Regularization](https://arxiv.org/abs/2403.03185) | 用占用度测量正则化方法可以有效防止奖励欺骗，通过考虑代理与真实奖励之间大的状态占用度偏差来避免潜在的灾难后果。 |
| [^6] | [Real-Time Recurrent Reinforcement Learning](https://arxiv.org/abs/2311.04830) | 本文提出了实时递归强化学习（RTRRL）方法，通过结合元-强化学习RNN架构、外部强化学习算法和RFLO局部在线学习，成功解决部分可观察马尔可夫决策过程中的离散和连续控制任务。实验结果表明，在计算复杂性相当的情况下，使用BPTT或RTRL替代RTRRL中的优化算法并不能提高回报。 |
| [^7] | [Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction.](http://arxiv.org/abs/2401.16796) | 本研究提出了一种新的训练协议，可学习提示作为伪插补（PAI），通过构建可学习的提示来模拟下游模型对缺失值的隐含偏好，显著提升所有电子健康记录（EHR）分析模型的性能。 |
| [^8] | [MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving.](http://arxiv.org/abs/2401.14361) | MoE-Infinity是一种成本高效的MoE服务系统，通过激活感知的专家卸载和缓存技术，显著降低了延迟，并提高了成本性能。 |
| [^9] | [MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks.](http://arxiv.org/abs/2312.15960) | MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。 |
| [^10] | [Federated Unlearning via Active Forgetting.](http://arxiv.org/abs/2307.03363) | 本文提出了一种基于增量学习的新型联邦遗忘框架，解决了现有联邦遗忘方法在时间效率、数据影响估计不精确和计算负荷大等方面的问题。 |
| [^11] | [Uncertainty-Aware Robust Learning on Noisy Graphs.](http://arxiv.org/abs/2306.08210) | 本文提出了一种基于分布式鲁棒优化的不确定性感知图学习框架，利用图神经网络嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入，从而缓解现实世界图中噪声测量挑战对图数据的负面影响。 |
| [^12] | [Networked Communication for Decentralised Agents in Mean-Field Games.](http://arxiv.org/abs/2306.02766) | 本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。 |
| [^13] | [Feasible Policy Iteration.](http://arxiv.org/abs/2304.08845) | 可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF). |

# 详细

[^1]: 消费者反应的社会动态：融合统计物理学与营销动态的统一框架

    Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics

    [https://arxiv.org/abs/2404.02175](https://arxiv.org/abs/2404.02175)

    使用统计物理学和市场营销动态的理论框架，本研究提出了一个创新的方程，准确捕捉了广告支出与消费者反应之间的复杂关系，并验证了其有效性。

    

    理解消费者对广告输入的反应对于旨在优化广告策略并提高广告活动有效性的营销人员至关重要。本研究通过应用源自物理学和社会心理学的理论框架，研究消费者行为的复杂性。我们提出了一个创新的方程，捕捉了广告支出与消费者反应之间的关系，利用了诸如对称性、标度律和相变等概念。通过将我们的方程验证与Michaelis-Menten和Hill方程等著名模型相比较，我们证明了其在准确表示消费者反应动态复杂性方面的有效性。分析强调了关键模型参数（如营销效果、反应敏感度和行为敏感度）对影响消费者行为的重要性。该研究探讨了广告商和营销人员的实际影响。

    arXiv:2404.02175v1 Announce Type: cross  Abstract: Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers,
    
[^2]: CMP：具有多智能体通信的合作运动预测

    CMP: Cooperative Motion Prediction with Multi-Agent Communication

    [https://arxiv.org/abs/2403.17916](https://arxiv.org/abs/2403.17916)

    该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。

    

    随着自动驾驶车辆（AVs）的发展和车联网（V2X）通信的成熟，合作连接的自动化车辆（CAVs）的功能变得可能。本文基于合作感知，探讨了合作运动预测的可行性和有效性。我们的方法CMP以LiDAR信号作为输入，以增强跟踪和预测能力。与过去专注于合作感知或运动预测的工作不同，我们的框架是我们所知的第一个解决CAVs在感知和预测模块中共享信息的统一问题。我们的设计中还融入了能够容忍现实V2X带宽限制和传输延迟的独特能力，同时处理庞大的感知表示。我们还提出了预测聚合模块，统一了预测

    arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
    
[^3]: 物理信息扩散模型

    Physics-Informed Diffusion Models

    [https://arxiv.org/abs/2403.14404](https://arxiv.org/abs/2403.14404)

    提出了一个信息化去噪扩散模型框架，可在模型训练期间对生成样本施加约束，以改善样本与约束的对齐程度并提供自然的正则化，适用性广泛。

    

    生成模型如去噪扩散模型正快速提升其逼近高度复杂数据分布的能力。它们也越来越多地被运用于科学机器学习中，预期从隐含数据分布中取样的样本将遵守特定的控制方程。我们提出了一个框架，用于在模型训练期间对生成样本的基础约束进行信息化。我们的方法改善了生成样本与施加约束的对齐程度，显著优于现有方法而不影响推理速度。此外，我们的研究结果表明，在训练过程中加入这些约束提供了自然的防止过拟合的正则化。我们的框架易于实现，适用性广泛，可用于施加等式和不等式约束以及辅助优化目标。

    arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
    
[^4]: 能量受限的无线边缘网络中的自适应分裂学习

    Adaptive Split Learning over Energy-Constrained Wireless Edge Networks

    [https://arxiv.org/abs/2403.05158](https://arxiv.org/abs/2403.05158)

    设计了一种在无线边缘网络中为设备动态选择分裂点并为服务器分配计算资源的自适应分裂学习方案，以最小化平均训练延迟为目标，并提出了一种名为OPEN的在线算法解决此问题。

    

    分裂学习（SL）是一种有希望的用于训练人工智能（AI）模型的方法，其中设备与服务器合作以分布式方式训练AI模型，基于相同的固定分裂点。然而，由于设备的异构性和信道条件的变化，这种方式在训练延迟和能量消耗方面并不是最优的。在本文中，我们设计了一种自适应分裂学习（ASL）方案，可以在无线边缘网络中为设备动态选择分裂点，并为服务器分配计算资源。我们制定了一个优化问题，旨在在满足长期能量消耗约束的情况下最小化平均训练延迟。解决这个问题的困难在于缺乏未来信息和混合整数规划（MIP）。为了解决这个问题，我们提出了一种利用Lyapunov理论的在线算法，名为OPEN，它将其分解为一个具有当前的新MIP问题。

    arXiv:2403.05158v1 Announce Type: cross  Abstract: Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the curren
    
[^5]: 用占用度测量正则化防止奖励欺骗

    Preventing Reward Hacking with Occupancy Measure Regularization

    [https://arxiv.org/abs/2403.03185](https://arxiv.org/abs/2403.03185)

    用占用度测量正则化方法可以有效防止奖励欺骗，通过考虑代理与真实奖励之间大的状态占用度偏差来避免潜在的灾难后果。

    

    当代理根据一个“代理”奖励函数（可能是手动指定或学习的）表现出色，但相对于未知的真实奖励却表现糟糕时，就会发生奖励欺骗。由于确保代理和真实奖励之间良好对齐极为困难，预防奖励欺骗的一种方法是保守地优化代理。以往的研究特别关注于通过惩罚他们的行为分布之间的KL散度来强制让学习到的策略表现类似于“安全”策略。然而，行为分布的正则化并不总是有效，因为在单个状态下行为分布的微小变化可能导致潜在的灾难性后果，而较大的变化可能并不代表任何危险活动。我们的见解是，当奖励欺骗时，代理访问的状态与安全策略达到的状态截然不同，导致状态占用度的巨大偏差。

    arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
    
[^6]: 实时递归强化学习

    Real-Time Recurrent Reinforcement Learning

    [https://arxiv.org/abs/2311.04830](https://arxiv.org/abs/2311.04830)

    本文提出了实时递归强化学习（RTRRL）方法，通过结合元-强化学习RNN架构、外部强化学习算法和RFLO局部在线学习，成功解决部分可观察马尔可夫决策过程中的离散和连续控制任务。实验结果表明，在计算复杂性相当的情况下，使用BPTT或RTRL替代RTRRL中的优化算法并不能提高回报。

    

    在本文中，我们提出了实时递归强化学习（RTRRL），这是一种对部分可观察马尔可夫决策过程（POMDPs）中的离散和连续控制任务进行求解的生物学合理方法。RTRRL由三部分组成：（1）一个元-强化学习循环神经网络（RNN）架构，独立实现了一个演员-评论家算法；（2）一个外部强化学习算法，利用时序差分学习和荷兰资格追踪来训练元-强化学习网络；和（3）随机反馈局部在线（RFLO）学习，一种用于计算网络参数梯度的在线自动微分算法。我们的实验结果表明，通过将RTRRL中的优化算法替换为生物不合理的时延反向传播（BPTT）或实时递归学习（RTRL），并不能改善回报，同时在匹配BPTT的计算复杂性的情况下，甚至会增加返回。

    arXiv:2311.04830v2 Announce Type: replace  Abstract: In this paper we propose real-time recurrent reinforcement learning (RTRRL), a biologically plausible approach to solving discrete and continuous control tasks in partially-observable markov decision processes (POMDPs). RTRRL consists of three parts: (1) a Meta-RL RNN architecture, implementing on its own an actor-critic algorithm; (2) an outer reinforcement learning algorithm, exploiting temporal difference learning and dutch eligibility traces to train the Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an online automatic differentiation algorithm for computing the gradients with respect to parameters of the network.Our experimental results show that by replacing the optimization algorithm in RTRRL with the biologically implausible back propagation through time (BPTT), or real-time recurrent learning (RTRL), one does not improve returns, while matching the computational complexity for BPTT, and even increasi
    
[^7]: 可学习提示作为伪插补方法：重新评估传统电子健康记录数据插补在下游临床预测中的必要性

    Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction. (arXiv:2401.16796v1 [cs.LG])

    [http://arxiv.org/abs/2401.16796](http://arxiv.org/abs/2401.16796)

    本研究提出了一种新的训练协议，可学习提示作为伪插补（PAI），通过构建可学习的提示来模拟下游模型对缺失值的隐含偏好，显著提升所有电子健康记录（EHR）分析模型的性能。

    

    基于电子健康记录（EHR）分析患者的健康状况是医学信息学中的一个基本研究问题。EHR中存在大量缺失值，这使得深度神经网络难以直接基于EHR模型化患者的健康状况。现有的深度学习训练协议需要使用统计信息或插补模型来重构缺失值，然而，这些协议会将非现实的数据注入到下游EHR分析模型中，极大地限制了模型的性能。本文介绍了一种新的训练协议——可学习提示作为伪插补（PAI）。PAI不再引入任何插补数据，而是构建一个可学习的提示来模拟下游模型对缺失值的隐含偏好，从而显著提高了所有EHR分析模型的性能。此外，我们的实验结果表明，在数据不足和高噪声的情况下，PAI表现出更高的鲁棒性。

    Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR. Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance. This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models. Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and hig
    
[^8]: MoE-Infinity：用于高效MoE服务的激活感知专家卸载系统

    MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving. (arXiv:2401.14361v1 [cs.LG])

    [http://arxiv.org/abs/2401.14361](http://arxiv.org/abs/2401.14361)

    MoE-Infinity是一种成本高效的MoE服务系统，通过激活感知的专家卸载和缓存技术，显著降低了延迟，并提高了成本性能。

    

    本文介绍了MoE-Infinity，一种成本高效的专家混合(MoE)服务系统，实现了激活感知的专家卸载。MoE-Infinity具有序列级专家激活追踪的特点，这是一种擅长识别稀疏激活并捕捉MoE推理的时间局部性的新方法。通过分析这些追踪，MoE-Infinity执行了新颖的激活感知专家预取和缓存，大大降低了通常与卸载专家相关的延迟开销，提高了成本性能。在一个集群中进行的大量实验表明，MoE-Infinity优于许多现有的系统和方法，对于各种MoEs，将延迟降低了420倍，将部署成本降低了8倍以上。MoE-Infinity的源代码可在https://github.com/TorchMoE/MoE-Infinity公开获取。

    This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
    
[^9]: MoTCoder: 使用思维模块提升大型语言模型在具有挑战性的编程任务中的能力。

    MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15960](http://arxiv.org/abs/2312.15960)

    MoTCoder是一个使用思维模块提升大型语言模型在挑战性编程任务中能力的框架，通过创新的指令调整促进任务的分解和模块化，显著提高生成解决方案的准确性和模块化程度。

    

    大型语言模型(LLMs)在处理简单的编程任务方面展示出了令人印象深刻的能力。然而，当面对更具挑战性的编程问题时，它们的性能往往表现不佳。我们观察到传统模型往往生成作为单一代码块的解决方案，限制了它们在解决复杂问题上的有效性。为了克服这个限制，我们提出了Modular-of-Thought Coder (MoTCoder)。我们引入了一种创新的MoT指令调整框架，旨在促进将任务分解为逻辑子任务和子模块。我们的研究发现，通过培养和利用子模块，MoTCoder显著提高了生成解决方案的模块化和正确性，导致在APPS上相对pass@1改进了12.9%，在CodeContests上相对pass@1改进了9.43%。我们的代码可在https://github.com/dvlab-research/MoTCoder获得。

    Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
    
[^10]: 通过主动遗忘实现联邦遗忘

    Federated Unlearning via Active Forgetting. (arXiv:2307.03363v1 [cs.LG])

    [http://arxiv.org/abs/2307.03363](http://arxiv.org/abs/2307.03363)

    本文提出了一种基于增量学习的新型联邦遗忘框架，解决了现有联邦遗忘方法在时间效率、数据影响估计不精确和计算负荷大等方面的问题。

    

    对机器学习模型隐私的关注日益增加，引发了对机器遗忘的探索，即一种消除训练数据对机器学习模型影响的过程。这种关注也出现在联邦学习的领域，促使研究人员解决联邦遗忘问题。然而，联邦遗忘仍然具有挑战性。现有的遗忘方法可以被广泛分为两种方法，即精确遗忘和近似遗忘。首先，在分布式情况下实施精确遗忘，通常依赖于分区-聚合框架，理论上不会提高时间效率。其次，现有的联邦（近似）遗忘方法在数据影响估计不精确、计算负荷大或两者都存在方面存在问题。为此，我们提出了一种基于增量学习的新型联邦遗忘框架，该框架不依赖于具体的模型和联邦设置。

    The increasing concerns regarding the privacy of machine learning models have catalyzed the exploration of machine unlearning, i.e., a process that removes the influence of training data on machine learning models. This concern also arises in the realm of federated learning, prompting researchers to address the federated unlearning problem. However, federated unlearning remains challenging. Existing unlearning methods can be broadly categorized into two approaches, i.e., exact unlearning and approximate unlearning. Firstly, implementing exact unlearning, which typically relies on the partition-aggregation framework, in a distributed manner does not improve time efficiency theoretically. Secondly, existing federated (approximate) unlearning methods suffer from imprecise data influence estimation, significant computational burden, or both. To this end, we propose a novel federated unlearning framework based on incremental learning, which is independent of specific models and federated se
    
[^11]: 基于不确定性的噪声图上的鲁棒学习

    Uncertainty-Aware Robust Learning on Noisy Graphs. (arXiv:2306.08210v1 [cs.LG])

    [http://arxiv.org/abs/2306.08210](http://arxiv.org/abs/2306.08210)

    本文提出了一种基于分布式鲁棒优化的不确定性感知图学习框架，利用图神经网络嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入，从而缓解现实世界图中噪声测量挑战对图数据的负面影响。

    

    图神经网络在解决各种图计算任务方面展现了出色的能力，特别是在节点分类方面表现出色。然而，在现实世界的图中，拓扑或节点信息中普遍存在的噪声测量挑战可能会影响其效果。观测中的这些不准确性可能会破坏图数据中的关键模式，最终导致实际应用中不良性能。为了解决这些问题，本文提出了一种新颖的基于分布式鲁棒优化的不确定性感知图学习框架。具体而言，我们使用基于图神经网络的编码器来嵌入节点特征，并通过极小极大形式最小化最坏情况风险来找到最优节点嵌入。这种不确定性感知的学习过程导致了改进的节点表示和更强壮的图预测模型，有效地缓解了图数据中噪声测量的负面影响。

    Graph neural networks have shown impressive capabilities in solving various graph learning tasks, particularly excelling in node classification. However, their effectiveness can be hindered by the challenges arising from the widespread existence of noisy measurements associated with the topological or nodal information present in real-world graphs. These inaccuracies in observations can corrupt the crucial patterns within the graph data, ultimately resulting in undesirable performance in practical applications. To address these issues, this paper proposes a novel uncertainty-aware graph learning framework motivated by distributionally robust optimization. Specifically, we use a graph neural network-based encoder to embed the node features and find the optimal node embeddings by minimizing the worst-case risk through a minimax formulation. Such an uncertainty-aware learning process leads to improved node representations and a more robust graph predictive model that effectively mitigates
    
[^12]: 分布式智能体在均场博弈中的网络通信

    Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2306.02766](http://arxiv.org/abs/2306.02766)

    本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。

    

    我们将网络通信引入均场博弈框架，特别是在无oracle的情况下，N个分布式智能体沿着经过的经验系统的单一非周期演化路径学习。我们证明，我们的架构在只有一些关于网络结构的合理假设的情况下，具有样本保证，在集中学习和独立学习情况之间有界。我们讨论了三个理论算法的样本保证实际上并不会导致实际收敛。因此，我们展示了在实际设置中，当理论参数未被观察到（导致Q函数的估计不准确）时，我们的通信方案显著加速了收敛速度，而无需依赖于一个不可取的集中式控制器的假设。我们对三个理论算法进行了几种实际的改进，使我们能够展示它们的第一个实证表现。

    We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
    
[^13]: 可行性策略迭代

    Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])

    [http://arxiv.org/abs/2304.08845](http://arxiv.org/abs/2304.08845)

    可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF).

    

    安全强化学习旨在在安全约束下解决最优控制问题。现有的 $\textit{直接}$ 安全强化学习方法会在整个学习过程中一直使用原始约束。它们或者缺乏策略迭代期间的理论保证，或者遭遇不可行性问题。为了解决这个问题，我们提出了一个叫做可行性策略迭代（FPI）的 $\textit{间接}$ 安全强化学习方法，它使用最后一个策略的可行域来迭代地限制当前策略。可行域由一个叫做约束衰减函数（CDF）的可行性函数表示。FPI 的核心是一个叫做可行性策略改进的区域性策略更新规则，它在可行域内最大化回报，在可行域外最小化 CDF。这个更新规则总是可行的，并确保可行域单调地扩展，状态值函数单调地增长。

    Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
    

