# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation](https://arxiv.org/abs/2404.01334) | 本研究引入了一种新颖的混合标注方法，将人力工作与大型语言模型相结合，旨在提高NER模型的性能，并以成本效益的方式实现这一目标。 |
| [^2] | [Sparse multimodal fusion with modal channel attention](https://arxiv.org/abs/2403.20280) | 研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力，并提出了模态通道注意力（MCA）机制，可以改善生成的嵌入空间质量和下游任务性能。 |
| [^3] | [Detecting Financial Bots on the Ethereum Blockchain](https://arxiv.org/abs/2403.19530) | 本研究提出了一种利用机器学习检测金融机器人在以太坊平台上的新方法，并建立了金融机器人的分类法，对机器人进行了检测并创建了地面实况数据集。 |
| [^4] | [EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union](https://arxiv.org/abs/2403.15474) | 通过EC-IoU度量，本文引入了一种定向安全性物体检测方法，可以在安全关键领域中提高物体检测器的性能，并在KITTI数据集上取得了比IoU更好的结果。 |
| [^5] | [A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients](https://arxiv.org/abs/2403.14742) | 使用神经网络分类器的倒数第二层作为异常检测的潜在空间，并提出了一种名为多类孤立森林（MCIF）的新方法来训练每个类别的孤立森林，以推导异常值 |
| [^6] | [Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples](https://arxiv.org/abs/2403.08618) | 提出了后训练校正的新范式，通过奇异值分解算法Verifix在初始训练后校正模型权重以减轻标签噪声，避免了重新训练的需求 |
| [^7] | [Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models](https://arxiv.org/abs/2403.02774) | 通过学习一致性模型，在不需要重新训练的情况下高效、准确地降尺度任意地球系统模型模拟，并产生概率性降尺度场。 |
| [^8] | [Graph Construction with Flexible Nodes for Traffic Demand Prediction](https://arxiv.org/abs/2403.00276) | 本文提出一种新型的图构建方法，利用基于密度的聚类算法确定了图中节点的灵活定位，克服了传统算法的计算瓶颈，并从乘客数据中提取有价值信息用于初始化GNNs的边权重。 |
| [^9] | [TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis](https://arxiv.org/abs/2402.16412) | TOTEM提出了一种简单的令牌化架构，通过自监督学习的离散矢量化表示嵌入不同领域的时间序列数据，能够实现通用、跨领域训练，在多个任务和领域上进行广泛评估。 |
| [^10] | [Neural Network Diffusion](https://arxiv.org/abs/2402.13144) | 扩散模型能够生成表现优异的神经网络参数，生成的模型在性能上与训练网络相媲美甚至更好，且成本极低。 |
| [^11] | [Text2Data: Low-Resource Data Generation with Textual Control](https://arxiv.org/abs/2402.10941) | Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。 |
| [^12] | [Mixture of Link Predictors](https://arxiv.org/abs/2402.08583) | 提出了一种用于链接预测的混合模型Link-MoE，通过选择合适的专家模型，利用不同类型的成对信息，能够显著提高预测性能。 |
| [^13] | [A Competition Winning Deep Reinforcement Learning Agent in microRTS](https://arxiv.org/abs/2402.08112) | 在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。 |
| [^14] | [Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach](https://arxiv.org/abs/2402.02954) | 本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。 |
| [^15] | [SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes](https://arxiv.org/abs/2402.01855) | SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。 |
| [^16] | [CPT: Competence-progressive Training Strategy for Few-shot Node Classification](https://arxiv.org/abs/2402.00450) | CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。 |
| [^17] | [Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web](https://arxiv.org/abs/2311.18751) | 本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。 |
| [^18] | [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations.](http://arxiv.org/abs/2401.14142) | 基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。 |
| [^19] | [Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification.](http://arxiv.org/abs/2309.09976) | Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。 |
| [^20] | [Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime.](http://arxiv.org/abs/2309.04522) | 本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。 |
| [^21] | [Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province.](http://arxiv.org/abs/2309.01115) | 本研究使用机器学习方法解决了多重共线性问题，针对四川省的碳排放情况进行了案例研究。研究结果确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略，以改善情况。 |
| [^22] | [The Initial Screening Order Problem.](http://arxiv.org/abs/2307.15398) | 本文研究了初始筛选顺序问题，在候选人筛选中起到关键作用。我们证明在候选人池不平衡情况下，类人筛选者可能对受保护、代表性不足的群体做出不公平的决策。这项研究的目的是与一家大公司合作，以更好地理解其潜在的自动化招聘流程。 |
| [^23] | [Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback.](http://arxiv.org/abs/2307.09295) | 嵌套消除是一种简单易实现的算法，通过利用创新的消除准则和嵌套结构，能够以最少的样本数量和高置信水平识别出最受欢迎的项目。 |
| [^24] | [Exploiting Observation Bias to Improve Matrix Completion.](http://arxiv.org/abs/2306.04775) | 本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。 |
| [^25] | [Approximation theory of transformer networks for sequence modeling.](http://arxiv.org/abs/2305.18475) | 本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。 |
| [^26] | [MADiff: Offline Multi-agent Learning with Diffusion Models.](http://arxiv.org/abs/2305.17330) | 本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。 |
| [^27] | [Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy.](http://arxiv.org/abs/2305.14528) | 本文提供了一种能够将数字特征编码为基函数向量的方法，通过在因子机中将该方法应用于因子机中，可以改善推荐系统的准确性。 |
| [^28] | [Bayesian approach to Gaussian process regression with uncertain inputs.](http://arxiv.org/abs/2305.11586) | 本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。 |
| [^29] | [Online Joint Assortment-Inventory Optimization under MNL Choices.](http://arxiv.org/abs/2304.02022) | 本文提出了一个算法解决在线联合组合库存优化问题，能够在平衡探索与开发的措施下实现最大化预期总利润，并为该算法建立了遗憾上界。 |
| [^30] | [InceptionNeXt: When Inception Meets ConvNeXt.](http://arxiv.org/abs/2303.16900) | 本论文提出了一种名为InceptionNeXt的新型神经网络，通过将大内核卷积沿通道维度分解为四个平行分支来提高模型效率，解决了保持性能的同时加快基于大内核的CNN模型的问题。 |
| [^31] | [Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions.](http://arxiv.org/abs/2210.07996) | 该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。 |
| [^32] | [Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network.](http://arxiv.org/abs/2210.07765) | 这个论文提出了一种基于层次图注意循环网络的活动感知人类移动预测方法，通过构建一个层次图和使用层次图注意模块来捕捉时间-活动-位置之间的依赖关系，以建模用户的偏好。同时引入了一种模型无关的历史增强置信标签，用于聚焦于每个用户的个体级偏好。 |
| [^33] | [Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution.](http://arxiv.org/abs/2208.04957) | 本研究首次研究了异构零样本协同问题，并提出了一种基于协同进化的通用方法，通过配对、更新和选择的过程，实现了多智能体零样本协同。实验结果表明，考虑异构情况的必要性，并证明了该方法对于异构零样本协同任务的有前景的解决方案。 |

# 详细

[^1]: 使用LLMs增强NER数据集：迈向自动化和精细化标注

    Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation

    [https://arxiv.org/abs/2404.01334](https://arxiv.org/abs/2404.01334)

    本研究引入了一种新颖的混合标注方法，将人力工作与大型语言模型相结合，旨在提高NER模型的性能，并以成本效益的方式实现这一目标。

    

    在自然语言处理（NLP）领域，命名实体识别（NER）被认为是一项关键技术，在各种应用中被广泛应用。传统的用于为NER模型标注数据集的方法面临着高成本和数据集质量变化的挑战。本研究介绍了一种新型的混合标注方法，将人力工作与大型语言模型（LLMs）的能力相结合。这种方法不仅旨在改善手动注释中固有的噪音，如遗漏，从而提高NER模型的性能，而且还以一种具有成本效益的方式实现这一目标。此外，通过采用标签混合策略，它解决了LLM-based注释中遇到的类别不平衡问题。通过对多个数据集的分析，这种方法一直表现出比传统注释方法更优异的性能，即使在co

    arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
    
[^2]: 带有模态通道注意力的稀疏多模态融合

    Sparse multimodal fusion with modal channel attention

    [https://arxiv.org/abs/2403.20280](https://arxiv.org/abs/2403.20280)

    研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力，并提出了模态通道注意力（MCA）机制，可以改善生成的嵌入空间质量和下游任务性能。

    

    通过测量生成的嵌入空间质量作为模态稀疏度函数的能力，研究了蒙特卡罗多模态变换器架构在模态样本稀疏对齐时学习稳健嵌入空间的能力。提出了一种扩展的蒙特卡罗多模态变压器模型，该模型在多头注意机制中引入了模态不完全通道，称为模态通道注意力（MCA）。使用了包含4种模态的两个数据集，CMU-MOSEI用于多模态情感识别，TCGA用于多组学。模型显示在大多数样本中只用了四种模态中的两种就学习出统一且对齐的嵌入空间。发现，即使没有模态稀疏，所提出的MCA机制也能提高生成的嵌入空间质量，召回指标，并提高下游任务的性能。

    arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
    
[^3]: 在以太坊区块链上检测金融机器人

    Detecting Financial Bots on the Ethereum Blockchain

    [https://arxiv.org/abs/2403.19530](https://arxiv.org/abs/2403.19530)

    本研究提出了一种利用机器学习检测金融机器人在以太坊平台上的新方法，并建立了金融机器人的分类法，对机器人进行了检测并创建了地面实况数据集。

    

    集成机器人在分布式账本技术（DLTs）中促进了效率和自动化。然而，它们的使用也与掠夺性交易和市场操纵相关，并可能对系统完整性构成威胁。因此，了解DLTs中机器人的部署程度至关重要；尽管如此，目前的检测系统主要基于规则，并且缺乏灵活性。在这项研究中，我们提出了一种利用机器学习检测以太坊平台上金融机器人的新方法。首先，我们系统化现有的科学文献并收集轶事证据，以建立金融机器人的分类法，包括7个类别和24个子类别。接下来，我们创建一个包含133个人类和137个机器人地址的地面实况数据集。第三，我们使用无监督和有监督的机器学习算法来检测部署在以太坊上的机器人。

    arXiv:2403.19530v1 Announce Type: cross  Abstract: The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algori
    
[^4]: EC-IoU: 通过自我中心交并联调整物体检测器的安全性

    EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union

    [https://arxiv.org/abs/2403.15474](https://arxiv.org/abs/2403.15474)

    通过EC-IoU度量，本文引入了一种定向安全性物体检测方法，可以在安全关键领域中提高物体检测器的性能，并在KITTI数据集上取得了比IoU更好的结果。

    

    本文介绍了通过一种新颖的自我中心交并联（EC-IoU）度量来定向安全性物体检测，解决了在自动驾驶等安全关键领域应用最先进的基于学习的感知模型时面临的实际问题。具体来说，我们提出了一种加权机制来优化广泛使用的IoU度量，使其能够根据自我代理人的视角覆盖更近的地面真实对象点的预测分配更高的分数。所提出的EC-IoU度量可以用于典型的评估过程，选择有更高安全性表现的物体检测器用于下游任务。它还可以集成到常见损失函数中进行模型微调。尽管面向安全性，但我们在KITTI数据集上的实验表明，使用EC-IoU训练的模型在均值平均精度方面的性能可能会优于使用IoU训练的变体。

    arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
    
[^5]: 一个基于分类器的多类异常检测方法，用于天文暂变星

    A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients

    [https://arxiv.org/abs/2403.14742](https://arxiv.org/abs/2403.14742)

    使用神经网络分类器的倒数第二层作为异常检测的潜在空间，并提出了一种名为多类孤立森林（MCIF）的新方法来训练每个类别的孤立森林，以推导异常值

    

    实时自动异常检测对于在大规模天文调查时代识别稀有的短暂现象至关重要。目前，大部分天文暂变检测算法要么依赖于从光变曲线中提取的手工特征，要么依赖于通过无监督表示学习生成的特征，然后与标准机器学习异常检测算法结合。在本研究中，我们引入了一种另类的检测异常的方法：使用神经网络分类器的倒数第二层作为异常检测的潜在空间。然后，我们提出了一种名为多类孤立森林（MCIF）的新方法，该方法为每个类别训练单独的孤立森林来推导异常值。

    arXiv:2403.14742v1 Announce Type: cross  Abstract: Automating real-time anomaly detection is essential for identifying rare transients in the era of large-scale astronomical surveys. Modern survey telescopes are generating tens of thousands of alerts per night, and future telescopes, such as the Vera C. Rubin Observatory, are projected to increase this number dramatically. Currently, most anomaly detection algorithms for astronomical transients rely either on hand-crafted features extracted from light curves or on features generated through unsupervised representation learning, which are then coupled with standard machine learning anomaly detection algorithms. In this work, we introduce an alternative approach to detecting anomalies: using the penultimate layer of a neural network classifier as the latent space for anomaly detection. We then propose a novel method, named Multi-Class Isolation Forests (MCIF), which trains separate isolation forests for each class to derive an anomaly sc
    
[^6]: Verifix: 后训练校正以改善具有经过验证样本的标签噪声鲁棒性

    Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples

    [https://arxiv.org/abs/2403.08618](https://arxiv.org/abs/2403.08618)

    提出了后训练校正的新范式，通过奇异值分解算法Verifix在初始训练后校正模型权重以减轻标签噪声，避免了重新训练的需求

    

    标签错误，即训练样本具有不正确的标签，可能严重损害机器学习模型的性能。这种错误往往来自非专家标注或敌对攻击。获取大型、完全标记的数据集成本高，当有干净的数据集可用时，重新训练大型模型就变得计算昂贵。为了解决这一挑战，我们提出了后训练校正，这是一种在初始训练后调整模型参数以减轻标签噪声的新范式，消除了重新训练的需要。我们引入了Verifix，这是一种基于奇异值分解（SVD）的新算法，利用一个小的、经过验证的数据集，通过单个更新校正模型权重。Verifix使用SVD估计干净激活空间，然后将模型的权重投影到这个空间上，以抑制对应于损坏数据的激活。我们展示了Verifix的有效性。

    arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
    
[^7]: 快速、自适应尺度和具有不确定性意识的地球系统模型场降尺度与生成基础模型

    Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models

    [https://arxiv.org/abs/2403.02774](https://arxiv.org/abs/2403.02774)

    通过学习一致性模型，在不需要重新训练的情况下高效、准确地降尺度任意地球系统模型模拟，并产生概率性降尺度场。

    

    精确和高分辨率的地球系统模型(ESM)模拟对于评估人为气候变化对生态和社会经济影响至关重要，但计算成本过高。最近的机器学习方法在ESM模拟的降尺度中表现出色，优于最先进的统计方法。然而，现有方法对每个ESM都需要计算昂贵的重新训练，并且在训练期间未见过的气候预测效果差。我们通过学习一个一致性模型(CM)，以零样本方式高效准确地降尺度任意ESM模拟来解决这些缺点。我们的基础模型方法以只受观测参考数据限制的分辨率产生概率性降尺度场。我们展示了CM在维持高可控性的同时以较低的计算成本优于最先进的扩散模型。

    arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
    
[^8]: 具有灵活节点的图构建用于交通需求预测

    Graph Construction with Flexible Nodes for Traffic Demand Prediction

    [https://arxiv.org/abs/2403.00276](https://arxiv.org/abs/2403.00276)

    本文提出一种新型的图构建方法，利用基于密度的聚类算法确定了图中节点的灵活定位，克服了传统算法的计算瓶颈，并从乘客数据中提取有价值信息用于初始化GNNs的边权重。

    

    图神经网络（GNNs）已被广泛应用于交通需求预测中，交通模式可分为基于站点的模式和自由漂浮交通模式。现有的交通图构建研究主要依赖于地图匹配，基于道路网络构建图。然而，在自由漂浮交通需求预测中，数据分布的复杂性和不均匀性使得道路网络匹配不够灵活。为了解决这些挑战，本文提出了一种针对自由漂浮交通模式量身定制的新型图构建方法。我们提出了一种新颖的基于密度的聚类算法（HDPC-L）来确定图中节点的灵活定位，克服传统聚类算法的计算瓶颈，实现对大规模数据集的有效处理。此外，我们从乘客数据中提取有价值的信息，以初始化GNNs的边权重。

    arXiv:2403.00276v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have been widely applied in traffic demand prediction, and transportation modes can be divided into station-based mode and free-floating traffic mode. Existing research in traffic graph construction primarily relies on map matching to construct graphs based on the road network. However, the complexity and inhomogeneity of data distribution in free-floating traffic demand forecasting make road network matching inflexible. To tackle these challenges, this paper introduces a novel graph construction method tailored to free-floating traffic mode. We propose a novel density-based clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in the graph, overcoming the computational bottlenecks of traditional clustering algorithms and enabling effective handling of large-scale datasets. Furthermore, we extract valuable information from ridership data to initialize the edge weights of GNNs. Comprehen
    
[^9]: TOTEM：用于一般时间序列分析的令牌化时间序列嵌入

    TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis

    [https://arxiv.org/abs/2402.16412](https://arxiv.org/abs/2402.16412)

    TOTEM提出了一种简单的令牌化架构，通过自监督学习的离散矢量化表示嵌入不同领域的时间序列数据，能够实现通用、跨领域训练，在多个任务和领域上进行广泛评估。

    

    最近，一般时间序列分析领域开始探索统一建模，其中一个通用的架构可以在特定任务和特定数据集上进行重新训练。本文从一个互补的角度接近统一化：跨任务和领域的统一化。为此，我们探讨了离散、学习到的时间序列数据表示对启用通用、跨领域训练的影响。我们的方法TOTEM，即TOkenized Time Series EMbeddings，提出了一种简单的标记器架构，通过以自监督方式学习的离散矢量化表示嵌入来自不同领域的时间序列数据。TOTEM可以跨多个任务和领域工作，几乎不需要调整。我们通过对3个任务上的17个真实世界时间序列数据集进行广泛评估来研究TOTEM的有效性。我们评估了专家（即在每个领域训练模型）和通用（即训练）的TOTEM。

    arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin
    
[^10]: 神经网络扩散

    Neural Network Diffusion

    [https://arxiv.org/abs/2402.13144](https://arxiv.org/abs/2402.13144)

    扩散模型能够生成表现优异的神经网络参数，生成的模型在性能上与训练网络相媲美甚至更好，且成本极低。

    

    扩散模型在图像和视频生成方面取得了显著成功。在这项工作中，我们展示了扩散模型也可以\textit{生成表现优异的神经网络参数}。我们的方法很简单，利用了自动编码器和标准的潜在扩散模型。自动编码器提取了部分受训网络参数的潜在表示。然后训练了一个扩散模型来从随机噪声中合成这些潜在参数表示。它生成了新的表示，经过自动编码器的解码器，输出准备用作新的网络参数子集。在各种架构和数据集上，我们的扩散过程始终生成性能与经过训练的网络相当或更好的模型，附加成本极小。值得注意的是，我们在实证研究中发现，生成的模型与经过训练的网络表现出差异。

    arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
    
[^11]: Text2Data：使用文本控制的低资源数据生成

    Text2Data: Low-Resource Data Generation with Textual Control

    [https://arxiv.org/abs/2402.10941](https://arxiv.org/abs/2402.10941)

    Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。

    

    自然语言作为人类与机器无缝交互的一种常见直接控制信号。意识到这一接口的重要性，机器学习社区正在投入大量精力生成与文本指令在语义上一致的数据。虽然在涵盖图像编辑、音频合成、视频生成等领域取得了进展，但低资源领域由于昂贵注释或复杂数据结构（如分子、运动动态和时序）等特点，往往缺乏文本标签。这种不足阻碍了监督学习，从而限制了将先进生成模型应用于文本到数据任务的可能性。为了应对低资源场景中的这些挑战，我们提出了Text2Data，这是一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法。

    arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
    
[^12]: 链接预测的混合模型

    Mixture of Link Predictors

    [https://arxiv.org/abs/2402.08583](https://arxiv.org/abs/2402.08583)

    提出了一种用于链接预测的混合模型Link-MoE，通过选择合适的专家模型，利用不同类型的成对信息，能够显著提高预测性能。

    

    链接预测是图机器学习中的一项基本任务，旨在预测图中未见连接。启发式方法利用一系列不同的成对度量，如共同邻居和最短路径，常常能够与纯粹的图神经网络（GNNs）性能相媲美。因此，近期GNNs在链接预测（GNN4LP）方面的进展主要集中在整合一种或少数几种成对信息上。在这项工作中，我们揭示了同一数据集中的不同节点对需要不同的成对信息进行准确预测，而只应用相同的成对信息的模型可能会导致次优性能。因此，我们提出了一种简单的专家模型Link-MoE用于链接预测。Link-MoE利用各种GNNs作为专家，并根据不同类型的成对信息为每个节点对选择合适的专家。在各种真实数据集上的实验结果表明，Link-MoE能够显著提高预测性能。

    Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets dem
    
[^13]: 一种在microRTS中获奖的深度强化学习代理

    A Competition Winning Deep Reinforcement Learning Agent in microRTS

    [https://arxiv.org/abs/2402.08112](https://arxiv.org/abs/2402.08112)

    在IEEE microRTS竞赛中，RAISocketAI成为第一个获胜的深度强化学习代理，它通过逐步优化基本策略和迁移学习来击败了前两位竞赛获胜者，在未来的竞赛中可以作为基准参考，并为DRL研究提供起点。

    

    在CIG和CoG举办的IEEE microRTS（$\mu$RTS）竞赛的五届中，脚本代理主导了比赛。尽管深度强化学习（DRL）算法在实时策略（RTS）游戏中取得了重大进展，但由于需要大量的培训资源以及创建和调试此类代理所固有的复杂性，它们在这个主要是学术竞赛中的采用仍然有限。RAISocketAI是第一个在IEEE microRTS竞赛中获胜的DRL代理。在一个没有性能限制的基准测试中，RAISocketAI经常击败前两位竞赛获胜者。这个第一个获胜的DRL提交可以成为未来microRTS竞赛的基准，并成为未来DRL研究的起点。逐步优化基本策略和对特定地图进行迁移学习对RAISocketAI的获胜表现至关重要。这些策略可以用于经济训练未来的DRL代理。在模仿学习方面的进一步工作可以进一步提高DRL代理的性能。

    Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
    
[^14]: 解决分层信息共享的分布式部分可观察马尔可夫决策过程：一种广义博弈方法

    Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach

    [https://arxiv.org/abs/2402.02954](https://arxiv.org/abs/2402.02954)

    本文通过应用最优性原理研究了分层信息共享的分布式部分可观察马尔可夫决策过程的解决方法。通过将问题分解成单阶段子游戏，并通过进一步分解子游戏，我们成功地解开了决策变量的纠缠，同时显著减少了时间复杂度。

    

    最近的理论表明，多人分散的部分可观察马尔可夫决策过程可以转化为等效的单人游戏，使得可以应用贝尔曼的最优性原理通过将其分解为单阶段子游戏来解决单人游戏。然而，这种方法在每个单阶段子游戏中纠缠了所有玩家的决策变量，导致指数复杂度的备份。本文展示了如何在保持分层信息共享的前提下解开这些决策变量的纠缠，这是我们社会中一种突出的管理风格。为了实现这个目标，我们应用最优性原理通过进一步将任何单阶段子游戏分解为更小的子游戏来解决它，使我们能够逐次进行单人决策。我们的方法揭示了存在于单阶段子游戏中的广义博弈解决方案，极大地减少了时间复杂度。我们的实验结果验证了我们方法的有效性，证明它可以在解决分层信息共享的分布式部分可观察马尔可夫决策过程中发挥重要作用。

    A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
    
[^15]: SPDE先验在端到端神经数据同化方案的不确定性量化中的应用

    SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes

    [https://arxiv.org/abs/2402.01855](https://arxiv.org/abs/2402.01855)

    SPDE先验在最优插值中的应用及其与神经网络的联合学习问题，为大规模地球物理数据集的时空插值提供了一种新的方法。

    

    大规模地球物理数据集的时空插值通常通过最优插值(Optimal Interpolation，OI)和更复杂的基于模型或数据驱动的数据同化技术来处理。在过去的十年中，随机偏微分方程(Spatio-temporal Partial Differential Equations，SPDE)和高斯马尔科夫随机场(Gaussian Markov Random Fields，GMRF)之间的联系开辟了一条新的途径，用于处理最优插值中的大数据集和物理诱导协方差矩阵。深度学习社区的最新进展也使得可以将这个问题视为嵌入数据同化变分框架的神经网络体系结构的联合学习问题。重建任务被视为一个包含在变分内部成本中的先验学习问题和后者的基于梯度的最小化：先验模型和求解器都被表示为具有自动微分的神经网络，可以通过最小化损失函数来训练，该损失函数通常被表示为一些真实值和重建值之间的均方误差。

    The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
    
[^16]: CPT: 应用于少样本节点分类的能 力递进式训练策略

    CPT: Competence-progressive Training Strategy for Few-shot Node Classification

    [https://arxiv.org/abs/2402.00450](https://arxiv.org/abs/2402.00450)

    CPT是一种新颖的两阶段课程学习方法，弥补了传统元学习方法在少样本节点分类上的困难。它使用能力递进的训练策略来提高元学习器的效果和稳定性。

    

    图神经网络（GNNs）在节点分类方面取得了显著的进展，但其成功仍然依赖于训练数据中每个类别有足够的标记节点。现实世界中的图数据通常呈现出长尾分布，标签稀疏，强调了GNN在少样本节点分类中的重要性，即使用有限的数据对节点进行分类。传统的情节元学习方法在这个领域显示出了潜力，但它们面临着固有的限制：随机和均匀任务分配可能导致模型收敛到次优解，忽视了任务的难度水平。这可能导致元学习器过早地面临复杂任务，阻碍了正常的学习。理想情况下，元学习器应该从简单概念开始，逐渐进入更复杂的概念，就像人类学习一样。因此，我们引入了CPT，一种新颖的两阶段课程学习方法，将任务难度与元学习器的递进能力相匹配，增强了元学习的效果和稳定性。

    Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
    
[^17]: 在Web上揭示语言模型代理在顺序任务组合中的局限性

    Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web

    [https://arxiv.org/abs/2311.18751](https://arxiv.org/abs/2311.18751)

    本文介绍了语言模型代理 (LMA) 在多步决策任务上的有希望的范例，在基本任务上具有出色的性能，但在组合任务上表现不佳。通过平衡数据分布，我们训练了一个新模型 HTML-T5++，在现实应用中取得了超越人类的性能，并在新基准测试中实现了最佳零-shot性能。

    

    最近，语言模型代理(LMA)作为一种在多步决策任务上的有希望的范例出现，通常表现优于人类和其他强化学习代理。尽管有这种希望，但它们在通常涉及任务组合的现实应用中的性能仍未得到充分探索。在这项工作中，我们引入了一个新的基准，叫做CompWoB-反映更现实假设的50个组合性网站自动化任务。我们发现，虽然现有的提示型LMA（gpt-3.5-turbo或gpt-4）在基本任务上实现了94.0％的平均成功率，但在组合任务上降至24.9％的成功率。另一方面，只在基本任务上进行微调的转移性LMA表现出更小的泛化性差距，从85.4％下降到54.8％。通过平衡任务之间的数据分布，我们训练了一个新模型HTML-T5++，在MiniWoB上超过了人类水平的性能（95.2％），并在CompWoB上实现了最佳的零-shot性能（61.5%）。

    Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
    
[^18]: 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释

    Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])

    [http://arxiv.org/abs/2401.14142](http://arxiv.org/abs/2401.14142)

    基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。

    

    现有方法，如概念瓶颈模型 (CBM)，在为黑盒深度学习模型提供基于概念的解释方面取得了成功。它们通常通过在给定输入的情况下预测概念，然后在给定预测的概念的情况下预测最终的类别标签。然而，它们经常无法捕捉到概念之间的高阶非线性相互作用，例如纠正一个预测的概念（例如“黄色胸部”）无法帮助纠正高度相关的概念（例如“黄色腹部”），导致最终准确率不理想；它们无法自然地量化不同概念和类别标签之间的复杂条件依赖关系（例如对于一个带有类别标签“Kentucky Warbler”和概念“黑色嘴巴”的图像，模型能够正确预测另一个概念“黑色冠”的概率是多少），因此无法提供关于黑盒模型工作原理更深层次的洞察。针对这些限制，我们提出了基于能量的概念瓶颈模型（Energy-based Concept Bottleneck Models）。

    Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
    
[^19]: Des-q: 一种用于回归和二分类的构建和高效重新训练决策树的量子算法

    Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2309.09976](http://arxiv.org/abs/2309.09976)

    Des-q是一种量子算法，用于在回归和二分类任务中构建和重新训练决策树。它显著减少了树重新训练所需的时间复杂度，并且能够处理新样本的加载时间。该算法通过 k 分段线性树分裂来构建决策树，将数据划分为不同的子空间。

    

    决策树由于其简单构造和可解释性而广泛应用于机器学习。然而，随着数据规模的增长，传统的决策树构建和重新训练方法变得越来越慢，与训练样本数量呈多项式规模。在本研究中，我们介绍了一种新颖的量子算法Des-q，用于在回归和二分类任务中构建和重新训练决策树。假设数据流产生较小的新训练样本增量，我们证明了我们的Des-q算法显著减少了树重新训练所需的时间，即使考虑将新样本加载到量子可访问内存所需的时间，其时间复杂度也达到了多对数级别。我们的方法涉及构建一个决策树算法，在每个内部节点执行k分段线性树分裂。这些分裂同时生成多个超平面，将数据划分为不同的子空间。

    Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
    
[^20]: 连接NTK和NNGP：神经网络学习动力学在核区域的统一理论框架

    Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])

    [http://arxiv.org/abs/2309.04522](http://arxiv.org/abs/2309.04522)

    本文提出了一个马尔可夫近似学习模型，统一了神经切向核（NTK）和神经网络高斯过程（NNGP）核，用于描述无限宽度深层网络的学习动力学。

    

    人工神经网络近年来在机器学习领域取得了革命性的进展，但其学习过程缺乏一个完整的理论框架。对于无限宽度网络，已经取得了重大进展。在这个范式中，使用了两种不同的理论框架来描述网络的输出：一种基于神经切向核（NTK）的框架，假设了线性化的梯度下降动力学；另一种是基于神经网络高斯过程（NNGP）核的贝叶斯框架。然而，这两种框架之间的关系一直不明确。本文通过一个马尔可夫近似学习模型，统一了这两种不同的理论，用于描述随机初始化的无限宽度深层网络的学习动力学。我们推导出了在学习过程中和学习后的网络输入-输出函数的精确分析表达式，并引入了一个新的时间相关的神经动态核（NDK），这个核可以同时产生NTK和NNGP。

    Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
    
[^21]: 基于机器学习的多重共线性解决方案：四川省碳排放案例研究

    Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01115](http://arxiv.org/abs/2309.01115)

    本研究使用机器学习方法解决了多重共线性问题，针对四川省的碳排放情况进行了案例研究。研究结果确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略，以改善情况。

    

    本研究使用矩阵归一化对四川省46个关键产业2000-2019年的能源消耗数据进行预处理。DBSCAN聚类识别了16个特征类别以客观地分组行业。接下来，采用罚函数回归模型，以应对过拟合控制、高维数据处理和特征选择等复杂能源数据处理的优势。结果表明，煤炭周围的第二个聚类因生产需求而产生的排放量最高。以汽油和焦炭为中心的聚类的排放量也很显著。基于此，减排建议包括清洁煤技术、交通管理、钢铁中的煤电替代和行业标准化。该研究引入了无监督学习的方法来客观选择因素，并旨在探索新的减排途径。总而言之，本研究确定了行业分组，评估了排放驱动因素，并提出了科学的减排策略以进一步改善情况。

    This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to bette
    
[^22]: 初始筛选顺序问题

    The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])

    [http://arxiv.org/abs/2307.15398](http://arxiv.org/abs/2307.15398)

    本文研究了初始筛选顺序问题，在候选人筛选中起到关键作用。我们证明在候选人池不平衡情况下，类人筛选者可能对受保护、代表性不足的群体做出不公平的决策。这项研究的目的是与一家大公司合作，以更好地理解其潜在的自动化招聘流程。

    

    本文介绍了初始筛选顺序问题，这是候选人筛选中的关键步骤。它涉及一个类似人类的筛选者，其目标是在给定初始筛选顺序的候选人池中找到前k个适合的候选人，而不是最好的k个适合的候选人。初始筛选顺序表示类人筛选者在筛选之前如何安排候选人池。初始筛选顺序的选择对所选的k个候选人有重要影响。我们证明，在候选人池不平衡的情况下（例如，男性候选人多于女性候选人），类人筛选者可能在决策过程中对受保护的、代表性不足的群体产生不平等的努力。其他公平性结果也在类人筛选者下得到证明。这项研究是与一家大公司合作的，旨在更好地了解其潜在自动化的招聘流程。

    In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
    
[^23]: 嵌套消除：一种从基于选择的反馈中识别最佳项目的简单算法

    Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])

    [http://arxiv.org/abs/2307.09295](http://arxiv.org/abs/2307.09295)

    嵌套消除是一种简单易实现的算法，通过利用创新的消除准则和嵌套结构，能够以最少的样本数量和高置信水平识别出最受欢迎的项目。

    

    我们研究了基于选择的反馈中识别最佳项目的问题。在这个问题中，公司依次向一群顾客展示显示集，并收集他们的选择。目标是以最少的样本数量和高置信水平识别出最受欢迎的项目。我们提出了一种基于消除的算法，即嵌套消除(Nested Elimination，NE)，它受到信息理论下界所暗示的嵌套结构的启发。NE的结构简单，易于实施，具有对样本复杂度的强大理论保证。具体而言，NE利用了一种创新的消除准则，并避免了解决任何复杂的组合优化问题的需要。我们提供了NE的特定实例和非渐近性的样本复杂度的上界。我们还展示了NE实现了高阶最坏情况渐近最优性。最后，来自合成和真实数据的数值实验验证了我们的理论。

    We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination (NE), which is inspired by the nested structure implied by the information-theoretic lower bound. NE is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of NE. We also show NE achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theore
    
[^24]: 利用观测偏差提高矩阵补全的方法研究

    Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])

    [http://arxiv.org/abs/2306.04775](http://arxiv.org/abs/2306.04775)

    本研究利用观测偏差来改进矩阵补全问题，提出一个简单的两阶段算法，实现了与对未观测协变量的监督学习性能相当的结果。

    

    我们考虑了一种变形的矩阵补全问题，其中输入数据以偏差的方式呈现，类似于Ma和Chen所引入的模型。我们的目标是利用偏差与感兴趣的结果之间的共享信息来改进预测。为此，我们提出了一个简单的两阶段算法：（i）将观测模式解释为完全观测的噪声矩阵，我们对观测模式应用传统的矩阵补全方法来估计潜在因素之间的距离； (ii)我们对恢复的特征应用监督学习来填补缺失观察。我们建立了有限样本误差率，这些误差率与相应的监督学习参数率相竞争，这表明我们的学习性能与使用未观测协变量相当。实证评估使用真实世界数据集反映了类似的表现。

    We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
    
[^25]: 序列建模的变压器网络的逼近理论

    Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])

    [http://arxiv.org/abs/2305.18475](http://arxiv.org/abs/2305.18475)

    本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。

    

    变压器是序列建模应用中广泛应用的架构，但其工作原理的理论理解有限。在本文中，我们研究了变压器逼近序列关系的能力。我们首先证明了变压器假设空间的普遍逼近定理。通过推导，我们确定了一种新的规律概念，在此概念下，我们可以证明一个明确的逼近速率估计。这个估计揭示了变压器的关键结构特性，并暗示了变压器适用于逼近哪些类型的序列关系。特别地，它使我们能够具体地讨论变压器与传统序列建模方法（如循环神经网络）之间的结构偏差。我们的研究结果得到了数字实验的支持。

    The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
    
[^26]: MADiff：离线多智能体学习与扩散模型

    MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])

    [http://arxiv.org/abs/2305.17330](http://arxiv.org/abs/2305.17330)

    本论文提出了基于注意力的扩散模型MADiff，解决了多智能体问题，是第一个扩散模型应用于多智能体离线RL的框架。

    

    扩散模型（DM）是一种强大的生成模型，最近在包括离线强化学习在内的各种场景中取得了巨大成功，其中策略通过在在线评估中产生轨迹来进行规划学习。然而，尽管单智能体学习显示了其有效性，但仍不清楚DM如何在多智能体问题中操作，其中代理商很难在独立建模每个代理商轨迹的情况下完成团队合作。在本文中，我们提出MADiff，一种新的生成式多智能体学习框架，以解决这个问题。MADiff是通过基于注意力的扩散模型来实现对多个扩散智能体行为的复杂协调建模。据我们所知，MADiff是第一个基于扩散的多智能体离线RL框架，它既可以行为为分散的政策，又可以为集中控制器，其中包括对手建模，并可用于多智能体轨迹预测。

    Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
    
[^27]: 基函数编码改善因子机中数字特征的准确性

    Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy. (arXiv:2305.14528v1 [cs.LG])

    [http://arxiv.org/abs/2305.14528](http://arxiv.org/abs/2305.14528)

    本文提供了一种能够将数字特征编码为基函数向量的方法，通过在因子机中将该方法应用于因子机中，可以改善推荐系统的准确性。

    

    因子机(FM)变体被广泛用于大规模实时内容推荐系统，因为它们在模型准确性和训练推理的低计算成本之间提供了出色的平衡。本文提供了一种系统、理论上合理的方法，通过将数值特征编码为所选函数集的函数值向量将数值特征纳入FM变体。

    Factorization machine (FM) variants are widely used for large scale real-time content recommendation systems, since they offer an excellent balance between model accuracy and low computational costs for training and inference. These systems are trained on tabular data with both numerical and categorical columns. Incorporating numerical columns poses a challenge, and they are typically incorporated using a scalar transformation or binning, which can be either learned or chosen a-priori. In this work, we provide a systematic and theoretically-justified way to incorporate numerical features into FM variants by encoding them into a vector of function values for a set of functions of one's choice.  We view factorization machines as approximators of segmentized functions, namely, functions from a field's value to the real numbers, assuming the remaining fields are assigned some given constants, which we refer to as the segment. From this perspective, we show that our technique yields a model
    
[^28]: 高斯过程回归的贝叶斯方法中融入不确定输入

    Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])

    [http://arxiv.org/abs/2305.11586](http://arxiv.org/abs/2305.11586)

    本文提出了一种新的高斯过程回归技术，通过贝叶斯方法将输入数据的不确定性纳入回归模型预测中。在数值实验中展示了该方法具有普适性和不错的表现。

    

    传统高斯过程回归仅假设模型观测数据的输出具有噪声。然而，在许多科学和工程应用中，由于建模假设、测量误差等因素，观测数据的输入位置可能也存在不确定性。在本文中，我们提出了一种贝叶斯方法，将输入数据的可变性融入到高斯过程回归中。考虑两种可观测量——具有固定输入的噪声污染输出和具有先验分布定义的不确定输入，通过贝叶斯框架估计后验分布以推断不确定的数据位置。然后，利用边际化方法将这些输入的量化不确定性纳入高斯过程预测中。通过几个数值实验，展示了这种新回归技术的有效性，在其中观察到不同水平输入数据不确定性下的普适良好表现。

    Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
    
[^29]: 基于MNL选择模型的在线联合组合库存优化问题研究

    Online Joint Assortment-Inventory Optimization under MNL Choices. (arXiv:2304.02022v1 [cs.LG])

    [http://arxiv.org/abs/2304.02022](http://arxiv.org/abs/2304.02022)

    本文提出了一个算法解决在线联合组合库存优化问题，能够在平衡探索与开发的措施下实现最大化预期总利润，并为该算法建立了遗憾上界。

    

    本文研究了一种在线联合组合库存优化问题，在该问题中，我们假设每个顾客的选择行为都遵循Multinomial Logit（MNL）选择模型，吸引力参数是先验未知的。零售商进行周期性组合和库存决策，以动态地从实现的需求中学习吸引力参数，同时在时间上最大化预期的总利润。本文提出了一种新算法，可以有效地平衡组合和库存在线决策中的探索和开发。我们的算法建立在一个新的MNL吸引力参数估计器，一种通过自适应调整某些已知和未知参数来激励探索的新方法，以及一个用于静态单周期组合库存规划问题的优化oracle基础之上。我们为我们的算法建立了遗憾上界，以及关于在线联合组合库存优化问题的下界。

    We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the realized demands about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance the exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, a novel approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimi
    
[^30]: InceptionNeXt：当Inception遇到ConvNeXt

    InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])

    [http://arxiv.org/abs/2303.16900](http://arxiv.org/abs/2303.16900)

    本论文提出了一种名为InceptionNeXt的新型神经网络，通过将大内核卷积沿通道维度分解为四个平行分支来提高模型效率，解决了保持性能的同时加快基于大内核的CNN模型的问题。

    

    受ViTs长程建模能力的启发，近期广泛研究和采用了大内核卷积来扩大感受野和提高模型性能，例如ConvNeXt采用了7x7深度卷积。虽然这种深度操作仅消耗少量FLOPs，但由于高内存访问成本，这在功能强大的计算设备上大大损害了模型效率。尽管缩小ConvNeXt的内核大小能提高速度，但会导致性能显着下降。如何在保持性能的同时加快基于大内核的CNN模型仍不清楚。为了解决这个问题，受Inceptions的启发，我们提出将大内核深度卷积沿通道维度分解为四个平行分支，即小方内核、两个正交带内核和一个互补内核。

    Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
    
[^31]: 退化是可以接受的：带有不连续分布的网络收入管理中的对数遗憾

    Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07996](http://arxiv.org/abs/2210.07996)

    该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。

    

    我们研究了具有接受/拒绝决策和T次独立同分布到达的经典网络收入管理（NRM）问题。我们考虑了一个分布形式，每个到达必须属于有限数量的可能类别之一，每个类别具有确定的资源消耗向量，但是一个在区间上连续分布的随机值。我们开发了一种在线算法，该算法在此模型下实现了O(log^2 T)的遗憾，唯一（必要）的假设是概率密度远离0。我们得到了第二个结果，在二阶增长的额外假设下，实现了O(log T)的遗憾。据我们所知，这是第一个在没有任何“退化”假设的情况下，在具有连续值的NRM模型中实现对数级别遗憾的结果。我们的结果是通过包括一种新的边界mypopic regret，离线分配的“半流体”放松以及改进边界的新技术实现的。

    We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
    
[^32]: 具有层次图注意力循环网络的活动感知人类移动预测

    Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network. (arXiv:2210.07765v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07765](http://arxiv.org/abs/2210.07765)

    这个论文提出了一种基于层次图注意循环网络的活动感知人类移动预测方法，通过构建一个层次图和使用层次图注意模块来捕捉时间-活动-位置之间的依赖关系，以建模用户的偏好。同时引入了一种模型无关的历史增强置信标签，用于聚焦于每个用户的个体级偏好。

    

    人类移动预测是一项基础任务，对于城市规划、基于位置的服务和智能交通系统等各种应用至关重要。现有的方法通常忽略了对行为信息的考虑，这是推理人类偏好和例行活动的关键，或者采用了简化的时间、活动和位置之间的依赖关系表示。为了解决这些问题，我们提出了一种基于层次图注意循环网络（HGARN）的人类移动预测方法。具体来说，我们基于所有用户的历史移动记录构建了一个层次图，并使用层次图注意模块来捕捉复杂的时间-活动-位置依赖关系。这样，HGARN可以学习具有丰富的人类出行语义的表示，以建模用户在全局层面上的偏好。我们还提出了一种模型无关的历史增强置信（MAHEC）标签，以便将我们的模型聚焦于每个用户的个体级偏好。最后，我们引入了一个时间模块...

    Human mobility prediction is a fundamental task essential for various applications, including urban planning, location-based services and intelligent transportation systems. Existing methods often ignore activity information crucial for reasoning human preferences and routines, or adopt a simplified representation of the dependencies between time, activities and locations. To address these issues, we present Hierarchical Graph Attention Recurrent Network (HGARN) for human mobility prediction. Specifically, we construct a hierarchical graph based on all users' history mobility records and employ a Hierarchical Graph Attention Module to capture complex time-activity-location dependencies. This way, HGARN can learn representations with rich human travel semantics to model user preferences at the global level. We also propose a model-agnostic history-enhanced confidence (MAHEC) label to focus our model on each user's individual-level preferences. Finally, we introduce a Temporal Module, wh
    
[^33]: 异构多智能体零样本协同进化研究

    Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2208.04957](http://arxiv.org/abs/2208.04957)

    本研究首次研究了异构零样本协同问题，并提出了一种基于协同进化的通用方法，通过配对、更新和选择的过程，实现了多智能体零样本协同。实验结果表明，考虑异构情况的必要性，并证明了该方法对于异构零样本协同任务的有前景的解决方案。

    

    在合作多智能体强化学习领域，生成能够与未知合作伙伴零样本协同的智能体是一个新的挑战。最近的一些研究在零样本协同方面取得了进展，通过训练过程中向智能体暴露多样化的合作伙伴。然而，这些方法通常在训练伙伴时涉及自我对弈，隐式地假设任务是同质的。然而，许多真实世界的任务是异构的，因此先前的方法可能效率低下。本文首次研究了异构零样本协同的问题，并提出了一种基于协同进化的通用方法，通过三个子过程：配对、更新和选择，对两个智能体和合作伙伴进行协同进化。对不同异构任务的实验结果突出了考虑异构情况的必要性，并证明我们提出的方法是解决异构零样本协同任务的一种有前景的解决方案。

    Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
    

