# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis](https://rss.arxiv.org/abs/2402.01371) | 本论文提出了一个评论家-演员算法，解决了长期平均奖励设置中的函数逼近问题，并进行了有限时间分析。实验结果表明，我们的算法能够在评论家的均方误差上界为$\epsilon$的情况下，获得样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，优于演员-评论家算法的结果。 |
| [^2] | [PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning](https://arxiv.org/abs/2404.00776) | PyTorch Frame是一个用于处理多模态表格数据的PyTorch框架，通过提供数据结构、模型抽象和外部基础模型整合等功能，实现了模块化的表格模型实现，并成功将这些模型应用于复杂的数据集。 |
| [^3] | [Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE](https://arxiv.org/abs/2403.19040) | 提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。 |
| [^4] | [Learning from Models and Data for Visual Grounding](https://arxiv.org/abs/2403.13804) | 结合数据驱动学习和模型知识传递的新框架，通过优化一致性目标来增强预训练视觉和语言模型的视觉定位能力。 |
| [^5] | [Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies](https://arxiv.org/abs/2403.11353) | 该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。 |
| [^6] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^7] | [Recurrent Drafter for Fast Speculative Decoding in Large Language Models](https://arxiv.org/abs/2403.09919) | 本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。 |
| [^8] | [Estimating the history of a random recursive tree](https://arxiv.org/abs/2403.09755) | 本文研究了估计随机递归树中顶点到达顺序的问题，提出了基于Jordan中心性度量的顺序估计器，并证明其几乎是最优的。 |
| [^9] | [Optimal Top-Two Method for Best Arm Identification and Fluid Analysis](https://arxiv.org/abs/2403.09123) | 提出了解决最佳臂识别问题中的样本复杂度挑战的最优Top-Two算法。 |
| [^10] | [Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)](https://arxiv.org/abs/2403.07573) | 本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。 |
| [^11] | [Data Collaboration Analysis Over Matrix Manifolds](https://arxiv.org/abs/2403.02780) | 本研究讨论了在矩阵流形上的数据协作分析，探讨了如何通过隐私保护机器学习来处理多来源数据的道德和隐私问题 |
| [^12] | [ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis](https://arxiv.org/abs/2403.01493) | ConvTimeNet是一种新颖的深度分层全卷积网络，通过自适应分段和全卷积块设计，有效捕捉了全局序列和跨变量的依赖性。 |
| [^13] | [Merging Text Transformer Models from Different Initializations](https://arxiv.org/abs/2403.00986) | 研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。 |
| [^14] | [Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation](https://arxiv.org/abs/2402.18329) | 通过增强和多样化LOTL恶意活动的存在，提出了一种用于改善恶意活动检测性能的数据增强框架 |
| [^15] | [Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models](https://arxiv.org/abs/2402.11989) | 提出了隐私保护的低秩适应解决方案PrivateLoRA，通过最小化适应损失和代理攻击模型的MI增益来抵御成员推断攻击。 |
| [^16] | [InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration](https://arxiv.org/abs/2402.11441) | 提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。 |
| [^17] | [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668) | 对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。 |
| [^18] | [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902) | L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。 |
| [^19] | [RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair](https://arxiv.org/abs/2312.15698) | 高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。 |
| [^20] | [A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning](https://arxiv.org/abs/2312.00502) | 本研究通过对比自监督学习应用于1D心音图样本中异常检测，进行了广泛的音频增强方法比较评估和在多个数据集上训练分类器的研究。 |
| [^21] | [Simulation Based Bayesian Optimization.](http://arxiv.org/abs/2401.10811) | 本文介绍了基于仿真的贝叶斯优化（SBBO）作为一种新方法，用于通过仅需基于采样的访问来优化获取函数。 |
| [^22] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^23] | [Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning.](http://arxiv.org/abs/2401.02329) | 本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。 |
| [^24] | [SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models.](http://arxiv.org/abs/2401.00793) | SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。 |
| [^25] | [An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l.](http://arxiv.org/abs/2310.07325) | 在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。 |
| [^26] | [xVal: A Continuous Number Encoding for Large Language Models.](http://arxiv.org/abs/2310.02989) | xVal是一种连续数字编码方案，通过使用单个标记来表示任何实数。与现有的数字编码方案相比，xVal更加高效，并且在泛化性能上表现更好。 |
| [^27] | [Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training.](http://arxiv.org/abs/2309.13254) | 这篇论文介绍了Zen，一种用于分布式DNN训练中近似最优稀疏张量同步的方法。通过分析流行的DNN模型中稀疏张量的特性，并系统地探索设计空间，找到了最佳的通信方案。通过减少通信流量和提高训练效率，Zen有效地提升了分布式训练的性能。 |
| [^28] | [Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples.](http://arxiv.org/abs/2308.14930) | 本研究探讨了量子预处理滤波器（QPF）在二值图像分类中的应用，并通过在MNIST、EMNIST和CIFAR-10上提高了分类准确率，并在GTSRB上降低了分类准确率。 |
| [^29] | [HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2307.01636) | 本文介绍了HAGNN框架，它是一种用于处理异构图的混合聚合方法。HAGNN同时利用元路径邻居和直接连接邻居进行节点聚合，将整个聚合过程分为基于元路径的内类型聚合和无元路径的跨类型聚合。这种方法能够全面利用异构图中的类型语义信息。 |
| [^30] | [Why can neural language models solve next-word prediction? A mathematical perspective.](http://arxiv.org/abs/2306.17184) | 本文研究了神经语言模型在下一个词预测任务中的成功，在形式语言理论背景下，提出了一种为什么神经语言模型能够学习到组合规则的解释，并在一个现实世界的英语句子示例中提供了零错误的证明。 |
| [^31] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^32] | [Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features.](http://arxiv.org/abs/2306.00934) | 基于PROVEXPLAINER框架，通过复制GNN-based security models的决策过程，利用决策树和图结构特征将抽象GNN决策边界投影到可解释的特征空间，以增强GNN安全模型的透明度和询问能力。 |
| [^33] | [Physics-Inspired Interpretability Of Machine Learning Models.](http://arxiv.org/abs/2304.02381) | 本文提出了一种以能量景观方法为启发的机器学习模型可解释性研究方法，可以识别决策的驱动因素，有助于解决机器学习模型在高度敏感领域应用的难题。 |

# 详细

[^1]: Critic-Actor算法在平均奖励MDPs中的函数逼近问题：有限时间分析

    Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis

    [https://rss.arxiv.org/abs/2402.01371](https://rss.arxiv.org/abs/2402.01371)

    本论文提出了一个评论家-演员算法，解决了长期平均奖励设置中的函数逼近问题，并进行了有限时间分析。实验结果表明，我们的算法能够在评论家的均方误差上界为$\epsilon$的情况下，获得样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，优于演员-评论家算法的结果。

    

    最近，关于两个时间尺度演员-评论家算法的渐近和非渐近收敛分析的研究工作非常活跃，其中演员的更新速度比评论家慢。在最近的一项工作中，提出了一个评论家-演员算法，用于无限时域折扣成本设置中的查找表情况，其中演员和评论家的时间尺度相反，并给出了渐近收敛分析。在我们的工作中，我们首次提出了一个具有函数逼近的评论家-演员算法，并在长期平均奖励设置中进行了首次有限时间（非渐近）分析。我们得到了最优的学习速率，并证明了我们的算法从评论家的均方误差上界为$\epsilon$，其样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，此结果比演员-评论家算法获得的结果要好。

    In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic
    
[^2]: PyTorch Frame: 一个用于多模态表格学习的模块化框架

    PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning

    [https://arxiv.org/abs/2404.00776](https://arxiv.org/abs/2404.00776)

    PyTorch Frame是一个用于处理多模态表格数据的PyTorch框架，通过提供数据结构、模型抽象和外部基础模型整合等功能，实现了模块化的表格模型实现，并成功将这些模型应用于复杂的数据集。

    

    我们提出了PyTorch Frame，这是一个基于PyTorch的框架，用于处理多模态表格数据的深度学习。PyTorch Frame通过提供基于PyTorch的数据结构来处理复杂的表格数据，引入模型抽象以实现表格模型的模块化实现，并允许整合外部基础模型来处理复杂列（例如，用于文本列的LLMs）。我们通过以模块化方式实现多样的表格模型，成功将这些模型应用于复杂的多模态表格数据，并将我们的框架与PyTorch Geometric集成，PyTorch Geometric是一个用于图神经网络（GNNs）的PyTorch库，以实现对关系数据库的端到端学习。

    arXiv:2404.00776v1 Announce Type: new  Abstract: We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.
    
[^3]: 使用方向感知t-SNE可视化高维时间数据

    Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE

    [https://arxiv.org/abs/2403.19040](https://arxiv.org/abs/2403.19040)

    提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。

    

    许多现实世界数据集包含时间组件或涉及从一个状态到另一个状态的转变。为了进行探索性数据分析，我们可以将这些高维数据集表示为二维地图，使用数据对象的嵌入进行探索，并用有向边表示它们的时间关系。大多数现有的降维技术，如t-SNE和UMAP，在构建嵌入时未考虑数据的时间性或关系性，导致时间上杂乱的可视化，使潜在有趣的模式变得模糊。为了解决这个问题，我们在t-SNE的优化函数中提出了两个互补的方向感知损失项，强调数据的时间方面，引导优化和结果嵌入以展示可能被忽略的时间模式。定向一致损失（DCL）鼓励附近的箭头

    arXiv:2403.19040v1 Announce Type: new  Abstract: Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows c
    
[^4]: 从模型和数据中学习进行视觉定位

    Learning from Models and Data for Visual Grounding

    [https://arxiv.org/abs/2403.13804](https://arxiv.org/abs/2403.13804)

    结合数据驱动学习和模型知识传递的新框架，通过优化一致性目标来增强预训练视觉和语言模型的视觉定位能力。

    

    我们介绍了SynGround，这是一个结合了数据驱动学习和从各种大规模预训练模型中进行知识传递的新型框架，以增强预训练视觉和语言模型的视觉定位能力。从模型中进行的知识传递引发了通过图像描述生成器生成图像描述。这些描述具有双重作用：它们作为文本到图像生成器合成图像的提示，以及作为查询来合成文本，从其中使用大型语言模型提取短语。最后，我们利用一个开放词汇的对象检测器为合成图像和文本生成合成边界框。通过优化一个遮罩-注意力一致性目标，在这个数据集上微调预训练的视觉和语言模型，该目标将区域注释与基于梯度的模型解释进行对齐。最终的模型提升了定位能力。

    arXiv:2403.13804v1 Announce Type: cross  Abstract: We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilit
    
[^5]: 溶剂感知的2D核磁共振预测：利用多任务训练和迭代自训练策略

    Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies

    [https://arxiv.org/abs/2403.11353](https://arxiv.org/abs/2403.11353)

    该论文提出了一种利用迭代自我训练方法来训练深度学习模型，从而解决二维核磁共振（2D NMR）预测中的挑战，弥补了缺乏标注NMR训练数据集的不足。

    

    核磁共振（NMR）光谱在各个科学领域中起着关键作用，提供了有关分子的结构信息、电子性质和动态行为的见解。准确的NMR光谱预测能够高效地生成候选分子，使化学家能够将它们与实际实验光谱进行比较。该过程有助于确认分子结构或指出差异，引导进一步的研究。机器学习（ML）已经成为一种有前途的替代方法，用于根据分子结构预测分子的原子NMR化学位移。虽然在预测一维（1D）NMR方面已经取得了显著进展，但通过机器学习进行二维（2D）NMR预测仍然是一项挑战，因为缺乏用于训练的标注的NMR数据集。为了解决这一差距，我们提出了一种迭代自训练（IST）方法，用于训练深度学习模型，以预测原子2DNMR位移。

    arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
    
[^6]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^7]: 大型语言模型中用于快速推测解码的循环草稿机制

    Recurrent Drafter for Fast Speculative Decoding in Large Language Models

    [https://arxiv.org/abs/2403.09919](https://arxiv.org/abs/2403.09919)

    本文介绍了一种适用于大型语言模型的循环草稿机制，结合了经典双模型和最新单模型方法，通过运用循环依赖设计，实现了高效的推测解码。

    

    在本文中，我们介绍一种改进的推测解码方法，旨在提高大型语言模型的效率。我们的方法利用了两种成熟技术的优势：经典的双模型推测解码方法和较新的单模型方法Medusa。从Medusa得到灵感，我们的方法采用了单模型策略进行推测解码。然而，我们的方法通过使用具有循环依赖设计的单个轻量级草稿头来区分自己，本质上类似于经典推测解码中使用的小型草稿模型，但避免了完整transformer架构的复杂性。由于循环依赖，我们可以使用波束搜索快速过滤出草稿头中不需要的候选项。其结果是一种结合了单模型设计简易性并避免了创建数据相关树依赖的方法。

    arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
    
[^8]: 估计随机递归树的历史

    Estimating the history of a random recursive tree

    [https://arxiv.org/abs/2403.09755](https://arxiv.org/abs/2403.09755)

    本文研究了估计随机递归树中顶点到达顺序的问题，提出了基于Jordan中心性度量的顺序估计器，并证明其几乎是最优的。

    

    本文研究了估计随机递归树中顶点到达顺序的问题。具体来说，我们研究了两个基本模型：均匀连接模型和线性优先连接模型。我们提出了一种基于Jordan中心性度量的顺序估计器，并定义了一族风险度量来量化排序过程的质量。此外，我们为这个问题建立了极小-最大下界，并证明所提出的估计器几乎是最优的。最后，我们通过数值实验表明所提出的估计器优于基于度数和谱排序程序。

    arXiv:2403.09755v1 Announce Type: cross  Abstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures.
    
[^9]: 最佳臂识别和流体分析的最优Top-Two方法

    Optimal Top-Two Method for Best Arm Identification and Fluid Analysis

    [https://arxiv.org/abs/2403.09123](https://arxiv.org/abs/2403.09123)

    提出了解决最佳臂识别问题中的样本复杂度挑战的最优Top-Two算法。

    

    Top-2方法在解决最佳臂识别（BAI）问题中变得流行。该方法通过一个算法识别最佳臂，即在有限数量臂中具有最大均值的臂，该算法在任何顺序步骤中独立地以固定概率 β 拉动经验最佳臂，并在其他情况下拉动最佳挑战者臂。选择错误的概率保证在指定的δ >0以下。对于BAI问题，已知信息理论下界的样本复杂度，并在δ → 0时与计算要求高的插件方法渐近匹配。 对于任何 β ∈（0,1）的上述Top 2算法的样本复杂度始终保持在下界的常数范围内。然而，确定与下界匹配的最佳 β 已被证明困难。在本文中，我们解决了这个问题并提出了一个最优的Top-2类型算法。我们考虑分配锚点的一个函数。

    arXiv:2403.09123v1 Announce Type: new  Abstract: Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\delta \rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchor
    
[^10]: 迈向具有可适应性计算和网络融合的动态未来（ACNC）

    Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)

    [https://arxiv.org/abs/2403.07573](https://arxiv.org/abs/2403.07573)

    本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    

    在推进6G的背景下，预计会出现实质性的范式转变，突出了由大量连接和严格遵守服务质量/体验（QoS/E）先决条件所特征化的全面的一切对一切交互。即将面临的挑战源于资源稀缺，促使有意识地向计算-网络融合（CNC）过渡，作为联合资源编排的有前途的方法。虽然基于CNC的机制引起了人们的关注，但它们在实现未来服务方面的有效性，特别是在类似Metaverse的使用情景中，可能会由于用户、服务和资源不断变化的特性而受到限制。因此，本文提出了可适应性CNC（ACNC）的概念，作为一种自主的机器学习（ML）辅助机制，旨在联合编排计算和网络资源，满足对动态和大量用户请求的严格要求。

    arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
    
[^11]: 矩阵流形上的数据协作分析

    Data Collaboration Analysis Over Matrix Manifolds

    [https://arxiv.org/abs/2403.02780](https://arxiv.org/abs/2403.02780)

    本研究讨论了在矩阵流形上的数据协作分析，探讨了如何通过隐私保护机器学习来处理多来源数据的道德和隐私问题

    

    机器学习(ML)算法的有效性与其训练数据集的质量和多样性密切相关。改进的数据集，标志着优越的质量，增强了预测的准确性，并扩展了模型在各种场景下的适用性。研究人员经常整合来自多个来源的数据，以减轻单一来源数据集的偏见和限制。然而，这种广泛的数据融合引发了重大的道德关切，特别是关于用户隐私和未经授权的数据披露风险。已建立了各种全球立法框架来解决这些隐私问题。虽然这些法规对保护隐私至关重要，但它们可能会使ML技术的实际部署变得复杂。隐私保护机器学习(PPML)通过保护从健康记录到地理位置数据等敏感信息，同时实现安全使用这些信息，来应对这一挑战。

    arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
    
[^12]: ConvTimeNet: 一种用于多变量时间序列分析的深度分层全卷积模型

    ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis

    [https://arxiv.org/abs/2403.01493](https://arxiv.org/abs/2403.01493)

    ConvTimeNet是一种新颖的深度分层全卷积网络，通过自适应分段和全卷积块设计，有效捕捉了全局序列和跨变量的依赖性。

    

    本文介绍了ConvTimeNet，这是一种新颖的深度分层全卷积网络，旨在作为时间序列分析的通用模型。该网络的关键设计是为了克服传统卷积网络的局限性。首先，我们提出了将时间序列划分为子序列级补丁的自适应分段，将其视为基本建模单元。这种设置避免了与原始点级时间步长相关联的稀疏语义。其次，我们设计了一个全卷积块，通过巧妙地集成深度和点卷积操作，遵循Transformer编码器中采用的先进构建块风格。这个骨干网络能够有效捕捉全局序列和跨变量依赖性，因为它不仅融合了Transformer架构的先进性，还继承了卷积的固有属性。

    arXiv:2403.01493v1 Announce Type: new  Abstract: This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks. Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution. Furtherm
    
[^13]: 合并来自不同初始化的文本变换器模型

    Merging Text Transformer Models from Different Initializations

    [https://arxiv.org/abs/2403.00986](https://arxiv.org/abs/2403.00986)

    研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。

    

    最近关于一次性基于排列的模型合并的工作表明，不同初始化的模型之间存在令人印象深刻的低或零障碍模连接。然而，尽管Transformer架构在语言领域中占主导地位，但这一领域的研究尚未延伸到Transformer架构。因此，在这项工作中，我们调查了独立Transformer极小值学习类似特征的程度，并提出了一种模型合并技术，以研究损失景观中这些极小值之间的关系。架构的具体细节，如其残差连接、多头注意力和离散的顺序输入，需要特定的干预措施，以便计算留在相同功能等价类中的模型排列。通过我们的方法合并这些模型，我们发现与对几个在一个maske上训练的模型进行模型平均相比，最小值之间的损失障碍一直较低。

    arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
    
[^14]: 依靠知情数据增强的抵制生活-依赖-土地反向外壳检测

    Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation

    [https://arxiv.org/abs/2402.18329](https://arxiv.org/abs/2402.18329)

    通过增强和多样化LOTL恶意活动的存在，提出了一种用于改善恶意活动检测性能的数据增强框架

    

    生活-依赖-土地(LOTL)进攻方法依赖于通过合法应用程序执行的命令链来犯罪行为，仅可通过系统日志分析来识别。LOTL技术隐藏在普通合法活动产生的事件流中，威胁行为者经常通过混淆来伪装活动，使其难以在不引起大量误警情况下检测，即使使用机器学习也是如此。为了在这样恶劣的环境中提高模型的性能，我们提出了一个增强框架，以增强和使多样化LOTL恶意活动在合法日志中的存在。在威胁情报的指导下，我们通过注入已知在野外使用的攻击模板生成数据集，进一步丰富合法活动的可塑模式，以复制回避威胁行为者行为的行为。

    arXiv:2402.18329v1 Announce Type: cross  Abstract: The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an
    
[^15]: 隐私保护的低秩适应Latent扩散模型

    Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models

    [https://arxiv.org/abs/2402.11989](https://arxiv.org/abs/2402.11989)

    提出了隐私保护的低秩适应解决方案PrivateLoRA，通过最小化适应损失和代理攻击模型的MI增益来抵御成员推断攻击。

    

    低秩适应（LoRA）是一种有效的策略，用于通过最小化适应损失，自训练数据集中适应Latent扩散模型（LDM）以生成特定对象。然而，通过LoRA适应的LDM容易受到成员推断（MI）攻击的影响，这种攻击可以判断特定数据点是否属于私人训练数据集，因此面临严重的隐私泄露风险。为了抵御MI攻击，我们首次提出了一个直接的解决方案：隐私保护的LoRA（PrivateLoRA）。PrivateLoRA被构建为一个最小最大优化问题，其中通过最大化MI增益来训练代理攻击模型，而LDM则通过最小化适应损失和代理攻击模型的MI增益之和来进行调整。然而，我们在实践中发现PrivateLoRA存在稳定性优化问题，即由于梯度规模的大幅波动而妨碍适应。

    arXiv:2402.11989v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, w
    
[^16]: InfuserKI：通过Infuser引导的知识集成增强大型语言模型与知识图谱

    InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration

    [https://arxiv.org/abs/2402.11441](https://arxiv.org/abs/2402.11441)

    提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。

    

    大型语言模型（LLMs）在各个领域展现出卓越的开放式生成能力，但在知识密集型任务中表现不佳。为了缓解这一问题，提出了知识集成方法，利用外部模块将领域特定知识图谱与LLMs结合起来。然而，它们存在数据效率低的问题，因为它们需要已知和未知的知识来进行微调。因此，我们研究了一个新颖的问题，即如何在不重复已知知识的情况下有效地将未知知识集成到LLMs中。注入新知识会导致遗忘先前获得的知识的风险。为了解决这个问题，我们提出了一种新颖的Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态来确定是否应该增强原始LLM输出信息，从而有效地减轻知识遗忘问题。在UMLS-2.5k和MetaQ上进行了评估。

    arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
    
[^17]: 对LLMs的越狱攻击的综合评估

    Comprehensive Assessment of Jailbreak Attacks Against LLMs

    [https://arxiv.org/abs/2402.05668](https://arxiv.org/abs/2402.05668)

    对大型语言模型（LLMs）的越狱攻击进行了全面的评估，揭示了一种绕过安全措施的不稳定漏洞。本研究是首次对多种越狱攻击方法进行大规模测量，实验证明优化的越狱提示能够持续达到最高的攻击成功率。

    

    对大型语言模型（LLMs）的滥用引起了广泛关注。为了解决这个问题，已经采取了安全措施以确保LLMs符合社会伦理。然而，最近的研究发现了一种绕过LLMs安全措施的不稳定漏洞，被称为越狱攻击。通过应用技术，如角色扮演场景、对抗性样本或对安全目标的微妙破坏作为提示，LLMs可以产生不适当甚至有害的回应。虽然研究人员已经研究了几种越狱攻击的类别，但他们都是孤立地进行的。为了填补这个空白，我们提出了对各种越狱攻击方法的首次大规模测量。我们集中在来自四个类别的13种尖端越狱方法、16种违规类别的160个问题以及六种流行的LLMs上。我们广泛的实验结果表明，优化的越狱提示始终能够达到最高的攻击成功率，并表现出...

    Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
    
[^18]: L4Q: 通过基于LoRA的量化训练在大型语言模型上提供参数高效的量化训练

    L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ

    [https://arxiv.org/abs/2402.04902](https://arxiv.org/abs/2402.04902)

    L4Q是一种参数高效的量化感知训练算法，通过基于LoRA的学习的量化步长，解决了大型语言模型中量化训练的挑战。

    

    后训练量化(PTQ)和量化感知训练(QAT)方法正在流行起来，以缓解大型语言模型(LLMs)所带来的高内存和计算成本。在资源受限的情况下，尽管后者具有更高的准确性潜力，但由于其减少的训练开销，通常首选后训练量化。同时，介绍了参数高效微调方法，如低秩适应（LoRA），并最近的工作已经探索了量化感知参数高效微调技术。然而，这些方法可能缺乏通用性，因为它们依赖于预量化模型的配置。由非线性量化或混合精度权重引起的效果可能会受到影响，并且重新训练特定量化参数可能会影响最优性能。为了应对这些挑战，我们提出了L4Q，一种参数高效的量化感知训练算法。L4Q利用了基于LoRA的学习的量化步长。

    Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
    
[^19]: RepairLLaMA：高效表示和微调适配器用于程序修复

    RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair

    [https://arxiv.org/abs/2312.15698](https://arxiv.org/abs/2312.15698)

    高效表示和微调适配器相结合的新型程序修复方法RepairLLaMA可为语言模型修复错误产生高效的适配器。

    

    自动程序修复（APR）随着大型语言模型（LLMs）的出现已有了显著发展。对于程序修复进行LLMs的微调是最近研究的一个新领域，有许多未被探索的维度。现有工作大多使用简单的代码表示对LLMs进行微调，并在能够微调更大型LLMs的能力方面存在根本性局限。为解决这个问题，我们提出了RepairLLaMA，一个结合了1）用于APR的代码表示和2）最先进的参数高效的LLM微调技术LoRA的新型程序修复方法。这使得RepairLLaMA产生了一个高效的“程序修复适配器”，用于使用语言模型修复错误。我们的实验证明了这两个概念的有效性。首先，使用具有程序修复特定代码表示的微调适配器使模型能够使用有意义的修复信号。其次，参数高效的微调有助于微调...

    arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
    
[^20]: 对稳健的OOD自监督对比心音图表示学习增强方法的全面评估

    A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning

    [https://arxiv.org/abs/2312.00502](https://arxiv.org/abs/2312.00502)

    本研究通过对比自监督学习应用于1D心音图样本中异常检测，进行了广泛的音频增强方法比较评估和在多个数据集上训练分类器的研究。

    

    尽管近年来深度学习模型的研究活动有所增加，但在医学等多个现实世界环境中，这些模型尚未被广泛接受。高质量标记数据的短缺经常阻碍了开发稳健且具有一般性的模型，当面临新收集的超出分布（OOD）数据集时，这些模型不会因效果下降而受损。对比自监督学习（SSL）为标记数据稀缺性提供了潜在解决方案，因为它利用未标记数据增加模型的效能和稳健性。本研究中，我们提出将对比SSL应用于检测1D心音图（PCG）样本中的异常，通过学习信号的广义表示。具体来说，我们进行了一项广泛的比较评估，涉及多种基于音频的增强方法，评估了在不同下游任务的多个数据集上训练的分类器，最终

    arXiv:2312.00502v2 Announce Type: replace  Abstract: Despite the recent increase in research activity, deep-learning models have not yet been widely accepted in several real-world settings, such as medicine. The shortage of high-quality annotated data often hinders the development of robust and generalizable models, which do not suffer from degraded effectiveness when presented with newly-collected, out-of-distribution (OOD) datasets. Contrastive Self-Supervised Learning (SSL) offers a potential solution to labeled data scarcity, as it takes advantage of unlabeled data to increase model effectiveness and robustness. In this research, we propose applying contrastive SSL for detecting abnormalities in 1D phonocardiogram (PCG) samples by learning a generalized representation of the signal. Specifically, we perform an extensive comparative evaluation of a wide range of audio-based augmentations, evaluate trained classifiers on multiple datasets across different downstream tasks, and finall
    
[^21]: 基于仿真的贝叶斯优化

    Simulation Based Bayesian Optimization. (arXiv:2401.10811v1 [stat.ML])

    [http://arxiv.org/abs/2401.10811](http://arxiv.org/abs/2401.10811)

    本文介绍了基于仿真的贝叶斯优化（SBBO）作为一种新方法，用于通过仅需基于采样的访问来优化获取函数。

    

    贝叶斯优化是一种将先验知识与持续函数评估相结合的强大方法，用于优化黑盒函数。贝叶斯优化通过构建与协变量相关的目标函数的概率代理模型来指导未来评估点的选择。对于平滑连续的搜索空间，高斯过程经常被用作代理模型，因为它们提供对后验预测分布的解析访问，从而便于计算和优化获取函数。然而，在涉及对分类或混合协变量空间进行优化的复杂情况下，高斯过程可能不是理想的选择。本文介绍了一种名为基于仿真的贝叶斯优化（SBBO）的新方法，该方法仅需要对后验预测分布进行基于采样的访问，以优化获取函数。

    Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \emph{sampling-based} access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for co
    
[^22]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^23]: 不是所有的少数群体都是平等的: 空类别感知的异质联邦学习方法

    Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])

    [http://arxiv.org/abs/2401.02329](http://arxiv.org/abs/2401.02329)

    本研究提出了一种异质联邦学习方法FedED，通过同时进行空类别蒸馏和逻辑抑制，解决了在联邦学习中尚未充分识别空类别的问题。

    

    数据异质性是联邦学习中的一个重大挑战，表现为客户端之间本地数据分布的差异。现有方法常常在本地训练过程中采用类别平衡的技术来解决本地类别分布的异质性问题。然而，在少数类别中由于过拟合本地不平衡数据而导致准确性较差的问题仍然存在。本文提出了FedED，这是一种新颖的异质联邦学习方法，同时整合了空类别蒸馏和逻辑抑制。具体而言，空类别蒸馏利用知识蒸馏的方法在每个客户端的本地训练中保留了与空类别相关的重要信息。此外，逻辑抑制直接阻断了预测结果中对空类别的输出。

    Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
    
[^24]: SecFormer：面向大型语言模型的快速准确隐私保护推理

    SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00793](http://arxiv.org/abs/2401.00793)

    SecFormer是一个优化框架，旨在实现Transformer模型的快速准确隐私保护推理。通过消除高成本的指数和线性操作，SecFormer能够有效解决在大型语言模型中应用SMPC时的性能问题。

    

    随着在云平台上部署大型语言模型以提供推理服务的使用增加，隐私问题日益加剧，尤其是涉及投资计划和银行账户等敏感数据。安全多方计算（SMPC）被视为保护推理数据和模型参数隐私的一种有前途的解决方案。然而，SMPC在大型语言模型（特别是基于Transformer架构的模型）的隐私保护推理中的应用往往会导致显著的减速或性能下降。这主要是由于Transformer架构中的众多非线性操作不适合SMPC，并且难以有效规避或优化。为了解决这个问题，我们引入了一个先进的优化框架，称为SecFormer，以实现Transformer模型的快速准确隐私保护推理。通过实施模型设计优化，我们成功消除了高成本的指数和线性操作，并取得了良好的性能。

    With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
    
[^25]: 直接逻辑属性的对抗性样本：gelu-4l中的内存管理

    An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])

    [http://arxiv.org/abs/2310.07325](http://arxiv.org/abs/2310.07325)

    在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。

    

    我们提供了一个4层transformer中内存管理的具体证据。具体来说，我们发现在前向传播过程中，模型组件一致地移除前面组件的输出，这是一种清理行为。我们的研究结果表明，解释性技术Direct Logit Attribution提供了误导性结果。我们展示了明确的例子，证明这种技术是不准确的，因为它没有考虑到清理行为。

    We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
    
[^26]: xVal: 大型语言模型的连续数字编码

    xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])

    [http://arxiv.org/abs/2310.02989](http://arxiv.org/abs/2310.02989)

    xVal是一种连续数字编码方案，通过使用单个标记来表示任何实数。与现有的数字编码方案相比，xVal更加高效，并且在泛化性能上表现更好。

    

    由于数字令牌化的独特困难，大型语言模型尚未广泛用于科学数据集的分析。我们提出了xVal，一种数字编码方案，可以使用单个标记来表示任何实数。xVal通过将专用嵌入向量按数字值进行缩放来表示给定的实数。结合修改后的数字推断方法，该策略使模型在考虑作为从输入字符串的数字到输出字符串的数字的映射时成为端到端连续的。这导致了一种更适用于科学领域应用的归纳偏差。我们在许多合成和现实世界数据集上进行了实证评估。与现有的数字编码方案相比，我们发现xVal在令牌效率和泛化性能上表现更好。

    Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
    
[^27]: Zen：用于分布式DNN训练的近似最优稀疏张量同步

    Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training. (arXiv:2309.13254v1 [cs.LG])

    [http://arxiv.org/abs/2309.13254](http://arxiv.org/abs/2309.13254)

    这篇论文介绍了Zen，一种用于分布式DNN训练中近似最优稀疏张量同步的方法。通过分析流行的DNN模型中稀疏张量的特性，并系统地探索设计空间，找到了最佳的通信方案。通过减少通信流量和提高训练效率，Zen有效地提升了分布式训练的性能。

    

    分布式训练是使用多个GPU扩展深度神经网络(DNN)训练的事实标准。分布式训练的性能瓶颈在于渐变同步的通信。最近，实践者观察到渐变张量中存在稀疏性，表明可以减少通信的流量并提高端到端的训练效率。然而，完全发挥稀疏性的最佳通信方案仍然缺失。本文旨在解决这一问题。我们首先分析了流行DNN模型中稀疏张量的特性，以了解稀疏性的基本原理。然后，我们系统地探索了稀疏张量通信方案的设计空间并找到了最优解。

    Distributed training is the de facto standard to scale up the training of Deep Neural Networks (DNNs) with multiple GPUs. The performance bottleneck of distributed training lies in communications for gradient synchronization. Recently, practitioners have observed sparsity in gradient tensors, suggesting the potential to reduce the traffic volume in communication and improve end-to-end training efficiency. Yet, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to address this gap. We first analyze the characteristics of sparse tensors in popular DNN models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal one. % We then find the optimal scheme based on the characteristics by systematically exploring the design space. We also develop a gradient synchronization system called Zen that approximately realizes it for sparse tensors. We demonstr
    
[^28]: 量子预处理滤波器在小样本二值图像分类中的应用

    Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples. (arXiv:2308.14930v1 [cs.CV])

    [http://arxiv.org/abs/2308.14930](http://arxiv.org/abs/2308.14930)

    本研究探讨了量子预处理滤波器（QPF）在二值图像分类中的应用，并通过在MNIST、EMNIST和CIFAR-10上提高了分类准确率，并在GTSRB上降低了分类准确率。

    

    过去几年来，量子机器学习（QML）在研究人员中引起了极大的兴趣，因为它有潜力改变机器学习领域。已开发出利用量子力学特性的几种模型用于实际应用。本研究探讨了我们之前提出的量子预处理滤波器（QPF）在二值图像分类中的应用。我们对四个数据集进行了QPF的评估：MNIST（手写数字）、EMNIST（手写数字和字母）、CIFAR-10（照片图像）和GTSRB（真实交通标志图像）。与我们之前的多类别分类结果类似，应用QPF使得使用神经网络对MNIST、EMNIST和CIFAR-10的二值图像分类准确率分别从98.9%提高到99.2%、从97.8%提高到98.3%和从71.2%提高到76.1%，但在GTSRB上的准确率下降了从93.5%到92.0%。然后我们将QPF应用于训练样本数量较少的情况下。

    Over the past few years, there has been significant interest in Quantum Machine Learning (QML) among researchers, as it has the potential to transform the field of machine learning. Several models that exploit the properties of quantum mechanics have been developed for practical applications. In this study, we investigated the application of our previously proposed quantum pre-processing filter (QPF) to binary image classification. We evaluated the QPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits and alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic sign images). Similar to our previous multi-class classification results, the application of QPF improved the binary image classification accuracy using neural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8% to 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from 93.5% to 92.0%. We then applied QPF in cases using a smaller number of training and 
    
[^29]: HAGNN: 混合聚合用于异构图神经网络

    HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks. (arXiv:2307.01636v1 [cs.LG])

    [http://arxiv.org/abs/2307.01636](http://arxiv.org/abs/2307.01636)

    本文介绍了HAGNN框架，它是一种用于处理异构图的混合聚合方法。HAGNN同时利用元路径邻居和直接连接邻居进行节点聚合，将整个聚合过程分为基于元路径的内类型聚合和无元路径的跨类型聚合。这种方法能够全面利用异构图中的类型语义信息。

    

    异构图神经网络在处理异构图方面取得了成功。在现有异构图神经网络中，元路径起着重要作用。然而，最近的研究指出，简单的无元路径的同质图模型也可以取得相近的结果，这对元路径的必要性提出了质疑。本文首先介绍了基于元路径和无元路径模型之间的内在差异，即如何选择节点聚合的邻居。然后，我们提出了一种新颖的框架来全面利用异构图中丰富的类型语义信息，即HAGNN（混合聚合用于异构图神经网络）。HAGNN的核心是同时利用元路径邻居和直接连接邻居进行节点聚合。HAGNN将整个聚合过程分为两个阶段：基于元路径的内类型聚合和无元路径的跨类型聚合。

    Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, w
    
[^30]: 为什么神经语言模型可以解决下一个词预测问题？数学视角

    Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])

    [http://arxiv.org/abs/2306.17184](http://arxiv.org/abs/2306.17184)

    本文研究了神经语言模型在下一个词预测任务中的成功，在形式语言理论背景下，提出了一种为什么神经语言模型能够学习到组合规则的解释，并在一个现实世界的英语句子示例中提供了零错误的证明。

    

    最近，深度学习在自然语言处理领域引起了革命，神经语言模型在下一个词预测方面证明了非常有效。然而，在形式语言理论的背景下，关于神经语言模型在此任务中可以学习到组合规则的成功的严格理论解释尚未被提出，因为尚不清楚为什么神经语言模型可以学习到控制下一个词预测任务的组合规则。在本文中，我们研究了一类可以用来模拟英语句子的现实世界示例的形式语言。我们构建了神经语言模型来解决这种情况下的下一个词预测任务，且错误率为零。我们的证明突出了嵌入层和全连接组件在神经语言模型中的不同作用。

    Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
    
[^31]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^32]: 基于权威图结构特征对基于GNN的IDS检测进行解释

    Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.00934](http://arxiv.org/abs/2306.00934)

    基于PROVEXPLAINER框架，通过复制GNN-based security models的决策过程，利用决策树和图结构特征将抽象GNN决策边界投影到可解释的特征空间，以增强GNN安全模型的透明度和询问能力。

    

    复杂神经网络模型的黑匣子本质妨碍了它们在安全领域的普及，因为它们缺乏逻辑解释和可执行后续行动的预测。为了增强在系统来源分析中使用的图神经网络（GNN）安全模型的透明度和问责制，我们提出了PROVEXPLAINER，一种将抽象GNN决策边界投影到可解释特征空间的框架。我们首先使用简单且可解释的模型，如决策树（DT），复制基于GNN的安全模型的决策过程。为了最大化替代模型的准确性和保真度，我们提出了一种基于经典图论的图结构特征，并通过安全领域知识的广泛数据研究对其进行了增强。我们的图结构特征与系统来源领域中的问题空间行动密切相关，这使检测结果可用人类语言描述和解释。

    The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
    
[^33]: 机器学习模型的物理启发式可解释性研究

    Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])

    [http://arxiv.org/abs/2304.02381](http://arxiv.org/abs/2304.02381)

    本文提出了一种以能量景观方法为启发的机器学习模型可解释性研究方法，可以识别决策的驱动因素，有助于解决机器学习模型在高度敏感领域应用的难题。

    

    机器学习模型的决策解释能力一直是限制其在高度敏感领域如医学、网络安全或自动驾驶中广泛应用的主要障碍之一。本文提出了一种新颖的方法，受物理学领域的能量景观研究方法启发，以识别输入数据的相关特征以促进模型决策。通过识别损失景观局部极小值组中的守恒权重，我们可以确定模型决策的驱动因素。在分子科学中存在类似的思想，使用坐标不变量或有序参数来确定分子的关键特征。然而，在机器学习损失景观中没有这样的方法。本文将展示能量景观方法在机器学习模型中的适用性，并举例说明其应用。

    The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
    

