# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons](https://arxiv.org/abs/2403.07688) | é‡æ–°è¯„ä¼°æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„æ­»äº¡ç¥ç»å…ƒç°è±¡ï¼Œæå‡ºäº†Demon Pruningï¼ˆDemPï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶æ­»äº¡ç¥ç»å…ƒçš„äº§ç”Ÿï¼ŒåŠ¨æ€å®ç°ç½‘ç»œç¨€ç–åŒ–ã€‚ |
| [^2] | [Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling](https://arxiv.org/abs/2402.18508) | å…°èŠ±å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®ç›¸å…³å·ç§¯æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å·ç§¯æ ¸ï¼Œå®ç°äº†é«˜è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚ |
| [^3] | [Stochastic Gradient Descent for Additive Nonparametric Regression](https://arxiv.org/abs/2401.00691) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œå¯ä»¥å®ç°æœ€å°å’Œæœ€ä¼˜çš„é£é™©ã€‚ |
| [^4] | [Are Ensembles Getting Better all the Time?](https://arxiv.org/abs/2311.17885) | åªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ¨¡å‹ä¸€ç›´åœ¨å˜å¾—æ›´å¥½ï¼Œå½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œå¥½æ¨¡å‹çš„é›†æˆå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆå˜å¾—æ›´ç³Ÿã€‚ |
| [^5] | [Content-based Recommendation Engine for Video Streaming Platform.](http://arxiv.org/abs/2308.08406) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå†…å®¹çš„æ¨èå¼•æ“ï¼Œé€šè¿‡è®¡ç®—æ–‡æ¡£ä¸­å•è¯çš„ç›¸å…³æ€§å’Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ–¹æ³•æ¥æ¨èè§†é¢‘ç»™ç”¨æˆ·ã€‚åŒæ—¶ï¼Œè¿˜é€šè¿‡è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å¾—åˆ†æ¥è¯„ä¼°å¼•æ“çš„æ€§èƒ½ã€‚ |
| [^6] | [Generative Modelling of L\'{e}vy Area for High Order SDE Simulation.](http://arxiv.org/abs/2308.02452) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹LÃ©vyGANï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ï¼Œè§£å†³äº†éé«˜æ–¯æ€§è´¨ä¸‹çš„æŠ½æ ·å›°éš¾é—®é¢˜ã€‚ |
| [^7] | [Machine learning for option pricing: an empirical investigation of network architectures.](http://arxiv.org/abs/2307.07657) | å¹¿ä¹‰é«˜é€Ÿå…¬è·¯ç½‘ç»œç»“æ„åœ¨æœŸæƒå®šä»·é—®é¢˜ä¸­çš„åº”ç”¨è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ |
| [^8] | [HiGen: Hierarchical Graph Generative Networks.](http://arxiv.org/abs/2305.19337) | HiGenæ˜¯ä¸€ç§æ–°é¢–çš„å›¾å½¢ç”Ÿæˆç½‘ç»œï¼Œèƒ½å¤Ÿä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ•æ‰å›¾å½¢çš„å±‚æ¬¡ç»“æ„ï¼Œå¹¶ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒæ¥ç”Ÿæˆå…·æœ‰æ•´æ•°å€¼çš„å­å›¾çš„è¾¹æƒã€‚å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å›¾å½¢çš„å±€éƒ¨å’Œå…¨å±€å±æ€§ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ |
| [^9] | [Hedonic Prices and Quality Adjusted Price Indices Powered by AI.](http://arxiv.org/abs/2305.00044) | æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨çš„ç»éªŒäº«ä¹æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤§é‡æœªç»“æ„åŒ–çš„äº§å“æ•°æ®ï¼Œå‡†ç¡®åœ°ä¼°è®¡äº§å“çš„äº«ä¹ä»·æ ¼å’Œæ´¾ç”ŸæŒ‡æ•°ã€‚ |
| [^10] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚ |

# è¯¦ç»†

[^1]: Maxwellçš„æ¶é­”ä¹‹å·¥ä½œï¼šé€šè¿‡åˆ©ç”¨ç¥ç»å…ƒé¥±å’Œå®ç°æœ‰æ•ˆä¿®å‰ª

    Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons

    [https://arxiv.org/abs/2403.07688](https://arxiv.org/abs/2403.07688)

    é‡æ–°è¯„ä¼°æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„æ­»äº¡ç¥ç»å…ƒç°è±¡ï¼Œæå‡ºäº†Demon Pruningï¼ˆDemPï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶æ­»äº¡ç¥ç»å…ƒçš„äº§ç”Ÿï¼ŒåŠ¨æ€å®ç°ç½‘ç»œç¨€ç–åŒ–ã€‚

    

    åœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œ$\textit{æ­»äº¡ç¥ç»å…ƒ}$ç°è±¡â€”â€”åœ¨è®­ç»ƒæœŸé—´å˜å¾—ä¸æ´»è·ƒæˆ–é¥±å’Œï¼Œè¾“å‡ºä¸ºé›¶çš„å•å…ƒâ€”ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºä¸å¯å–çš„ï¼Œä¸ä¼˜åŒ–æŒ‘æˆ˜æœ‰å…³ï¼Œå¹¶å¯¼è‡´åœ¨ä¸æ–­å­¦ä¹ çš„æƒ…å†µä¸‹ä¸§å¤±å¯å¡‘æ€§ã€‚æœ¬æ–‡é‡æ–°è¯„ä¼°äº†è¿™ä¸€ç°è±¡ï¼Œä¸“æ³¨äºç¨€ç–æ€§å’Œä¿®å‰ªã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢å„ç§è¶…å‚æ•°é…ç½®å¯¹æ­»äº¡ç¥ç»å…ƒçš„å½±å“ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬æœ‰åŠ©äºä¿ƒè¿›ç®€å•è€Œæœ‰æ•ˆçš„ç»“æ„åŒ–ä¿®å‰ªç®—æ³•çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†$\textit{Demon Pruning}$ï¼ˆDemPï¼‰ï¼Œä¸€ç§æ§åˆ¶æ­»äº¡ç¥ç»å…ƒæ‰©å¼ ï¼ŒåŠ¨æ€å¯¼è‡´ç½‘ç»œç¨€ç–æ€§çš„æ–¹æ³•ã€‚é€šè¿‡åœ¨æ´»è·ƒå•å…ƒä¸Šæ³¨å…¥å™ªå£°å’Œé‡‡ç”¨å•å‘¨æœŸè°ƒåº¦æ­£åˆ™åŒ–ç­–ç•¥çš„ç»„åˆï¼ŒDemPå› å…¶ç®€å•æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§è€Œè„±é¢–è€Œå‡ºã€‚åœ¨CIFAR10ä¸Šçš„å®éªŒä¸­...

    arXiv:2403.07688v1 Announce Type: cross  Abstract: When training deep neural networks, the phenomenon of $\textit{dying neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero during training$\unicode{x2013}$ has traditionally been viewed as undesirable, linked with optimization challenges, and contributing to plasticity loss in continual learning scenarios. In this paper, we reassess this phenomenon, focusing on sparsity and pruning. By systematically exploring the impact of various hyperparameter configurations on dying neurons, we unveil their potential to facilitate simple yet effective structured pruning algorithms. We introduce $\textit{Demon Pruning}$ (DemP), a method that controls the proliferation of dead neurons, dynamically leading to network sparsity. Achieved through a combination of noise injection on active units and a one-cycled schedule regularization strategy, DemP stands out for its simplicity and broad applicability. Experiments on CIFAR10 an
    
[^2]: å…°èŠ±ï¼šçµæ´»ä¸”æ•°æ®ç›¸å…³çš„å·ç§¯ç”¨äºåºåˆ—å»ºæ¨¡

    Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling

    [https://arxiv.org/abs/2402.18508](https://arxiv.org/abs/2402.18508)

    å…°èŠ±å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®ç›¸å…³å·ç§¯æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å·ç§¯æ ¸ï¼Œå®ç°äº†é«˜è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ã€‚

    

    åœ¨æ·±åº¦å­¦ä¹ ä¸æ–­å‘å±•çš„æ ¼å±€ä¸­ï¼Œå¹³è¡¡è¡¨è¾¾èƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡çš„æ¨¡å‹å·²ç»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå…°èŠ±ï¼ˆOrchidï¼‰çš„æ–°å‹æ¶æ„ï¼Œé€šè¿‡åŒ…å«ä¸€ç§æ–°çš„æ•°æ®ç›¸å…³å·ç§¯æœºåˆ¶æ¥é‡æ–°æ„æƒ³åºåˆ—å»ºæ¨¡ã€‚å…°èŠ±æ—¨åœ¨è§£å†³ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶å›ºæœ‰çš„é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„äºŒæ¬¡å¤æ‚æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ•æ‰è¿œç¨‹ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ã€‚å…°èŠ±çš„æ ¸å¿ƒæ˜¯æ•°æ®ç›¸å…³å·ç§¯å±‚ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨çš„æ¡ä»¶åŒ–ç¥ç»ç½‘ç»œæ ¹æ®è¾“å…¥æ•°æ®åŠ¨æ€è°ƒæ•´å…¶å·ç§¯æ ¸ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªç®€å•çš„æ¡ä»¶åŒ–ç½‘ç»œï¼Œä»¥åœ¨è‡ªé€‚åº”å·ç§¯æ“ä½œä¸­ç»´æŒå¹³ç§»ç­‰å˜æ€§ã€‚æ•°æ®ç›¸å…³å·ç§¯æ ¸çš„åŠ¨æ€ç‰¹æ€§ï¼ŒåŠ ä¸Šé—¨æ§æ“ä½œï¼Œèµ‹äºˆäº†å…°èŠ±é«˜è¡¨è¾¾èƒ½åŠ›ï¼ŒåŒæ—¶ç»´æŒäº†è®¡ç®—æ•ˆç‡ã€‚

    arXiv:2402.18508v1 Announce Type: new  Abstract: In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent convolution mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and in-context learning. At the core of Orchid lies the data-dependent convolution layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive convolution operation. The dynamic nature of data-dependent convolution kernel, coupled with gating operations, grants Orchid high expressivity while mai
    
[^3]: æ·»åŠ éå‚æ•°å›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™

    Stochastic Gradient Descent for Additive Nonparametric Regression

    [https://arxiv.org/abs/2401.00691](https://arxiv.org/abs/2401.00691)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œå¯ä»¥å®ç°æœ€å°å’Œæœ€ä¼˜çš„é£é™©ã€‚

    

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„è¿­ä»£ç®—æ³•ï¼Œè¯¥ç®—æ³•å…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚è¯¥ç®—æ³•å¯ä»¥çœ‹ä½œæ˜¯å¯¹ç»„ä»¶å‡½æ•°çš„æˆªæ–­åŸºæ‰©å±•çš„ç³»æ•°åº”ç”¨éšæœºæ¢¯åº¦ä¸‹é™çš„å‡½æ•°å¯¹åº”ç‰©ã€‚æˆ‘ä»¬è¯æ˜äº†å¾—åˆ°çš„ä¼°è®¡é‡æ»¡è¶³ä¸€ä¸ªå¥¥æ‹‰å…‹ä¸ç­‰å¼ï¼Œå…è®¸æ¨¡å‹é”™è¯¯è§„èŒƒã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åœ¨è®­ç»ƒçš„ä¸‰ä¸ªä¸åŒé˜¶æ®µä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬è¯æ˜äº†å…¶é£é™©åœ¨æ•°æ®ç»´åº¦å’Œè®­ç»ƒæ ·æœ¬å¤§å°çš„ä¾èµ–æ–¹é¢æ˜¯æœ€å°å’Œæœ€ä¼˜çš„ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªå®é™…æ•°æ®é›†ä¸Šå°†è¯¥æ–¹æ³•ä¸ä¼ ç»Ÿçš„åå‘æ‹Ÿåˆè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯´æ˜äº†è®¡ç®—ä¼˜åŠ¿ã€‚

    This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
    
[^4]: é›†æˆæ¨¡å‹æ˜¯å¦ä¸€ç›´åœ¨ä¸æ–­è¿›æ­¥ï¼Ÿ

    Are Ensembles Getting Better all the Time?

    [https://arxiv.org/abs/2311.17885](https://arxiv.org/abs/2311.17885)

    åªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ¨¡å‹ä¸€ç›´åœ¨å˜å¾—æ›´å¥½ï¼Œå½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œå¥½æ¨¡å‹çš„é›†æˆå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆå˜å¾—æ›´ç³Ÿã€‚

    

    é›†æˆæ–¹æ³•ç»“åˆäº†å‡ ä¸ªåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å§‹ç»ˆå°†æ›´å¤šæ¨¡å‹çº³å…¥é›†æˆä¼šæå‡å…¶å¹³å‡æ€§èƒ½ã€‚è¿™ä¸ªé—®é¢˜å–å†³äºæ‰€è€ƒè™‘çš„é›†æˆç±»å‹ï¼Œä»¥åŠé€‰æ‹©çš„é¢„æµ‹åº¦é‡ã€‚æˆ‘ä»¬ä¸“æ³¨äºæ‰€æœ‰é›†æˆæˆå‘˜è¢«é¢„æœŸè¡¨ç°ç›¸åŒçš„æƒ…å†µï¼Œè¿™æ˜¯å‡ ç§æµè¡Œæ–¹æ³•ï¼ˆå¦‚éšæœºæ£®æ—æˆ–æ·±åº¦é›†æˆï¼‰çš„æƒ…å†µã€‚åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ‰ä¼šä¸€ç›´å˜å¾—æ›´å¥½ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé›†æˆçš„å¹³å‡æŸå¤±æ˜¯æ¨¡å‹æ•°é‡çš„å‡å‡½æ•°ã€‚å½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç³»åˆ—ç»“æœï¼Œå¯ä»¥æ€»ç»“ä¸ºï¼šå¥½æ¨¡å‹çš„é›†æˆä¼šå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆä¼šå˜å¾—æ›´ç³Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†å…³äºå°¾æ¦‚ç‡å•è°ƒæ€§çš„æ–°ç»“æœã€‚

    arXiv:2311.17885v2 Announce Type: replace-cross  Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabiliti
    
[^5]: åŸºäºå†…å®¹çš„è§†é¢‘æµåª’ä½“å¹³å°æ¨èå¼•æ“

    Content-based Recommendation Engine for Video Streaming Platform. (arXiv:2308.08406v1 [cs.IR])

    [http://arxiv.org/abs/2308.08406](http://arxiv.org/abs/2308.08406)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå†…å®¹çš„æ¨èå¼•æ“ï¼Œé€šè¿‡è®¡ç®—æ–‡æ¡£ä¸­å•è¯çš„ç›¸å…³æ€§å’Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ–¹æ³•æ¥æ¨èè§†é¢‘ç»™ç”¨æˆ·ã€‚åŒæ—¶ï¼Œè¿˜é€šè¿‡è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å¾—åˆ†æ¥è¯„ä¼°å¼•æ“çš„æ€§èƒ½ã€‚

    

    æ¨èå¼•æ“ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•å‘ç”¨æˆ·å»ºè®®å†…å®¹ã€äº§å“æˆ–æœåŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå†…å®¹çš„æ¨èå¼•æ“ï¼Œæ ¹æ®ç”¨æˆ·ä¹‹å‰çš„å…´è¶£å’Œé€‰æ‹©ï¼Œå‘ç”¨æˆ·æä¾›è§†é¢‘æ¨èã€‚æˆ‘ä»¬å°†ä½¿ç”¨TF-IDFæ–‡æœ¬å‘é‡åŒ–æ–¹æ³•æ¥ç¡®å®šæ–‡æ¡£ä¸­å•è¯çš„ç›¸å…³æ€§ã€‚ç„¶åé€šè¿‡è®¡ç®—å®ƒä»¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ‰¾å‡ºæ¯ä¸ªå†…å®¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æœ€åï¼Œæ ¹æ®å¾—åˆ°çš„ç›¸ä¼¼åº¦åˆ†æ•°å€¼ï¼Œå¼•æ“å°†å‘ç”¨æˆ·æ¨èè§†é¢‘ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å°†é€šè¿‡è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1å¾—åˆ†æ¥è¡¡é‡å¼•æ“çš„æ€§èƒ½ã€‚

    Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
    
[^6]: å¯¹é«˜é˜¶SDEæ¨¡æ‹Ÿçš„LÃ©vyåŒºåŸŸè¿›è¡Œç”Ÿæˆå»ºæ¨¡

    Generative Modelling of L\'{e}vy Area for High Order SDE Simulation. (arXiv:2308.02452v1 [stat.ML])

    [http://arxiv.org/abs/2308.02452](http://arxiv.org/abs/2308.02452)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹LÃ©vyGANï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ï¼Œè§£å†³äº†éé«˜æ–¯æ€§è´¨ä¸‹çš„æŠ½æ ·å›°éš¾é—®é¢˜ã€‚

    

    ä¼—æ‰€å‘¨çŸ¥ï¼Œå½“æ•°å€¼æ¨¡æ‹ŸSDEçš„è§£æ—¶ï¼Œè¦å®ç°å¼ºæ”¶æ•›é€Ÿç‡è¶…è¿‡O(\sqrt{h})ï¼ˆå…¶ä¸­hä¸ºæ­¥é•¿ï¼‰ï¼Œéœ€è¦ä½¿ç”¨æŸäº›å¸ƒæœ—è¿åŠ¨çš„è¿­ä»£ç§¯åˆ†ï¼Œé€šå¸¸ç§°ä¸ºå…¶â€œLÃ©vyåŒºåŸŸâ€ã€‚ç„¶è€Œï¼Œç”±äºå…¶éé«˜æ–¯æ€§è´¨ï¼Œå¯¹äºdç»´å¸ƒæœ—è¿åŠ¨ï¼ˆd>2ï¼‰ï¼Œç›®å‰æ²¡æœ‰å¿«é€Ÿè¿‘ä¼¼æŠ½æ ·ç®—æ³•ã€‚æœ¬æ–‡æå‡ºäº†LÃ©vyGANï¼Œä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ã€‚æˆ‘ä»¬çš„ç”Ÿæˆå™¨é‡‡ç”¨ç»è¿‡é‡èº«å®šåˆ¶çš„GNN-inspiredæ¶æ„ï¼Œå¼ºåˆ¶è¾“å‡ºåˆ†å¸ƒä¸æ¡ä»¶å˜é‡ä¹‹é—´çš„æ­£ç¡®ä¾èµ–ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†åŸºäºç‰¹å¾å‡½æ•°çš„æ•°å­¦åŸç†çš„åˆ¤åˆ«æ€§å½’ä¸€åŒ–æ“ä½œã€‚

    It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.  In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discrim
    
[^7]: æœºå™¨å­¦ä¹ ç”¨äºæœŸæƒå®šä»·ï¼šå¯¹ç½‘ç»œç»“æ„çš„å®è¯ç ”ç©¶

    Machine learning for option pricing: an empirical investigation of network architectures. (arXiv:2307.07657v1 [q-fin.CP])

    [http://arxiv.org/abs/2307.07657](http://arxiv.org/abs/2307.07657)

    å¹¿ä¹‰é«˜é€Ÿå…¬è·¯ç½‘ç»œç»“æ„åœ¨æœŸæƒå®šä»·é—®é¢˜ä¸­çš„åº”ç”¨è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚

    

    æœ¬æ–‡è€ƒè™‘äº†ä½¿ç”¨é€‚å½“çš„è¾“å…¥æ•°æ®ï¼ˆæ¨¡å‹å‚æ•°ï¼‰å’Œç›¸åº”è¾“å‡ºæ•°æ®ï¼ˆæœŸæƒä»·æ ¼æˆ–éšå«æ³¢åŠ¨ç‡ï¼‰æ¥å­¦ä¹ æœŸæƒä»·æ ¼æˆ–éšå«æ³¢åŠ¨ç‡çš„ç›‘ç£å­¦ä¹ é—®é¢˜ã€‚å¤§éƒ¨åˆ†ç›¸å…³æ–‡çŒ®éƒ½ä½¿ç”¨ï¼ˆæ™®é€šçš„ï¼‰å‰é¦ˆç¥ç»ç½‘ç»œç»“æ„æ¥è¿æ¥ç”¨äºå­¦ä¹ å°†è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºçš„ç¥ç»å…ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œå—åˆ°å›¾åƒåˆ†ç±»æ–¹æ³•å’Œç”¨äºåå¾®åˆ†æ–¹ç¨‹æœºå™¨å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶æ¥æ¢ç©¶ç½‘ç»œç»“æ„çš„é€‰æ‹©å¦‚ä½•å½±å“æœºå™¨å­¦ä¹ ç®—æ³•çš„ç²¾ç¡®åº¦å’Œè®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æœŸæƒå®šä»·é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨Black-Scholeså’ŒHestonæ¨¡å‹ï¼Œå¹¿ä¹‰é«˜é€Ÿå…¬è·¯ç½‘ç»œç»“æ„ç›¸è¾ƒäºå…¶ä»–å˜ä½“åœ¨å‡æ–¹è¯¯å·®å’Œè®­ç»ƒæ—¶é—´æ–¹é¢è¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼Œåœ¨è®¡ç®—éšå«æ³¢åŠ¨ç‡æ–¹é¢ï¼Œ

    We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that for option pricing problems, where we focus on the Black--Scholes and the Heston model, the generalized highway network architecture outperforms all other variants, when considering the mean squared error and the training time as criteria. Moreover, for the computation of the implied volatility, a
    
[^8]: HiGenï¼šå±‚æ¬¡å›¾ç”Ÿæˆç½‘ç»œ

    HiGen: Hierarchical Graph Generative Networks. (arXiv:2305.19337v1 [cs.LG])

    [http://arxiv.org/abs/2305.19337](http://arxiv.org/abs/2305.19337)

    HiGenæ˜¯ä¸€ç§æ–°é¢–çš„å›¾å½¢ç”Ÿæˆç½‘ç»œï¼Œèƒ½å¤Ÿä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ•æ‰å›¾å½¢çš„å±‚æ¬¡ç»“æ„ï¼Œå¹¶ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒæ¥ç”Ÿæˆå…·æœ‰æ•´æ•°å€¼çš„å­å›¾çš„è¾¹æƒã€‚å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å›¾å½¢çš„å±€éƒ¨å’Œå…¨å±€å±æ€§ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

    

    å¤§å¤šæ•°çœŸå®ä¸–ç•Œçš„å›¾è¡¨ç°å‡ºå±‚æ¬¡ç»“æ„ï¼Œè¿™é€šå¸¸è¢«ç°æœ‰çš„å›¾å½¢ç”Ÿæˆæ–¹æ³•æ‰€å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›¾å½¢ç”Ÿæˆç½‘ç»œï¼Œèƒ½å¤Ÿä»¥ç²—åˆ°ç»†çš„æ–¹å¼æ•æ‰å›¾å½¢çš„å±‚æ¬¡ç»“æ„å¹¶æˆåŠŸåœ°ç”Ÿæˆå›¾å½¢å­ç»“æ„ã€‚åœ¨æ¯ä¸ªå±‚æ¬¡ä¸Šï¼Œè¯¥æ¨¡å‹å¹¶è¡Œç”Ÿæˆç¤¾åŒºï¼Œä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å‹é¢„æµ‹ç¤¾åŒºä¹‹é—´çš„è·¨è¾¹ã€‚è¿™ç§æ¨¡å—åŒ–æ–¹æ³•ä½¿ç”Ÿæˆçš„å›¾å½¢ç½‘ç»œé«˜åº¦å¯æ‰©å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”¨å¤šé¡¹å¼åˆ†å¸ƒå»ºæ¨¡å±‚æ¬¡å›¾å½¢çš„è¾“å‡ºåˆ†å¸ƒï¼Œå¹¶é’ˆå¯¹æ­¤åˆ†å¸ƒæ¨å¯¼äº†é€’å½’åˆ†è§£ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆå…·æœ‰æ•´æ•°å€¼çš„å­å›¾çš„è¾¹æƒã€‚å®è¯ç ”ç©¶è¯æ˜ï¼Œæ‰€æå‡ºçš„ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å›¾å½¢çš„å±€éƒ¨å’Œå…¨å±€å±æ€§ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

    Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using a separate model. This modular approach results in a highly scalable graph generative network. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution, enabling us to generate sub-graphs with integer-valued edge weights in an autoregressive approach. Empirical studies demonstrate that the proposed generative model can effectively capture both local and global properties of graphs and achieves state-of-the-art performanc
    
[^9]: ç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„äº«ä¹ä»·æ ¼å’Œè´¨é‡è°ƒæ•´ä»·æ ¼æŒ‡æ•°

    Hedonic Prices and Quality Adjusted Price Indices Powered by AI. (arXiv:2305.00044v1 [econ.GN])

    [http://arxiv.org/abs/2305.00044](http://arxiv.org/abs/2305.00044)

    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨çš„ç»éªŒäº«ä¹æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤§é‡æœªç»“æ„åŒ–çš„äº§å“æ•°æ®ï¼Œå‡†ç¡®åœ°ä¼°è®¡äº§å“çš„äº«ä¹ä»·æ ¼å’Œæ´¾ç”ŸæŒ‡æ•°ã€‚

    

    åœ¨å½“ä»Šçš„ç»æµç¯å¢ƒä¸‹ï¼Œä½¿ç”¨ç”µå­è®°å½•å‡†ç¡®åœ°å®æ—¶æµ‹é‡ä»·æ ¼æŒ‡æ•°çš„å˜åŒ–å¯¹äºè·Ÿè¸ªé€šèƒ€å’Œç”Ÿäº§ç‡è‡³å…³é‡è¦ã€‚æœ¬æ–‡å¼€å‘äº†ç»éªŒäº«ä¹æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤§é‡æœªç»“æ„åŒ–çš„äº§å“æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€ä»·æ ¼å’Œæ•°é‡ï¼‰ï¼Œå¹¶è¾“å‡ºç²¾ç¡®çš„äº«ä¹ä»·æ ¼ä¼°è®¡å’Œæ´¾ç”ŸæŒ‡æ•°ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œä»æ–‡æœ¬æè¿°å’Œå›¾åƒä¸­ç”ŸæˆæŠ½è±¡çš„äº§å“å±æ€§æˆ–â€ç‰¹å¾â€œï¼Œç„¶åä½¿ç”¨è¿™äº›å±æ€§æ¥ä¼°ç®—äº«ä¹ä»·æ ¼å‡½æ•°ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºtransformersçš„å¤§å‹è¯­è¨€æ¨¡å‹å°†æœ‰å…³äº§å“çš„æ–‡æœ¬ä¿¡æ¯è½¬æ¢ä¸ºæ•°å­—ç‰¹å¾ï¼Œä½¿ç”¨è®­ç»ƒæˆ–å¾®è°ƒè¿‡çš„äº§å“æè¿°ä¿¡æ¯ï¼Œä½¿ç”¨æ®‹å·®ç½‘ç»œæ¨¡å‹å°†äº§å“å›¾åƒè½¬æ¢ä¸ºæ•°å­—ç‰¹å¾ã€‚ä¸ºäº†äº§ç”Ÿä¼°è®¡çš„äº«ä¹ä»·æ ¼å‡½æ•°ï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨å¤šä»»åŠ¡ç¥ç»ç½‘ç»œï¼Œè®­ç»ƒä»¥åœ¨æ‰€æœ‰æ—¶é—´æ®µåŒæ—¶é¢„æµ‹äº§å“çš„ä»·æ ¼ã€‚

    Accurate, real-time measurements of price index changes using electronic records are essential for tracking inflation and productivity in today's economic environment. We develop empirical hedonic models that can process large amounts of unstructured product data (text, images, prices, quantities) and output accurate hedonic price estimates and derived indices. To accomplish this, we generate abstract product attributes, or ``features,'' from text descriptions and images using deep neural networks, and then use these attributes to estimate the hedonic price function. Specifically, we convert textual information about the product to numeric features using large language models based on transformers, trained or fine-tuned using product descriptions, and convert the product image to numeric features using a residual network model. To produce the estimated hedonic price function, we again use a multi-task neural network trained to predict a product's price in all time periods simultaneousl
    
[^10]: é¢„æ¡ä»¶å¯¹è¶…å‚åŒ–ä½ç§©çŸ©é˜µæ„ŸçŸ¥çš„å½±å“

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚

    

    æœ¬æ–‡æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•æ¥è§£å†³ä½ç§©çŸ©é˜µæ„ŸçŸ¥ä¸­çŸ©é˜µå¯èƒ½ç—…æ€ä»¥åŠçœŸå®ç§©æœªçŸ¥çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…å‚å¼è¡¨ç¤ºï¼Œä»ä¸€ä¸ªå°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šå½¢å¼çš„é˜»å°¼é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™æ¥å¯¹æŠ—è¶…å‚åŒ–å’Œç—…æ€æ›²ç‡çš„å½±å“ã€‚ä¸åŸºå‡†æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ç›¸æ¯”ï¼Œå°½ç®¡é¢„å¤„ç†éœ€è¦è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œä½†ScaledGDï¼ˆğœ†ï¼‰åœ¨é¢å¯¹ç—…æ€é—®é¢˜æ—¶è¡¨ç°å‡ºäº†å‡ºè‰²çš„é²æ£’æ€§ã€‚åœ¨é«˜æ–¯è®¾è®¡ä¸‹ï¼ŒScaledGD($\lambda$) ä¼šåœ¨ä»…è¿­ä»£æ•°å¯¹æ•°çº§åˆ«çš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„ä½ç§©çŸ©é˜µã€‚

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    

