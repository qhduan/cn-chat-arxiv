# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Recurrent Transformers with Dynamic Halt](https://rss.arxiv.org/abs/2402.00976) | 本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。 |
| [^2] | [A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights](https://arxiv.org/abs/2404.00204) | 该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。 |
| [^3] | [The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access](https://arxiv.org/abs/2403.18846) | 提出了一种基于盲归一化斯坦变分梯度下降的检测器，用于解决智能大规模随机接入中的前导碰撞问题，并通过开发改进Hadamard变换和设计块MHT层来提高检测性能。 |
| [^4] | [Multi-Scale Texture Loss for CT denoising with GANs](https://arxiv.org/abs/2403.16640) | 该研究提出了一种利用灰度共生矩阵的多尺度纹理损失函数，以帮助生成对抗网络更好地捕捉复杂的图像关系。 |
| [^5] | [Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach](https://arxiv.org/abs/2403.16144) | 该研究应用循环神经网络方法，特别是LSTM模型，来预测受表面张力影响的流体流动中的能量预算。 |
| [^6] | [Useful Compact Representations for Data-Fitting](https://arxiv.org/abs/2403.12206) | 我们提出了新的紧凑表示方法，在大型确定性问题的软件实现中表现良好，特别适用于大型特征值计算、张量分解和非线性回归。 |
| [^7] | [FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update](https://arxiv.org/abs/2403.11464) | 提出了一种具有随机参数更新机制的个性化联邦学习方法，以解决资源受限设备在非iid数据场景下的性能下降问题。 |
| [^8] | ['One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification](https://arxiv.org/abs/2403.06402) | 本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。 |
| [^9] | [Transferable Reinforcement Learning via Generalized Occupancy Models](https://arxiv.org/abs/2403.06328) | 通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。 |
| [^10] | [Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation](https://arxiv.org/abs/2403.02302) | 本研究评估了多模态大型语言模型（MLLMs）在年龄和性别估计中的能力，对不同模型进行了比较，揭示了它们在特定任务上的优势和劣势。 |
| [^11] | [Q-FOX Learning: Breaking Tradition in Reinforcement Learning](https://arxiv.org/abs/2402.16562) | Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。 |
| [^12] | [Global Safe Sequential Learning via Efficient Knowledge Transfer](https://arxiv.org/abs/2402.14402) | 提出了考虑转移安全的全局顺序学习方法，以加速安全学习，并通过预先计算源组件来减少额外的计算负载。 |
| [^13] | [UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language](https://arxiv.org/abs/2402.13630) | UniGraph框架旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型 |
| [^14] | [GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence](https://arxiv.org/abs/2402.12566) | GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。 |
| [^15] | [Behind the Myth of Exploration in Policy Gradients](https://arxiv.org/abs/2402.00162) | 本论文提出了对政策梯度算法中探索项的新分析方法，区分了其平滑学习目标和增加梯度估计的两种不同作用。同时，详细讨论和实证了基于熵奖励的探索策略的局限性，并开辟了未来对这些策略设计和分析的研究方向。 |
| [^16] | [Selective Uncertainty Propagation in Offline RL](https://arxiv.org/abs/2302.00284) | 本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。 |
| [^17] | [Graph-level Representation Learning with Joint-Embedding Predictive Architectures.](http://arxiv.org/abs/2309.16014) | 本文提出了一种用于图级表示学习的联合嵌入预测架构（JEPA），通过预测输入图的不同子图的潜在表示在2维单位双曲线上的坐标，实现了对图级表示的有效建模。 |
| [^18] | [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature.](http://arxiv.org/abs/2308.12420) | 本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。 |
| [^19] | [PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models.](http://arxiv.org/abs/2307.09254) | 本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。 |
| [^20] | [MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters.](http://arxiv.org/abs/2307.07343) | 本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。 |
| [^21] | [Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning.](http://arxiv.org/abs/2306.14522) | 该论文提出了一种非凸随机Bregman近端梯度法（SBPG），它通过使用Bregman近似测度替代了随机梯度法中的上二次逼近，并在捕捉非Lipschitz梯度的非凸目标函数方面得到更好的近似模型。论文证明了SBPG的收敛性质，并提出了一种基于动量的改进版本，称为MSBPG，并证明了它具有更好的收敛性质。 |
| [^22] | [On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models.](http://arxiv.org/abs/2305.17583) | 本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。 |
| [^23] | [Conditional Generative Modeling is All You Need for Marked Temporal Point Processes.](http://arxiv.org/abs/2305.12569) | 本文提出了一种从标记时间点过程中提取其统计直觉的事件生成模型，通过条件生成器以历史观察作为输入，生成可能发生的高质量随后事件。该模型具有高效、灵活和表示能力等方面的优势。 |
| [^24] | [SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization.](http://arxiv.org/abs/2304.04169) | 本论文提出了一种利用慢查询点技术改进Local-SGD算法的方法，可以对抗异构情况下本地更新的偏差。 |
| [^25] | [Explainable Performance: Measuring the Driving Forces of Predictive Performance.](http://arxiv.org/abs/2212.05866) | XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。 |

# 详细

[^1]: 具有动态停止的循环Transformer

    Recurrent Transformers with Dynamic Halt

    [https://rss.arxiv.org/abs/2402.00976](https://rss.arxiv.org/abs/2402.00976)

    本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。

    

    本文研究了两种主要方法在增强Transformer与循环机制方面的归纳偏好——（1）类似于Universal Transformers的深度逐层循环方法；和（2）类似于Temporal Latent Bottleneck的分块时态循环方法。此外，我们提出并研究了扩展和组合上述方法的新方式，例如，我们提出了一种基于全局均值的Universal Transformer动态停止机制，并将Universal Transformer的元素融入到Temporal Latent Bottleneck中。我们通过多个诊断任务（如Long Range Arena（LRA），翻转-翻转语言建模，ListOps和逻辑推理）比较了模型并探索了它们的归纳偏好。

    In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
    
[^2]: 基于PPO的DRL自调PID非线性无人机控制器用于稳健自主飞行

    A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights

    [https://arxiv.org/abs/2404.00204](https://arxiv.org/abs/2404.00204)

    该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。

    

    该项目旨在通过将非线性深度强化学习（DRL）代理作为传统线性比例积分微分（PID）控制器的替代品，从而彻底改变无人机飞行控制。主要目标是在手动和自主模式之间实现无缝过渡，提高响应速度和稳定性。我们在Gazebo模拟器中利用近端策略优化（PPO）强化学习策略来训练DRL代理。添加20000美元的室内Vicon跟踪系统提供<1mm的定位精度，显着提高了自主飞行精度。为了在最短的无碰撞轨迹中导航无人机，我们还建立了一个三维A*路径规划器并成功地将其实施到实际飞行中。

    arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers <1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
    
[^3]: 基于盲归一化斯坦变分梯度下降的智能大规模随机接入检测

    The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access

    [https://arxiv.org/abs/2403.18846](https://arxiv.org/abs/2403.18846)

    提出了一种基于盲归一化斯坦变分梯度下降的检测器，用于解决智能大规模随机接入中的前导碰撞问题，并通过开发改进Hadamard变换和设计块MHT层来提高检测性能。

    

    缺乏高效的前导检测算法仍然是解决实际通信场景中智能大规模随机接入(RA)中前导碰撞问题的挑战。为解决这一问题，我们提出了一种新颖的基于最大似然估计(MLE)模型的早期前导检测方案，在授予式RA流程的第一步。提出了一种新颖的基于盲归一化斯坦变分梯度下降(SVGD)的检测器，以获得MLE模型的近似解。首先，通过探索Hadamard变换和小波变换之间的关系，开发了一种新的改进Hadamard变换(MHT)，使用二阶导数滤波器将高频分离出重要部分。接下来，为了消除SVGD检测器中的噪声并减轻梯度消失问题，设计了基于MHT的块MHT层，该层基于MHT、缩放层、软阈值层构建。

    arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
    
[^4]: 使用GAN进行CT去噪的多尺度纹理损失

    Multi-Scale Texture Loss for CT denoising with GANs

    [https://arxiv.org/abs/2403.16640](https://arxiv.org/abs/2403.16640)

    该研究提出了一种利用灰度共生矩阵的多尺度纹理损失函数，以帮助生成对抗网络更好地捕捉复杂的图像关系。

    

    生成对抗网络（GANs）已被证明在医学影像中的去噪应用中是一个强大的框架。然而，基于GAN的去噪算法仍然存在捕捉图像内复杂关系的局限性。为了在训练过程中掌握高度复杂和非线性的纹理关系，本文提出了一种利用灰度共生矩阵（GLCM）固有的多尺度性质的损失函数。尽管深度学习的最新进展在分类和检测任务中表现出优越性能，我们假设将其信息内容整合到GANs的训练中会是有价值的。因此，我们提出了适用于基于梯度优化的GLCM的可微分实现。

    arXiv:2403.16640v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimiza
    
[^5]: 预测液滴动力学中的能量预算：一种循环神经网络方法

    Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach

    [https://arxiv.org/abs/2403.16144](https://arxiv.org/abs/2403.16144)

    该研究应用循环神经网络方法，特别是LSTM模型，来预测受表面张力影响的流体流动中的能量预算。

    

    arXiv:2403.16144v1 公告类型: 交叉摘要: 流体力学中的神经网络为探索复杂流动（包括多相和自由表面流动）提供了一种高效方法。 循环神经网络，特别是长短期记忆（LSTM）模型，证明了对学习从瞬态输入到动态输出的映射具有吸引力。 本研究应用LSTM来预测受表面张力影响的流体流动的瞬态和静态输出。 具体而言，我们探索了两种不同的液滴动力学场景：具有不同初始形状的液滴与固体表面碰撞，以及两个液滴碰撞后凝聚。 仅使用无量纲数和来自数值模拟的几何时间序列数据，LSTM可以预测能量预算。 采用标记格点法前向跟踪方法结合标记格点法有限差分策略来模拟液滴动力学。 使用馈送循环神经网络（RNN）架构

    arXiv:2403.16144v1 Announce Type: cross  Abstract: Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed wit
    
[^6]: 用于数据拟合的实用紧凑表示

    Useful Compact Representations for Data-Fitting

    [https://arxiv.org/abs/2403.12206](https://arxiv.org/abs/2403.12206)

    我们提出了新的紧凑表示方法，在大型确定性问题的软件实现中表现良好，特别适用于大型特征值计算、张量分解和非线性回归。

    

    在没有2阶导数信息的最小化问题中，估计Hessian矩阵的方法非常有效。然而，传统技术生成的稠密矩阵对于大型问题是难以承受的。有限内存紧凑表示将稠密数组表示为低秩表示，已成为大型确定性问题软件实现的最新技术。我们开发了由向量选择参数化的新紧凑表示，并且对于特定选择，它们可以简化为现有的著名公式。我们展示了紧凑表示在大型特征值计算、张量分解和非线性回归中的有效性。

    arXiv:2403.12206v1 Announce Type: cross  Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.
    
[^7]: FedSPU：具有随机参数更新的资源受限设备个性化联邦学习

    FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update

    [https://arxiv.org/abs/2403.11464](https://arxiv.org/abs/2403.11464)

    提出了一种具有随机参数更新机制的个性化联邦学习方法，以解决资源受限设备在非iid数据场景下的性能下降问题。

    

    个性化的联邦学习（PFL）被广泛应用于物联网应用中，用于处理大量的非iid客户端数据，同时确保数据隐私。然而，客户拥有的异构边缘设备可能施加不同程度的资源约束，给PFL造成计算和通信瓶颈。联邦Dropout已成为应对这一挑战的一种流行策略，其中仅在客户端设备上训练全局模型的一个子模型，从而降低计算和通信开销。然而，基于Dropout的模型修剪策略可能引入偏差，特别是对非iid本地数据。当有偏见的子模型吸收来自其他客户端的高度分散参数时，性能下降是不可避免的。为了应对这一情况，我们提出了具有随机参数更新的联邦学习（FedSPU）。与专门为小型本地子模型定制全局模型的Dropout不同，FedSPU引入了个性化的随机参数更新机制，以在保持数据隐私的同时降低计算和通信开销。

    arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model
    
[^8]: 一刀切不适用：学习在文本分类中使用多少例为了改进上下文学习

    'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification

    [https://arxiv.org/abs/2403.06402](https://arxiv.org/abs/2403.06402)

    本文提出了自适应上下文学习（AICL）的工作流程，通过动态调整示例数量来提高文本分类的性能，类似于k最近邻（k-NN）中的可变大小邻域。

    

    arXiv:2403.06402v1 发表类型：新 Abstract: 自然语言处理（NLP）中的预测模型已经从从头训练模型发展到使用标记数据微调预训练模型。这种微调的极端形式涉及到上下文学习（ICL），其中一个预先训练的生成模型的输出（冻结的解码器参数）只受到输入字符串的变化（称为指令或提示）的控制。ICL的一个重要组成部分是在提示中使用少量标记数据实例作为示例。尽管现有工作在推理过程中为每个数据实例使用静态数量的示例，但在本文中，我们提出了一种动态调整示例数量的新方法。这类似于k最近邻（k-NN）分类器中使用可变大小邻域的方法。在我们提出的自适应ICL（AICL）的工作流程中，对于特定数据实例进行推理时使用的演示数量是动态调整的。

    arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
    
[^9]: 通过广义占有模型实现可迁移的强化学习

    Transferable Reinforcement Learning via Generalized Occupancy Models

    [https://arxiv.org/abs/2403.06328](https://arxiv.org/abs/2403.06328)

    通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。

    

    智能代理必须是通用的 - 具有快速适应和概括到不同任务的能力。在强化学习（RL）框架内，基于模型的RL算法学习世界的任务不可知动态模型，原则上使它们能够概括到任意奖励。然而，一步模型自然会受到累积错误的影响，使它们在具有长时间跨度和大状态空间的问题上失效。在这项工作中，我们提出了一类新型模型 - 广义占有模型（GOMs），保留了基于模型的RL的通用性，同时避免了累积性错误。GOMs的关键思想是在一个固定数据集的覆盖下，建模给定状态的所有可能长期结果的分布，以及实现给定状态的特定结果的策略。然后，这些模型可以迅速用于为任意新任务选择最优操作，而无需担心累积错误。

    arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
    
[^10]: 超越专业化：评估MLLMs在年龄和性别估计中的能力

    Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation

    [https://arxiv.org/abs/2403.02302](https://arxiv.org/abs/2403.02302)

    本研究评估了多模态大型语言模型（MLLMs）在年龄和性别估计中的能力，对不同模型进行了比较，揭示了它们在特定任务上的优势和劣势。

    

    最近，多模态大型语言模型（MLLMs）变得异常流行。像ChatGPT-4V和Gemini这样功能强大的商用模型，以及像LLaVA这样的开源模型，本质上都是通用模型，应用于解决各种各样的任务，包括计算机视觉中的任务。这些神经网络具有如此强大的通用知识和推理能力，以至于它们已被证明能够处理甚至未经专门训练的任务。我们将迄今为止最强大的MLLMs的能力进行了比较：ShareGPT4V、ChatGPT、LLaVA-Next 进行了专门任务的年龄和性别估计，与我们的最新专业化模型MiVOLO进行了比较。我们还更新了MiVOLO，并在本文中提供了详细信息和新的指标。这种比较产生了一些有趣的结果和关于参与模型的优点和缺点的见解。此外，我们尝试了各种微调方法

    arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
    
[^11]: Q-FOX学习：颠覆传统的强化学习

    Q-FOX Learning: Breaking Tradition in Reinforcement Learning

    [https://arxiv.org/abs/2402.16562](https://arxiv.org/abs/2402.16562)

    Q-FOX学习是一种新颖的自动超参数调整方法，结合了FOX优化器和Q-learning算法，提出了使用新的目标函数来解决强化学习中超参数调整的问题。

    

    强化学习（RL）是人工智能（AI）的一个子集，代理通过与环境的交互来学习最佳动作，因此适用于不需要标记数据或直接监督的任务。 本文提出了一种名为Q-FOX的新颖自动调参方法，该方法使用了FOX优化器和常用的易于实现的RL Q-learning算法解决了调参的问题。此外，还提出了一个新的目标函数，该函数将奖励放在均方误差（MSE）和学习时间之上。

    arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
    
[^12]: 全局安全顺序学习通过高效知识转移

    Global Safe Sequential Learning via Efficient Knowledge Transfer

    [https://arxiv.org/abs/2402.14402](https://arxiv.org/abs/2402.14402)

    提出了考虑转移安全的全局顺序学习方法，以加速安全学习，并通过预先计算源组件来减少额外的计算负载。

    

    arXiv:2402.14402v1 公告类型: 新摘要: 顺序学习方法例如主动学习和贝叶斯优化选择最具信息量的数据来学习一个任务。在许多医学或工程应用中，数据选择受先验未知的安全条件限制。一条有前途的安全学习方法利用高斯过程（GPs）来建模安全概率，并在具有较高安全置信度的区域中进行数据选择。然而，准确的安全建模需要先验知识或消耗数据。此外，安全置信度集中在给定的观测值周围，导致局部探索。由于在安全关键实验中通常存在可转移的源知识，我们提出考虑转移安全顺序学习来加速安全学习。我们进一步考虑先计算源组件，以减少引入源数据带来的额外计算负载。

    arXiv:2402.14402v1 Announce Type: new  Abstract: Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this pap
    
[^13]: UniGraph: 从自然语言中学习跨领域图基础模型

    UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language

    [https://arxiv.org/abs/2402.13630](https://arxiv.org/abs/2402.13630)

    UniGraph框架旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型

    

    arXiv:2402.13630v1 公告类型: 新摘要: ChatGPT 和 GPT-4 等基础模型已经彻底改变了人工智能，展示出在各种任务和应用中泛化的显著能力，超越了它们最初的训练目标。然而，当这个概念应用于图学习时，出现了鲜明的对比。图学习主要集中在针对特定任务或数据集定制的单个图模型上，缺乏将学到的知识转移到不同领域的能力。这种限制源于图结构的内在复杂性和多样性，以及特定于图数据的不同特征和标签空间。在本文中，我们提出了我们的UniGraph框架，旨在训练一个能够泛化到不同领域的未见图和任务的图基础模型。

    arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T
    
[^14]: GenAudit：利用证据修复语言模型输出中的事实错误

    GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence

    [https://arxiv.org/abs/2402.12566](https://arxiv.org/abs/2402.12566)

    GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。

    

    LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。

    arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
    
[^15]: 政策梯度探索背后的神话

    Behind the Myth of Exploration in Policy Gradients

    [https://arxiv.org/abs/2402.00162](https://arxiv.org/abs/2402.00162)

    本论文提出了对政策梯度算法中探索项的新分析方法，区分了其平滑学习目标和增加梯度估计的两种不同作用。同时，详细讨论和实证了基于熵奖励的探索策略的局限性，并开辟了未来对这些策略设计和分析的研究方向。

    

    政策梯度算法是解决具有连续状态和动作空间的控制问题的有效强化学习方法。为了计算接近最优的策略，在实践中必须在学习目标中包含探索项。尽管这些项的有效性通常通过对探索环境的内在需求进行证明，但我们提出了一种新的分析方法，区分了这些技术的两种不同含义。首先，它们使得平滑学习目标成为可能，并在保持全局最大值的同时消除了局部最优解。其次，它们修改了梯度估计，增加了随机参数更新最终提供最优策略的概率。基于这些效应，我们讨论并实证了基于熵奖励的探索策略，突出了其局限性，并为设计和分析这些策略的未来研究开辟了新方向。

    Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
    
[^16]: 选择性不确定性传播在离线强化学习中的应用

    Selective Uncertainty Propagation in Offline RL

    [https://arxiv.org/abs/2302.00284](https://arxiv.org/abs/2302.00284)

    本论文提出了一种名为选择性不确定性传播的方法，用于解决离线强化学习中的分布偏移问题。该方法通过自适应的方式建立置信区间，有效地处理了实际问题中策略学习的挑战。

    

    本研究考虑有限时间段离线强化学习的情景，目标在于应对动态规划算法中每一步策略学习的挑战。通过评估离开行为策略在第h步时的处理效果，就可以学习到这一步的策略。由于每一步策略都会影响下一状态的分布，相关的分布偏移问题使得这一问题在统计学上比随机情境挑战下的处理效果估计更加困难。然而，许多现实强化学习问题的难度介于这两种情境之间。我们开发了一种灵活且通用的方法，名为选择性不确定性传播，用于建立置信区间，并根据相关分布偏移问题的难度进行自适应。在玩具环境中展示了我们方法的优势，并证明了这些技术在离线策略学习中的好处。

    We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
    
[^17]: 用联合嵌入预测架构进行图级表示学习

    Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])

    [http://arxiv.org/abs/2309.16014](http://arxiv.org/abs/2309.16014)

    本文提出了一种用于图级表示学习的联合嵌入预测架构（JEPA），通过预测输入图的不同子图的潜在表示在2维单位双曲线上的坐标，实现了对图级表示的有效建模。

    

    联合嵌入预测架构（JEPAs）作为一种新颖而强大的自监督表示学习技术最近出现。它们旨在通过从上下文信号x中预测目标信号y的潜在表示来学习基于能量的模型。JEPAs绕过了对数据增强和负样本的需求，这通常是对比学习所要求的，同时避免了与生成式预训练相关的过拟合问题。在本文中，我们展示了该范式可以有效地对图级表示进行建模，并提出了Graph-JEPA，这是图领域的第一个JEPA。特别是，我们采用掩码建模的方式来学习输入图的不同子图的嵌入。为了赋予表示隐含的层次结构，我们设计了一种替代性的训练目标，该目标是预测编码子图在2维单位双曲线上的坐标。

    Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2
    
[^18]: ESG主导的DLT研究的演化：对文献进行NLP分析

    Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])

    [http://arxiv.org/abs/2308.12420](http://arxiv.org/abs/2308.12420)

    本研究通过NLP分析了ESG主导的DLT研究的演化，通过构建引用网络和命名实体识别任务，对DLT在ESG背景下的发展进行了文献综述。

    

    分布式账本技术(DLT)迅速发展，需要全面了解其各个组成部分。然而，针对DLT的环境、可持续性和治理(ESG)组成部分的系统文献综述还不足。为填补这一空白，我们选择了107篇种子文献，构建了一个包含63,083个参考文献的引用网络，并将其精炼为24,539篇文献的语料库进行分析。然后，我们根据一个已建立的技术分类法从46篇论文中标记了命名实体，并通过找出DLT的ESG要素来完善这个分类法。利用基于transformer的语言模型，我们对一个预先训练的语言模型进行了细化调整，用于命名实体识别任务，使用我们标记的数据集。我们利用我们调整后的语言模型对语料库进行了精简，得到了505篇关键论文，通过命名实体和时间图分析，促进了对DLT在ESG背景下的演化的文献综述。

    Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
    
[^19]: 用于量化生成式语言模型不确定性的PAC神经预测集学习

    PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])

    [http://arxiv.org/abs/2307.09254](http://arxiv.org/abs/2307.09254)

    本文提出了一种使用神经网络来量化生成式语言模型不确定性的PAC神经预测集学习方法，通过在多种语言数据集和模型上的实验证明，相比于标准基准方法，我们的方法平均提高了63％的量化不确定性。

    

    学习和量化模型的不确定性是增强模型可信度的关键任务。由于对生成虚构事实的担忧，最近兴起的生成式语言模型（GLM）特别强调可靠的不确定性量化的需求。本文提出了一种学习神经预测集模型的方法，该方法能够以可能近似正确（PAC）的方式量化GLM的不确定性。与现有的预测集模型通过标量值参数化不同，我们提出通过神经网络参数化预测集，实现更精确的不确定性量化，但仍满足PAC保证。通过在四种类型的语言数据集和六种类型的模型上展示，我们的方法相比标准基准方法平均提高了63％的量化不确定性。

    Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
    
[^20]: MaxMin-L2-SVC-NCH:一种用于训练支持向量分类器并选择模型参数的新方法

    MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])

    [http://arxiv.org/abs/2307.07343](http://arxiv.org/abs/2307.07343)

    本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。

    

    模型参数的选择在支持向量分类（SVC）的应用中发挥着重要作用。常用的选择模型参数的方法是k折交叉验证与格点搜索（CV）。由于需要训练大量的SVC模型，这个方法非常耗时。本文提出了一种新方法，用于训练SVC并选择模型参数。首先，将具有模型参数选择的SVC训练建模为极小化极大优化问题（MaxMin-L2-SVC-NCH），其中极小化问题是寻找两个正常凸壳之间最接近点的优化问题（L2-SVC-NCH），而极大化问题是寻找最优模型参数的优化问题。MaxMin-L2-SVC-NCH具有较低的时间复杂度，因为放弃了CV。然后，提出了一种基于梯度的算法来求解MaxMin-L2-SVC-NCH，其中L2-SVC-NCH通过投影梯度算法求解。

    The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
    
[^21]: 非凸随机 Bregman 近端梯度法及其在深度学习中的应用

    Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.14522](http://arxiv.org/abs/2306.14522)

    该论文提出了一种非凸随机Bregman近端梯度法（SBPG），它通过使用Bregman近似测度替代了随机梯度法中的上二次逼近，并在捕捉非Lipschitz梯度的非凸目标函数方面得到更好的近似模型。论文证明了SBPG的收敛性质，并提出了一种基于动量的改进版本，称为MSBPG，并证明了它具有更好的收敛性质。

    

    广泛使用的随机梯度方法用于最小化非凸复合目标函数时需要可微部分的Lipschitz平滑性, 但这一要求对于包括二次逆问题和训练神经网络在内的问题类别并不成立。为了解决这个问题，我们研究了一族随机 Bregman 近端梯度 (SBPG) 方法，这些方法只需要可微部分的平滑适应性。

    The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
    
[^22]: 关于神经网络作为无限树状概率图模型的论文研究

    On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])

    [http://arxiv.org/abs/2305.17583](http://arxiv.org/abs/2305.17583)

    本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    

    深度神经网络(DNNs)缺乏概率图模型(PGMs)的精确语义和明确定义的概率解释。本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决这个问题。我们的研究揭示了DNNs在前向传播期间确实执行PGM推断的近似，这与曾经的神经网络描述为核机器或无限大小的高斯过程的现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似。潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
    
[^23]: 有条件生成模型是标记时间点过程的必备工具。

    Conditional Generative Modeling is All You Need for Marked Temporal Point Processes. (arXiv:2305.12569v1 [stat.ML])

    [http://arxiv.org/abs/2305.12569](http://arxiv.org/abs/2305.12569)

    本文提出了一种从标记时间点过程中提取其统计直觉的事件生成模型，通过条件生成器以历史观察作为输入，生成可能发生的高质量随后事件。该模型具有高效、灵活和表示能力等方面的优势。

    

    近年来，生成建模的进步使得从上下文信息中生成高质量内容成为可能，但一个关键问题仍然存在：如何教模型知道何时生成内容？为了回答这个问题，本研究提出了一种新的事件生成模型，从标记时间点过程中提取其统计直觉，并提供了一个干净、灵活和计算效率高的解决方案，适用于涉及多维标记的各种应用。我们旨在捕捉点过程的分布而不需明确指定条件强度或概率密度。我们使用一个条件生成器，以事件历史为输入并生成在先前观察到的事件下，可能发生的高质量随后事件。所提出的框架提供了一系列利益，包括在学习模型和生成样本方面的异常效率以及相当大的表示能力来捕捉。

    Recent advancements in generative modeling have made it possible to generate high-quality content from context information, but a key question remains: how to teach models to know when to generate content? To answer this question, this study proposes a novel event generative model that draws its statistical intuition from marked temporal point processes, and offers a clean, flexible, and computationally efficient solution for a wide range of applications involving multi-dimensional marks. We aim to capture the distribution of the point process without explicitly specifying the conditional intensity or probability density. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including exceptional efficiency in learning the model and generating samples, as well as considerable representational power to capture
    
[^24]: SLowcal-SGD：慢查询点提高了随机凸优化的Local-SGD算法（arXiv:2304.04169v1 [cs.LG]）

    SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization. (arXiv:2304.04169v1 [cs.LG])

    [http://arxiv.org/abs/2304.04169](http://arxiv.org/abs/2304.04169)

    本论文提出了一种利用慢查询点技术改进Local-SGD算法的方法，可以对抗异构情况下本地更新的偏差。

    

    在分布式学习场景中，M台计算机与参数服务器交互，通过多个通信轮次来最小化联合目标函数。我们关注异构情况，即不同计算机可能从不同的数据分布中抽取样本，设计了第一种可以证明优于两个最突出的分布式基准线——Minibatch-SGD和Local-SGD的本地更新方法。我们方法的关键在于慢查询技术，我们将其定制给分布式设置使用，从而更好地减轻本地更新带来的偏差。

    We consider distributed learning scenarios where M machines interact with a parameter server along several communication rounds in order to minimize a joint objective function. Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approach is a slow querying technique that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates.
    
[^25]: 可解释的性能：衡量预测性能的驱动力

    Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.05866](http://arxiv.org/abs/2212.05866)

    XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。

    

    我们引入了XPER（eXplainable PERformance）方法来衡量输入特征对模型预测性能的具体贡献。我们的方法在理论上基于Shapley值，既不依赖于模型，也不依赖于性能度量。此外，XPER可在模型级别或个体级别实现。我们证明XPER具有标准解释性方法（SHAP）的特殊情况。在贷款违约预测应用中，我们展示了如何利用XPER处理异质性问题，并显著提高样本外性能。为此，我们通过基于个体XPER值对他们进行聚类来构建同质化的个体群体。我们发现估计群体特定的模型比一个模型适用于所有个体具有更高的预测精度。

    We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
    

