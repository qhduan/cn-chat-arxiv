<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38598;&#21512;&#20540;&#39044;&#27979;&#21644;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#20013;&#30340;&#37096;&#20998;&#35782;&#21035;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35268;&#33539;&#27491;&#30830;&#21644;&#38169;&#35823;&#26102;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2401.11046</link><description>&lt;p&gt;
&#20855;&#26377;&#38598;&#21512;&#20540;&#39044;&#27979;&#21644;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#20449;&#24687;&#30340;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Information Based Inference in Models with Set-Valued Predictions and Misspecification. (arXiv:2401.11046v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38598;&#21512;&#20540;&#39044;&#27979;&#21644;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#27169;&#22411;&#20013;&#30340;&#37096;&#20998;&#35782;&#21035;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35268;&#33539;&#27491;&#30830;&#21644;&#38169;&#35823;&#26102;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#23436;&#25972;&#27169;&#22411;&#20013;&#37096;&#20998;&#35782;&#21035;&#30340;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35268;&#33539;&#27491;&#30830;&#21644;&#38169;&#35823;&#26102;&#22343;&#26377;&#25928;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65306;&#65288;i&#65289;&#22522;&#20110;&#26368;&#23567;&#21270;&#36866;&#24403;&#23450;&#20041;&#30340;Kullback-Leibler&#20449;&#24687;&#20934;&#21017;&#65292;&#32771;&#34385;&#27169;&#22411;&#30340;&#19981;&#23436;&#25972;&#24615;&#65292;&#24182;&#25552;&#20379;&#38750;&#31354;&#20266;&#30495;&#38598;&#65307;&#65288;ii&#65289;&#35745;&#31639;&#21487;&#34892;&#65307;&#65288;iii&#65289;&#26080;&#35770;&#27169;&#22411;&#35268;&#33539;&#27491;&#30830;&#19982;&#21542;&#65292;&#23454;&#29616;&#26041;&#27861;&#30456;&#21516;&#65307;&#65288;iv&#65289;&#21033;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#21327;&#21464;&#37327;&#30340;&#21464;&#24322;&#25552;&#20379;&#30340;&#25152;&#26377;&#20449;&#24687;&#65307;&#65288;v&#65289;&#20381;&#36182;&#20110;Rao&#30340;&#35780;&#20998;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#22522;&#26412;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an information-based inference method for partially identified parameters in incomplete models that is valid both when the model is correctly specified and when it is misspecified. Key features of the method are: (i) it is based on minimizing a suitably defined Kullback-Leibler information criterion that accounts for incompleteness of the model and delivers a non-empty pseudo-true set; (ii) it is computationally tractable; (iii) its implementation is the same for both correctly and incorrectly specified models; (iv) it exploits all information provided by variation in discrete and continuous covariates; (v) it relies on Rao's score statistic, which is shown to be asymptotically pivotal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.16158</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#36130;&#25253;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact. (arXiv:2303.16158v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#27604;&#20154;&#31867;&#26356;&#20026;&#20934;&#30830;&#12290;&#20294;&#26159;&#65292;&#25991;&#29486;&#24182;&#26410;&#27979;&#35797;&#31639;&#27861;&#39044;&#27979;&#26159;&#21542;&#26356;&#20026;&#29702;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;&#31639;&#27861;&#65288;&#21253;&#25324;&#32447;&#24615;&#22238;&#24402;&#21644;&#19968;&#31181;&#21517;&#20026;Gradient Boosted Regression Trees&#30340;&#27969;&#34892;&#31639;&#27861;&#65289;&#23545;&#20110;&#20844;&#21496;&#30408;&#21033;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;GBRT&#24179;&#22343;&#32988;&#36807;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#31867;&#32929;&#24066;&#20998;&#26512;&#24072;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#19988;&#26080;&#27861;&#28385;&#36275;&#29702;&#24615;&#39044;&#26399;&#26631;&#20934;&#12290;&#36890;&#36807;&#38477;&#20302;&#23398;&#20064;&#29575;&#65292;&#21487;&#26368;&#23567;&#31243;&#24230;&#19978;&#20943;&#23569;&#36807;&#24230;&#21453;&#24212;&#31243;&#24230;&#65292;&#20294;&#36825;&#20250;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#36807;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#27604;&#20256;&#32479;&#35757;&#32451;&#30340;&#20998;&#26512;&#24072;&#20135;&#29983;&#30340;&#36807;&#24230;&#21453;&#24212;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#32929;&#24066;&#20998;&#26512;&#24072;&#30340;&#39044;&#27979;&#21453;&#26144;&#20986;&#26426;&#22120;&#31639;&#27861;&#27809;&#26377;&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable evidence that machine learning algorithms have better predictive abilities than humans in various financial settings. But, the literature has not tested whether these algorithmic predictions are more rational than human predictions. We study the predictions of corporate earnings from several algorithms, notably linear regressions and a popular algorithm called Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformed both linear regressions and human stock analysts, but it still overreacted to news and did not satisfy rational expectation as normally defined. By reducing the learning rate, the magnitude of overreaction can be minimized, but it comes with the cost of poorer out-of-sample prediction accuracy. Human stock analysts who have been trained in machine learning methods overreact less than traditionally trained analysts. Additionally, stock analyst predictions reflect information not otherwise available to machine algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20309;&#28151;&#21512;&#26465;&#20214;&#19979;&#30340;&#20999;&#25442;&#35797;&#39564;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#22312;&#35813;&#35774;&#32622;&#19979;&#65292;&#26631;&#20934;&#30340;&#20999;&#25442;&#35774;&#35745;&#21463;&#21040;&#20102;&#24310;&#32493;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#36890;&#36807;&#35880;&#24910;&#20351;&#29992;&#21021;&#22987;&#29123;&#28903;&#26399;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24773;&#20917;&#65292;&#23454;&#29616;&#35823;&#24046;&#20197;&#26356;&#24930;&#30340;&#36895;&#24230;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2209.00197</link><description>&lt;p&gt;
&#20960;&#20309;&#28151;&#21512;&#26465;&#20214;&#19979;&#30340;&#20999;&#25442;&#35797;&#39564;
&lt;/p&gt;
&lt;p&gt;
Switchback Experiments under Geometric Mixing. (arXiv:2209.00197v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20309;&#28151;&#21512;&#26465;&#20214;&#19979;&#30340;&#20999;&#25442;&#35797;&#39564;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#22312;&#35813;&#35774;&#32622;&#19979;&#65292;&#26631;&#20934;&#30340;&#20999;&#25442;&#35774;&#35745;&#21463;&#21040;&#20102;&#24310;&#32493;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#36890;&#36807;&#35880;&#24910;&#20351;&#29992;&#21021;&#22987;&#29123;&#28903;&#26399;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24773;&#20917;&#65292;&#23454;&#29616;&#35823;&#24046;&#20197;&#26356;&#24930;&#30340;&#36895;&#24230;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#25442;&#35797;&#39564;&#26159;&#19968;&#31181;&#36890;&#36807;&#21453;&#22797;&#23545;&#25972;&#20010;&#31995;&#32479;&#24320;&#21551;&#21644;&#20851;&#38381;&#24178;&#39044;&#26469;&#27979;&#37327;&#27835;&#30103;&#25928;&#26524;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#20999;&#25442;&#35797;&#39564;&#26159;&#20811;&#26381;&#21333;&#20803;&#38388;&#28322;&#20986;&#25928;&#24212;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#24310;&#32493;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#20960;&#20309;&#28151;&#21512;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#30340;&#20999;&#25442;&#35797;&#39564;&#24615;&#36136;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#30340;&#20999;&#25442;&#35774;&#35745;&#22312;&#24310;&#32493;&#20559;&#24046;&#26041;&#38754;&#21463;&#21040;&#20102;&#36739;&#22823;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#30340;&#20272;&#35745;&#35823;&#24046;&#38543;&#30528;&#23454;&#39564;&#26102;&#38388;&#36328;&#24230;$T$&#30340;&#22686;&#21152;&#32780;&#34928;&#20943;&#20026;$T^{-1/3}$&#65292;&#32780;&#22312;&#27809;&#26377;&#24310;&#32493;&#25928;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#24555;&#30340;$T^{-1/2}$&#34928;&#20943;&#36895;&#24230;&#26159;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35880;&#24910;&#20351;&#29992;&#21021;&#22987;&#29123;&#28903;&#26399;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#24773;&#20917;&#65292;&#24182;&#19988;&#23454;&#29616;&#35823;&#24046;&#20197;$\log(T)^{1/2}T^{-1/2}$&#30340;&#36895;&#24230;&#34928;&#20943;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#32467;&#26524;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The switchback is an experimental design that measures treatment effects by repeatedly turning an intervention on and off for a whole system. Switchback experiments are a robust way to overcome cross-unit spillover effects; however, they are vulnerable to bias from temporal carryovers. In this paper, we consider properties of switchback experiments in Markovian systems that mix at a geometric rate. We find that, in this setting, standard switchback designs suffer considerably from carryover bias: Their estimation error decays as $T^{-1/3}$ in terms of the experiment horizon $T$, whereas in the absence of carryovers a faster rate of $T^{-1/2}$ would have been possible. We also show, however, that judicious use of burn-in periods can considerably improve the situation, and enables errors that decay as $\log(T)^{1/2}T^{-1/2}$. Our formal results are mirrored in an empirical evaluation.
&lt;/p&gt;</description></item></channel></rss>