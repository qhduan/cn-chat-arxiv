<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#30340;&#36873;&#25321;&#21487;&#20197;&#34987;&#36275;&#22815;&#19981;&#30456;&#20851;&#30340;&#20559;&#22909;&#30340;&#38750;&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#25152;&#20195;&#34920;&#65292;&#21363;&#38750;&#29702;&#24615;RUM&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;RUMs&#21487;&#20197;&#36890;&#36807;&#19968;&#37096;&#20998;&#20915;&#31574;&#32773;&#26159;&#38750;&#29702;&#24615;&#30340;&#32676;&#20307;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#20182;&#20204;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#26159;&#19981;&#21463;&#38480;&#21046;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.10208</link><description>&lt;p&gt;
&#38750;&#29702;&#24615;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Irrational Random Utility Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10208
&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#30340;&#36873;&#25321;&#21487;&#20197;&#34987;&#36275;&#22815;&#19981;&#30456;&#20851;&#30340;&#20559;&#22909;&#30340;&#38750;&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#25152;&#20195;&#34920;&#65292;&#21363;&#38750;&#29702;&#24615;RUM&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;RUMs&#21487;&#20197;&#36890;&#36807;&#19968;&#37096;&#20998;&#20915;&#31574;&#32773;&#26159;&#38750;&#29702;&#24615;&#30340;&#32676;&#20307;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#20182;&#20204;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#26159;&#19981;&#21463;&#38480;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#30340;&#38598;&#21512;&#36873;&#25321; - &#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65288;RUMs&#65289; - &#21482;&#26377;&#24403;&#20182;&#20204;&#30340;&#20559;&#22909;&#36275;&#22815;&#19981;&#30456;&#20851;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#38750;&#29702;&#24615;&#20915;&#31574;&#32773;&#32676;&#20307;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#34920;&#31034;&#20026;&#65306;&#38750;&#29702;&#24615;RUM&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20960;&#20046;&#25152;&#26377;&#30340;RUMs&#37117;&#21487;&#20197;&#36890;&#36807;&#33267;&#23569;&#19968;&#37096;&#20998;&#20915;&#31574;&#32773;&#26159;&#38750;&#29702;&#24615;&#20915;&#31574;&#32773;&#30340;&#32676;&#20307;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#20182;&#20204;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#26159;&#19981;&#21463;&#38480;&#21046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10208v1 Announce Type: new  Abstract: We show that the set of aggregate choices of a population of rational decision-makers - random utility models (RUMs) - can be represented by a population of irrational ones if, and only if, their preferences are sufficiently uncorrelated. We call this representation: Irrational RUM. We then show that almost all RUMs can be represented by a population in which at least some decision-makers are irrational and that under specific conditions their irrational behavior is unconstrained.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#20915;&#31574;&#32773;&#36890;&#36807;&#24809;&#32602;&#26469;&#38450;&#27490;&#22312;&#29305;&#23450;&#20154;&#32676;&#20013;&#30340;&#19981;&#20844;&#24179;&#32467;&#26524;&#20998;&#24067;&#65292;&#35813;&#26694;&#26550;&#23545;&#30446;&#26631;&#20989;&#25968;&#21644;&#27495;&#35270;&#24230;&#37327;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#20855;&#22791;&#36951;&#25022;&#21644;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2401.17909</link><description>&lt;p&gt;
&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#20013;&#27491;&#21017;&#21270;&#27495;&#35270;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regularizing Discrimination in Optimal Policy Learning with Distributional Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17909
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#20915;&#31574;&#32773;&#36890;&#36807;&#24809;&#32602;&#26469;&#38450;&#27490;&#22312;&#29305;&#23450;&#20154;&#32676;&#20013;&#30340;&#19981;&#20844;&#24179;&#32467;&#26524;&#20998;&#24067;&#65292;&#35813;&#26694;&#26550;&#23545;&#30446;&#26631;&#20989;&#25968;&#21644;&#27495;&#35270;&#24230;&#37327;&#20855;&#26377;&#24456;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#20855;&#22791;&#36951;&#25022;&#21644;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#36890;&#24120;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#27835;&#30103;&#30340;&#30456;&#23545;&#25928;&#26524;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#23454;&#26045;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#26681;&#25454;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#39044;&#27979;&#20102;&#8220;&#26368;&#20248;&#8221;&#32467;&#26524;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24847;&#35782;&#21040;&#27495;&#35270;&#38382;&#39064;&#30340;&#20915;&#31574;&#32773;&#21487;&#33021;&#19981;&#28385;&#24847;&#20197;&#20005;&#37325;&#27495;&#35270;&#20154;&#32676;&#23376;&#32452;&#30340;&#20195;&#20215;&#26469;&#23454;&#29616;&#35813;&#20248;&#21270;&#65292;&#21363;&#22312;&#23376;&#32452;&#20013;&#30340;&#32467;&#26524;&#20998;&#24067;&#26126;&#26174;&#20559;&#31163;&#25972;&#20307;&#26368;&#20248;&#32467;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#20915;&#31574;&#32773;&#24809;&#32602;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#30446;&#26631;&#20989;&#25968;&#21644;&#27495;&#35270;&#24230;&#37327;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#25968;&#25454;&#39537;&#21160;&#35843;&#21442;&#30340;&#32463;&#39564;&#25104;&#21151;&#31574;&#30053;&#24314;&#31435;&#20102;&#36951;&#25022;&#21644;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#23454;&#35777;&#22330;&#26223;&#36827;&#34892;&#20102;&#31616;&#35201;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
A decision maker typically (i) incorporates training data to learn about the relative effectiveness of the treatments, and (ii) chooses an implementation mechanism that implies an "optimal" predicted outcome distribution according to some target functional. Nevertheless, a discrimination-aware decision maker may not be satisfied achieving said optimality at the cost of heavily discriminating against subgroups of the population, in the sense that the outcome distribution in a subgroup deviates strongly from the overall optimal outcome distribution. We study a framework that allows the decision maker to penalize for such deviations, while allowing for a wide range of target functionals and discrimination measures to be employed. We establish regret and consistency guarantees for empirical success policies with data-driven tuning parameters, and provide numerical results. Furthermore, we briefly illustrate the methods in two empirical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#27169;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#20869;&#22312;&#38556;&#30861;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#65292;&#22686;&#21152;&#20934;&#30830;&#24615;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#12290;&#36890;&#36807;&#23558;&#23398;&#20064;&#30340;&#38556;&#30861;&#23450;&#20041;&#20026;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#31283;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#27700;&#24179;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;limitQR&#22343;&#34913;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#31574;&#30053;&#38480;&#21046;&#22312;&#20943;&#23569;&#25110;&#22686;&#21152;&#23398;&#20064;&#38556;&#30861;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.16904</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#20869;&#22312;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Endogenous Barriers to Learning. (arXiv:2306.16904v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#27169;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#20869;&#22312;&#38556;&#30861;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#65292;&#22686;&#21152;&#20934;&#30830;&#24615;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#12290;&#36890;&#36807;&#23558;&#23398;&#20064;&#30340;&#38556;&#30861;&#23450;&#20041;&#20026;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#31283;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#27700;&#24179;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;limitQR&#22343;&#34913;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#31574;&#30053;&#38480;&#21046;&#22312;&#20943;&#23569;&#25110;&#22686;&#21152;&#23398;&#20064;&#38556;&#30861;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#22312;&#20110;&#32570;&#20047;&#32463;&#39564;&#20250;&#23548;&#33268;&#38169;&#35823;&#65292;&#32780;&#32463;&#39564;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#38169;&#35823;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#27169;&#22411;&#26469;&#24314;&#27169;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#23558;&#20854;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#35774;&#23450;&#20026;&#20869;&#29983;&#21464;&#37327;&#12290;&#22312;&#26576;&#20123;&#28216;&#25103;&#20013;&#65292;&#22686;&#21152;&#20934;&#30830;&#24615;&#26377;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;&#23398;&#20064;&#30340;&#38556;&#30861;&#23450;&#20041;&#20026;&#20445;&#25345;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#31283;&#23450;&#25152;&#38656;&#30340;&#26368;&#23567;&#22122;&#22768;&#27700;&#24179;&#12290;&#20351;&#29992;&#36923;&#36753;&#37327;&#21270;&#21709;&#24212;&#65292;&#36825;&#23450;&#20041;&#20102;&#19968;&#20010;limitQR&#22343;&#34913;&#12290;&#25105;&#20204;&#23558;&#35813;&#27010;&#24565;&#24212;&#29992;&#20110;&#34568;&#34467;&#12289;&#26053;&#34892;&#32773;&#22256;&#22659;&#21644;11-20&#38065;&#27714;&#28216;&#25103;&#65292;&#20197;&#21450;&#19968;&#20215;&#21644;&#20840;&#20184;&#25293;&#21334;&#65292;&#24182;&#35752;&#35770;&#31574;&#30053;&#38480;&#21046;&#22312;&#20943;&#23569;&#25110;&#22686;&#21152;&#23398;&#20064;&#38556;&#30861;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the idea that lack of experience is a source of errors but that experience should reduce them, we model agents' behavior using a stochastic choice model, leaving endogenous the accuracy of their choice. In some games, increased accuracy is conducive to unstable best-response dynamics. We define the barrier to learning as the minimum level of noise which keeps the best-response dynamic stable. Using logit Quantal Response, this defines a limitQR Equilibrium. We apply the concept to centipede, travelers' dilemma, and 11-20 money-request games and to first-price and all-pay auctions, and discuss the role of strategy restrictions in reducing or amplifying barriers to learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12647</link><description>&lt;p&gt;
&#22522;&#20110;Q&#30340;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Q-based Equilibria. (arXiv:2304.12647v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;Q&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21017;&#65292;&#20854;&#20026;&#27599;&#20010;&#26367;&#20195;&#26041;&#26696;&#25552;&#20379;&#20272;&#35745;&#20540;(&#21363;Q&#20540;)&#65292;&#35813;&#20540;&#19982;&#20043;&#21069;&#30340;&#20915;&#31574;&#30456;&#20851;&#12290;&#19968;&#20010;&#26420;&#32032;&#30340;&#31574;&#30053;&#26159;&#22987;&#32456;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;Q&#20540;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#26063;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#31995;&#32479;&#22320;&#25903;&#25345;&#26576;&#20123;&#26367;&#20195;&#26041;&#26696;&#32780;&#19981;&#26159;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#21253;&#21547;&#26377;&#21033;&#21512;&#20316;&#30340;&#23485;&#23481;&#20559;&#24046;&#30340;&#35268;&#21017;&#12290;&#22312; Compte &#21644; Postlewaite [2018] &#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010; Q-based &#35268;&#21017;&#26063;&#20013;&#23547;&#25214;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#32463;&#20856;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic environments, Q-learning is an adaptative rule that provides an estimate (a Q-value) of the continuation value associated with each alternative. A naive policy consists in always choosing the alternative with highest Q-value. We consider a family of Q-based policy rules that may systematically favor some alternatives over others, for example rules that incorporate a leniency bias that favors cooperation. In the spirit of Compte and Postlewaite [2018], we look for equilibrium biases (or Qb-equilibria) within this family of Q-based rules. We examine classic games under various monitoring technologies.
&lt;/p&gt;</description></item></channel></rss>