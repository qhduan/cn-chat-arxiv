<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21453;&#21333;&#35843;&#20998;&#37197;&#26469;&#26368;&#20248;&#21270;&#20849;&#20139;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#24403;&#25152;&#26377;&#20195;&#29702;&#37117;&#39118;&#38505;&#36861;&#27714;&#26102;&#65292;&#24085;&#32047;&#25176;&#26368;&#20248;&#20998;&#37197;&#24517;&#39035;&#26159;&#22823;&#22870;&#20998;&#37197;&#65307;&#24403;&#25152;&#26377;&#20195;&#29702;&#30340;&#25928;&#29992;&#20989;&#25968;&#19981;&#36830;&#32493;&#26102;&#65292;&#26367;&#32618;&#32650;&#20998;&#37197;&#20351;&#24471;&#36229;&#36807;&#19981;&#36830;&#32493;&#38408;&#20540;&#30340;&#27010;&#29575;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.03328</link><description>&lt;p&gt;
&#36127;&#30456;&#20851;&#30340;&#26368;&#20248;&#39118;&#38505;&#20849;&#25285;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Negatively dependent optimal risk sharing. (arXiv:2401.03328v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21453;&#21333;&#35843;&#20998;&#37197;&#26469;&#26368;&#20248;&#21270;&#20849;&#20139;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#24403;&#25152;&#26377;&#20195;&#29702;&#37117;&#39118;&#38505;&#36861;&#27714;&#26102;&#65292;&#24085;&#32047;&#25176;&#26368;&#20248;&#20998;&#37197;&#24517;&#39035;&#26159;&#22823;&#22870;&#20998;&#37197;&#65307;&#24403;&#25152;&#26377;&#20195;&#29702;&#30340;&#25928;&#29992;&#20989;&#25968;&#19981;&#36830;&#32493;&#26102;&#65292;&#26367;&#32618;&#32650;&#20998;&#37197;&#20351;&#24471;&#36229;&#36807;&#19981;&#36830;&#32493;&#38408;&#20540;&#30340;&#27010;&#29575;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#34920;&#29616;&#20986;&#21453;&#21333;&#35843;&#24615;&#30340;&#20998;&#37197;&#26041;&#24335;&#26469;&#20248;&#21270;&#20849;&#20139;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#21453;&#21333;&#35843;&#20998;&#37197;&#30340;&#24418;&#24335;&#26377;&#8220;&#36194;&#32773;&#36890;&#21507;&#8221;&#25110;&#8220;&#36755;&#32773;&#20840;&#20891;&#35206;&#27809;&#8221;&#22411;&#24425;&#31080;&#65292;&#25105;&#20204;&#20998;&#21035;&#23558;&#20854;&#24402;&#20026;&#26631;&#20934;&#21270;&#30340;&#8220;&#22823;&#22870;&#8221;&#25110;&#8220;&#26367;&#32618;&#32650;&#8221;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#23450;&#29702;&#8212;&#8212;&#21453;&#21333;&#35843;&#25913;&#36827;&#23450;&#29702;&#65292;&#35828;&#26126;&#23545;&#20110;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#20840;&#37096;&#19979;&#30028;&#26377;&#30028;&#36824;&#26159;&#20840;&#37096;&#19978;&#30028;&#26377;&#30028;&#65292;&#24635;&#26159;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#21453;&#21333;&#35843;&#38543;&#26426;&#21464;&#37327;&#65292;&#20854;&#20013;&#27599;&#20010;&#20998;&#37327;&#37117;&#22823;&#20110;&#25110;&#31561;&#20110;&#20984;&#24207;&#20013;&#23545;&#24212;&#30340;&#20998;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#24085;&#32047;&#25176;&#26368;&#20248;&#20998;&#37197;&#23384;&#22312;&#19988;&#25152;&#26377;&#20195;&#29702;&#37117;&#36861;&#27714;&#39118;&#38505;&#65292;&#37027;&#20040;&#23427;&#20204;&#24517;&#39035;&#26159;&#22823;&#22870;&#20998;&#37197;&#12290;&#32780;&#24403;&#25152;&#26377;&#20195;&#29702;&#30340;&#19981;&#36830;&#32493;&#20271;&#21162;&#21033;&#25928;&#29992;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#30456;&#21453;&#30340;&#32467;&#35770;&#65292;&#26367;&#32618;&#32650;&#20998;&#37197;&#20351;&#24471;&#36229;&#36807;&#19981;&#36830;&#32493;&#38408;&#20540;&#30340;&#27010;&#29575;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either "winner-takes-all" lotteries or "loser-loses-all" lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also c
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.08854</link><description>&lt;p&gt;
&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08854
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#21644;&#25512;&#26029;&#65307;&#21363;&#22312;&#27599;&#20010;&#26102;&#26399;&#65288;&#20063;&#31216;&#20026;&#24773;&#33410;&#65289;&#20197;&#39034;&#24207;&#26041;&#24335;&#19982;&#21333;&#20010;&#21463;&#35797;&#21333;&#20803;&#22810;&#27425;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#21518;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#24182;&#20272;&#35745;&#32467;&#26500;&#21442;&#25968;&#65292;&#22914;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20449;&#29992;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#31532;&#19968;&#20010;&#26102;&#26399;&#30340;&#34892;&#21160;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#65289;&#12290;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#21487;&#20197;&#26500;&#25104;&#30697;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#19981;&#26159;&#24635;&#20307;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#22312;&#38745;&#24577;&#25968;&#25454;&#24773;&#20917;&#19979;&#23548;&#33268;&#20102;$Z$-&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20272;&#35745;&#37327;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#19981;&#33021;&#28176;&#36817;&#27491;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#36825;&#26159;&#30001;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
&lt;/p&gt;</description></item></channel></rss>