<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21270;&#21516;&#26102;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#21306;&#38388;&#30340;&#19978;&#19979;&#30028;&#25910;&#32553;&#20026;&#38646;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#19981;&#37325;&#35201;&#30340;&#21327;&#21464;&#37327;&#24182;&#23558;&#20854;&#25490;&#38500;&#22312;&#26368;&#32456;&#27169;&#22411;&#20043;&#22806;&#65292;&#21516;&#26102;&#36890;&#36807;&#20854;&#20182;&#21306;&#38388;&#21028;&#26029;&#20986;&#21487;&#20449;&#21644;&#26174;&#33879;&#30340;&#21327;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.07574</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#30340;&#31232;&#30095;&#21270;&#21516;&#26102;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear Models. (arXiv:2307.07574v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21270;&#21516;&#26102;&#32622;&#20449;&#21306;&#38388;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#21306;&#38388;&#30340;&#19978;&#19979;&#30028;&#25910;&#32553;&#20026;&#38646;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#19981;&#37325;&#35201;&#30340;&#21327;&#21464;&#37327;&#24182;&#23558;&#20854;&#25490;&#38500;&#22312;&#26368;&#32456;&#27169;&#22411;&#20043;&#22806;&#65292;&#21516;&#26102;&#36890;&#36807;&#20854;&#20182;&#21306;&#38388;&#21028;&#26029;&#20986;&#21487;&#20449;&#21644;&#26174;&#33879;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#38590;&#20197;&#32771;&#34385;&#65292;&#23545;&#39640;&#32500;&#22238;&#24402;&#31995;&#25968;&#30340;&#32479;&#35745;&#25512;&#26029;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20197;&#21450;&#22914;&#20309;&#23558;&#27169;&#22411;&#30340;&#25512;&#26029;&#23884;&#20837;&#21040;&#31995;&#25968;&#30340;&#21516;&#26102;&#25512;&#26029;&#20013;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31232;&#30095;&#21270;&#21516;&#26102;&#32622;&#20449;&#21306;&#38388;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#21306;&#38388;&#22312;&#26576;&#20123;&#19978;&#19979;&#30028;&#19978;&#36827;&#34892;&#20102;&#31232;&#30095;&#65292;&#21363;&#32553;&#23567;&#20026;&#38646;&#65288;&#20363;&#22914;&#65292;$[0,0]$&#65289;&#65292;&#34920;&#31034;&#30456;&#24212;&#21327;&#21464;&#37327;&#30340;&#19981;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#21327;&#21464;&#37327;&#24212;&#35813;&#20174;&#26368;&#32456;&#27169;&#22411;&#20013;&#25490;&#38500;&#12290;&#20854;&#20313;&#30340;&#21306;&#38388;&#65292;&#26080;&#35770;&#26159;&#21253;&#21547;&#38646;&#65288;&#20363;&#22914;&#65292;$[-1,1]$&#25110;$[0,1]$&#65289;&#36824;&#26159;&#19981;&#21253;&#21547;&#38646;&#65288;&#20363;&#22914;&#65292;$[2,3]$&#65289;&#65292;&#20998;&#21035;&#34920;&#31034;&#21487;&#20449;&#21644;&#26174;&#33879;&#30340;&#21327;&#21464;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#21508;&#31181;&#36873;&#25321;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#27604;&#36739;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical inference of the high-dimensional regression coefficients is challenging because the uncertainty introduced by the model selection procedure is hard to account for. A critical question remains unsettled; that is, is it possible and how to embed the inference of the model into the simultaneous inference of the coefficients? To this end, we propose a notion of simultaneous confidence intervals called the sparsified simultaneous confidence intervals. Our intervals are sparse in the sense that some of the intervals' upper and lower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance of the corresponding covariates. These covariates should be excluded from the final model. The rest of the intervals, either containing zero (e.g., $[-1,1]$ or $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and significant covariates, respectively. The proposed method can be coupled with various selection procedures, making it ideal for comparing their u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Probit&#21644;Tobit&#27169;&#22411;&#20013;&#30340;&#31471;&#28857;&#25928;&#24212;&#19982;&#20869;&#29983;&#24615;&#65292;&#35777;&#26126;&#20102;&#26410;&#21306;&#20998;&#20004;&#31181;&#20869;&#29983;&#24615;&#20250;&#23548;&#33268;&#23545;&#20559;&#25928;&#24212;&#30340;&#20302;&#20272;&#25110;&#39640;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#25928;&#24212;&#33539;&#22260;&#20272;&#35745;&#22120;&#24182;&#25552;&#20379;&#26131;&#20110;&#23454;&#29616;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.14862</link><description>&lt;p&gt;
Probit&#21644;Tobit&#27169;&#22411;&#20013;&#30340;&#31471;&#28857;&#25928;&#24212;&#19982;&#20869;&#29983;&#24615;
&lt;/p&gt;
&lt;p&gt;
Marginal Effects for Probit and Tobit with Endogeneity. (arXiv:2306.14862v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Probit&#21644;Tobit&#27169;&#22411;&#20013;&#30340;&#31471;&#28857;&#25928;&#24212;&#19982;&#20869;&#29983;&#24615;&#65292;&#35777;&#26126;&#20102;&#26410;&#21306;&#20998;&#20004;&#31181;&#20869;&#29983;&#24615;&#20250;&#23548;&#33268;&#23545;&#20559;&#25928;&#24212;&#30340;&#20302;&#20272;&#25110;&#39640;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#25928;&#24212;&#33539;&#22260;&#20272;&#35745;&#22120;&#24182;&#25552;&#20379;&#26131;&#20110;&#23454;&#29616;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35780;&#20272;&#20559;&#25928;&#24212;&#26102;&#65292;&#21306;&#20998;&#32467;&#26500;&#24615;&#20869;&#29983;&#24615;&#21644;&#27979;&#37327;&#35823;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#20004;&#31181;&#20869;&#29983;&#24615;&#26469;&#28304;&#22312;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#23545;&#20559;&#25928;&#24212;&#30340;&#24433;&#21709;&#19981;&#21516;&#12290;&#26412;&#25991;&#20197;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;Probit&#21644;Tobit&#27169;&#22411;&#20026;&#37325;&#28857;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#26377;&#25928;&#30340;IV&#65292;&#26410;&#33021;&#21306;&#20998;&#36825;&#20004;&#31181;&#20869;&#29983;&#24615;&#31867;&#22411;&#20063;&#21487;&#33021;&#23548;&#33268;&#20559;&#25928;&#24212;&#34987;&#20302;&#20272;&#25110;&#39640;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#31616;&#21333;&#30340;&#20559;&#25928;&#24212;&#33539;&#22260;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#26131;&#20110;&#23454;&#29616;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#35813;&#21306;&#38388;&#27491;&#30830;&#22320;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20869;&#29983;&#24615;&#12290;&#25105;&#20204;&#22312;Monte Carlo&#27169;&#25311;&#21644;&#23454;&#35777;&#24212;&#29992;&#20013;&#35828;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When evaluating partial effects, it is important to distinguish between structural endogeneity and measurement errors. In contrast to linear models, these two sources of endogeneity affect partial effects differently in nonlinear models. We study this issue focusing on the Instrumental Variable (IV) Probit and Tobit models. We show that even when a valid IV is available, failing to differentiate between the two types of endogeneity can lead to either under- or over-estimation of the partial effects. We develop simple estimators of the bounds on the partial effects and provide easy to implement confidence intervals that correctly account for both types of endogeneity. We illustrate the methods in a Monte Carlo simulation and an empirical application.
&lt;/p&gt;</description></item></channel></rss>