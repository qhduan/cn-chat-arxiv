<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37325;&#22797;&#30456;&#21516;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#26356;&#20542;&#21521;&#20110;&#36981;&#24490;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#22312;&#26368;&#21518;&#19968;&#32452;&#20915;&#31574;&#20013;&#26377;&#26356;&#22810;&#20010;&#20307;&#34920;&#29616;&#20026;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.16538</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#65288;&#39044;&#26399;&#65289;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize (Expected) Utility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37325;&#22797;&#30456;&#21516;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#26356;&#20542;&#21521;&#20110;&#36981;&#24490;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#30340;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#22312;&#26368;&#21518;&#19968;&#32452;&#20915;&#31574;&#20013;&#26377;&#26356;&#22810;&#20010;&#20307;&#34920;&#29616;&#20026;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#23454;&#39564;&#20013;&#65292;&#21442;&#19982;&#32773;&#26159;&#21542;&#20250;&#22312;&#37325;&#22797;&#20174;&#30456;&#21516;&#33756;&#21333;&#20013;&#20570;&#20986;&#20915;&#31574;&#19988;&#27809;&#26377;&#25509;&#25910;&#20219;&#20309;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20250;&#34920;&#29616;&#20986;&#19982;&#24207;&#25968;&#21644;&#26399;&#26395;&#25928;&#29992;&#29702;&#35770;&#39044;&#27979;&#26356;&#21152;&#25509;&#36817;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#39033;&#38750;&#24378;&#21046;&#36873;&#25321;&#30340;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37329;&#38065;&#24425;&#31080;&#65292;&#24182;&#27599;&#20010;&#33756;&#21333;&#37325;&#22797;&#20116;&#27425;&#65292;&#26088;&#22312;&#20174;&#22810;&#20010;&#34892;&#20026;&#35282;&#24230;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#25105;&#20204;&#20174;&#33521;&#22269;&#21644;&#24503;&#22269;&#30340;308&#21517;&#21463;&#35797;&#32773;&#20013;&#30340;&#25968;&#25454;&#20013;&#65292;&#26174;&#33879;&#26356;&#22810;&#30340;&#20010;&#20307;&#22312;&#20182;&#20204;&#26368;&#21518;15&#20010;&#30456;&#21516;&#20915;&#31574;&#38382;&#39064;&#20013;&#26159;&#24207;&#25968;&#25928;&#29992;&#21644;&#26399;&#26395;&#25928;&#29992;&#30340;&#26368;&#22823;&#21270;&#32773;&#65292;&#32780;&#19981;&#26159;&#22312;&#31532;&#19968;&#20010;15&#20010;&#20013;&#12290;&#27492;&#22806;&#65292;&#22823;&#32422;&#22235;&#20998;&#20043;&#19968;&#21644;&#20116;&#20998;&#20043;&#19968;&#30340;&#25152;&#26377;&#21463;&#35797;&#32773;&#65292;&#22312;&#23454;&#39564;&#20013;&#37117;&#20197;&#36825;&#20123;&#27169;&#24335;&#20570;&#20915;&#31574;&#65292;&#20960;&#20046;&#19968;&#21322;&#26174;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#19981;&#21516;&#12290;&#22312;&#37027;&#20123;&#22987;&#32456;&#29702;&#24615;&#30340;&#20010;&#20307;&#19982;&#28385;&#36275;&#38543;&#26426;&#25928;&#29992;&#29702;&#35770;&#26680;&#24515;&#21407;&#21017;&#30340;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16538v1 Announce Type: new  Abstract: We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Fi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25919;&#24220;&#22312;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#31119;&#21033;&#24182;&#36827;&#34892;&#31579;&#36873;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#25919;&#24220;&#26080;&#27861;&#36890;&#36807;&#21333;&#19968;&#24037;&#20855;&#36827;&#34892;&#20844;&#24179;&#31579;&#36873;&#65292;&#20294;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#31579;&#36873;&#24182;&#20135;&#29983;&#20844;&#24179;&#30340;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.08781</link><description>&lt;p&gt;
&#20844;&#24179;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Equitable screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#25919;&#24220;&#22312;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#31119;&#21033;&#24182;&#36827;&#34892;&#31579;&#36873;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#25919;&#24220;&#26080;&#27861;&#36890;&#36807;&#21333;&#19968;&#24037;&#20855;&#36827;&#34892;&#20844;&#24179;&#31579;&#36873;&#65292;&#20294;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#31579;&#36873;&#24182;&#20135;&#29983;&#20844;&#24179;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#25919;&#24220;&#22312;&#32771;&#34385;&#32467;&#26524;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#26102;&#25552;&#20379;&#31119;&#21033;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#20851;&#27880;&#36890;&#36807;&#20844;&#24179;&#24615;&#38480;&#21046;&#26469;&#24314;&#27169;&#65292;&#35201;&#27714;&#24212;&#35813;&#24179;&#31561;&#22320;&#32473;&#20104;&#24179;&#31561;&#20540;&#24471;&#30340;&#20195;&#29702;&#20154;&#20998;&#37197;&#12290;&#25105;&#30740;&#31350;&#20102;&#21738;&#20123;&#31579;&#36873;&#24418;&#24335;&#19982;&#20844;&#24179;&#30456;&#23481;&#65292;&#24182;&#23637;&#31034;&#20102;&#34429;&#28982;&#25919;&#24220;&#26080;&#27861;&#36890;&#36807;&#21333;&#19968;&#24037;&#20855;&#65288;&#22914;&#25903;&#20184;&#25110;&#31561;&#24453;&#26102;&#38388;&#65289;&#36827;&#34892;&#20844;&#24179;&#31579;&#36873;&#65292;&#20294;&#23558;&#22810;&#20010;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#24037;&#20855;&#26412;&#36523;&#23601;&#26377;&#21033;&#20110;&#19981;&#21516;&#30340;&#32676;&#20307;&#65292;&#21487;&#20197;&#23454;&#29616;&#31579;&#36873;&#21516;&#26102;&#20135;&#29983;&#20844;&#24179;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08781v1 Announce Type: new Abstract: I study the problem of a government providing benefits while considering the perceived equity of the resulting allocation. Such concerns are modeled through an equity constraint requiring that equally deserving agents receive equal allocations. I ask what forms of screening are compatible with equity and show that while the government cannot equitably screen with a single instrument (e.g. payments or wait times), combining multiple instruments, which on their own favor different groups, allows it to screen while still producing an equitable allocation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#27835;&#30103;&#26465;&#20214;&#19979;&#30340;&#25919;&#31574;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26680;&#26041;&#27861;&#20272;&#35745;&#25919;&#31574;&#31119;&#21033;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21322;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#24179;&#34913;&#31119;&#21033;&#25439;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.02535</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36830;&#32493;&#27835;&#30103;&#25919;&#31574;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-driven Policy Learning for a Continuous Treatment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#27835;&#30103;&#26465;&#20214;&#19979;&#30340;&#25919;&#31574;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26680;&#26041;&#27861;&#20272;&#35745;&#25919;&#31574;&#31119;&#21033;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21322;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#24179;&#34913;&#31119;&#21033;&#25439;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#27835;&#30103;&#21464;&#37327;&#26465;&#20214;&#19979;&#30340;&#25919;&#31574;&#23398;&#20064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#20498;&#25968;&#20272;&#35745;&#26435;&#37325;(IPW)&#26041;&#27861;&#26469;&#20272;&#35745;&#25919;&#31574;&#31119;&#21033;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#30001;&#26080;&#31351;&#30340;Vapnik-Chervonenkis(VC)&#32500;&#24230;&#29305;&#24449;&#30340;&#20840;&#23616;&#25919;&#31574;&#31867;&#20013;&#36817;&#20284;&#26368;&#20248;&#25919;&#31574;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#20855;&#26377;&#26377;&#38480;VC&#32500;&#24230;&#30340;&#31579;&#36873;&#25919;&#31574;&#31867;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#31119;&#21033;&#25439;&#22833;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#20840;&#23616;&#31119;&#21033;&#19981;&#36275;&#12289;&#26041;&#24046;&#21644;&#20559;&#24046;&#12290;&#36825;&#23548;&#33268;&#21516;&#26102;&#36873;&#25321;&#20272;&#35745;&#30340;&#26368;&#20248;&#24102;&#23485;&#21644;&#31119;&#21033;&#36817;&#20284;&#30340;&#26368;&#20248;&#25919;&#31574;&#31867;&#25104;&#20026;&#24517;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#37319;&#29992;&#20102;&#24809;&#32602;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#22885;&#25289;&#20811;&#19981;&#31561;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#20107;&#20808;&#20102;&#35299;&#31119;&#21033;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#28789;&#27963;&#24179;&#34913;&#31119;&#21033;&#25439;&#22833;&#30340;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficie
&lt;/p&gt;</description></item></channel></rss>