<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Luce&#30340;&#36873;&#25321;&#20844;&#29702;&#20026;&#22522;&#30784;&#30340;&#19968;&#31867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#25928;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;Sinkhorn&#31639;&#27861;&#65292;&#23558;&#36873;&#25321;&#24314;&#27169;&#31639;&#27861;&#32479;&#19968;&#20026;&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.00260</link><description>&lt;p&gt;
&#20851;&#20110;Sinkhorn&#31639;&#27861;&#21644;&#36873;&#25321;&#24314;&#27169;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Sinkhorn's Algorithm and Choice Modeling. (arXiv:2310.00260v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Luce&#30340;&#36873;&#25321;&#20844;&#29702;&#20026;&#22522;&#30784;&#30340;&#19968;&#31867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#25928;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;Sinkhorn&#31639;&#27861;&#65292;&#23558;&#36873;&#25321;&#24314;&#27169;&#31639;&#27861;&#32479;&#19968;&#20026;&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;Luce&#36873;&#25321;&#20844;&#29702;&#30340;&#24191;&#27867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#21253;&#25324;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#20851;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#20215;&#20110;&#20855;&#26377;&#30446;&#26631;&#34892;&#21644;&#21015;&#21644;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#35266;&#28857;&#25171;&#24320;&#20102;&#20004;&#20010;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#20043;&#38376;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36873;&#25321;&#24314;&#27169;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#32479;&#19968;&#20026;Sinkhorn&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#27530;&#23454;&#20363;&#25110;&#31867;&#20284;&#29289;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#32852;&#31995;&#20013;&#33719;&#24471;&#21551;&#21457;&#65292;&#24182;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#23384;&#22312;&#26377;&#38480;&#35299;&#26102;&#65292;Sinkhorn&#31639;&#27861;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#26500;&#24314;&#30340;&#20108;&#20998;&#22270;&#30340;&#20195;&#25968;&#36830;&#36890;&#24615;&#26469;&#25551;&#36848;&#36825;&#31181;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a broad class of choice and ranking models based on Luce's choice axiom, including the Bradley--Terry--Luce and Plackett--Luce models, we show that the associated maximum likelihood estimation problems are equivalent to a classic matrix balancing problem with target row and column sums. This perspective opens doors between two seemingly unrelated research areas, and allows us to unify existing algorithms in the choice modeling literature as special instances or analogs of Sinkhorn's celebrated algorithm for matrix balancing. We draw inspirations from these connections and resolve important open problems on the study of Sinkhorn's algorithm. We first prove the global linear convergence of Sinkhorn's algorithm for non-negative matrices whenever finite solutions to the matrix balancing problem exist. We characterize this global rate of convergence in terms of the algebraic connectivity of the bipartite graph constructed from data. Next, we also derive the sharp asymptotic rate of line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.03816</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#19982;&#21322;&#21442;&#25968;Bernstein-von-Mises&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reparametrization and the Semiparametric Bernstein-von-Mises Theorem. (arXiv:2306.03816v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#37096;&#20998;&#32447;&#24615;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22238;&#24402;&#20989;&#25968;&#30340;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#25152;&#20851;&#24515;&#30340;&#20302;&#32500;&#21442;&#25968;&#12290;&#21442;&#25968;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#29983;&#25104;&#20102;&#19968;&#20010;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#20272;&#35745;&#20302;&#32500;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#22823;&#26679;&#26412;&#20998;&#26512;&#25903;&#25345;&#20102;&#36825;&#31181;&#35828;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#20302;&#32500;&#21442;&#25968;&#30340;&#21518;&#39564;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#30495;&#23454;&#20540;&#25910;&#32553;&#65292;&#24182;&#19988;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#28176;&#36817;&#22320;&#27491;&#24577;&#20998;&#24067;&#12290;&#36825;&#20123;&#26465;&#20214;&#30456;&#23545;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#21407;&#22987;&#21442;&#25968;&#21270;&#20801;&#35768;&#26356;&#22823;&#31867;&#30340;&#24178;&#25200;&#21442;&#25968;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19968;&#20010;&#23884;&#20837;&#20102;Neyman&#27491;&#20132;&#24615;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#25104;&#20026;&#21322;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26377;&#29992;&#24037;&#20855;&#65292;&#20197;&#21435;&#20559;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers Bayesian inference for the partially linear model. Our approach exploits a parametrization of the regression function that is tailored toward estimating a low-dimensional parameter of interest. The key property of the parametrization is that it generates a Neyman orthogonal moment condition meaning that the low-dimensional parameter is less sensitive to the estimation of nuisance parameters. Our large sample analysis supports this claim. In particular, we derive sufficient conditions under which the posterior for the low-dimensional parameter contracts around the truth at the parametric rate and is asymptotically normal with a variance that coincides with the semiparametric efficiency bound. These conditions allow for a larger class of nuisance parameters relative to the original parametrization of the regression model. Overall, we conclude that a parametrization that embeds Neyman orthogonality can be a useful device for debiasing posterior distributions in semipa
&lt;/p&gt;</description></item></channel></rss>