# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large-Scale Education Reform in General Equilibrium: Regression Discontinuity Evidence from India: Comment.](http://arxiv.org/abs/2303.11956) | 本文重新分析了Khanna (2023)的研究，指出缺失数据及绘图软件等因素对结果造成了干扰，表明他们之前的结论不能被有效支持。 |
| [^2] | [Algorithmic Assistance with Recommendation-Dependent Preferences.](http://arxiv.org/abs/2208.07626) | 本研究提出了一个联合人机决策的委托代理模型，探讨了算法推荐对选择的影响和设计，特别关注算法对偏好的改变，以解决算法辅助可能带来的意外后果。 |
| [^3] | [Policy Choice in Time Series by Empirical Welfare Maximization.](http://arxiv.org/abs/2205.03970) | 本文提出了一种新方法以在一定时间段内进行政策选择，通过经验福利最大化方法来估计这一政策规则，旨在达到更好的条件福利。我们表征了此方法可达到最优策略选择的条件，并用模拟研究和实证应用来证明其可行性，在宏观经济领域应用上也能够起到积极的作用。 |

# 详细

[^1]: 基于一般均衡的大规模教育改革：印度回归不连续证据的评论

    Large-Scale Education Reform in General Equilibrium: Regression Discontinuity Evidence from India: Comment. (arXiv:2303.11956v1 [econ.GN])

    [http://arxiv.org/abs/2303.11956](http://arxiv.org/abs/2303.11956)

    本文重新分析了Khanna (2023)的研究，指出缺失数据及绘图软件等因素对结果造成了干扰，表明他们之前的结论不能被有效支持。

    

    本文重新分析了 Khanna (2023) 中通过回归不连续设计研究印度教育对劳动力市场的影响的内容。在图形初步分析中，反转绘图软件默认值的覆盖极大地减少了不连续性的出现。在数据中缺少离不连续点四个街区；修复后削减了对学校和对数工资的简化形式影响分别为62％和75％。使用一致的方差估计器，并将其聚类处理到地理治疗单元，进一步削弱了积极影响的推断。一般均衡效应和替代弹性的估计不是无偏的，且有效方差为无限大。

    This paper reanalyzes Khanna (2023), which studies labor market effects of schooling in India through a regression discontinuity design. In graphical preliminaries, reversing overrides of the plotting software's defaults greatly reduces the appearance of discontinuities. Absent from the data are four districts close to the discontinuity; restoring them cuts the reduced-form impacts on schooling and log wages by 62% and 75%. Using a consistent variance estimator, and clustering it at the geographic unit of treatment, further weakens the inference of positive impact. The estimates of general equilibrium effects and elasticities of substitution are not unbiased and have effectively infinite variance.
    
[^2]: 算法辅助下的推荐相关偏好

    Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07626](http://arxiv.org/abs/2208.07626)

    本研究提出了一个联合人机决策的委托代理模型，探讨了算法推荐对选择的影响和设计，特别关注算法对偏好的改变，以解决算法辅助可能带来的意外后果。

    

    当算法提供风险评估时，我们通常将其视为对人类决策的有益输入，例如将风险评分呈现给法官或医生。然而，决策者可能不仅仅只针对算法提供的信息做出反应。决策者还可能将算法推荐视为默认操作，使其难以偏离，例如法官在对被告进行高风险评估的时候不愿意推翻，或医生担心偏离推荐的程序会带来后果。为了解决算法辅助的这种意外后果，我们提出了一个联合人机决策的委托代理模型。在该模型中，我们考虑了算法推荐对选择的影响和设计，这种影响不仅仅是通过改变信念，还通过改变偏好。我们从制度因素和行为经济学中的已有模型等方面进行了这个假设的动机论证。

    When an algorithm provides risk assessments, we typically think of them as helpful inputs to human decisions, such as when risk scores are presented to judges or doctors. However, a decision-maker may not only react to the information provided by the algorithm. The decision-maker may also view the algorithmic recommendation as a default action, making it costly for them to deviate, such as when a judge is reluctant to overrule a high-risk assessment for a defendant or a doctor fears the consequences of deviating from recommended procedures. To address such unintended consequences of algorithmic assistance, we propose a principal-agent model of joint human-machine decision-making. Within this model, we consider the effect and design of algorithmic recommendations when they affect choices not just by shifting beliefs, but also by altering preferences. We motivate this assumption from institutional factors, such as a desire to avoid audits, as well as from well-established models in behav
    
[^3]: 基于经验福利最大化的时间序列政策选择方法研究

    Policy Choice in Time Series by Empirical Welfare Maximization. (arXiv:2205.03970v3 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2205.03970](http://arxiv.org/abs/2205.03970)

    本文提出了一种新方法以在一定时间段内进行政策选择，通过经验福利最大化方法来估计这一政策规则，旨在达到更好的条件福利。我们表征了此方法可达到最优策略选择的条件，并用模拟研究和实证应用来证明其可行性，在宏观经济领域应用上也能够起到积极的作用。

    

    本文提出了一种在多元时间序列条件下进行政策选择的新方法。建立在统计治疗选择框架的基础上，我们提出了时间序列经验福利最大化（T-EWM）方法，通过最大化使用非参数潜在结果时间序列构造的经验福利准则来估计当前或多个时期的最优政策规则。我们表征了T-EWM在什么条件下能够一致地学习到在时间序列历史给定条件福利下最优的政策选择。然后推导出了条件福利遗憾及其极小下界的非渐进上限。为了说明T-EWM的实现和应用，我们进行了模拟研究，并将该方法应用于从宏观经济时间序列数据中估计最优货币政策规则。

    This paper develops a novel method for policy choice in a dynamic setting where the available data is a multivariate time series. Building on the statistical treatment choice framework, we propose Time-series Empirical Welfare Maximization (T-EWM) methods to estimate an optimal policy rule for the current period or over multiple periods by maximizing an empirical welfare criterion constructed using nonparametric potential outcome time-series. We characterize conditions under which T-EWM consistently learns a policy choice that is optimal in terms of conditional welfare given the time-series history. We then derive a nonasymptotic upper bound for conditional welfare regret and its minimax lower bound. To illustrate the implementation and uses of T-EWM, we perform simulation studies and apply the method to estimate optimal monetary policy rules from macroeconomic time-series data.
    

