# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning to Maximize (Expected) Utility](https://arxiv.org/abs/2402.16538) | 该研究探讨了在重复相同决策过程中，参与者是否会更倾向于遵循序数和期望效用理论的预测行为，并发现在最后一组决策中有更多个体表现为序数和期望效用最大化者。 |
| [^2] | [Equitable screening](https://arxiv.org/abs/2402.08781) | 这项研究探讨了政府在考虑公平性的情况下提供福利并进行筛选的问题。研究结果表明，虽然政府无法通过单一工具进行公平筛选，但通过结合多种工具，可以实现筛选并产生公平的分配。 |
| [^3] | [Data-driven Policy Learning for a Continuous Treatment](https://arxiv.org/abs/2402.02535) | 本论文研究了在连续治疗条件下的政策学习，通过使用核方法估计政策福利，并引入一种半数据驱动的策略来平衡福利损失的组成部分。 |

# 详细

[^1]: 学习最大化（预期）效用

    Learning to Maximize (Expected) Utility

    [https://arxiv.org/abs/2402.16538](https://arxiv.org/abs/2402.16538)

    该研究探讨了在重复相同决策过程中，参与者是否会更倾向于遵循序数和期望效用理论的预测行为，并发现在最后一组决策中有更多个体表现为序数和期望效用最大化者。

    

    我们研究了在选择实验中，参与者是否会在重复从相同菜单中做出决策且没有接收任何反馈的情况下，学会表现出与序数和期望效用理论预测更加接近的行为。我们设计并实施了一项非强制选择的实验室实验，使用了金钱彩票，并每个菜单重复五次，旨在从多个行为角度测试这一假设。在我们从英国和德国的308名受试者中的数据中，显著更多的个体在他们最后15个相同决策问题中是序数效用和期望效用的最大化者，而不是在第一个15个中。此外，大约四分之一和五分之一的所有受试者，在实验中都以这些模式做决策，几乎一半显示出非平凡的不同。在那些始终理性的个体与满足随机效用理论核心原则的个体之间存在明显的重叠。

    arXiv:2402.16538v1 Announce Type: new  Abstract: We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Fi
    
[^2]: 公平筛选

    Equitable screening

    [https://arxiv.org/abs/2402.08781](https://arxiv.org/abs/2402.08781)

    这项研究探讨了政府在考虑公平性的情况下提供福利并进行筛选的问题。研究结果表明，虽然政府无法通过单一工具进行公平筛选，但通过结合多种工具，可以实现筛选并产生公平的分配。

    

    我研究了政府在考虑结果分配的公平性时提供福利的问题。这些关注通过公平性限制来建模，要求应该平等地给予平等值得的代理人分配。我研究了哪些筛选形式与公平相容，并展示了虽然政府无法通过单一工具（如支付或等待时间）进行公平筛选，但将多个工具结合起来，这些工具本身就有利于不同的群体，可以实现筛选同时产生公平的分配。

    arXiv:2402.08781v1 Announce Type: new Abstract: I study the problem of a government providing benefits while considering the perceived equity of the resulting allocation. Such concerns are modeled through an equity constraint requiring that equally deserving agents receive equal allocations. I ask what forms of screening are compatible with equity and show that while the government cannot equitably screen with a single instrument (e.g. payments or wait times), combining multiple instruments, which on their own favor different groups, allows it to screen while still producing an equitable allocation.
    
[^3]: 基于数据驱动的连续治疗政策学习

    Data-driven Policy Learning for a Continuous Treatment

    [https://arxiv.org/abs/2402.02535](https://arxiv.org/abs/2402.02535)

    本论文研究了在连续治疗条件下的政策学习，通过使用核方法估计政策福利，并引入一种半数据驱动的策略来平衡福利损失的组成部分。

    

    本研究针对连续治疗变量条件下的政策学习进行了研究。我们采用基于核方法的倒数估计权重(IPW)方法来估计政策福利。我们的目标是在由无穷的Vapnik-Chervonenkis(VC)维度特征的全局政策类中近似最优政策。通过使用一系列具有有限VC维度的筛选政策类，实现了这一目标。初步分析表明，福利损失包括三个组成部分：全局福利不足、方差和偏差。这导致同时选择估计的最优带宽和福利近似的最优政策类成为必要。为了应对这一挑战，我们引入了一种半数据驱动的策略，采用了惩罚技术。这种方法产生了奥拉克不等式，能够在不事先了解福利不足的情况下，灵活平衡福利损失的三个组成部分。

    This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficie
    

