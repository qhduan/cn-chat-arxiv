<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#31034;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.19390</link><description>&lt;p&gt;
&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#26816;&#26597;&#28857;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Checkpoint Merging via Bayesian Optimization in LLM Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#31034;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Gemini&#30340;&#36805;&#36895;&#22686;&#38271;&#31361;&#26174;&#20102;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36164;&#28304;&#30340;&#24378;&#28872;&#38656;&#27714;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#29615;&#22659;&#25104;&#26412;&#65292;&#36825;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#26816;&#26597;&#28857;&#21512;&#24182;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#20849;&#20139;&#35757;&#32451;&#36712;&#36857;&#30340;LLM&#26816;&#26597;&#28857;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#23545;&#26368;&#20339;&#21512;&#24182;&#26435;&#37325;&#36827;&#34892;&#24191;&#27867;&#30340;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#12290;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#22312;&#26368;&#23567;&#25104;&#26412;&#19979;&#33719;&#24471;&#37325;&#22823;&#25910;&#30410;&#30340;&#26426;&#20250;&#65307;&#65288;2&#65289;&#23613;&#31649;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#32473;&#23450;&#30340;&#20445;&#30041;&#25968;&#25454;&#38598;&#65292;&#20294;&#20173;&#23637;&#31034;&#20102;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#31283;&#20581;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#39044;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19390v1 Announce Type: new  Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;</title><link>https://arxiv.org/abs/2403.09073</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24182;&#34892;&#22810;&#35821;&#35328;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Parallel Multilingual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65306;&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#20026;LLMs&#25552;&#20379;&#20102;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65288;PiM&#65289;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21253;&#25324;8&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#12289;7&#31181;&#35821;&#35328;&#21644;8&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LLMs&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;&#25972;&#21512;&#26356;&#22810;&#35821;&#35328;&#21487;&#20197;&#24110;&#21161;PiM&#36827;&#19968;&#27493;&#36229;&#36234;&#20256;&#32479;&#30340;ICL&#65307;&#65288;2&#65289;&#21363;&#20351;&#19982;&#22522;&#20934;&#24615;&#33021;&#20302;&#21155;&#30340;&#32763;&#35793;&#32467;&#21512;&#20063;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26816;&#26597;LLMs&#20013;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24847;&#22806;&#20294;&#26377;&#36259;&#30340;&#29616;&#35937;&#12290;&#19982;&#24120;&#35265;&#35266;&#28857;&#30456;&#21453;&#65292;PiM&#24182;&#19981;&#20250;&#28608;&#27963;&#27604;&#21333;&#35821;&#36755;&#20837;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#26469;&#21033;&#29992;&#20174;&#22810;&#31181;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#32780;&#23454;&#38469;&#19978;&#26159;&#25233;&#21046;&#31070;&#32463;&#20803;&#24182;&#20419;&#36827;&#26356;&#31934;&#30830;&#30340;&#31070;&#32463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09073v1 Announce Type: new  Abstract: In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neu
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04247</link><description>&lt;p&gt;
UltraWiki: &#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04247
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;(ESE)&#26088;&#22312;&#35782;&#21035;&#23646;&#20110;&#19982;&#32473;&#23450;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#26032;&#23454;&#20307;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#27491;&#31181;&#23376;&#23454;&#20307;&#26469;&#34920;&#31034;&#30446;&#26631;&#35821;&#20041;&#31867;&#21035;&#65292;&#36825;&#23545;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#30340;&#34920;&#31034;&#26500;&#25104;&#25361;&#25112;&#12290;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#26159;&#22522;&#20110;&#24102;&#26377;&#26356;&#20855;&#20307;&#23646;&#24615;&#32422;&#26463;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#23450;&#20041;&#30340;&#12290;&#20165;&#20351;&#29992;&#27491;&#31181;&#23376;&#23454;&#20307;&#25551;&#36848;&#20250;&#24341;&#36215;&#20004;&#20010;&#38382;&#39064;&#65306;(i) &#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;(ii) &#26080;&#27861;&#23450;&#20041;&#8220;&#19981;&#24819;&#35201;&#8221;&#30340;&#35821;&#20041;&#12290;&#30001;&#20110;&#36825;&#20123;&#22266;&#26377;&#32570;&#38519;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#20915;&#36229;&#32454;&#31890;&#24230;ESE(Ultra-ESE)&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36755;&#20837;&#20013;&#30340;&#36127;&#31181;&#23376;&#23454;&#20307;&#65292;&#23427;&#20204;&#23646;&#20110;&#19982;&#27491;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#65292;&#20294;&#22312;&#26576;&#20123;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36127;&#31181;&#23376;&#23454;&#20307;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04247v1 Announce Type: new  Abstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define "unwanted" semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15319</link><description>&lt;p&gt;
GPTVQ&#65306;LLM&#37327;&#21270;&#20013;&#32500;&#24230;&#30340;&#31119;&#38899;
&lt;/p&gt;
&lt;p&gt;
GPTVQ: The Blessing of Dimensionality for LLM Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15319
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#65292;GPTVQ&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#19981;&#20165;&#26174;&#33879;&#25913;&#21892;&#20102;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#36824;&#25552;&#39640;&#20102;&#22788;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22686;&#21152;&#37327;&#21270;&#32500;&#24230;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPTVQ&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24555;&#36895;&#21518;&#35757;&#32451;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20132;&#26367;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#21015;&#30340;&#37327;&#21270;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#27599;&#23618;&#36755;&#20986;&#37325;&#24314;MSE&#30340;Hessian&#20449;&#24687;&#26469;&#26356;&#26032;&#20854;&#20313;&#26410;&#37327;&#21270;&#30340;&#26435;&#37325;&#12290;&#37327;&#21270;&#30721;&#20070;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;EM&#31639;&#27861;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#25968;&#37327;&#21270;&#21644;&#22522;&#20110;SVD&#30340;&#21387;&#32553;&#36827;&#19968;&#27493;&#21387;&#32553;&#30721;&#20070;&#12290;GPTVQ&#22312;&#35832;&#22914;Llama-v2&#21644;Mistral&#31561;&#21508;&#31181;LLMs&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#22823;&#23567;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65306;&#22312;&#21333;&#20010;H100&#19978;&#65292;&#22788;&#29702;&#19968;&#20010;Llamav2-70B&#38656;&#35201;3&#33267;11&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15319v1 Announce Type: cross  Abstract: In this work we show that the size versus accuracy trade-off of neural network quantization can be significantly improved by increasing the quantization dimensionality. We propose the GPTVQ method, a new fast method for post-training vector quantization (VQ) that scales well to Large Language Models (LLMs). Our method interleaves quantization of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. Quantization codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer quantization and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B
&lt;/p&gt;</description></item><item><title>Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16061</link><description>&lt;p&gt;
Conti&#20844;&#21496;&#65306;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20102;&#35299;&#19968;&#20010;&#22823;&#22411;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#30340;&#20869;&#37096;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16061
&lt;/p&gt;
&lt;p&gt;
Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#65288;RaaS&#65289;&#27491;&#22312;&#22686;&#21152;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#12290;&#20102;&#35299;RaaS&#32972;&#21518;&#30340;&#20869;&#37096;&#36816;&#20316;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#27492;&#31867;&#27963;&#21160;&#26159;&#38750;&#27861;&#30340;&#12290;&#26368;&#36817;Conti&#20844;&#21496;&#27844;&#38706;&#30340;&#32842;&#22825;&#35760;&#24405;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20102;&#35299;&#36825;&#31867;&#32452;&#32455;&#20869;&#37096;&#36816;&#20316;&#30340;&#33391;&#26426;&#12290;&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;Conti&#20844;&#21496;&#32842;&#22825;&#35760;&#24405;&#20013;&#30340;&#20027;&#35201;&#20027;&#39064;&#35752;&#35770;&#12290;&#21457;&#29616;&#20102;&#20116;&#20010;&#35752;&#35770;&#20027;&#39064;&#65306;1&#65289;&#19994;&#21153;&#65292;2&#65289;&#25216;&#26415;&#65292;3&#65289;&#20869;&#37096;&#20219;&#21153;/&#31649;&#29702;&#65292;4&#65289;&#24694;&#24847;&#36719;&#20214;&#65292;5&#65289;&#23458;&#25143;&#26381;&#21153;/&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;Conti&#25104;&#21592;&#30340;&#20027;&#39064;&#20998;&#24067;&#26174;&#31034;&#65292;&#21482;&#26377;4%&#30340;&#20154;&#36827;&#34892;&#20102;&#19987;&#38376;&#30340;&#35752;&#35770;&#65292;&#32780;&#20960;&#20046;&#25152;&#26377;&#20154;&#65288;96%&#65289;&#37117;&#26159;&#20840;&#33021;&#22411;&#65292;&#24847;&#21619;&#30528;&#20182;&#20204;&#30340;&#35752;&#35770;&#37117;&#22260;&#32469;&#30528;&#36825;&#20116;&#20010;&#20027;&#39064;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve aro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12892</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#21465;&#36848;&#20998;&#31867;&#30340;&#23567;&#35268;&#27169;&#20132;&#25442;&#21464;&#21387;&#22120;&#21644;&#22522;&#20110;NLP&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification. (arXiv:2303.12892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#22312;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#37319;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#12289;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#20132;&#25442;&#21464;&#21387;&#22120;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36807;&#20110;&#22797;&#26434;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26377;&#38480;&#25968;&#25454;&#30340;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;Switch Transformer&#26694;&#26550;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#22312;CHU Sainte-Justine&#21307;&#38498;&#30340;&#23567;&#22411;&#27861;&#35821;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21270;&#30340;&#23567;&#35268;&#27169;&#21464;&#21387;&#22120;&#27169;&#22411;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#21253;&#25324;DistillBERT&#12289;CamemBERT&#12289;FlauBERT&#21644;FrALBERT&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Switch Transformer&#30340;&#19987;&#23478;&#28151;&#21512;&#26426;&#21046;&#26377;&#21161;&#20110;&#25429;&#33719;&#22810;&#26679;&#30340;&#27169;&#24335;&#65307;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20855;&#26377;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#20256;&#32479;&#21464;&#21387;&#22120;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#20102;87&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;87&#65285;&#30340;&#31934;&#24230;&#21644;86&#65285;&#30340;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#23567;&#22411;&#20020;&#24202;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based models such as the Switch Transformer have achieved remarkable results in natural language processing tasks. However, these models are often too complex and require extensive pre-training, which limits their effectiveness for small clinical text classification tasks with limited data. In this study, we propose a simplified Switch Transformer framework and train it from scratch on a small French clinical text classification dataset at CHU Sainte-Justine hospital. Our results demonstrate that the simplified small-scale Transformer models outperform pre-trained BERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT. Additionally, using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer with the self-attention mechanism. Finally, our proposed framework achieves an accuracy of 87\%, precision at 87\%, and recall 
&lt;/p&gt;</description></item></channel></rss>