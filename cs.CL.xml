<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.11170</link><description>&lt;p&gt;
UMLS-KGI-BERT&#65306;&#22522;&#20110;&#36716;&#21270;&#22120;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#30693;&#35782;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition. (arXiv:2307.11170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#36716;&#21270;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#25104;&#20026;&#20027;&#23548;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26723;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23558;&#35813;&#33539;&#24335;&#36866;&#24212;&#20110;&#38656;&#35201;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20197;&#21450;&#35821;&#35328;&#30340;&#32479;&#35745;&#24314;&#27169;&#30340;NLP&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#26500;&#24314;LMs&#65292;&#26082;&#32771;&#34385;&#21307;&#23398;&#25991;&#26412;&#20013;&#20196;&#29260;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#21448;&#32771;&#34385;UMLS&#31561;&#26415;&#35821;&#36164;&#28304;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be comb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.15932</link><description>&lt;p&gt;
BUCA&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#19988;&#22312;&#33539;&#22260;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#38480;&#65292;&#26080;&#30417;&#30563;&#30340;&#24120;&#35782;&#25512;&#29702;(UCR)&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;UCR&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23558;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;(&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;)&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#25490;&#21517;&#26469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#21512;&#29702;&#21644;&#19981;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;KG&#30340;&#29616;&#26377;UCR&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#33410;&#30465;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/probe2/BUCA&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
&lt;/p&gt;</description></item></channel></rss>