<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;</title><link>https://arxiv.org/abs/2403.02839</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#21028;&#22120;&#30340;LLM&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#31934;&#35843;&#35780;&#21028;&#22120;&#27169;&#22411;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02839
&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20854;&#20182;LLM&#36136;&#37327;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;GPT4&#65292;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#21478;&#22806;&#65292;&#20854;&#20182;&#30740;&#31350;&#21033;&#29992;&#24320;&#28304;LLM&#26469;&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35780;&#21028;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#31934;&#35843;&#30340;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;GPT4&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#36828;&#20302;&#20110;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02839v1 Announce Type: new  Abstract: Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08491</link><description>&lt;p&gt;
&#23545;&#27604;&#22256;&#24785;&#24230;&#22312;&#21463;&#25511;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65306;&#28165;&#27905;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models. (arXiv:2401.08491v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#19981;&#21487;&#21462;&#21644;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#20869;&#23481;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#20010;&#25361;&#25112;&#21644;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#38544;&#24335;&#30693;&#35782;&#32534;&#36753;&#21644;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#27604;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#23545;&#40784;&#25991;&#26412;&#30340;&#22256;&#24785;&#24230;&#12290;&#20026;&#20102;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#23545;&#20110;&#24120;&#35782;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#20294;&#32463;&#39564;&#19978;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.
&lt;/p&gt;</description></item><item><title>StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2312.10741</link><description>&lt;p&gt;
StyleSinger: &#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis. (arXiv:2312.10741v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10741
&lt;/p&gt;
&lt;p&gt;
StyleSinger&#26159;&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#30340;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#36890;&#36807;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#28436;&#21809;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#22806;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#65288;SVS&#65289;&#30340;&#39118;&#26684;&#36716;&#31227;&#19987;&#27880;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28436;&#21809;&#22768;&#38899;&#65292;&#35813;&#22768;&#38899;&#20855;&#26377;&#20174;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#20013;&#34893;&#29983;&#30340;&#26410;&#35265;&#39118;&#26684;&#65288;&#22914;&#38899;&#33394;&#12289;&#24773;&#24863;&#12289;&#21457;&#38899;&#21644;&#21457;&#38899;&#25216;&#24039;&#65289;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#28436;&#21809;&#22768;&#38899;&#39118;&#26684;&#30340;&#31934;&#32454;&#24046;&#24322;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28436;&#21809;&#22768;&#38899;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#34920;&#29616;&#21147;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;SVS&#26041;&#27861;&#22312;&#39046;&#22495;&#22806;&#22330;&#26223;&#20013;&#21512;&#25104;&#30340;&#28436;&#21809;&#22768;&#38899;&#36136;&#37327;&#19979;&#38477;&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#35757;&#32451;&#38454;&#27573;&#21487;&#36776;&#21035;&#20986;&#30446;&#26631;&#22768;&#38899;&#23646;&#24615;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StyleSinger&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#22806;&#21442;&#32771;&#28436;&#21809;&#22768;&#38899;&#26679;&#26412;&#30340;&#38646;&#26679;&#24335;&#36716;&#31227;&#30340;&#28436;&#21809;&#22768;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;StyleSinger&#37319;&#29992;&#20102;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#26524;&#65306;1&#65289;&#27531;&#24046;&#39118;&#26684;&#36866;&#37197;&#22120;&#65288;RSA&#65289;&#65292;&#23427;&#20351;&#29992;&#27531;&#24046;&#37327;&#21270;&#27169;&#22359;&#26469;&#25429;&#25417;&#22810;&#26679;&#30340;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses on generating high-quality singing voices with unseen styles (such as timbre, emotion, pronunciation, and articulation skills) derived from reference singing voice samples. However, the endeavor to model the intricate nuances of singing voice styles is an arduous task, as singing voices possess a remarkable degree of expressiveness. Moreover, existing SVS methods encounter a decline in the quality of synthesized singing voices in OOD scenarios, as they rest upon the assumption that the target vocal attributes are discernible during the training phase. To overcome these challenges, we propose StyleSinger, the first singing voice synthesis model for zero-shot style transfer of out-of-domain reference singing voice samples. StyleSinger incorporates two critical approaches for enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a residual quantization module to capture diverse style character
&lt;/p&gt;</description></item></channel></rss>