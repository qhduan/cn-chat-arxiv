<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.05136</link><description>&lt;p&gt;
LV-Eval:&#19968;&#20010;&#24179;&#34913;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#20855;&#26377;5&#20010;&#38271;&#24230;&#32423;&#21035;&#65292;&#26368;&#22810;&#21487;&#36798;256K
&lt;/p&gt;
&lt;p&gt;
LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05136
&lt;/p&gt;
&lt;p&gt;
LV-Eval&#26159;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;256k&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#24182;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#21644;&#25552;&#20379;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#22768;&#31216;&#25903;&#25345;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#36798;&#21040;256k&#29978;&#33267;&#26356;&#22810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#30340;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#19981;&#36275;&#65288;5k-21k&#65289;&#65292;&#24182;&#19988;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#30693;&#35782;&#27844;&#28431;&#21644;&#19981;&#20934;&#30830;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23548;&#33268;&#35780;&#20272;&#32467;&#26524;&#20559;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LV-Eval&#65292;&#19968;&#20010;&#20855;&#26377;&#20116;&#20010;&#38271;&#24230;&#32423;&#21035;&#65288;16k&#65292;32k&#65292;64k&#65292;128k&#21644;256k&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#22810;&#21487;&#36798;256k&#20010;&#21333;&#35789;&#12290;LV-Eval&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21333;&#36339;&#38382;&#31572;&#21644;&#22810;&#36339;&#38382;&#31572;&#65292;&#21253;&#21547;11&#20010;&#21452;&#35821;&#25968;&#25454;&#38598;&#12290;LV-Eval&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#21363;&#28151;&#28102;&#20107;&#23454;&#25554;&#20837;&#12289;&#20851;&#38190;&#35789;&#21644;&#30701;&#35821;&#26367;&#25442;&#20197;&#21450;&#22522;&#20110;&#20851;&#38190;&#35789;&#22238;&#24518;&#30340;&#24230;&#37327;&#35774;&#35745;&#12290;LV-Eval&#30340;&#20248;&#28857;&#21253;&#25324;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#21487;&#25511;&#35780;&#20272;&#12289;&#20855;&#26377;&#28151;&#28102;&#20107;&#23454;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#23454;&#20363;&#12289;&#20943;&#23569;&#30340;&#30693;&#35782;&#27844;&#28431;&#20197;&#21450;&#26356;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;LV-Eval&#19978;&#35780;&#20272;&#20102;10&#20010;LLMs&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies o
&lt;/p&gt;</description></item></channel></rss>