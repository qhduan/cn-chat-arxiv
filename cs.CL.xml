<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01383</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#65306;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
LLM-based NLG Evaluation: Current Status and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#25361;&#25112;&#37325;&#37325;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#36890;&#36807;&#31995;&#32479;&#36755;&#20986;&#21644;&#21442;&#32771;&#25991;&#26412;&#20043;&#38388;&#30340;&#20869;&#23481;&#65288;&#22914;n-gram&#65289;&#37325;&#21472;&#24230;&#26469;&#25429;&#25417;&#65292;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#32780;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;NLG&#35780;&#20272;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#22522;&#20110;LLM&#30340;NLG&#35780;&#20272;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20998;&#21035;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#29992;&#20110;NLG&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.
&lt;/p&gt;</description></item></channel></rss>