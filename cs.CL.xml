<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.14562</link><description>&lt;p&gt;
&#35821;&#20041;&#35299;&#30721;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
The Era of Semantic Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#29616;&#20102;&#22312;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#32534;&#25490;&#21327;&#20316;&#20197;&#35299;&#20915;LLM&#22266;&#26377;&#23616;&#38480;&#24615;&#30340;&#24819;&#27861;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;&#36825;&#20123;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;LLM&#27010;&#24565;&#21270;&#20026;&#25805;&#32437;&#25105;&#20204;&#31216;&#20043;&#20026;&#35821;&#20041;&#26631;&#35760;&#65288;&#24050;&#30693;&#24605;&#24819;&#65289;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#29255;&#27573;&#30340;&#35821;&#20041;&#22788;&#29702;&#22120;&#12290;LLM&#26159;&#20247;&#22810;&#20854;&#20182;&#35821;&#20041;&#22788;&#29702;&#22120;&#20043;&#19968;&#65292;&#21253;&#25324;&#20154;&#31867;&#21644;&#24037;&#20855;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#25110;&#20195;&#30721;&#25191;&#34892;&#22120;&#12290;&#35821;&#20041;&#22788;&#29702;&#22120;&#38598;&#20307;&#21442;&#19982;&#35821;&#20041;&#26631;&#35760;&#30340;&#21160;&#24577;&#20132;&#27969;&#65292;&#36880;&#27493;&#26500;&#24314;&#39640;&#25928;&#36755;&#20986;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#25628;&#32034;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#35821;&#20041;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#24050;&#24191;&#20026;&#30740;&#31350;&#30340;&#35821;&#20041;&#35299;&#30721;&#38382;&#39064;&#30452;&#25509;&#24179;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14562v1 Announce Type: cross  Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12059</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#24615;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25191;&#34892;&#22810;&#39033;&#36873;&#25321;&#31526;&#21495;&#32465;&#23450;&#65288;MCSB&#65289;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#22240;&#20026;&#36234;&#21335;&#35821;&#20013;&#30340;&#25361;&#25112;&#24615;MCQA&#25968;&#25454;&#38598;&#36739;&#33521;&#35821;&#23569;&#12290;&#29616;&#26377;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;ViMMRC 1.0&#21644;ViMMRC 2.0&#65292;&#19987;&#27880;&#20110;&#25991;&#23398;&#38382;&#39064;&#12290;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;ChatGPT&#22312;2019&#24180;&#33267;2023&#24180;&#30340;&#36234;&#21335;&#22269;&#23478;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;ChatGPT&#22914;&#20309;&#36880;&#27493;&#35299;&#20915;VNHSGE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20026;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#30340;LaTeX&#20844;&#24335;&#36755;&#20837;&#25552;&#20379;&#32467;&#26500;&#21270;&#25351;&#21335;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;MCSB&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#35201;&#27714;&#20351;&#29992;&#20005;&#26684;&#30340;LaTeX&#26679;&#24335;&#36827;&#34892;&#36755;&#20837;&#12290;&#25105;&#20204;&#37325;&#28857;&#39044;&#27979;&#23383;&#31526;&#65288;A&#12289;B&#12289;C&#25110;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03903</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Multi-Agent Coordination Abilities in Large Language Models. (arXiv:2310.03903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#29087;&#32451;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#26377;&#25928;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#31995;&#32479;&#21512;&#20316;&#30340;&#26234;&#33021;&#20307;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#26174;&#33879;&#30340;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#35299;&#37322;&#35821;&#35328;&#30340;&#33021;&#21147;&#25104;&#20026;&#24320;&#21457;&#36825;&#31181;&#26234;&#33021;&#20307;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20351;&#29992;LLM&#26500;&#24314;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#21035;&#35774;&#35745;&#30340;LLM-Co&#26694;&#26550;&#65292;&#20351;LLM&#33021;&#22815;&#21442;&#19982;&#21327;&#35843;&#28216;&#25103;&#12290;&#36890;&#36807;LLM-Co&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#35780;&#20272;&#20998;&#20026;&#20116;&#20010;&#26041;&#38754;&#65306;&#24515;&#26234;&#29702;&#35770;&#12289;&#24773;&#22659;&#25512;&#29702;&#12289;&#25345;&#32493;&#21327;&#35843;&#12289;&#23545;&#21512;&#20316;&#20249;&#20276;&#30340;&#31283;&#20581;&#24615;&#21644;&#26126;&#30830;&#36741;&#21161;&#12290;&#39318;&#20808;&#65292;&#24515;&#26234;&#29702;&#35770;&#21644;&#24773;&#22659;&#25512;&#29702;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;LLM&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions acco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01903</link><description>&lt;p&gt;
&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#36229;&#20986;&#22270;&#20687;&#33539;&#22260;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#26816;&#32034;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#36825;&#32463;&#24120;&#20250;&#24341;&#20837;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-3&#65289;&#20316;&#20026;&#38544;&#21547;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#22238;&#31572;&#25152;&#38656;&#30340;&#24517;&#35201;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36824;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;GPT-3&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#36755;&#20837;&#20449;&#24687;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prophet&#8212;&#8212;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#21551;&#21457;&#24335;&#26041;&#24335;&#65292;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29305;&#23450;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#31572;&#26696;&#21551;&#21457;&#24335;&#65306;&#31572;&#26696;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
&lt;/p&gt;</description></item></channel></rss>