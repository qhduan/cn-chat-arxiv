<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04311</link><description>&lt;p&gt;
ALTO&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#21512;AI&#31995;&#32479;&#30340;&#39640;&#25928;&#32593;&#32476;&#32534;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALTO: An Efficient Network Orchestrator for Compound AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04311
&lt;/p&gt;
&lt;p&gt;
ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ALTO&#65292;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#20026;&#35832;&#22914;&#35821;&#35328;&#27169;&#22411;&#31649;&#36947;&#20043;&#31867;&#30340;&#22797;&#21512;AI&#31995;&#32479;&#25552;&#20379;&#26381;&#21153;&#30340;&#32593;&#32476;&#32534;&#25490;&#22120;&#12290;ALTO&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29305;&#26377;&#30340;&#20248;&#21270;&#26426;&#20250;&#65306;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;&#29983;&#25104;token&#30340;&#36755;&#20986;&#65292;ALTO&#22312;&#21487;&#33021;&#26102;&#26292;&#38706;&#20102;&#22312;&#38454;&#27573;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#36755;&#20986;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36328;&#20998;&#24067;&#24335;&#31649;&#36947;&#38454;&#27573;&#23454;&#20363;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32858;&#21512;&#24863;&#30693;&#36335;&#30001;&#25509;&#21475;&#21644;&#20998;&#24067;&#24335;&#25552;&#31034;&#24863;&#30693;&#35843;&#24230;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#39564;&#35777;&#31649;&#36947;&#19978;&#23637;&#31034;&#20102;ALTO&#37096;&#20998;&#36755;&#20986;&#27969;&#24335;&#20256;&#36755;&#30340;&#24433;&#21709;&#65292;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#21516;&#26102;&#23558;&#22266;&#23450;&#24310;&#36831;&#30446;&#26631;&#35774;&#32622;&#20026;4&#31186;/&#35831;&#27714;&#65292;&#36824;&#20943;&#23569;&#20102;&#23614;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04311v1 Announce Type: new  Abstract: We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item></channel></rss>