<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11802</link><description>&lt;p&gt;
Counting-Stars&#65306;&#19968;&#31181;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11802
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Counting-Stars&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;GPT-4 Turbo&#21644;Kimi Chat&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#20855;&#26377;&#24378;&#22823;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#23545;&#39046;&#20808;&#30340;LLMs&#65288;&#20363;&#22914;ChatGPT&#21644;KimiChat&#65289;&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21512;&#29702;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#35780;&#20272;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;Counting-Stars&#12290;Counting-Stars&#26088;&#22312;&#35201;&#27714;LLMs&#20805;&#20998;&#29702;&#35299;&#21644;&#25429;&#25417;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#33021;&#22815;&#25910;&#38598;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#22810;&#20010;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22522;&#20110;Counting-Stars&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#20102;&#20004;&#20010;&#39046;&#20808;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#65292;&#21363;GPT-4 Turbo&#21644;Kimi Chat&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 Turbo&#21644;Kimi Chat&#22312;Counting-Stars&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11802v1 Announce Type: new  Abstract: While recent research endeavors have concentrated on developing Large Language Models (LLMs) with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading LLMs (e.g., ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context LLMs as a new benchmark, named Counting-Stars. The Counting-Stars is designed to require LLMs to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve significant performance in th
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35789;-&#19978;&#19979;&#25991;&#23884;&#20837;&#22120;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.02285</link><description>&lt;p&gt;
&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;
&lt;/p&gt;
&lt;p&gt;
Detection of Non-recorded Word Senses in English and Swedish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#26816;&#27979;&#26410;&#35760;&#24405;&#30340;&#35789;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35789;-&#19978;&#19979;&#25991;&#23884;&#20837;&#22120;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33521;&#35821;&#21644;&#29790;&#20856;&#35821;&#20013;&#36827;&#34892;&#26410;&#30693;&#35789;&#20041;&#26816;&#27979;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#29305;&#23450;&#35789;&#35821;&#29992;&#27861;&#30340;&#21547;&#20041;&#26159;&#21542;&#22312;&#35789;&#20856;&#20013;&#26377;&#35760;&#24405;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#22120;&#26469;&#27604;&#36739;&#35789;&#20041;&#26465;&#30446;&#19982;&#29616;&#20195;&#21644;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#35789;&#35821;&#29992;&#27861;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24314;&#27169;&#36825;&#19968;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#19982;&#20174;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#22686;&#21152;&#26816;&#27979;&#21040;&#20855;&#26377;&#26410;&#35760;&#24405;&#35789;&#20041;&#30340;&#35789;&#35821;&#29992;&#27861;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02285v1 Announce Type: new  Abstract: This study addresses the task of Unknown Sense Detection in English and Swedish. The primary objective of this task is to determine whether the meaning of a particular word usage is documented in a dictionary or not. For this purpose, sense entries are compared with word usages from modern and historical corpora using a pre-trained Word-in-Context embedder that allows us to model this task in a few-shot scenario. Additionally, we use human annotations to adapt and evaluate our models. Compared to a random sample from a corpus, our model is able to considerably increase the detected number of word usages with non-recorded senses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#25968;&#23398;&#21453;&#39304;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#27880;&#37322;&#20154;&#24037;&#32534;&#20889;&#30340;&#21644;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01304</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Automatically Generated Feedback via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#25968;&#23398;&#21453;&#39304;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#23637;&#31034;&#20102;GPT-4&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#27880;&#37322;&#20154;&#24037;&#32534;&#20889;&#30340;&#21644;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#21644;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#21453;&#39304;&#20855;&#26377;&#25913;&#21892;&#35768;&#22810;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27491;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01304v1 Announce Type: new  Abstract: Automatically generating feedback via large language models (LLMs) in intelligent tutoring systems and online learning platforms has the potential to improve the learning outcomes of many students. However, both feedback generation and evaluation are challenging: feedback content has to be valid especially in subjects like math, which requires models to understand the problem, the solution, and where the student's error lies. Feedback also has to be pedagogically valid to reflect effective tutoring strategies, such as explaining possible misconceptions and encouraging the student, among other desirable features. In this work, we address both problems of automatically generating and evaluating feedback while considering both correctness and alignment. First, we propose a rubric for evaluating math feedback and show that GPT-4 is able to effectively use it to annotate human-written and LLM-generated feedback. Second, we propose a framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#23553;&#38381;&#39046;&#22495;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;Galactica&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.16995</link><description>&lt;p&gt;
&#36136;&#37327;&gt;&#25968;&#37327;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#23553;&#38381;&#39046;&#22495;&#25277;&#21462;&#24335;&#38382;&#31572;&#30340;&#21512;&#25104;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Quality &gt; Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering. (arXiv:2310.16995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16995
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#23553;&#38381;&#39046;&#22495;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;Galactica&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26159;&#23558;&#27169;&#22411;&#22312;&#19968;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#36807;&#31243;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#30784;&#27169;&#22411;(FM)&#26159;&#19968;&#20010;&#36873;&#25321;&#65292;&#20294;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;FM&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21738;&#31181;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#37117;&#26080;&#27861;&#22987;&#32456;&#36798;&#21040;&#26368;&#20808;&#36827;( SOTA)&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23553;&#38381;&#39046;&#22495;&#20869;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24182;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#12290;&#36825;&#24847;&#21619;&#30528;&#30830;&#23450;&#21644;&#29983;&#25104;&#30456;&#20851;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21033;&#29992;&#22312;&#24191;&#27867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#29305;&#23450;&#39046;&#22495;&#30340;FM&#30340;&#29702;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;Galactica&#29983;&#25104;&#19982;&#29305;&#23450;&#20889;&#20316;&#39118;&#26684;&#21644;&#20027;&#39064;(&#22914;&#30740;&#31350;&#35770;&#25991;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;)&#30456;&#19968;&#33268;&#30340;&#21512;&#25104;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge 
&lt;/p&gt;</description></item></channel></rss>