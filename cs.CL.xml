<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CheckEval&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#21644;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;SummEval&#22522;&#20934;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18771</link><description>&lt;p&gt;
CheckEval: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#28165;&#21333;&#26500;&#24314;&#20581;&#22766;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CheckEval: Robust Evaluation Framework using Large Language Model via Checklist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18771
&lt;/p&gt;
&lt;p&gt;
CheckEval&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#21644;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;SummEval&#22522;&#20934;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CheckEval&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#19968;&#33268;&#24615;&#25361;&#25112;&#12290;CheckEval&#36890;&#36807;&#23558;&#35780;&#20272;&#26631;&#20934;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#65292;&#24182;&#20026;&#27599;&#20010;&#26500;&#24314;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20351;&#36807;&#31243;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#36890;&#36807;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#35780;&#20272;&#32500;&#24230;&#26174;&#30528;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;SummEval&#22522;&#20934;&#36827;&#34892;&#30340;&#19987;&#27880;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#65292;CheckEval&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23637;&#31034;&#20102;&#39640;&#24230;&#19968;&#33268;&#30340;&#20114;&#27880;&#32773;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;CheckEval&#22312;&#23458;&#35266;&#12289;&#28789;&#27963;&#21644;&#31934;&#30830;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#25552;&#20379;&#21487;&#23450;&#21046;&#21644;&#20114;&#21160;&#30340;&#26694;&#26550;&#65292;CheckEval&#20026;LL&#30340;&#20351;&#29992;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18771v1 Announce Type: new  Abstract: We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LL
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#31995;&#32479;&#35843;&#26597;&#65306;&#25216;&#26415;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#24050;&#25104;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#21147;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#22312;&#19981;&#20462;&#25913;&#26680;&#24515;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25552;&#31034;&#20801;&#35768;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#25552;&#31034;&#24341;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#25552;&#31034;&#21487;&#20197;&#26159;&#25552;&#20379;&#19978;&#19979;&#25991;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20063;&#21487;&#20197;&#26159;&#35843;&#29992;&#30456;&#20851;&#30693;&#35782;&#30340;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20174;&#38382;&#31572;&#21040;&#24120;&#35782;&#25512;&#29702;&#37117;&#26377;&#28041;&#21450;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#26679;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#21644;&#25216;&#26415;&#32570;&#20047;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#29702;&#35299;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#23545;&#26368;&#36817;&#36827;&#23637;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Evidence to Generate&#65288;E2G&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30446;&#21069;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#20316;&#20026;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.05787</link><description>&lt;p&gt;
&#29983;&#25104;&#35777;&#25454;&#65288;E2G&#65289;&#65306;&#19968;&#31181;&#21333;&#20195;&#29702;&#30340;&#20004;&#27493;&#25552;&#31034;&#29992;&#20110;&#19978;&#19979;&#25991;&#36741;&#21161;&#21644;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Evidence to Generate&#65288;E2G&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30446;&#21069;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#20316;&#20026;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#38761;&#26032;&#20102;LLMs&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#20294;&#20854;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#21464;&#20307;&#65288;&#20363;&#22914;&#65292;&#33258;&#19968;&#33268;&#24615;&#65292;&#21453;&#24212;&#65292;&#21453;&#23556;&#65292;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#65292;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65289;&#23384;&#22312;&#32531;&#24930;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#25509;&#22320;&#12289;&#24187;&#35937;&#21644;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Evidence to Generate&#65288;E2G&#65289;&#36825;&#19968;&#26032;&#39062;&#30340;&#21333;&#20195;&#29702;&#12289;&#20004;&#27493;&#25552;&#31034;&#26694;&#26550;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#8220;&#20915;&#31574;&#30340;&#35777;&#25454;&#8221;&#30340;&#21147;&#37327;&#65292;&#32780;&#19981;&#26159;&#26410;&#32463;&#39564;&#35777;&#30340;&#25512;&#29702;&#20027;&#24352;&#65292;&#39318;&#20808;&#19987;&#27880;&#20110;&#22312;&#19978;&#19979;&#25991;&#20013;&#26126;&#30830;&#25552;&#21450;&#30340;&#24605;&#32500;&#24207;&#21015;&#65288;&#20013;&#38388;&#27493;&#39588;&#30340;&#31995;&#21015;&#65289;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#25552;&#21462;&#30340;&#35777;&#25454;&#65292;&#20197;&#26356;&#39640;&#30340;&#31934;&#30830;&#24230;&#21644;&#25928;&#29575;&#24341;&#23548;LLM&#30340;&#36755;&#20986;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#27861;&#35299;&#38145;&#20102;&#20687;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#28508;&#21147;&#65292;&#20026;LLM&#20013;&#26356;&#24555;&#12289;&#26356;&#21487;&#38752;&#21644;&#26356;&#20855;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#25512;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of "evidence for decision making" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.04928</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#20316;&#29615;&#22659;&#20013;&#65292;&#22914;&#20250;&#35758;&#23433;&#25490;&#12289;&#21512;&#20316;&#21644;&#39033;&#30446;&#35268;&#21010;&#20013;&#65292;&#38598;&#20307;&#20915;&#31574;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20559;&#22909;&#22810;&#26679;&#24615;&#12289;&#24037;&#20316;&#28966;&#28857;&#19981;&#21516;&#21644;&#25104;&#21592;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#31561;&#22240;&#32032;&#65292;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20419;&#36827;&#32676;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20010;&#20307;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#28385;&#36275;&#25104;&#21592;&#20559;&#22909;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#27492;&#31995;&#32479;&#24212;&#29992;&#20110;&#20225;&#19994;&#20250;&#35758;&#23433;&#25490;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#21019;&#24314;&#20102;&#21512;&#25104;&#21592;&#24037;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#35780;&#20272;&#31995;&#32479;&#34920;&#29616;&#26469;&#20316;&#20026;&#24320;&#23637;&#29992;&#25143;&#30740;&#31350;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#23454;&#29616;&#25104;&#21592;&#19982;LLM&#31995;&#32479;&#20043;&#38388;&#30340;&#39640;&#25928;&#21327;&#35843;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23545;&#20854;&#25552;&#20986;&#30340;&#36873;&#39033;&#36827;&#34892;&#25913;&#36827;&#21644;&#23436;&#21892;&#65292;&#30830;&#20445;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from conversations and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#21033;&#29992;Thrust&#25351;&#26631;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.10442</link><description>&lt;p&gt;
Thrust: &#29992;&#22806;&#37096;&#30693;&#35782;&#33258;&#36866;&#24212;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Thrust: Adaptively Propels Large Language Models with External Knowledge. (arXiv:2307.10442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#21033;&#29992;Thrust&#25351;&#26631;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20854;&#27169;&#22411;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;PTLM&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#21487;&#33021;&#26159;&#19981;&#36879;&#26126;&#25110;&#38745;&#24577;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#29978;&#33267;&#21487;&#33021;&#24341;&#20837;&#22122;&#38899;&#21644;&#35823;&#23548;&#24615;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#65288;IAPEK&#65289;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Thrust&#65292;&#21033;&#29992;&#23569;&#37327;&#24050;&#35265;&#23454;&#20363;&#30340;&#34920;&#31034;&#20998;&#24067;&#26469;&#34913;&#37327;PTLM&#26159;&#21542;&#21253;&#21547;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#19968;&#20010;&#23454;&#20363;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Thrust&#26159;&#34913;&#37327;PTLM&#27169;&#22411;&#23454;&#20363;&#32423;&#30693;&#35782;&#33021;&#21147;&#30340;&#33391;&#22909;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;Thrust&#20998;&#25968;&#20316;&#20026;&#26816;&#32034;&#25351;&#26631;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#39640;&#20110;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#26420;&#32032;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that thrust is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve significantly higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.03972</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task. (arXiv:2307.03972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#20013;&#26410;&#33021;&#36798;&#21040;&#36229;&#36234;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#26412;&#25253;&#21578;&#26088;&#22312;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#22312;4&#20010;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20102;3&#20010;&#19981;&#21516;&#27169;&#22411;&#35268;&#27169;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#19981;&#21450;&#20043;&#21069;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#22240;&#20026;&#23384;&#22312;&#36807;&#24230;&#20462;&#27491;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;LLMs&#22312;&#35780;&#20272;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.07207</link><description>&lt;p&gt;
Valley: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35270;&#39057;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#21331;&#36234;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25104;&#20026;&#24378;&#22823;&#30340;AI&#21161;&#25163;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26500;&#24314;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#24212;&#29992;AI&#21161;&#25163;&#65311;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#27169;&#22359;&#26469;&#23545;&#40784;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#65292;&#28982;&#21518;&#22312;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Valley&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#35270;&#39057;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
&lt;/p&gt;</description></item></channel></rss>