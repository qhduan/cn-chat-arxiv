<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2404.02043</link><description>&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#65306;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02043
&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20294;&#21508;&#31181;&#35821;&#35328;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#20381;&#28982;&#26174;&#32780;&#26131;&#35265;&#12290;&#20044;&#20811;&#20848;&#35821;&#20316;&#20026;&#19968;&#31181;&#20173;&#21487;&#20174;&#36328;&#35821;&#35328;&#26041;&#27861;&#30340;&#25345;&#32493;&#23436;&#21892;&#20013;&#21463;&#30410;&#30340;&#35821;&#35328;&#12290;&#37492;&#20110;&#25105;&#20204;&#25152;&#20102;&#35299;&#65292;&#38024;&#23545;&#20856;&#22411;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20044;&#20811;&#20848;&#35821;&#35821;&#26009;&#24211;&#26497;&#24230;&#21294;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#36991;&#20813;&#25163;&#21160;&#25968;&#25454;&#25972;&#29702;&#65306;&#22823;&#22411;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#32763;&#35793;&#31995;&#32479;&#12289;LLMs&#65292;&#20197;&#21450;&#35821;&#35328;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;--&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;--&#25552;&#20379;&#20102;&#26368;&#20339;&#35774;&#32622;&#30340;"&#37197;&#26041;"&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02043v1 Announce Type: cross  Abstract: Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the "recipe" for the optimal setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.10020</link><description>&lt;p&gt;
&#22312;&#37325;&#21472;&#20013;&#36855;&#22833;&#65306;&#25506;&#32034;LLMs&#20013;&#30340;&#27700;&#21360;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Lost in Overlap: Exploring Watermark Collision in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#25991;&#26412;&#29256;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;logit&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21487;&#23519;&#35273;&#30340;&#26631;&#35782;&#23884;&#20837;&#25991;&#26412;&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#27700;&#21360;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#24120;&#35265;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#21644;&#25913;&#20889;&#65289;&#20013;&#21457;&#29983;&#30340;&#27700;&#21360;&#20914;&#31361;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21452;&#27700;&#21360;&#20914;&#31361;&#65292;&#21363;&#21516;&#19968;&#25991;&#26412;&#20013;&#21516;&#26102;&#23384;&#22312;&#20004;&#20010;&#27700;&#21360;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27700;&#21360;&#20914;&#31361;&#23545;&#19978;&#28216;&#21644;&#19979;&#28216;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#24615;&#33021;&#26500;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10020v1 Announce Type: new  Abstract: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.12806</link><description>&lt;p&gt;
SymBa&#65306;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#29992;&#20110;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12806
&lt;/p&gt;
&lt;p&gt;
SymBa&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#25552;&#21319;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#24605;&#32500;&#25552;&#31034;&#20013;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#24544;&#23454;&#30340;&#22810;&#27493;&#25512;&#29702;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21521;&#21518;&#25512;&#29702;&#65292;&#21363;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#36882;&#24402;&#22320;&#20998;&#35299;&#26597;&#35810;&#65292;&#30452;&#21040;&#35777;&#26126;&#20026;&#27490;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#21521;&#21518;&#25512;&#29702;&#23454;&#29616;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymBa&#65288;&#31526;&#21495;&#21270;&#21521;&#21518;&#25512;&#29702;&#65289;&#12290;&#22312;SymBa&#20013;&#65292;&#31526;&#21495;&#21270;&#33258;&#39030;&#21521;&#19979;&#27714;&#35299;&#22120;&#25511;&#21046;&#25972;&#20010;&#35777;&#26126;&#36807;&#31243;&#65292;&#24403;&#27714;&#35299;&#22120;&#36935;&#21040;&#27515;&#32993;&#21516;&#26102;&#65292;&#25165;&#35843;&#29992;LLM&#29983;&#25104;&#21333;&#20010;&#25512;&#29702;&#27493;&#39588;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;-LLM&#38598;&#25104;&#65292;SymBa&#22312;&#21508;&#31181;&#22810;&#27493;&#25512;&#29702;&#22522;&#20934;&#65288;ProofWriter&#65292;Birds-Electricity&#65292;GSM8k&#65292;CLUTRR-TF&#65292;ECtHR Article 6&#65289;&#20013;&#30456;&#27604;&#21521;&#21518;&#25512;&#29702;&#22522;&#32447;&#21462;&#24471;&#20102;&#24615;&#33021;&#12289;&#35777;&#26126;&#24544;&#23454;&#24615;&#21644;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12806v1 Announce Type: new  Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning ability as in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the LLM is called to generate a single reasoning step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01487</link><description>&lt;p&gt;
&#20248;&#31168;&#30340;&#35270;&#35273;&#25351;&#23548;&#26377;&#20160;&#20040;&#29305;&#28857;&#65311;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#29992;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#25552;&#39640;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#30528;&#30524;&#20110;&#19981;&#21516;&#28966;&#28857;&#21644;&#29305;&#24449;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;MLLMs&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#19968;&#20010;&#26356;&#22522;&#26412;&#30340;&#38382;&#39064;&#65306;&#8220;&#20160;&#20040;&#26679;&#30340;&#35270;&#35273;&#25351;&#23548;&#25165;&#26159;&#22909;&#30340;&#65311;&#8221;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20391;&#37325;&#20110;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#25351;&#23548;&#23545;&#20110;&#25913;&#21892;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#29305;&#21035;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21512;&#25104;-&#22797;&#26434;&#21270;-&#37325;&#26500;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#22686;&#21152;&#25351;&#20196;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual instruction tuning is an essential approach to improving the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). A surge of visual instruction datasets with various focuses and characteristics have been proposed recently, enabling MLLMs to achieve surprising results on evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to investigate a more fundamental question: ``what makes for good visual instructions?''. By conducting a comprehensive empirical study, we find that instructions focused on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs on evaluation benchmarks. Building upon this finding, we design a systematic approach to automatically creating high-quality complex visual reasoning instructions. Our approach employs a synthesis-complication-reformulation paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. B
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item></channel></rss>