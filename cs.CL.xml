<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item></channel></rss>