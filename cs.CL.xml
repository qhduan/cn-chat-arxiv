<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#25351;&#20986;&#39321;&#33609;&#27169;&#22411;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#65292;&#25351;&#23548;MoE&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.01994</link><description>&lt;p&gt;
&#39321;&#33609;&#21464;&#21387;&#22120;&#26159;&#36801;&#31227;&#33021;&#21147;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Vanilla Transformers are Transfer Capability Teachers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01994
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#25351;&#20986;&#39321;&#33609;&#27169;&#22411;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#65292;&#25351;&#23548;MoE&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#20256;&#36755;&#33021;&#21147;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21464;&#21387;&#22120;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;MoE&#21464;&#21387;&#22120;&#30340;&#34920;&#29616;&#19981;&#21450;&#39321;&#33609;&#21464;&#21387;&#22120;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;MoE&#27169;&#22411;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#26159;&#24433;&#21709;&#20854;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#32852;&#21512;&#20915;&#23450;&#22240;&#32032;&#12290;&#19982;&#39321;&#33609;&#27169;&#22411;&#30456;&#27604;&#65292;MoE&#27169;&#22411;&#30340;&#36801;&#31227;&#33021;&#21147;&#36739;&#24046;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36801;&#31227;&#33021;&#21147;&#33976;&#39311;&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#34429;&#28982;&#39321;&#33609;&#27169;&#22411;&#24615;&#33021;&#36739;&#24369;&#65292;&#20294;&#23427;&#20204;&#26159;&#36801;&#31227;&#33021;&#21147;&#30340;&#26377;&#25928;&#25945;&#24072;&#12290;&#30001;&#39321;&#33609;&#27169;&#22411;&#25351;&#23548;&#30340;MoE&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#24615;&#33021;&#21644;&#36801;&#31227;&#33021;&#21147;&#65292;&#26368;&#32456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01994v1 Announce Type: new  Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20110;BERT&#34920;&#31034;&#65292;&#21457;&#29616;&#35821;&#35328;&#29305;&#24449;&#22312;&#19981;&#21516;&#23618;&#20013;&#26377;&#24207;&#20998;&#31163;&#65292;&#31070;&#32463;&#34920;&#31034;&#23618;&#32423;&#32452;&#32455;&#65292;&#20013;&#38388;&#23618;&#35299;&#32806;&#65292;&#20248;&#20110;&#20854;&#20182;&#35299;&#30721;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11608</link><description>&lt;p&gt;
&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#35782;&#21035;BERT&#34920;&#31034;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#22788;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11608
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20110;BERT&#34920;&#31034;&#65292;&#21457;&#29616;&#35821;&#35328;&#29305;&#24449;&#22312;&#19981;&#21516;&#23618;&#20013;&#26377;&#24207;&#20998;&#31163;&#65292;&#31070;&#32463;&#34920;&#31034;&#23618;&#32423;&#32452;&#32455;&#65292;&#20013;&#38388;&#23618;&#35299;&#32806;&#65292;&#20248;&#20110;&#20854;&#20182;&#35299;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20316;&#20026;&#19968;&#31181;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#22914;&#20309;&#34920;&#31034;&#20854;&#22788;&#29702;&#23545;&#35937;&#30340;&#29702;&#35770;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;MLEMs&#24212;&#29992;&#20110;&#20174;BERT&#20013;&#25552;&#21462;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#36319;&#36394;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#26102;&#24577;&#12289;&#20027;&#35821;&#20154;&#31216;&#12289;&#20174;&#21477;&#31867;&#22411;&#12289;&#20174;&#21477;&#23884;&#22871;&#31561;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#35821;&#35328;&#29305;&#24449;&#26159;&#26377;&#24207;&#30340;&#65306;&#23427;&#20204;&#22312;&#19981;&#21516;&#23618;&#20013;&#20197;&#19981;&#21516;&#31243;&#24230;&#23558;&#21477;&#23376;&#30340;&#34920;&#31034;&#20998;&#24320;&#65307;&#65288;2&#65289;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#23618;&#32452;&#32455;&#30340;&#65306;&#22312;&#26576;&#20123;&#23618;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#30340;&#32676;&#38598;&#23884;&#22871;&#22312;&#26356;&#22823;&#30340;&#32676;&#38598;&#20869;&#37096;&#65292;&#36981;&#24490;&#36880;&#28176;&#37325;&#35201;&#30340;&#35821;&#35328;&#29305;&#24449;&#65307;&#65288;3&#65289;&#35821;&#35328;&#29305;&#24449;&#22312;&#20013;&#38388;&#23618;&#20013;&#26159;&#35299;&#32806;&#30340;&#65306;&#19981;&#21516;&#30340;&#12289;&#36873;&#25321;&#24615;&#21333;&#20301;&#30001;&#19981;&#21516;&#30340;&#35821;&#35328;&#29305;&#24449;&#28608;&#27963;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;MLEMs&#65288;4&#65289;&#20248;&#20110;&#22810;&#21464;&#37327;&#35299;&#30721;&#26041;&#27861;&#65292;&#26356;&#20855;&#25239;&#31867;&#22411;-I&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;5&#65289;&#20248;&#20110;&#21333;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11608v1 Announce Type: new  Abstract: We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2401.11323</link><description>&lt;p&gt;
&#36776;&#35782;&#24182;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Identifying and Analyzing Task-Encoding Tokens in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;ICL&#30340;&#24037;&#20316;&#26426;&#21046;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#22914;&#20309;&#20174;ICL&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#26495;&#26631;&#35760;&#21644;&#20572;&#29992;&#35789;&#26631;&#35760;&#26368;&#23481;&#26131;&#25104;&#20026;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#24847;&#24605;&#12289;&#37325;&#22797;&#21644;&#25991;&#26412;&#26684;&#24335;&#26159;&#36825;&#20123;&#26631;&#35760;&#30340;&#20027;&#35201;&#21306;&#21035;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11323v2 Announce Type: replace  Abstract: In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL's working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing task-encoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to per
&lt;/p&gt;</description></item></channel></rss>