<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13517</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36870;&#21521;&#32763;&#35793;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Round Trip Translation Defence against Large Language Model Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13517
&lt;/p&gt;
&lt;p&gt;
&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#23545;&#20154;&#31867;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#38656;&#35201;LLMs&#20855;&#26377;&#39640;&#27700;&#24179;&#30340;&#29702;&#35299;&#33021;&#21147;&#25165;&#33021;&#25269;&#25239;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#26368;&#22810;&#21482;&#33021;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19981;&#21040;&#19968;&#21322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;LLMs&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;RTT&#20250;&#25913;&#20889;&#23545;&#25239;&#24615;&#25552;&#31034;&#24182;&#25512;&#24191;&#34920;&#36798;&#30340;&#24605;&#24819;&#65292;&#20351;LLMs&#26356;&#23481;&#26131;&#26816;&#27979;&#20986;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#12289;&#36731;&#37327;&#19988;&#21487;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#36229;&#36807;70%&#30340;Prompt Automatic Iterative Refinement (PAIR)&#25915;&#20987;&#65292;&#36825;&#26159;&#30446;&#21069;&#25105;&#20204;&#25152;&#30693;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#23581;&#35797;&#32531;&#35299;MathsAttack&#65292;&#24182;&#23558;&#20854;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#20102;&#36817;40%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2310.18964</link><description>&lt;p&gt;
LLMs&#19982;Fine-tuning: &#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#36328;&#39046;&#22495;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20132;&#27969;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#25968;&#23383;&#24179;&#21488;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;1&#65289;&#27169;&#22411;&#24615;&#33021;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24494;&#35843;&#21644;&#35757;&#32451;&#21442;&#25968;&#65311;&#65288;2&#65289;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#31243;&#24230;&#22914;&#20309;&#65311;&#20197;&#21450;&#65288;3&#65289;&#24433;&#21709;&#27867;&#21270;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#30340;&#20855;&#20307;&#29305;&#24449;&#26159;&#20160;&#20040;&#65311;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#20063;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#20026;&#20102;&#22238;&#31572;&#38382;&#39064;&#65288;1&#65289;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;36&#20010;&#39046;&#22495;&#20869;&#20998;&#31867;&#22120;&#65292;&#28085;&#30422;&#20102;LLaMA&#12289;Vicuna&#21450;&#20854;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29366;&#24577;&#65292;&#36328;&#36234;&#20102;&#20061;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18964v2 Announce Type: replace  Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. To answer (1) we analyze 36 in-domain classifiers comprising LLaMA, Vicuna, and their variations in pre-trained and fine-tuned states across nine publicly available datasets that span a wide range of platforms a
&lt;/p&gt;</description></item></channel></rss>