<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01172</link><description>&lt;p&gt;
&#27969;&#24335;&#24207;&#21015;&#36716;&#23548;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Sequence Transduction through Dynamic Compression
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01172
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;STAR&#65288;&#24102;&#26377;&#38170;&#23450;&#34920;&#31034;&#30340;&#27969;&#24335;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#12290;STAR&#21160;&#24577;&#22320;&#23545;&#36755;&#20837;&#27969;&#36827;&#34892;&#20998;&#27573;&#65292;&#21019;&#24314;&#21387;&#32553;&#30340;&#38170;&#23450;&#34920;&#31034;&#65292;&#23454;&#29616;&#36817;&#20046;&#26080;&#25439;&#30340;&#21387;&#32553;&#65288;12&#20493;&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;STAR&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#21644;&#24310;&#36831;-&#36136;&#37327;&#25240;&#34935;&#65292;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.12264</link><description>&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12264
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12289;&#36951;&#24536;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#20449;&#20219;&#20854;&#39044;&#27979;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#19968;&#33324;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#23545;&#31934;&#35843;LLMs&#36827;&#34892;&#22522;&#20110;&#21518;&#39564;&#36924;&#36817;&#30340;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Mistral-7b&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#20102;&#19977;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#23545;&#19981;&#21516;&#30446;&#26631;&#39046;&#22495;&#30340;&#24863;&#30693;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#25928;&#33021;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#25105;&#20204;&#23545;&#37027;&#20123;&#23545;&#20110;&#32473;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#29109;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#25552;&#20986;&#20102;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item></channel></rss>