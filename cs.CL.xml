<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00530</link><description>&lt;p&gt;
&#23558;&#22351;&#33529;&#26524;&#19982;&#22909;&#27224;&#23376;&#36827;&#34892;&#27604;&#36739;&#65306;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20559;&#22909;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#36890;&#36807;&#27604;&#36739;&#22312;&#22266;&#23450;&#19978;&#19979;&#25991;&#20013;&#26465;&#20214;&#29983;&#25104;&#30340;&#22810;&#20010;&#29983;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29983;&#25104;&#25918;&#32622;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#36825;&#20165;&#21033;&#29992;&#20102;&#25104;&#23545;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26465;&#20214;&#25490;&#21517;&#36890;&#24120;&#26080;&#27861;&#25429;&#33719;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#21644;&#22810;&#32500;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20559;&#22909;&#33719;&#21462;&#30340;&#20256;&#32479;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#25351;&#20196;-&#21709;&#24212;&#23545;&#19978;&#32852;&#21512;&#24341;&#21457;&#20559;&#22909;&#30340;&#26032;&#36724;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#20559;&#22909;&#20248;&#21270;&#26159;&#38024;&#23545;&#26465;&#20214;&#25490;&#21517;&#21327;&#35758;&#65288;&#20363;&#22914;&#65292;DPO&#65289;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#33719;&#21462;&#21327;&#35758;&#24341;&#20837;&#20102;DOVE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20559;&#22909;&#20248;&#21270;&#30446;&#26631;&#65292;&#36890;&#36807;&#25552;&#21319;&#25152;&#36873;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#26469;&#38477;&#20302;&#25152;&#25298;&#32477;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17464</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#38142;&#25512;&#29702;&#30340;&#39640;&#25928;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Tool Use with Chain-of-Abstraction Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#20934;&#30830;&#25512;&#29702;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#23558;&#25512;&#29702;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#32593;&#32476;&#20107;&#23454;&#12289;&#25968;&#23398;&#21644;&#29289;&#29702;&#35268;&#21017;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;LLM&#33719;&#21462;&#36825;&#20123;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#26159;&#22312;&#22810;&#27493;&#25512;&#29702;&#38382;&#39064;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#31934;&#32454;&#35843;&#25972;LLM&#20195;&#29702;&#65288;&#20363;&#22914;Toolformer&#65289;&#20197;&#35843;&#29992;&#24037;&#20855;&#65292;&#20854;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#24037;&#20855;&#35843;&#29992;&#38656;&#35201;&#25972;&#20307;&#21270;&#21644;&#39640;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;LLM&#39318;&#20808;&#29992;&#25277;&#35937;&#21344;&#20301;&#31526;&#35299;&#30721;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#20197;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#26469;&#23454;&#29616;&#27599;&#20010;&#25512;&#29702;&#38142;&#12290;&#25277;&#35937;&#38142;&#30340;&#35268;&#21010;&#20351;LLM&#33021;&#22815;&#23398;&#20064;&#26356;&#36890;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23545;&#20110;&#19982;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#30456;&#20851;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#20363;&#22914;&#25968;&#23398;&#32467;&#26524;&#65289;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#36824;&#20801;&#35768;LLM&#25191;&#34892;&#35299;&#30721;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00398</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#26631;&#35760;&#32423;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Preference-grounded Token-level Guidance for Language Model Fine-tuning. (arXiv:2306.00398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20559;&#22909;&#19982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30456;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#20559;&#22909;&#36890;&#24120;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#25552;&#20379;&#65292;&#32780;LM&#35757;&#32451;&#21644;&#29983;&#25104;&#37117;&#21457;&#29983;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#12290;&#22240;&#27492;&#65292;&#20559;&#22909;&#21644;LM&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#23384;&#22312;&#39063;&#31890;&#24230;&#19981;&#21305;&#37197;&#65292;&#36825;&#21487;&#33021;&#20250;&#22797;&#26434;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#24207;&#21015;&#32423;&#21035;&#20559;&#22909;&#19982;&#26631;&#35760;&#32423;&#21035;&#35757;&#32451;&#25351;&#23548;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#25913;&#36827;LM&#12290;&#20026;&#20102;&#23398;&#20064;&#25351;&#23548;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;&#25193;&#23637;&#21040;&#21487;&#21464;&#38271;&#24230;LM&#29983;&#25104;&#21644;&#21033;&#29992;&#22810;&#20010;&#29983;&#25104;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;LM&#35757;&#32451;&#65292;&#22522;&#20110;&#30417;&#30563;&#25968;&#25454;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#25351;&#23548;&#30340;&#26497;&#31616;&#20027;&#20041;&#23398;&#20064;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#25351;&#23548;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20801;&#35768;&#26356;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and utilizing the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs 
&lt;/p&gt;</description></item></channel></rss>