<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#20063;&#23384;&#22312;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.14640</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs. (arXiv:2401.14640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#22238;&#31572;&#24402;&#22240;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#20063;&#23384;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#30340;&#24402;&#22240;&#26159;&#20026;&#29983;&#25104;&#30340;&#38472;&#36848;&#25552;&#20379;&#24341;&#29992;, &#24182;&#19988;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#24402;&#22240;&#30340;&#26041;&#27861;&#24448;&#24448;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM), &#20294;&#20173;&#28982;&#19981;&#36275;, &#29305;&#21035;&#26159;&#22312;&#35782;&#21035;&#24402;&#22240;&#20043;&#38388;&#32454;&#24494;&#24046;&#21035;&#21644;&#24341;&#29992;&#19982;&#38472;&#36848;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#12290;&#20026;&#20102;&#27604;&#36739;&#36825;&#20123;&#24402;&#22240;&#35780;&#20272;&#26041;&#27861;&#24182;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;, &#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#31867;&#21035;(&#21363;&#25903;&#25345;, &#19981;&#36275;, &#30683;&#30462;&#21644;&#26080;&#20851;), &#29992;&#20110;&#34913;&#37327;&#24402;&#22240;, &#24182;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#20026;&#38382;&#39064;-&#22238;&#31572;&#23545;&#33258;&#21160;&#29983;&#25104;&#19981;&#21516;&#31867;&#21035;&#30340;&#24402;&#22240;, &#24320;&#21457;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#24402;&#22240;&#38382;&#39064;&#22238;&#31572;(CAQA)&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;, &#29616;&#26377;&#30340;&#35780;&#20272;&#22120;&#22312;&#32454;&#31890;&#24230;&#30340;&#24402;&#22240;&#35774;&#32622;&#19979;&#34920;&#29616;&#19981;&#20339;, &#22312;&#22797;&#26434;&#30340;&#24341;&#25991;-&#38472;&#36848;&#25512;&#29702;&#20013;&#23384;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The attribution of question answering is to provide citations for supporting generated statements, and has attracted wide research attention. The current methods for automatically evaluating the attribution, which are often based on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and complex relationships between citations and statements. To compare these attribution evaluation methods and develop new ones, we introduce a set of fine-grained categories (i.e., supportive, insufficient, contradictory and irrelevant) for measuring the attribution, and develop a Complex Attributed Question Answering (CAQA) benchmark by leveraging knowledge graphs (KGs) for automatically generating attributions of different categories to question-answer pairs. Our analysis reveals that existing evaluators perform poorly under fine-grained attribution settings and exhibit weaknesses in complex citation-statement reasoning. Our CAQA benc
&lt;/p&gt;</description></item></channel></rss>