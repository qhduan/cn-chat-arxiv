<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08978</link><description>&lt;p&gt;
AutoGuide: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#36873;&#25321;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08978
&lt;/p&gt;
&lt;p&gt;
AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#21463;&#38480;&#12290;&#36825;&#32473;&#22522;&#20110;LLMs&#30340;&#20195;&#29702;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AutoGuide&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#32463;&#39564;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoGuide&#36890;&#36807;&#25552;&#21462;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#26377;&#25928;&#22320;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#12290;&#27599;&#20010;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#20197;&#31616;&#27905;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#36981;&#24490;&#26465;&#20214;&#32467;&#26500;&#65292;&#28165;&#26224;&#25551;&#36848;&#36866;&#29992;&#30340;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25351;&#21335;&#20026;&#21521;&#20195;&#29702;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#22823;&#24133;&#39046;&#20808;&#20110;&#31454;&#20105;&#30340;&#22522;&#20110;LLMs&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#25552;&#20379;&#26426;&#36935;&#65292;&#35813;&#35843;&#26597;&#29420;&#29305;&#20851;&#27880;LLM&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#36129;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#31561;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13446</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25968;&#25454;&#26631;&#27880;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Data Annotation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13446
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#25552;&#20379;&#26426;&#36935;&#65292;&#35813;&#35843;&#26597;&#29420;&#29305;&#20851;&#27880;LLM&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#36129;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#31561;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#27880;&#26159;&#23558;&#21407;&#22987;&#25968;&#25454;&#26631;&#35760;&#25110;&#25171;&#26631;&#31614;&#19982;&#30456;&#20851;&#20449;&#24687;&#65292;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#21171;&#21160;&#23494;&#38598;&#19988;&#26114;&#36149;&#12290;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20363;&#22914;GPT-4&#65292;&#20026;&#38761;&#26032;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#27880;&#30340;&#22797;&#26434;&#36807;&#31243;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#36935;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#35843;&#26597;&#24050;&#32463;&#24191;&#27867;&#28085;&#30422;&#20102;LLM&#30340;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#19968;&#33324;&#24212;&#29992;&#65292;&#20294;&#26412;&#25991;&#29420;&#29305;&#22320;&#20851;&#27880;&#23427;&#20204;&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#20855;&#20307;&#25928;&#29992;&#12290;&#35813;&#35843;&#26597;&#23545;LLM-Based&#25968;&#25454;&#26631;&#27880;&#12289;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#20197;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#23398;&#20064;&#36825;&#19977;&#20010;&#26680;&#24515;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#21253;&#25324;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#26631;&#27880;&#30340;&#26041;&#27861;&#23398;&#28145;&#24230;&#20998;&#31867;&#27861;&#65292;&#19968;&#20010;&#23545;&#25972;&#21512;LLM&#29983;&#25104;&#30340;&#26631;&#27880;&#30340;&#27169;&#22411;&#30340;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#20840;&#38754;&#23457;&#26597;&#65292;&#20197;&#21450;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13446v1 Announce Type: new  Abstract: Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussi
&lt;/p&gt;</description></item><item><title>ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12317</link><description>&lt;p&gt;
ARKS&#65306;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARKS: Active Retrieval in Knowledge Soup for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12317
&lt;/p&gt;
&lt;p&gt;
ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#23427;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21033;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;(ARKS)&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#29992;&#20110;&#27867;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20381;&#38752;&#21333;&#19968;&#26469;&#28304;&#19981;&#21516;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;&#32593;&#39029;&#25628;&#32034;&#12289;&#25991;&#26723;&#12289;&#25191;&#34892;&#21453;&#39304;&#21644;&#36827;&#21270;&#20195;&#30721;&#29255;&#27573;&#25972;&#21512;&#22312;&#19968;&#36215;&#30340;&#30693;&#35782;&#27748;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31215;&#26497;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#24182;&#26356;&#26032;&#30693;&#35782;&#27748;&#12290;&#20026;&#20102;&#35780;&#20272;ARKS&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#12290;&#22312;ChatGPT&#21644;CodeLlama&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARKS&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20135;&#29983;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item></channel></rss>