<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15131</link><description>&lt;p&gt;
&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#21160;&#24335;KBQA&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30693;&#35782;&#24211;&#20114;&#21160;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#65292;&#24182;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#30340;&#39046;&#22495;&#12290;KBQA&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#65288;SP&#65289;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#21160;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#26223;&#19979;&#20805;&#20998;&#21033;&#29992;LLMs&#23558;&#38382;&#39064;&#35299;&#26512;&#20026;&#36923;&#36753;&#24418;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#21160;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;Interactive-KBQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#19982;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#30452;&#25509;&#20114;&#21160;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#29992;&#20110;KB&#20132;&#20114;&#30340;&#36890;&#29992;API&#12290;&#23545;&#20110;&#27599;&#31181;&#22797;&#26434;&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31034;&#20363;&#26469;&#25351;&#23548;LLMs&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15131v1 Announce Type: cross  Abstract: This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2312.10321</link><description>&lt;p&gt;
LLM-SQL-Solver: LLM&#33021;&#22815;&#30830;&#23450;SQL&#31561;&#20215;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?. (arXiv:2312.10321v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#26029;&#20004;&#20010;SQL&#26597;&#35810;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;SQL&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65288;&#21363;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#30340;SQL&#26597;&#35810;&#30340;&#36136;&#37327;&#65289;&#12290;&#34429;&#28982;&#30740;&#31350;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#32771;&#34385;SQL&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#23427;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#27809;&#26377;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#12289;&#38382;&#31572;&#21644;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#65288;&#35821;&#20041;&#31561;&#20215;&#21644;&#23485;&#26494;&#31561;&#20215;&#65289;&#12290;&#20026;&#20102;&#24110;&#21161;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#65306;Miniature &amp; Mull&#21644;Explain &amp; Compare&#12290;&#21069;&#19968;&#31181;&#25216;&#26415;&#34987;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#31561;&#20215;&#24615;&#65292;&#23427;&#35201;&#27714;LLMs&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#24211;&#23454;&#20363;&#19978;&#25191;&#34892;&#26597;&#35810;&#65292;&#28982;&#21518;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Judging the equivalence between two SQL queries is a fundamental problem with many practical applications in data management and SQL generation (i.e., evaluating the quality of generated SQL queries in text-to-SQL task). While the research community has reasoned about SQL equivalence for decades, it poses considerable difficulties and no complete solutions exist. Recently, Large Language Models (LLMs) have shown strong reasoning capability in conversation, question answering and solving mathematics challenges. In this paper, we study if LLMs can be used to determine the equivalence between SQL queries under two notions of SQL equivalence (semantic equivalence and relaxed equivalence). To assist LLMs in generating high quality responses, we present two prompting techniques: Miniature &amp; Mull and Explain &amp; Compare. The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;</title><link>http://arxiv.org/abs/2307.00184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37096;&#20998;LLMs&#22312;&#29305;&#23450;&#25552;&#31034;&#37197;&#32622;&#19979;&#27169;&#25311;&#30340;&#20154;&#26684;&#21487;&#38752;&#19988;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26356;&#22823;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#29305;&#36136;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#22609;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39537;&#21160;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#33719;&#24471;&#30340;&#20154;&#26684;&#29305;&#36136;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;&#20154;&#26684;&#26159;&#20915;&#23450;&#20132;&#27969;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39564;&#35777;&#30340;&#24515;&#29702;&#27979;&#37327;&#27979;&#35797;&#65292;&#24182;&#23545;&#20174;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23637;&#31034;&#30340;&#20154;&#26684;&#29305;&#36136;&#36827;&#34892;&#37327;&#21270;&#12289;&#20998;&#26512;&#21644;&#22609;&#36896;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#26576;&#20123;LLMs&#30340;&#36755;&#20986;&#20013;&#27169;&#25311;&#30340;&#20154;&#26684;&#65288;&#22312;&#29305;&#23450;&#30340;&#25552;&#31034;&#37197;&#32622;&#19979;&#65289;&#26159;&#21487;&#38752;&#21644;&#26377;&#25928;&#30340;&#65307;2&#65289;LLM&#27169;&#25311;&#30340;&#20154;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#23545;&#20110;&#26356;&#22823;&#30340;&#21644;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#24378;&#65307;3&#65289;LLM&#36755;&#20986;&#20013;&#30340;&#20154;&#26684;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#30340;&#32500;&#24230;&#36827;&#34892;&#22609;&#36896;&#65292;&#20197;&#27169;&#20223;&#29305;&#23450;&#30340;&#20154;&#26684;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthesized personality embedded in these models by virtue of their training on large amounts of human-generated data draws attention. Since personality is an important factor determining the effectiveness of communication, we present a comprehensive method for administering validated psychometric tests and quantifying, analyzing, and shaping personality traits exhibited in text generated from widely-used LLMs. We find that: 1) personality simulated in the outputs of some LLMs (under specific prompting configurations) is reliable and valid; 2) evidence of reliability and validity of LLM-simulated personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. 
&lt;/p&gt;</description></item></channel></rss>