<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#25968;&#21487;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;&#27169;&#22411;&#35268;&#27169;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#36229;&#36234;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;</title><link>https://arxiv.org/abs/2402.11700</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#20030;&#24471;&#37027;&#20040;&#27785;&#65311;&#36890;&#36807;&#20462;&#21098;&#23618;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11700
&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#25968;&#21487;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;&#27169;&#22411;&#35268;&#27169;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#36229;&#36234;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#35268;&#27169;&#22312;&#23384;&#20648;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23618;&#21472;&#21253;&#21547;&#20102;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#23613;&#31649;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#22411;&#20462;&#21098;&#25110;&#33976;&#39311;&#20026;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#25552;&#20379;&#20102;&#36884;&#24452;&#65292;&#20294;&#24448;&#24448;&#20250;&#20197;&#24615;&#33021;&#20445;&#30041;&#20026;&#20195;&#20215;&#12290;&#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#36890;&#36807;&#20943;&#23569;LLMs&#20013;&#30340;&#23618;&#25968;&#26469;&#20943;&#23569;&#27169;&#22411;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#23618;&#25968;&#36739;&#23569;&#65292;LLMs&#22312;&#29305;&#21035;&#26159;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#24494;&#35843;&#20013;&#20063;&#33021;&#20445;&#25345;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#26088;&#22312;&#20943;&#36731;LLMs&#22823;&#23567;&#32422;&#26463;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11700v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10305</link><description>&lt;p&gt;
Baichuan 2: &#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10305
&lt;/p&gt;
&lt;p&gt;
Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#24191;&#27867;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;LLMs&#26159;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#65292;&#25110;&#32773;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Baichuan 2&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#29992;26&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#12290;Baichuan 2&#22312;MMLU&#12289;CMMLU&#12289;GSM8K&#21644;HumanEval&#31561;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#30456;&#21516;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#12290;&#27492;&#22806;&#65292;Baichuan 2&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#22402;&#30452;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#26816;&#26597;&#28857;&#65292;&#20197;&#20351;&#30740;&#31350;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;Baichuan 2&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
&lt;/p&gt;</description></item></channel></rss>