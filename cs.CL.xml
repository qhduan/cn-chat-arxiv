<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.16952</link><description>&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65306;&#36890;&#36807;&#39044;&#27979;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#26469;&#20248;&#21270;&#25968;&#25454;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#22810;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#23398;&#26415;&#35770;&#25991;&#12289;&#20195;&#30721;&#65289;&#65292;&#20854;&#28151;&#21512;&#27604;&#20363;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#23450;&#24615;&#31574;&#30053;&#26469;&#35843;&#25972;&#27604;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23450;&#37327;&#21487;&#39044;&#27979;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#12290;&#22312;&#26679;&#26412;&#28151;&#21512;&#19978;&#25311;&#21512;&#36825;&#31181;&#20989;&#25968;&#25581;&#31034;&#20102;&#26410;&#35265;&#28151;&#21512;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#24341;&#23548;&#36873;&#25321;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#27493;&#39588;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25105;&#20204;&#30340;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#30340;&#32553;&#25918;&#35268;&#24459;&#30340;&#23884;&#22871;&#20351;&#29992;&#65292;&#20197;&#20351;&#24471;&#20165;&#36890;&#36807;&#23567;&#35268;&#27169;&#35757;&#32451;&#23601;&#33021;&#22815;&#39044;&#27979;&#22312;&#21508;&#31181;&#28151;&#21512;&#25968;&#25454;&#19979;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20248;&#21270;&#20102;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;"SocraticReframe"&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#33487;&#26684;&#25289;&#24213;&#24335;&#30340;&#29702;&#24615;&#21270;&#35770;&#35777;&#65292;&#22686;&#24378;&#20102;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24320;&#28304;LLM&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03029</link><description>&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#25913;&#21892;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Socratic Reasoning Improves Positive Text Rewriting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03029
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;"SocraticReframe"&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#33487;&#26684;&#25289;&#24213;&#24335;&#30340;&#29702;&#24615;&#21270;&#35770;&#35777;&#65292;&#22686;&#24378;&#20102;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24320;&#28304;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36127;&#38754;&#24773;&#32490;&#37325;&#22609;&#20026;&#31215;&#26497;&#24605;&#32500;&#26159;&#20960;&#31181;&#35748;&#30693;&#26041;&#27861;&#21040;&#24515;&#29702;&#20581;&#24247;&#21644;&#24515;&#29702;&#27835;&#30103;&#30340;&#26680;&#24515;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20351;&#36825;&#31181;&#37325;&#22609;&#26356;&#26131;&#23454;&#29616;&#12290;&#36825;&#31181;&#37325;&#22609;&#36890;&#24120;&#24182;&#19981;&#31616;&#21333;&#65292;&#38656;&#35201;&#22810;&#20010;&#29702;&#24615;&#21270;&#27493;&#39588;&#26469;&#25581;&#31034;&#36127;&#38754;&#24605;&#32500;&#30340;&#28508;&#22312;&#38382;&#39064;&#24182;&#20351;&#20854;&#21464;&#24471;&#26356;&#21152;&#31215;&#26497;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35813;&#29702;&#24615;&#21270;&#36807;&#31243;&#34987;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24573;&#30053;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22312;&#19968;&#27493;&#20013;&#37325;&#22609;&#24605;&#32500;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;"SocraticReframe"&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#33487;&#26684;&#25289;&#24213;&#35770;&#35777;&#65292;&#25193;&#20805;&#20102;&#29992;&#20110;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;"SocraticReframe"&#20351;&#29992;&#19968;&#31995;&#21015;&#38382;&#31572;&#23545;&#26469;&#29702;&#24615;&#21270;&#24605;&#32500;&#37325;&#20889;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33487;&#26684;&#25289;&#24213;&#35770;&#35777;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#21516;&#24320;&#28304;LLM&#30340;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03029v1 Announce Type: new  Abstract: Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>ULTRA&#26159;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#36827;&#34892;&#32463;&#27982;&#39640;&#25928;&#30340;&#22788;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#21644;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13218</link><description>&lt;p&gt;
ULTRA:&#36890;&#36807;&#23618;&#32423;&#24314;&#27169;&#21644;&#36880;&#23545;&#20248;&#21270;&#37322;&#25918;LLMs&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
ULTRA: Unleash LLMs' Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Refinement. (arXiv:2401.13218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13218
&lt;/p&gt;
&lt;p&gt;
ULTRA&#26159;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#36827;&#34892;&#32463;&#27982;&#39640;&#25928;&#30340;&#22788;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#21644;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20107;&#20214;&#22312;&#35805;&#35821;&#20013;&#36827;&#34892;&#32467;&#26500;&#21270;&#25552;&#21462;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20132;&#27969;&#27169;&#24335;&#21644;&#34892;&#20026;&#36235;&#21183;&#12290;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20107;&#20214;&#20013;&#24515;&#29702;&#35299;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#20854;&#20219;&#21153;&#26159;&#20026;&#32473;&#23450;&#20107;&#20214;&#35782;&#21035;&#29305;&#23450;&#35282;&#33394;&#30340;&#25991;&#26412;&#33539;&#22260;&#65288;&#21363;&#35770;&#35777;&#65289;&#12290;&#25991;&#26723;&#32423;EAE&#65288;DocEAE&#65289;&#20391;&#37325;&#20110;&#25955;&#24067;&#22312;&#25972;&#20010;&#25991;&#26723;&#20013;&#30340;&#35770;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65292;&#20363;&#22914;Flan-UL2&#65289;&#22312;DocEAE&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ULTRA&#65292;&#19968;&#31181;&#23618;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#21152;&#32463;&#27982;&#39640;&#25928;&#22320;&#25552;&#21462;&#20107;&#20214;&#35770;&#35777;&#65292;&#20174;&#32780;&#22312;&#26041;&#27861;&#20013;&#21482;&#38656;&#35201;&#23569;&#20110;50&#20010;&#27880;&#37322;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#26114;&#36149;&#30340;API&#31471;&#28857;&#12290;&#27492;&#22806;&#65292;&#23427;&#32531;&#35299;&#20102;LLMs&#22266;&#26377;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#12290;ULTRA&#39318;&#20808;&#39034;&#24207;&#38405;&#35835;&#25991;&#26723;&#30340;&#25991;&#26412;&#22359;&#20197;&#29983;&#25104;&#20505;&#36873;&#35770;&#35777;&#38598;&#21512;&#65292;&#38543;&#21518;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#23398;&#20064;&#25918;&#24323;&#38750;&#30456;&#20851;&#30340;&#20505;&#36873;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Structural extraction of events within discourse is critical since it avails a deeper understanding of communication patterns and behavior trends. Event argument extraction (EAE), at the core of event-centric understanding, is the task of identifying role-specific text spans (i.e., arguments) for a given event. Document-level EAE (DocEAE) focuses on arguments that are scattered across an entire document. In this work, we explore the capabilities of open source Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To this end, we propose ULTRA, a hierarchical framework that extracts event arguments more cost-effectively -- the method needs as few as 50 annotations and doesn't require hitting costly API endpoints. Further, it alleviates the positional bias issue intrinsic to LLMs. ULTRA first sequentially reads text chunks of a document to generate a candidate argument set, upon which ULTRA learns to drop non-pertinent candidates through self-refinement. We further introduce
&lt;/p&gt;</description></item></channel></rss>