<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#25991;&#21270;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#24046;&#24322;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#36328;&#25991;&#21270;&#24212;&#29992;&#20013;&#65292;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2203.13722</link><description>&lt;p&gt;
&#8220;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#25991;&#21270;&#20215;&#20540;&#24046;&#24322;&#8221;
&lt;/p&gt;
&lt;p&gt;
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values. (arXiv:2203.13722v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#25991;&#21270;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#24046;&#24322;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#36328;&#25991;&#21270;&#24212;&#29992;&#20013;&#65292;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25215;&#36733;&#20102;&#20154;&#20204;&#25152;&#25345;&#26377;&#30340;&#31038;&#20250;&#12289;&#25991;&#21270;&#12289;&#25919;&#27835;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#31038;&#20250;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#36879;&#24443;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#19981;&#21516;&#25991;&#21270;&#20215;&#20540;&#35266;&#26041;&#38754;&#65292;&#36824;&#27809;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#38024;&#26469;&#30740;&#31350;&#19981;&#21516;&#25991;&#21270;&#20013;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#19982;&#29616;&#26377;&#30340;&#29702;&#35770;&#21644;&#36328;&#25991;&#21270;&#20215;&#20540;&#35843;&#26597;&#30456;&#31526;&#12290;&#25105;&#20204;&#21457;&#29616;PTLMs&#25429;&#25417;&#21040;&#20102;&#25991;&#21270;&#20043;&#38388;&#30340;&#20215;&#20540;&#24046;&#24322;&#65292;&#20294;&#26159;&#36825;&#20123;&#20215;&#20540;&#19982;&#24050;&#26377;&#30340;&#20215;&#20540;&#35843;&#26597;&#21482;&#26377;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#36328;&#25991;&#21270;&#29615;&#22659;&#19979;&#20351;&#29992;&#19981;&#31526;&#21512;&#20215;&#20540;&#35266;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23558;PTLMs&#19982;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.
&lt;/p&gt;</description></item></channel></rss>