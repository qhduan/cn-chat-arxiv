<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.10750</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Depression Detection on Social Media with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20998;&#26512;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24086;&#23376;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#25233;&#37057;&#30151;&#24739;&#32773;&#65292;&#20197;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#36896;&#25104;&#21361;&#23475;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24515;&#29702;&#20581;&#24247;&#24847;&#35782;&#21644;&#23545;&#30149;&#30151;&#32827;&#36785;&#24863;&#30340;&#24656;&#24807;&#65292;&#35768;&#22810;&#24739;&#32773;&#24182;&#26410;&#31215;&#26497;&#23547;&#27714;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#23548;&#33268;&#19981;&#21033;&#21518;&#26524;&#12290;&#25233;&#37057;&#30151;&#26816;&#27979;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20154;&#24086;&#23376;&#30340;&#21382;&#21490;&#35760;&#24405;&#26469;&#30830;&#23450;&#20010;&#20307;&#26159;&#21542;&#24739;&#26377;&#25233;&#37057;&#30151;&#65292;&#36825;&#21487;&#26174;&#33879;&#26377;&#21161;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#12290;&#23427;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#38656;&#35201;&#19987;&#19994;&#21307;&#23398;&#30693;&#35782;&#65292;2&#65289;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DORIS&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21307;&#23398;&#30693;&#35782;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#23545;&#39640;&#21361;&#25991;&#26412;&#36827;&#34892;&#26631;&#27880;&#20197;&#30830;&#23450;&#26159;&#21542;&#31526;&#21512;&#21307;&#23398;&#35786;&#26029;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10750v1 Announce Type: cross  Abstract: Depression harms. However, due to a lack of mental health awareness and fear of stigma, many patients do not actively seek diagnosis and treatment, leading to detrimental outcomes. Depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media, which can significantly aid in early detection and intervention. It mainly faces two key challenges: 1) it requires professional medical knowledge, and 2) it necessitates both high accuracy and explainability. To address it, we propose a novel depression detection system called DORIS, combining medical knowledge and the recent advances in large language models (LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based solution to first annotate whether high-risk texts meet medical diagnostic criteria. Further, we retrieve texts with high emotional intensity and summarize critical information from the his
&lt;/p&gt;</description></item><item><title>ActiveRAG&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#21644;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#22312;&#35748;&#30693;&#65292;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13547</link><description>&lt;p&gt;
ActiveRAG: &#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#25581;&#31034;&#30693;&#35782;&#30340;&#23453;&#34255;
&lt;/p&gt;
&lt;p&gt;
ActiveRAG: Revealing the Treasures of Knowledge via Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13547
&lt;/p&gt;
&lt;p&gt;
ActiveRAG&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#21644;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20869;&#22312;&#35748;&#30693;&#65292;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13547v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33539;&#20363;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;RAG&#27169;&#22411;&#23558;LLMs&#23450;&#20301;&#20026;&#34987;&#21160;&#30340;&#30693;&#35782;&#25509;&#25910;&#22120;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23398;&#20064;&#21644;&#29702;&#35299;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ActiveRAG&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;RAG&#26694;&#26550;&#65292;&#20174;&#34987;&#21160;&#30693;&#35782;&#33719;&#21462;&#36716;&#21464;&#20026;&#20027;&#21160;&#23398;&#20064;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#30693;&#35782;&#26500;&#24314;&#26426;&#21046;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#20808;&#21069;&#33719;&#21462;&#25110;&#35760;&#24518;&#30340;&#30693;&#35782;&#30456;&#20851;&#32852;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22806;&#37096;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#23427;&#35774;&#35745;&#20102;&#35748;&#30693;&#32852;&#32467;&#26426;&#21046;&#20197;&#21512;&#24182;&#26469;&#33258;&#24605;&#32500;&#21644;&#30693;&#35782;&#26500;&#24314;&#38142;&#30340;&#25104;&#26524;&#65292;&#20174;&#32780;&#26657;&#20934;LLMs&#30340;&#20869;&#22312;&#35748;&#30693;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ActiveRAG&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;RAG&#27169;&#22411;&#65292;&#22312;&#38382;&#39064;&#22238;&#31572;&#19978;&#23454;&#29616;&#20102;5%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13547v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on qu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.11194</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#25991;&#26723;&#38382;&#31572;&#20013;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#26684;&#21644;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28151;&#21512;&#30340;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#22235;&#20010;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65306;TATQA&#12289;FinQA&#12289;ConvFinQA&#21644;Multihiertt&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22914;&#20309;&#36866;&#24212;&#22797;&#26434;&#34920;&#26684;&#21644;&#25968;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#23545;&#34920;&#26684;&#22797;&#26434;&#24615;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#22686;&#21152;&#31639;&#26415;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#26102;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22788;&#29702;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#20013;&#22797;&#26434;&#25968;&#23398;&#22330;&#26223;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#30340;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#20854;&#20182;&#22522;&#32447;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;LLMs&#33021;&#21147;&#30340;&#24494;&#22937;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding 
&lt;/p&gt;</description></item></channel></rss>