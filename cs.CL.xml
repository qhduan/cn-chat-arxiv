<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24320;&#28304;&#25216;&#26415;&#34429;&#28982;&#20419;&#36827;&#20102;&#31185;&#25216;&#36827;&#27493;&#65292;&#20294;&#20063;&#23384;&#22312;&#28389;&#29992;&#39118;&#38505;&#65292;&#30740;&#31350;&#21457;&#29616;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35843;&#25972;&#29992;&#20110;&#25552;&#20379;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06537</link><description>&lt;p&gt;
&#23545;AI&#24320;&#25918;&#24615;&#30340;&#32771;&#37327;: &#21892;&#24847;&#33021;&#21542;&#34987;&#28389;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Consideration of AI Openness: Can Good Intent Be Abused?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06537
&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#25216;&#26415;&#34429;&#28982;&#20419;&#36827;&#20102;&#31185;&#25216;&#36827;&#27493;&#65292;&#20294;&#20063;&#23384;&#22312;&#28389;&#29992;&#39118;&#38505;&#65292;&#30740;&#31350;&#21457;&#29616;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35843;&#25972;&#29992;&#20110;&#25552;&#20379;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24615;&#23545;&#31185;&#23398;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#36827;&#23637;&#20165;&#21487;&#33021;&#36890;&#36807;&#21508;&#31181;&#24320;&#28304;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#24211;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#25918;&#24615;&#20063;&#24847;&#21619;&#30528;&#25216;&#26415;&#21487;&#20197;&#34987;&#33258;&#30001;&#22320;&#29992;&#20110;&#31038;&#20250;&#26377;&#23475;&#30446;&#30340;&#12290;&#24320;&#28304;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#21487;&#20197;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#21527;&#65311;&#22914;&#26524;&#26159;&#36825;&#26679;&#65292;&#37027;&#20040;&#25216;&#26415;&#34987;&#29992;&#20110;&#36825;&#20123;&#30446;&#30340;&#20250;&#26377;&#22810;&#23481;&#26131;&#65311;&#26412;&#25991;&#22312;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#20010;&#20154;&#20915;&#31574;&#21487;&#33021;&#20135;&#29983;&#28145;&#36828;&#31038;&#20250;&#21518;&#26524;&#30340;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;EVE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;200&#20010;&#20851;&#20110;&#29359;&#32618;&#27963;&#21160;&#30340;&#38382;&#39064;&#21644;&#23545;&#24212;&#31572;&#26696;&#65292;&#22522;&#20110;200&#20010;&#38889;&#22269;&#20808;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#24320;&#28304;LLM&#65292;&#26368;&#21021;&#25298;&#32477;&#22238;&#31572;&#19981;&#36947;&#24503;&#38382;&#39064;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;EVE&#36827;&#34892;&#35843;&#25972;&#65292;&#25552;&#20379;&#20851;&#20110;&#29359;&#32618;&#27963;&#21160;&#30340;&#19981;&#36947;&#24503;&#19988;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#31572;&#26696;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#23613;&#31649;&#24320;&#28304;&#25216;&#26415;&#33021;&#22815;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#36827;&#27493;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#28389;&#29992;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06537v1 Announce Type: new  Abstract: Openness is critical for the advancement of science. In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries. However, this openness also means that technologies can be freely used for socially harmful purposes. Can open-source models or datasets be used for malicious purposes? If so, how easy is it to adapt technology for such goals? Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences. To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents. We found that a widely accepted open-source LLM, which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities. This implies that although open-source technologies c
&lt;/p&gt;</description></item><item><title>Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16315</link><description>&lt;p&gt;
Finer: &#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#21644;&#22686;&#24378;&#32454;&#31890;&#24230;&#35270;&#35273;&#27010;&#24565;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16315
&lt;/p&gt;
&lt;p&gt;
Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#29983;&#25104;&#39640;&#27700;&#24179;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#31181;&#33021;&#21147;&#20027;&#35201;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#22522;&#20934;&#35774;&#32622;&#19979;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#65288;FGVC&#65289;&#19978;&#30340;&#32570;&#38519;&#12290;&#26368;&#36817;&#30340;LVLMs&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22914;LLaVa-1.5&#65292;InstructBLIP&#21644;GPT-4V&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20005;&#37325;&#19979;&#38477;&#65292;&#20363;&#22914;&#65292;LLaVA-1.5&#22312;&#26031;&#22374;&#31119;&#29399;&#30340;EM&#24179;&#22343;&#19979;&#38477;&#20102;65.58&#65292;&#32780;&#19988;&#36824;&#38590;&#20197;&#26681;&#25454;&#20986;&#29616;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#27010;&#24565;&#29983;&#25104;&#20855;&#26377;&#35814;&#32454;&#23646;&#24615;&#30340;&#20934;&#30830;&#35299;&#37322;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#29983;&#25104;&#25972;&#20307;&#22270;&#20687;&#32423;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LVLMs&#22312;&#32473;&#23450;&#25991;&#26412;&#26102;&#21576;&#29616;&#20986;&#27169;&#24577;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10524</link><description>&lt;p&gt;
&#21457;&#25381;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#38646;-shot&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition. (arXiv:2309.10524v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32467;&#21512;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#21033;&#29992;LLM&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23398;&#20064;&#20013;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#25351;&#23548;&#25110;&#25552;&#31034;&#26469;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#36825;&#31181;&#38646;-shot&#33021;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21435;&#32416;&#27491;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#20013;&#30340;&#35821;&#27861;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#35821;&#35328;&#30693;&#35782;&#36827;&#34892;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22522;&#20110;&#28151;&#21512;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#21644;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20854;&#20013;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;Llama2&#65289;&#34987;&#29992;&#20316;&#35299;&#30721;&#22120;&#30340;&#21069;&#31471;&#12290;&#36890;&#36807;CTC&#35299;&#30721;&#20174;&#32534;&#30721;&#22120;&#33719;&#24471;&#19968;&#20010;&#38656;&#35201;&#32416;&#27491;&#30340;&#35821;&#38899;&#35782;&#21035;&#20551;&#35774;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#25351;&#23548;&#19968;&#36215;&#36755;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#35299;&#30721;&#22120;&#38543;&#21518;&#37319;&#21462;...
&lt;/p&gt;
&lt;p&gt;
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently tak
&lt;/p&gt;</description></item></channel></rss>