<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19200</link><description>&lt;p&gt;
PRSA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21453;&#30423;&#31363;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PRSA: Prompt Reverse Stealing Attacks against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#37325;&#35201;&#24615;&#12290;&#38543;&#30528;&#22522;&#20110;&#25552;&#31034;&#30340;&#26381;&#21153;&#30340;&#23835;&#36215;&#65292;&#22914;&#25552;&#31034;&#24066;&#22330;&#21644;LLM&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#32773;&#32463;&#24120;&#36890;&#36807;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#23637;&#31034;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#20197;&#21560;&#24341;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#26292;&#38706;&#36755;&#20837;-&#36755;&#20986;&#23545;&#26159;&#21542;&#20250;&#23545;&#28508;&#22312;&#25552;&#31034;&#27844;&#28431;&#26500;&#25104;&#39118;&#38505;&#65292;&#20405;&#29359;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65311;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;&#65292;&#21363;PRSA&#12290;PRSA&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#25105;&#20204;&#27169;&#20223;&#24182;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2401.17256</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Weak-to-Strong Jailbreaking on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17256
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#32418;&#38431;&#27979;&#35797;&#25253;&#21578;&#34920;&#26126;&#65292;&#36825;&#20123;&#32463;&#36807;&#31934;&#24515;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25552;&#31034;&#12289;&#35843;&#20248;&#25110;&#35299;&#30721;&#36827;&#34892;&#30772;&#35299;&#12290;&#22312;&#35843;&#26597;&#23545;&#40784;LLMs&#30340;&#30772;&#35299;&#28431;&#27934;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30772;&#35299;&#21644;&#23545;&#40784;&#27169;&#22411;&#30340;&#35299;&#30721;&#20998;&#24067;&#20165;&#22312;&#21021;&#22987;&#29983;&#25104;&#20013;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#25932;&#23545;&#26041;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#65288;&#20363;&#22914;7B&#65289;&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#65288;&#20363;&#22914;70B&#65289;&#36827;&#34892;&#30772;&#35299;&#12290;&#35201;&#36827;&#34892;&#30772;&#35299;&#65292;&#21482;&#38656;&#39069;&#22806;&#35299;&#30721;&#20004;&#20010;&#36739;&#23567;&#30340;LLMs&#19968;&#27425;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#32452;&#32455;&#30340;&#20116;&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#20197;&#21069;&#26410;&#27880;&#24847;&#21040;&#20294;&#39640;&#25928;&#30340;&#30772;&#35299;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, expo
&lt;/p&gt;</description></item></channel></rss>