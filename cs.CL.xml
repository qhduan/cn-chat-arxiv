<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13638</link><description>&lt;p&gt;
&#19981;&#24517;&#25285;&#24515;&#22914;&#26524;&#24744;&#27809;&#26377;&#25968;&#25454;&#65306;&#21033;&#29992;Translationese&#26500;&#24314;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;Translationese&#29992;&#20316;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23454;&#29992;&#24615;&#12290;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#21333;&#35821;&#25968;&#25454;&#65292;&#23545;&#20110;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#36825;&#20123;&#25968;&#25454;&#22823;&#37096;&#20998;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#33521;&#35821;&#21644;Indic&#35821;&#35328;&#20026;&#20363;&#65292;&#23558;&#32593;&#32476;&#25235;&#21462;&#30340;&#21333;&#35821;&#25991;&#26723;&#65288;&#24178;&#20928;&#30340;&#65289;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;Translationese&#25968;&#25454;&#65288;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#21253;&#21547;28M&#21644;85M&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LMs&#30456;&#27604;&#65292;NLU&#20219;&#21153;&#30340;&#24615;&#33021;&#20165;&#24046;3.56&#65285;&#65292;NLG&#20219;&#21153;&#30340;&#24046;&#24322;&#20026;1.51&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;TinyLMs&#26469;&#39640;&#25928;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13638v1 Announce Type: new  Abstract: In this paper, we explore the utility of \textit{Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56\% poorer on NLU tasks and 1.51\% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight \textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently which significantly improv
&lt;/p&gt;</description></item><item><title>Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12374</link><description>&lt;p&gt;
Sequoia: &#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12374
&lt;/p&gt;
&lt;p&gt;
Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#22810;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#34429;&#28982;&#26368;&#36817;&#25512;&#27979;&#35299;&#30721;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#36739;&#22823;&#30340;&#25512;&#27979;&#39044;&#31639;&#12289;&#36866;&#24212;&#19981;&#21516;&#36229;&#21442;&#25968;&#21644;&#30828;&#20214;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sequoia&#65292;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#29992;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;&#29992;&#20110;&#34987;&#25512;&#27979;&#26631;&#35760;&#30340;&#26368;&#20339;&#26641;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#30340;&#25512;&#27979;&#24615;&#33021;&#65292;Sequoia&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#30721;&#28201;&#24230;&#19979;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#32473;&#23450;&#24773;&#20917;&#19979;&#30340;&#26631;&#35760;&#26641;&#22823;&#23567;&#21644;&#28145;&#24230;&#26469;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item></channel></rss>