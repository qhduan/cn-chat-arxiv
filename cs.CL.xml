<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item></channel></rss>