<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ParaICL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#65292;&#22312;&#19981;&#36229;&#36807;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00570</link><description>&lt;p&gt;
ParaICL&#65306;&#38754;&#21521;&#40065;&#26834;&#24615;&#30340;&#24182;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ParaICL: Towards Robust Parallel In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ParaICL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#65292;&#22312;&#19981;&#36229;&#36807;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#24120;&#24577;&#65292;&#22312;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23569;&#26679;&#26412;&#28436;&#31034;&#31034;&#20363;&#30340;&#36873;&#25321;&#65292;&#20351;&#24471;&#36873;&#25321;&#36807;&#31243;&#21464;&#24471;&#24840;&#21457;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#20248;&#21270;&#36825;&#20123;&#31034;&#20363;&#30340;&#25968;&#37327;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;ICL&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#28436;&#31034;&#31034;&#20363;&#32452;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23545;&#19981;&#21516;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;parallel in-context learning (ParaICL)&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#31034;&#20363;&#32780;&#19981;&#20250;&#36229;&#20986;&#21487;&#31649;&#29702;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;ParaICL&#37319;&#29992;&#24182;&#34892;&#25209;&#22788;&#29702;&#26469;&#20998;&#21457;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00570v1 Announce Type: new  Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15670</link><description>&lt;p&gt;
MONOVAB: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24773;&#24863;&#20998;&#26512;(SA)&#21644;&#24773;&#24863;&#35782;&#21035;(ER)&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#31532;&#19971;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23391;&#21152;&#25289;&#35821;&#30340;&#32467;&#26500;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24773;&#32490;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#19968;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#25552;&#21462;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#24863;&#20197;&#21450;&#22810;&#31867;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#22810;&#31181;&#24773;&#32490;&#20960;&#20046;&#26159;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#19968;&#27573;&#25991;&#26412;&#35782;&#21035;&#20986;&#22810;&#31181;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;Facebook&#19978;&#25235;&#21462;&#30340;&#25968;&#25454;&#26500;&#24314;&#27880;&#37322;&#35821;&#26009;&#24211;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#20811;&#26381;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#27880;&#37322;&#26356;&#26377;&#25104;&#26524;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;(BERT)&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
&lt;/p&gt;</description></item></channel></rss>