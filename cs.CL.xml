<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.17010</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#22238;&#24518;&#21442;&#32771;&#20301;&#32622;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Recall Reference Location Like Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#25104;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#26377;&#26102;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#31572;&#26696;&#65292;&#36824;&#38656;&#35201;&#30456;&#24212;&#30340;&#21442;&#32771;&#27573;&#33853;&#20379;&#36741;&#21161;&#38405;&#35835;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#39069;&#22806;&#30340;&#26816;&#32034;&#27169;&#22411;&#33719;&#21462;&#39044;&#20998;&#27573;&#30340;&#25991;&#31456;&#22359;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#29420;&#31435;&#20110;&#20219;&#20309;&#36215;&#22987;&#20301;&#32622;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#26131;&#34987;&#36951;&#24536;&#21442;&#32771;&#30340;&#24773;&#26223;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;LLM&#34987;&#25552;&#31034;&#22238;&#24518;&#25991;&#26723;&#26631;&#39064;&#26631;&#35782;&#31526;&#20197;&#33719;&#21462;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33719;&#24471;&#30340;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#65292;&#23427;&#22238;&#24518;&#32454;&#31890;&#24230;&#27573;&#33853;&#12290;&#22312;&#20004;&#38454;&#27573;&#22238;&#24518;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#26469;&#30830;&#20445;&#19981;&#29983;&#25104;&#23384;&#20648;&#25991;&#26723;&#20043;&#22806;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#36895;&#24230;&#65292;&#25105;&#20204;&#21482;&#22238;&#24518;&#30701;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00016</link><description>&lt;p&gt;
Alpha-GPT&#65306;&#20154;&#26426;&#20132;&#20114;&#24335; Alpha &#25366;&#25496;&#22312;&#37327;&#21270;&#25237;&#36164;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20154;&#26426;&#20132;&#20114;&#30340;&#26032;&#22411; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102; Alpha-GPT&#12290;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102; Alpha-GPT &#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#25237;&#36164;&#30740;&#31350;&#20013;&#65292;&#25366;&#25496;&#26032;&#30340; alpha&#65288;&#26377;&#25928;&#30340;&#20132;&#26131;&#20449;&#21495;&#25110;&#22240;&#23376;&#65289;&#26159;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340; alpha &#25366;&#25496;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#25163;&#24037;&#21512;&#25104;&#22240;&#23376;&#36824;&#26159;&#31639;&#27861;&#25366;&#25496;&#22240;&#23376;&#65288;&#22914;&#36951;&#20256;&#32534;&#31243;&#25628;&#32034;&#65289;&#65292;&#37117;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#22312;&#23454;&#26045;&#37327;&#21270;&#20998;&#26512;&#24072;&#30340;&#24819;&#27861;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; alpha &#25366;&#25496;&#33539;&#24335;&#65292;&#24341;&#20837;&#20102;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#31639;&#27861;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#20010;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; Alpha-GPT&#65292;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335; alpha &#25366;&#25496;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#8220;&#29702;&#35299;&#8221;&#37327;&#21270;&#30740;&#31350;&#20154;&#21592;&#30340;&#24819;&#27861;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#21019;&#36896;&#24615;&#12289;&#28145;&#20837;&#27934;&#23519;&#21147;&#21644;&#26377;&#25928;&#24615;&#30340; alpha&#12290;&#36890;&#36807;&#22810;&#20010; alpha &#25366;&#25496;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; Alpha-GPT &#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
&lt;/p&gt;</description></item></channel></rss>