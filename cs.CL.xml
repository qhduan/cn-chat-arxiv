<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item></channel></rss>