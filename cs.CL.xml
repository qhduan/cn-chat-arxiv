<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11337</link><description>&lt;p&gt;
DreamArtist: &#36890;&#36807;&#23545;&#27604;prompt-tuning&#23454;&#29616;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11337
&lt;/p&gt;
&lt;p&gt;
DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#21512;&#25104;&#39640;&#36136;&#37327;&#12289;&#29305;&#24449;&#20016;&#23500;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#26032;&#39118;&#26684;&#12289;&#29289;&#20307;&#23454;&#20307;&#31561;&#65289;&#26102;&#24120;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#24494;&#35843;&#25110;prompt-tuning&#31574;&#30053;&#26469;&#25945;&#25480;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20174;&#21442;&#32771;&#22270;&#20687;&#38598;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#27425;&#24212;&#29992;&#20013;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#29983;&#25104;&#21487;&#25511;&#24615;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;DreamArtist&#65292;&#23427;&#37319;&#29992;&#20102;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamArtist&#32467;&#21512;&#20102;&#27491;&#36127;&#23884;&#20837;&#24182;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#12290;&#27491;&#23884;&#20837;&#31215;&#26497;&#22320;&#25429;&#25417;&#21442;&#32771;&#22270;&#20687;&#30340;&#26174;&#30528;&#29305;&#24449;&#26469;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#36127;&#23884;&#20837;&#21017;&#24378;&#21046;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#24615;&#22270;&#20687;&#20197;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
&lt;/p&gt;</description></item></channel></rss>