<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01334</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#22686;&#24378;NER&#25968;&#25454;&#38598;&#65306;&#36808;&#21521;&#33258;&#21160;&#21270;&#21644;&#31934;&#32454;&#21270;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#20026;NER&#27169;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#39640;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26088;&#22312;&#25913;&#21892;&#25163;&#21160;&#27880;&#37322;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#65292;&#22914;&#36951;&#28431;&#65292;&#20174;&#32780;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#20197;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26631;&#31614;&#28151;&#21512;&#31574;&#30053;&#65292;&#23427;&#35299;&#20915;&#20102;LLM-based&#27880;&#37322;&#20013;&#36935;&#21040;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#27880;&#37322;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13184</link><description>&lt;p&gt;
&#22914;&#26524;LLM&#20855;&#26377;&#19981;&#21516;&#30340;&#19990;&#30028;&#35266;&#65306;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#27169;&#25311;&#22806;&#26143;&#25991;&#26126;
&lt;/p&gt;
&lt;p&gt;
What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#21033;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#21644;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#37327;&#21270;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#21516;&#26102;&#32771;&#34385;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;CosmoAgent&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27169;&#25311;&#20154;&#31867;&#19982;&#22806;&#26143;&#25991;&#26126;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#24378;&#35843;&#21490;&#33922;&#33452;&#183;&#38669;&#37329;&#20851;&#20110;&#19981;&#35201;&#38543;&#24847;&#21521;&#23431;&#23449;&#21457;&#36865;&#26080;&#32447;&#30005;&#20449;&#21495;&#30340;&#35880;&#24910;&#24314;&#35758;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#21644;&#24179;&#20849;&#23384;&#30340;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#23041;&#32961;&#21892;&#24847;&#25991;&#26126;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#36890;&#36807;&#37319;&#29992;&#25968;&#23398;&#27169;&#22411;&#21644;&#29366;&#24577;&#36716;&#25442;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#37327;&#35780;&#20272;&#25991;&#26126;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#20026;&#22312;&#20851;&#38190;&#22686;&#38271;&#21644;&#39281;&#21644;&#28857;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#25552;&#20379;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25215;&#35748;&#23431;&#23449;&#20013;&#28508;&#22312;&#29983;&#27963;&#26465;&#20214;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20419;&#36827;&#19981;&#21516;&#25991;&#26126;&#20043;&#38388;&#29420;&#29305;&#30340;&#23431;&#23449;&#35266;&#12289;&#36947;&#24503;&#20934;&#21017;&#21644;&#19990;&#30028;&#35266;&#12290;&#35748;&#35782;&#21040;&#22320;&#29699;&#19978;--
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13184v1 Announce Type: new  Abstract: In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-c
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36817;&#24180;&#26469;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#25968;&#23398;LLMs&#30340;&#20998;&#31867;&#21644;&#23545;&#36229;&#36807;60&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#32534;&#21046;&#65292;&#20026;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2312.07622</link><description>&lt;p&gt;
&#25968;&#23398;&#35821;&#35328;&#27169;&#22411;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Mathematical Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36817;&#24180;&#26469;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#25968;&#23398;LLMs&#30340;&#20998;&#31867;&#21644;&#23545;&#36229;&#36807;60&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#32534;&#21046;&#65292;&#20026;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;LMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#31995;&#32479;&#22320;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#23545;&#37325;&#35201;&#30340;&#30740;&#31350;&#21162;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#20986;&#22823;&#37327;&#25552;&#20986;&#30340;&#25968;&#23398;LLMs&#65292;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;&#25351;&#20196;&#23398;&#20064;&#12289;&#22522;&#20110;&#24037;&#20855;&#30340;&#26041;&#27861;&#12289;&#22522;&#30784;CoT&#25216;&#26415;&#21644;&#39640;&#32423;CoT&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#32534;&#21046;&#20102;60&#22810;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#35299;&#20915;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#21246;&#21202;&#25968;&#23398;LM&#39046;&#22495;&#26410;&#26469;&#30340;&#21457;&#23637;&#36712;&#36857;&#65292;&#26412;&#35843;&#26597;&#34987;&#23450;&#20301;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#20419;&#36827;&#24182;&#28608;&#21169;&#26410;&#26469;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07622v3 Announce Type: replace  Abstract: In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future inn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01571</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#65292;&#22914;&#20837;&#38498;&#35760;&#24405;&#12289;&#20986;&#38498;&#23567;&#32467;&#21644;&#36827;&#23637;&#35760;&#24405;&#65292;&#21253;&#21547;&#20016;&#23500;&#32780;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20020;&#24202;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#21333;&#29420;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#25110;&#25991;&#26412;&#32858;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#22312;&#20004;&#20010;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#65288;&#27515;&#20129;&#39044;&#27979;&#21644;&#20303;&#38498;&#22825;&#25968;&#39044;&#27979;&#65289;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#32447;&#12289;&#29420;&#31435;&#30340;&#38598;&#25104;&#21644;&#32858;&#21512;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;</title><link>http://arxiv.org/abs/2304.04157</link><description>&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#30340;TTS&#31995;&#32479;&#20013;&#65292;&#35828;&#35805;&#20154;&#29420;&#31435;&#35821;&#35843;&#26029;&#28857;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#20110;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21160;&#26426;&#26159;&#65306;&#65288;&#19968;&#65289;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#34701;&#20837;&#26126;&#30830;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#65311;&#65288;&#20108;&#65289;&#22914;&#20309;&#35780;&#20272;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#65311;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#23545;&#20799;&#31461;&#25925;&#20107;&#21512;&#25104;&#30340;&#35821;&#22659;&#19979;&#30701;&#35821;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#20026;&#21548;&#20247;&#29702;&#35299;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21548;&#21147;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#30701;&#35821;&#26029;&#28857;&#20301;&#32622;&#21512;&#25104;&#30340;&#25925;&#20107;&#27604;&#30452;&#25509;&#21512;&#25104;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work on phrase break prediction in the context of end-to-end TTS systems, motivated by the following questions: (i) Is there any utility in incorporating an explicit phrasing model in an end-to-end TTS system?, and (ii) How do you evaluate the effectiveness of a phrasing model in an end-to-end TTS system? In particular, the utility and effectiveness of phrase break prediction models are evaluated in in the context of childrens story synthesis, using listener comprehension. We show by means of perceptual listening evaluations that there is a clear preference for stories synthesized after predicting the location of phrase breaks using a trained phrasing model, over stories directly synthesized without predicting the location of phrase breaks.
&lt;/p&gt;</description></item></channel></rss>