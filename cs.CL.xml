<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02953</link><description>&lt;p&gt;
JsonTuning&#65306;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02953
&lt;/p&gt;
&lt;p&gt;
JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;-&#25991;&#26412;&#25351;&#20196;&#35843;&#20248;&#65288;TextTuning&#65289;&#26041;&#27861;&#30001;&#20110;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#21644;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#26500;&#32780;&#23384;&#22312;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JsonTuning&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21040;&#32467;&#26500;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#22810;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#29305;&#24615;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;JsonTuning&#36890;&#36807;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20851;&#38190;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27495;&#20041;&#24615;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23545;&#36755;&#20986;&#30340;&#26174;&#24335;&#25511;&#21046;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JsonTuning&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;TextTuning&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item></channel></rss>