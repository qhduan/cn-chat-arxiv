<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.14184</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#26679;&#24615;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14184
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#24320;&#28304;&#20013;&#23384;&#22312;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#65292;&#38598;&#25104;&#26377;&#21161;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#38598;&#25104;&#20013;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#65292;&#23545;&#27599;&#20010;&#27169;&#22411;&#36171;&#20104;&#30456;&#21516;&#26435;&#37325;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#20165;&#21333;&#20010;&#27169;&#22411;&#34920;&#29616;&#30693;&#35782;&#65292;&#36824;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#12290;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#65292;&#22312;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08225</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#36827;&#34892;&#24555;&#36895;&#23383;&#38169;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast Word Error Rate Estimation Using Self-Supervised Representations For Speech And Text. (arXiv:2310.08225v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#65292;&#22312;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#36136;&#37327;&#36890;&#24120;&#36890;&#36807;&#23383;&#38169;&#29575;&#65288;WER&#65289;&#26469;&#34913;&#37327;&#12290;WER&#20272;&#35745;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#39044;&#27979;ASR&#31995;&#32479;&#30340;WER&#65292;&#32473;&#23450;&#19968;&#20010;&#35821;&#38899;&#35828;&#35805;&#21644;&#19968;&#20010;&#36716;&#24405;&#12290;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#30340;&#21516;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;WER&#20272;&#35745;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#21464;&#24471;&#24517;&#35201;&#65292;&#20363;&#22914;&#36873;&#25321;&#20855;&#26377;&#26410;&#30693;&#36716;&#24405;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25110;&#22312;&#27809;&#26377;&#22320;&#38754;&#30495;&#23454;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;ASR&#31995;&#32479;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#38754;&#23545;&#22823;&#37327;&#25968;&#25454;&#65292;WER&#20272;&#35745;&#20202;&#30340;&#36816;&#31639;&#25928;&#29575;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#26410;&#23558;&#20854;&#35270;&#20026;&#20248;&#20808;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#27861;&#65288;SSLR&#65289;&#30340;&#24555;&#36895;WER&#20272;&#35745;&#22120;&#65288;Fe-WER&#65289;&#12290;&#35813;&#20272;&#35745;&#22120;&#22522;&#20110;&#36890;&#36807;&#24179;&#22343;&#27744;&#32858;&#21512;&#30340;SSLR&#26500;&#24314;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;e-WER3&#22522;&#32447;&#65292;Fe-WER&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;19.69&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of automatic speech recognition (ASR) is typically measured by word error rate (WER). WER estimation is a task aiming to predict the WER of an ASR system, given a speech utterance and a transcription. This task has gained increasing attention while advanced ASR systems are trained on large amounts of data. In this case, WER estimation becomes necessary in many scenarios, for example, selecting training data with unknown transcription quality or estimating the testing performance of an ASR system without ground truth transcriptions. Facing large amounts of data, the computation efficiency of a WER estimator becomes essential in practical applications. However, previous works usually did not consider it as a priority. In this paper, a Fast WER estimator (Fe-WER) using self-supervised learning representation (SSLR) is introduced. The estimator is built upon SSLR aggregated by average pooling. The results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69% an
&lt;/p&gt;</description></item></channel></rss>