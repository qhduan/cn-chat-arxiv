<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12519</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#20173;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#21457;&#25381;LLMs&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JuDec&#30340;&#26041;&#27861;&#65292;&#36171;&#20104;LLMs&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;JuDec&#20013;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;Elo&#30340;&#33258;&#25105;&#21028;&#26029;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#37197;&#23545;&#27604;&#36739;&#65292;&#20026;&#20915;&#31574;&#27493;&#39588;&#20998;&#37197;Elo&#20998;&#25968;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#30340;&#20215;&#20540;&#21644;&#25928;&#29992;&#65292;&#24182;&#30456;&#24212;&#22320;&#24341;&#23548;&#20915;&#31574;&#25628;&#32034;&#36807;&#31243;&#26397;&#21521;&#26368;&#20248;&#35299;&#12290;&#22312;ToolBench&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JuDec&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#36890;&#36807;&#29575;&#25552;&#39640;&#20102;10%&#20197;&#19978;&#12290;&#23427;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;(ChatGPT API&#35843;&#29992;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its 
&lt;/p&gt;</description></item><item><title>ViMMRC 2.0&#26159;&#19968;&#20010;&#38024;&#23545;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#65292;&#20849;&#26377;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#19988;&#38382;&#39064;&#38590;&#24230;&#22686;&#21152;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.18162</link><description>&lt;p&gt;
&#29992;&#20110;&#36234;&#21335;&#35821;&#25945;&#32946;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A Multiple Choices Reading Comprehension Corpus for Vietnamese Language Education. (arXiv:2303.18162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18162
&lt;/p&gt;
&lt;p&gt;
ViMMRC 2.0&#26159;&#19968;&#20010;&#38024;&#23545;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#65292;&#20849;&#26377;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#19988;&#38382;&#39064;&#38590;&#24230;&#22686;&#21152;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26159;&#36817;&#24180;&#26469;&#19968;&#20010;&#26377;&#36259;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#22312;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ViMMRC 2.0&#65292;&#36825;&#26159;&#23545;&#20043;&#21069;ViMMRC&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#36234;&#21335;&#25945;&#26448;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#65292;&#36825;&#20123;&#25945;&#26448;&#21253;&#21547;&#20102;&#19968;&#24180;&#32423;&#33267;&#21313;&#20108;&#24180;&#32423;&#23398;&#29983;&#30340;&#38405;&#35835;&#25991;&#31456;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;699&#31687;&#25955;&#25991;&#21644;&#35799;&#27468;&#65292;&#20197;&#21450;5,273&#20010;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#30340;&#29256;&#26412;&#19981;&#21516;&#65292;&#26032;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#36873;&#39033;&#19981;&#22266;&#23450;&#20026;&#22235;&#20010;&#65292;&#21516;&#26102;&#36824;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#38656;&#35201;&#23547;&#25214;&#27491;&#30830;&#30340;&#36873;&#25321;&#12290;&#30005;&#33041;&#24517;&#39035;&#29702;&#35299;&#25972;&#20010;&#38405;&#35835;&#25991;&#31456;&#30340;&#19978;&#19979;&#25991;&#12289;&#38382;&#39064;&#20197;&#21450;&#27599;&#20010;&#36873;&#39033;&#30340;&#20869;&#23481;&#25165;&#33021;&#25552;&#21462;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#27493;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MAN&#65289;&#19982;&#21464;&#21387;&#22120;&#30456;&#32467;&#21512;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reading comprehension has been an interesting and challenging task in recent years, with the purpose of extracting useful information from texts. To attain the computer ability to understand the reading text and answer relevant information, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for the task of multiple-choice reading comprehension in Vietnamese Textbooks which contain the reading articles for students from Grade 1 to Grade 12. This dataset has 699 reading passages which are prose and poems, and 5,273 questions. The questions in the new dataset are not fixed with four options as in the previous version. Moreover, the difficulty of questions is increased, which challenges the models to find the correct choice. The computer must understand the whole context of the reading passage, the question, and the content of each choice to extract the right answers. Hence, we propose the multi-stage approach that combines the multi-step attention network (MAN) with the
&lt;/p&gt;</description></item></channel></rss>