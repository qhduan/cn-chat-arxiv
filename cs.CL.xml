<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13534</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24120;&#24120;&#21463;&#30410;&#20110;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20570;&#27861;&#24341;&#20837;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#20351;&#27169;&#22411;&#21464;&#24471;&#22797;&#26434;&#65292;&#23548;&#33268;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#12290;TCL&#26694;&#26550;&#36890;&#36807;&#36880;&#28176;&#24341;&#20837;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#30340;&#19981;&#21516;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#65288;CWS&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#39640;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;TCL&#21152;&#36895;&#20102;&#35757;&#32451;&#24182;&#32531;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13534v1 Announce Type: cross  Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the 
&lt;/p&gt;</description></item></channel></rss>