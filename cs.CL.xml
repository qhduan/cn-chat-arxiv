<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.09949</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#23396;&#31435;&#30340;&#23581;&#35797;&#26469;&#21387;&#32553;Transformer&#65292;&#20294;&#30740;&#31350;&#20013;&#30340;&#35774;&#32622;&#21644;&#25351;&#26631;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#19981;&#21516;&#21387;&#32553;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#21387;&#32553;&#25216;&#26415;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#36825;&#20123;&#23396;&#31435;&#32467;&#26524;&#25552;&#20379;&#32972;&#26223;&#65292;&#30740;&#31350;&#20960;&#31181;&#24120;&#29992;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22681;&#38047;&#26102;&#38388;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#20056;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#21387;&#32553;&#26041;&#27861;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
&lt;/p&gt;</description></item></channel></rss>