<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.18140</link><description>&lt;p&gt;
Juru: &#26469;&#33258;&#21487;&#38752;&#26469;&#28304;&#30340;&#24052;&#35199;&#27861;&#24459;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Juru: Legal Brazilian Large Language Model from Reputable Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18140
&lt;/p&gt;
&lt;p&gt;
Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#30456;&#20851;&#30740;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39046;&#22495;&#19987;&#38376;&#21270;&#21644;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20026;&#25506;&#32034;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21487;&#38752;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#19987;&#38376;&#21270;&#20102;Sabi\'a-2 Small&#27169;&#22411;&#65292;&#24182;&#22312;&#27861;&#24459;&#21644;&#19968;&#33324;&#30693;&#35782;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Juru&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19987;&#38376;&#21270;&#26159;&#20197;&#22312;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22686;&#21152;&#30340;&#31185;&#23398;&#35777;&#25454;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21487;&#33021;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12295</link><description>&lt;p&gt;
&#24265;&#20215;&#23398;&#20064;&#65306;&#26368;&#22823;&#21270;&#31038;&#20250;&#25968;&#25454;&#31185;&#23398;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#26368;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#25552;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22312;&#26500;&#24314;&#26032;&#27169;&#22411;&#26102;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#38477;&#20302;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#20123;&#8220;&#24265;&#20215;&#8221;&#23398;&#20064;&#25216;&#26415;&#22312;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#22240;&#20026;&#24320;&#21457;&#22823;&#22411;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20998;&#26512;&#20219;&#21153;&#30340;&#23454;&#38469;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#19977;&#31181;&#8220;&#24265;&#20215;&#8221;&#25216;&#26415;&#65306;&#24369;&#30417;&#30563;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#38024;&#23545;&#27599;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24037;&#20316;&#21407;&#29702;&#30340;&#25351;&#21335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#23454;&#38469;&#31038;&#20250;&#31185;&#23398;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65288;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19982;&#19977;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#25216;&#26415;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very
&lt;/p&gt;</description></item><item><title>Otter&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#22522;&#20110;OpenFlamingo&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#20196;&#36319;&#38543;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03726</link><description>&lt;p&gt;
Otter: &#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#21450;&#20854;&#19978;&#19979;&#25991;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Otter: A Multi-Modal Model with In-Context Instruction Tuning. (arXiv:2305.03726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03726
&lt;/p&gt;
&lt;p&gt;
Otter&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#22522;&#20110;OpenFlamingo&#35757;&#32451;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#20196;&#36319;&#38543;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#39044;&#35757;&#32451;&#20102;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#32780;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;&#38646;/&#23569;&#25968;&#25454;&#23398;&#20064;&#30340;&#26174;&#33879;&#26222;&#36866;&#33021;&#21147;&#65292;&#20363;&#22914;GPT-3&#65292;&#23427;&#25512;&#20986;&#20102;InstrctGPT&#21644;ChatGPT&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#25351;&#20196;&#35843;&#25972;&#24341;&#20837;&#21040;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24819;&#27861;&#65292;&#21463;&#21040;Flamingo&#27169;&#22411;&#19978;&#28216;&#20132;&#26367;&#26684;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#37319;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;MultI-Modal In-Context Instruction Tuning (MIMIC-IT)&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Otter&#65292;&#19968;&#31181;&#22522;&#20110;OpenFlamingo&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;(DeepMind&#30340;Flamingo&#30340;&#24320;&#28304;&#29256;&#26412;)&#65292;&#23427;&#22312;MIMIC-IT&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#30740;&#31350;&#20154;&#21592;&#20248;&#21270;&#20102;OpenFlamingo&#30340;&#23454;&#29616;&#65292;&#23558;&#25152;&#38656;&#30340;&#35757;&#32451;&#36164;&#28304;&#20174;1&#20010;A100 GPU&#38477;&#33267;4&#20010;RTX-3090 GPU&#65292;&#20174;&#32780;&#20351;&#30740;&#31350;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\times$ A100 GPU to 4$\times$ RTX-3090 GPUs, and integrate both Op
&lt;/p&gt;</description></item></channel></rss>