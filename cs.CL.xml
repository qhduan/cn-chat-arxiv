<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item></channel></rss>