<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15449</link><description>&lt;p&gt;
&#24974;&#24680;&#28304;&#20110;&#26080;&#30693;&#65281;&#23545;&#25239;&#20250;&#35805;&#24615;&#20167;&#24680;&#35328;&#35770;&#20013;&#35828;&#26381;&#26041;&#24335;&#30340;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#25239;&#35328;&#35770;&#20351;&#29992;&#30340;&#22240;&#32032;&#26159;&#29702;&#35299;&#22312;&#32447;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#26680;&#24515;&#12290;&#21508;&#31181;&#30740;&#31350;&#35780;&#20272;&#23545;&#25239;&#35328;&#35770;&#20013;&#20351;&#29992;&#30340;&#24773;&#24863;&#22522;&#30784;&#22240;&#32032;&#65292;&#22914;&#24773;&#24863;&#20849;&#40483;&#12289;&#20882;&#29359;&#31243;&#24230;&#21644;&#25932;&#24847;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20250;&#35805;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#26412;&#30740;&#31350;&#23558;&#35828;&#26381;&#26041;&#24335;&#20998;&#35299;&#20026;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#22312;&#28041;&#21450;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#23447;&#25945;&#38382;&#39064;&#30340;&#20004;&#31181;&#23545;&#35805;&#20132;&#20114;&#31867;&#22411;&#20013;&#30340;&#20351;&#29992;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20154;&#31867;&#19982;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22238;&#22797;&#30340;&#31435;&#22330;&#19982;&#27599;&#31181;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#35828;&#26381;&#26041;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#20132;&#20114;&#30340;&#23545;&#25239;&#35328;&#35770;&#35828;&#26381;&#26041;&#24335;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322; -- &#23588;&#20854;&#26159;&#22312;&#35805;&#39064;&#23618;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15449v1 Announce Type: cross  Abstract: Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12280</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39592;&#26550;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Adaptive Skeleton Graph Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;70&#20159;+&#65289;&#65307;&#28982;&#32780;&#65292;LLM&#25512;&#26029;&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65292;&#20363;&#22914;&#8220;&#24605;&#24819;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#20998;&#35299;&#20026;&#21487;&#20197;&#24182;&#34892;&#35299;&#30721;&#30340;&#23376;&#38382;&#39064;&#26469;&#25913;&#21892;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#36973;&#21463;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#29983;&#25104;&#23376;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#35831;&#27714;&#39069;&#22806;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#20851;&#31995;&#21644;&#38590;&#24230;&#65292;&#20197;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#26292;&#38706;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20381;&#36182;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20449;&#24687;&#36716;&#21457;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#21516;&#26102;&#26292;&#38706;&#29420;&#31435;&#23376;&#38382;&#39064;&#35299;&#30721;&#30340;&#24182;&#34892;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#26631;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#35813;&#35780;&#20272;&#30340;EC-FUNSD&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.02379</link><description>&lt;p&gt;
&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#23454;&#20307;&#20013;&#24515;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#26631;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#35813;&#35780;&#20272;&#30340;EC-FUNSD&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#22411;&#65288;PTLMs&#65289;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#19978;&#30340;&#22810;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20934;&#25968;&#25454;&#20013;&#30340;&#27880;&#37322;&#19981;&#36275;&#65292;&#30446;&#21069;&#30340;&#35780;&#20272;&#27969;&#31243;&#21487;&#33021;&#19981;&#22815;&#31283;&#20581;&#65292;&#26080;&#27861;&#20805;&#20998;&#35780;&#20272;PTLMs&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;PTLMs&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30340;&#29702;&#24819;&#22522;&#20934;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;EC-FUNSD&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#19978;&#35821;&#20041;&#23454;&#20307;&#35782;&#21035;&#21644;&#23454;&#20307;&#38142;&#25509;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19981;&#21516;&#26684;&#24335;&#30340;&#25991;&#26723;&#24067;&#23616;&#21644;&#35821;&#20041;&#39537;&#21160;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#35299;&#24320;&#20102;&#30001;FUNSD&#30340;&#20998;&#27573;&#32423;&#27880;&#37322;&#24102;&#26469;&#30340;&#27573;&#33853;&#21644;&#23454;&#20307;&#38169;&#35823;&#32806;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;PTLMs&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed pre-trained text-and-layout models (PTLMs) have shown remarkable success in multiple information extraction tasks on visually-rich documents. However, the prevailing evaluation pipeline may not be sufficiently robust for assessing the information extraction ability of PTLMs, due to inadequate annotations within the benchmarks. Therefore, we claim the necessary standards for an ideal benchmark to evaluate the information extraction ability of PTLMs. We then introduce EC-FUNSD, an entity-centric benckmark designed for the evaluation of semantic entity recognition and entity linking on visually-rich documents. This dataset contains diverse formats of document layouts and annotations of semantic-driven entities and their relations. Moreover, this dataset disentangles the falsely coupled annotation of segment and entity that arises from the block-level annotation of FUNSD. Experiment results demonstrate that state-of-the-art PTLMs exhibit overfitting tendencies on the pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.03667</link><description>&lt;p&gt;
&#22312;11&#31181;&#35821;&#35328;&#20013;&#27979;&#35797;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#36890;&#36807;&#30740;&#31350;11&#31181;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#27979;&#35797;&#20102;Surprisal&#29702;&#35770;&#30340;&#19977;&#20010;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20102;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#23545;&#38405;&#35835;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26524;&#26159;&#65292;&#21487;&#39044;&#27979;&#24615;&#36739;&#20302;&#30340;&#35789;&#35821;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#26469;&#22788;&#29702;&#12290;Surprisal&#29702;&#35770;&#65288;Hale, 2001; Levy, 2008&#65289;&#26159;&#23545;&#36825;&#19968;&#21457;&#29616;&#30340;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23427;&#23558;&#19968;&#20010;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#37327;&#21270;&#20026;&#20854;surprisal&#65292;&#21363;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#36127;&#23545;&#25968;&#27010;&#29575;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#25903;&#25345;Surprisal&#29702;&#35770;&#30340;&#39044;&#27979;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#65292;&#21363;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#20154;&#38405;&#35835;&#33521;&#35821;&#25991;&#26412;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#20116;&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#20998;&#24067;&#30340;&#21313;&#19968;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;surprisal&#19982;&#38405;&#35835;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#22635;&#34917;&#24403;&#21069;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#20174;&#21333;&#35821;&#21644;&#22810;&#35821;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#20272;&#35745;&#20540;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19982;surprisal&#29702;&#35770;&#30456;&#20851;&#30340;&#19977;&#20010;&#39044;&#27979;&#65306;(i) surprisal&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#38405;&#35835;&#26102;&#38388;&#65307;(ii) &#39044;&#26399;surprisal&#65292;&#21363;&#19978;&#19979;&#25991;&#29109;&#65292;&#26159;&#21542;&#24433;&#21709;&#38405;&#35835;&#26102;&#38388;&#65307;(iii) &#19982;surprisal&#30456;&#20851;&#30340;&#20854;&#20182;&#35821;&#35328;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#35299;&#37322;&#38405;&#35835;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2211.13613</link><description>&lt;p&gt;
Ham2Pose&#65306;&#23558;&#25163;&#35821;&#31526;&#21495;&#36716;&#21270;&#25104;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21475;&#35821;&#32763;&#35793;&#25104;&#25163;&#35821;&#23545;&#20110;&#32843;&#21548;&#31038;&#21306;&#20043;&#38388;&#30340;&#24320;&#25918;&#24615;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;HamNoSys&#65292;&#19968;&#31181;&#35789;&#27719;&#25163;&#35821;&#31526;&#21495;&#65292;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;&#12290;&#30001;&#20110;HamNoSys&#26159;&#36890;&#29992;&#35774;&#35745;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19981;&#21463;&#30446;&#26631;&#25163;&#35821;&#38480;&#21046;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#36880;&#28176;&#29983;&#25104;&#23039;&#21183;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#65292;&#24182;&#19988;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#37096;&#20998;&#21644;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26102;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#32570;&#22833;&#20851;&#38190;&#28857;&#65292;&#20351;&#29992;DTW-MJE&#26469;&#27979;&#37327;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;AUTSL&#36825;&#20010;&#22823;&#35268;&#27169;&#25163;&#35821;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#23427;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#23427;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correctness using AUTSL, a large-scale Sign language dataset, show that it measures the distance betw
&lt;/p&gt;</description></item></channel></rss>