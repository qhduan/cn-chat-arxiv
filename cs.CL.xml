<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15226</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#27880;&#24847;&#21147;&#37117;&#26159;&#24517;&#35201;&#30340;&#65306;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#35843;&#21442;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#31216;&#20026;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#65288;EAS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65288;MHA&#65289;&#20316;&#20026;MLLM&#30340;&#20027;&#35201;&#35745;&#31639;&#24320;&#38144;&#65292;&#36890;&#24120;&#23545;&#19979;&#28216;&#20219;&#21153;&#26469;&#35828;&#26159;&#22810;&#20313;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;EAS&#35780;&#20272;&#27880;&#24847;&#21147;&#20887;&#20313;&#24182;&#36339;&#36807;&#36739;&#19981;&#37325;&#35201;&#30340;MHA&#20197;&#21152;&#36895;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20256;&#25773;&#36866;&#37197;&#22120;&#65288;PIA&#65289;&#26469;&#26381;&#21153;EAS&#30340;&#27880;&#24847;&#21147;&#36339;&#36807;&#24182;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#65292;&#23427;&#21487;&#20197;&#36827;&#19968;&#27493;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#38646;&#39069;&#22806;&#24310;&#36831;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#12290;&#20026;&#20102;&#39564;&#35777;EAS&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;LaVIN&#21644;&#32463;&#20856;&#30340;VL&#39044;&#35757;&#32451;&#27169;&#22411;METER&#65292;&#24182;&#22312;&#19968;&#32452;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EAS&#19981;&#20165;&#20445;&#25345;&#20102;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15226v1 Announce Type: cross  Abstract: In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency,
&lt;/p&gt;</description></item></channel></rss>