<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65292;&#35777;&#23454;&#20854;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#20013;&#30340;&#39640;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18397</link><description>&lt;p&gt;
&#20998;&#35299;&#25552;&#31034;&#65306;&#25581;&#31034;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18397
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65292;&#35777;&#23454;&#20854;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#20013;&#30340;&#39640;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33521;&#35821;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#31867;&#20284;GPT-3&#21644;LLaMA&#36825;&#26679;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#22810;&#35821;&#35328;&#20219;&#21153;&#33021;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#36328;&#35821;&#35328;&#33021;&#21147;&#28145;&#24230;&#21644;&#24615;&#36136;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20197;&#25506;&#31350;&#36825;&#20123;LLMs&#22312;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#12290;&#19982;&#21333;&#19968;&#25991;&#26412;&#21040;&#25991;&#26412;&#25552;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#27599;&#20010;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;&#25552;&#31034;&#65292;&#35810;&#38382;&#20854;&#35821;&#35328;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;38&#31181;&#35821;&#35328;&#30340;&#36890;&#29992;&#20381;&#36182;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#35299;&#25552;&#31034;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#36845;&#20195;&#35774;&#32622;&#19979;&#30340;&#25928;&#21147;&#21644;&#25928;&#29575;&#22343;&#36229;&#36807;&#20102;&#36845;&#20195;&#25552;&#31034;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#20182;&#20204;&#20043;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18397v1 Announce Type: new  Abstract: Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the us
&lt;/p&gt;</description></item></channel></rss>