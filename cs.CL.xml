<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.16646</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#36830;&#36143;&#27010;&#29575;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Incoherent Probability Judgments in Large Language Models. (arXiv:2401.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16646
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36830;&#36143;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#26159;&#21542;&#21516;&#26679;&#25797;&#38271;&#24418;&#25104;&#36830;&#36143;&#30340;&#27010;&#29575;&#21028;&#26029;&#65311;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#36523;&#20221;&#21644;&#37325;&#22797;&#21028;&#26029;&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#27010;&#29575;&#29702;&#35770;&#35268;&#21017;&#20559;&#31163;&#12290;&#27492;&#22806;&#65292;&#24403;&#35201;&#27714;&#23545;&#21516;&#19968;&#20107;&#20214;&#36827;&#34892;&#21028;&#26029;&#26102;&#65292;LLMs&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#22343;&#20540;-&#26041;&#24046;&#20851;&#31995;&#21576;&#29616;&#20986;&#20154;&#31867;&#25152;&#35265;&#21040;&#30340;&#20498;U&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#38750;&#29702;&#24615;&#30340;&#20559;&#31163;&#21487;&#20197;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19982;&#20154;&#31867;&#27010;&#29575;&#21028;&#26029;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#22120;&#27169;&#22411;&#36827;&#34892;&#31867;&#27604;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
&lt;/p&gt;</description></item></channel></rss>