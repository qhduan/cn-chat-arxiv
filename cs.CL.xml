<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.09606</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21327;&#20316;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09606
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#21508;&#31181;NLP&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#35843;&#26597;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;LLMs&#30340;&#22240;&#26524;&#35270;&#35282;&#65292;&#22312;&#20197;&#19979;&#39046;&#22495;&#23637;&#24320;&#65306;&#29702;&#35299;&#21644;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#20026;LLMs&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#21516;&#26102;&#65292;LLMs&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21453;&#36807;&#26469;&#21487;&#20197;&#36890;&#36807;&#24110;&#21161;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26469;&#20419;&#36827;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19982;LLMs&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#38598;&#20307;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#20998;&#25968;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#25429;&#25417;&#31867;&#20284;&#20110;$n$-gram&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#30340;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25253;&#21578;&#20998;&#25968;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.00553</link><description>&lt;p&gt;
&#35268;&#33539;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#27979;&#37327;&#65306;&#19968;&#20010;&#24037;&#20855;&#21644;&#23545;&#20998;&#25968;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#26631;&#20934;&#20998;&#25968;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#25429;&#25417;&#31867;&#20284;&#20110;$n$-gram&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#30340;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25253;&#21578;&#20998;&#25968;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#22609;&#36896;&#20102;&#20154;&#20204;&#23545;&#20854;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#33521;&#35821;&#25991;&#26412;&#30340;&#22810;&#26679;&#24615;&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21387;&#32553;&#31639;&#27861;&#25429;&#25417;&#21040;&#19982;$n$-gram&#30340;&#37325;&#21472;&#21516;&#36136;&#24615;&#24471;&#20998;&#25152;&#34913;&#37327;&#30340;&#20449;&#24687;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21387;&#32553;&#27604;&#12289;&#38271;$n$-gram&#30340;&#33258;&#37325;&#22797;&#12289;Self-BLEU&#21644;BERTScore&#8212;&#8212;&#36275;&#20197;&#25253;&#21578;&#65292;&#22240;&#20026;&#23427;&#20204;&#24444;&#27492;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#36739;&#20302;&#12290;&#36825;&#20123;&#20998;&#25968;&#30340;&#36866;&#29992;&#24615;&#36229;&#20986;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#65307;&#20363;&#22914;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22312;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00553v1 Announce Type: new  Abstract: The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity sc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.01788</link><description>&lt;p&gt;
LitLLM&#65306;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LitLLM: A Toolkit for Scientific Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01788
&lt;/p&gt;
&lt;p&gt;
"LitLLM: A Toolkit for Scientific Literature Review" &#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110; RAG &#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#33258;&#21160;&#21270;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#36716;&#21270;&#25688;&#35201;&#20026;&#20851;&#38190;&#35789;&#36827;&#34892;&#25991;&#29486;&#26816;&#32034;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#36827;&#34892;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#29486;&#32508;&#36848;&#23545;&#20110;&#29702;&#35299;&#30740;&#31350;&#12289;&#20854;&#38480;&#21046;&#20197;&#21450;&#26500;&#24314;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#36825;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#33258;&#21160;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#22120;&#21464;&#24471;&#26377;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#27492;&#31867;&#32508;&#36848;&#30340;&#29616;&#26377;&#24037;&#20316;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#38750;&#23454;&#38469;&#20449;&#24687;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#26410;&#21463;&#36807;&#35757;&#32451;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21407;&#21017;&#30340;&#24037;&#20855;&#21253;&#65292;&#22312;LLM&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;&#25552;&#31034;&#21644;&#25351;&#23548;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25688;&#35201;&#36716;&#21270;&#20026;&#20851;&#38190;&#35789;&#26469;&#36827;&#34892;&#32593;&#32476;&#25628;&#32034;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#35770;&#25991;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#12290;&#20316;&#32773;&#21487;&#20197;&#36890;&#36807;&#34917;&#20805;&#30456;&#20851;&#35770;&#25991;&#25110;&#20851;&#38190;&#35789;&#26469;&#25913;&#36827;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#26816;&#32034;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#31995;&#32479;&#26681;&#25454;-
&lt;/p&gt;
&lt;p&gt;
Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on t
&lt;/p&gt;</description></item></channel></rss>