<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07147</link><description>&lt;p&gt;
QFT: &#20351;&#29992;&#21487;&#25215;&#25285;&#36164;&#28304;&#23545;LLMs&#36827;&#34892;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources. (arXiv:2310.07147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07147
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QFT&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#23545;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#36825;&#19968;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#29616;&#26377;&#30340;&#21162;&#21147;&#37117;&#38598;&#20013;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#19978;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#20840;&#21442;&#25968;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QFT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;LLMs&#30340;&#37327;&#21270;&#20840;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#20869;&#23384;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#24605;&#24819;&#65306;&#65288;i&#65289;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;Lion&#20248;&#21270;&#22120;&#65292;&#20165;&#36319;&#36394;&#21160;&#37327;&#24182;&#20855;&#26377;&#27599;&#20010;&#21442;&#25968;&#19968;&#33268;&#30340;&#26356;&#26032;&#24133;&#24230;&#65292;&#36825;&#23545;&#20110;&#31283;&#20581;&#30340;&#37327;&#21270;&#26159;&#19968;&#31181;&#20869;&#22312;&#20248;&#21183;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#29366;&#24577;&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20197;&#25972;&#25968;&#20540;&#23384;&#20648;&#65292;&#21516;&#26102;&#25552;&#20379;&#26799;&#24230;&#27969;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel Quantized Full-parameter Tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update s
&lt;/p&gt;</description></item></channel></rss>