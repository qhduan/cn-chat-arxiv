<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19723</link><description>&lt;p&gt;
HGT&#65306;&#21033;&#29992;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19723
&lt;/p&gt;
&lt;p&gt;
HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#29702;&#35299; (TU) &#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38754;&#20020;&#25163;&#21160;&#26631;&#35760;&#34920;&#26684;&#30340;&#31232;&#32570;&#24615;&#21644;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HGT &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#24322;&#36136;&#22270; (HG) &#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412; TU &#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#25351;&#23548;&#36716;&#25442;&#23558;&#34920;&#26684;&#35821;&#20041;&#19982;LLM&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#19977;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#26696;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;HGT&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#23569;&#26679;&#26412;&#22797;&#26434;TU&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#20013;&#30340;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17524</link><description>&lt;p&gt;
&#21487;&#35777;&#23433;&#20840;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#28040;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Secure Disambiguating Neural Linguistic Steganography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17524
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#20013;&#30340;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#35777;&#23433;&#20840;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#21457;&#36865;&#26041;&#24517;&#39035;&#23545;&#38544;&#20889;&#25991;&#26412;&#36827;&#34892;&#21435;&#35760;&#21495;&#21270;&#65292;&#20197;&#36991;&#20813;&#24341;&#36215;&#31363;&#21548;&#32773;&#30340;&#24576;&#30097;&#12290;&#22522;&#20110;&#23376;&#35789;&#30340;&#35821;&#35328;&#27169;&#22411;&#20250;&#23548;&#33268;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#65292;&#22312;&#25152;&#26377;&#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#23454;&#29616;&#20013;&#20598;&#23572;&#20250;&#20986;&#29616;&#35299;&#30721;&#22833;&#36133;&#12290;&#30446;&#21069;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#21253;&#25324;&#26356;&#25913;&#20505;&#36873;&#35789;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20351;&#20854;&#19982;&#21487;&#35777;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#19981;&#30456;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#38544;&#20889;&#23884;&#20837;&#31639;&#27861;&#36816;&#34892;&#20043;&#21069;&#23558;&#25152;&#26377;&#20855;&#26377;&#21069;&#32512;&#20851;&#31995;&#30340;&#20196;&#29260;&#20998;&#32452;&#22312;&#20505;&#36873;&#27744;&#20013;&#65292;&#20197;&#28040;&#38500;&#27169;&#31946;&#20196;&#29260;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20351;&#25509;&#25910;&#26041;&#33021;&#22815;&#21516;&#27493;&#21457;&#36865;&#26041;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20849;&#20139;&#23494;&#30721;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17524v1 Announce Type: cross  Abstract: Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper. The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models. Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography. We propose a novel secure disambiguation method named SyncPool, which effectively addresses the segmentation ambiguity problem. We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens. To enable the receiver to synchronize the sampling process of the sender, a shared cryptograph
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15268</link><description>&lt;p&gt;
&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65306;&#23398;&#20064;&#24819;&#35937;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#26469;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29983;&#25104;&#22686;&#24378;&#29983;&#25104;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#19988;&#20004;&#32773;&#37117;&#38656;&#35201;&#23558;&#26174;&#24335;&#25991;&#26723;&#21512;&#24182;&#21040;&#19978;&#19979;&#25991;&#20013;&#65292;&#23548;&#33268;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#28040;&#32791;&#26356;&#22810;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24050;&#32463;&#24314;&#27169;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#27809;&#26377;&#34987;&#26377;&#25928;&#22320;&#35302;&#21457;&#25110;&#28608;&#27963;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#36890;&#36807;&#24819;&#35937;&#21147;&#22312;&#20165;&#20973;&#24819;&#35937;&#22238;&#31572;&#38382;&#39064;&#26102;&#24357;&#34917;&#30693;&#35782;&#32570;&#38519;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;IAG&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#65292;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27169;&#22359;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#65306;&#36890;&#36807;&#29983;&#25104;&#31616;&#21333;&#30340;&#24819;&#35937;&#23454;&#29616;&#26174;&#24335;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13804</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Learning from Models and Data for Visual Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13804
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SynGround&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20174;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;&#20174;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#30693;&#35782;&#20256;&#36882;&#24341;&#21457;&#20102;&#36890;&#36807;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#22120;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#23427;&#20204;&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21512;&#25104;&#22270;&#20687;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#20316;&#20026;&#26597;&#35810;&#26469;&#21512;&#25104;&#25991;&#26412;&#65292;&#20174;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30701;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#20026;&#21512;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#26694;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#36974;&#32617;-&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30446;&#26631;&#23558;&#21306;&#22495;&#27880;&#37322;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#23545;&#40784;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#25552;&#21319;&#20102;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13804v1 Announce Type: cross  Abstract: We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08604</link><description>&lt;p&gt;
DevBench&#65306;&#36719;&#20214;&#24320;&#21457;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DevBench: A Comprehensive Benchmark for Software Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08604
&lt;/p&gt;
&lt;p&gt;
DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1&#23459;&#24067;&#31867;&#22411;&#65306;&#26032;&#30340;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#20851;&#27880;&#32534;&#31243;&#30340;&#31616;&#21270;&#25110;&#23396;&#31435;&#26041;&#38754;&#65292;&#22914;&#21333;&#25991;&#20214;&#20195;&#30721;&#29983;&#25104;&#25110;&#23384;&#20648;&#24211;&#38382;&#39064;&#35843;&#35797;&#65292;&#26410;&#33021;&#20840;&#38754;&#34913;&#37327;&#30001;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#27963;&#21160;&#25552;&#20986;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#20840;&#35889;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DevBench&#65292;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;LLMs&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#36719;&#20214;&#35774;&#35745;&#12289;&#29615;&#22659;&#35774;&#32622;&#12289;&#23454;&#29616;&#12289;&#39564;&#25910;&#27979;&#35797;&#21644;&#21333;&#20803;&#27979;&#35797;&#12290;DevBench&#20855;&#26377;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#39046;&#22495;&#65292;&#39640;&#36136;&#37327;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#31934;&#24515;&#35774;&#35745;&#21644;&#39564;&#35777;&#30340;&#25351;&#26631;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#65292;&#21253;&#25324;GPT-4-Turbo&#65292;&#26080;&#27861;&#35299;&#20915;DevBench&#25552;&#20986;&#30340;&#25361;&#25112;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understand
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01002</link><description>&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01002
&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#24635;&#32467;&#20020;&#24202;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#29983;&#25104;&#20934;&#30830;&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#19982;&#22522;&#30784;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#32467;&#26500;&#21270;&#65288;AS&#65289;&#20316;&#20026;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#26500;&#21270;&#20102;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#12290;&#23427;&#23558;&#35780;&#20272;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#20010;&#22522;&#20110;LLM&#25191;&#34892;&#30456;&#23545;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#21644;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#32508;&#21512;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AS&#22987;&#32456;&#25913;&#21892;&#20102;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#20013;&#20154;&#31867;&#27880;&#37322;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;AS&#36890;&#36807;&#30701;&#25991;&#26412;&#24418;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01002v1 Announce Type: cross  Abstract: Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short te
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.16313</link><description>&lt;p&gt;
Chain-of-Discussion&#65306;&#22797;&#26434;&#35777;&#25454;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#27169;&#22411;&#25214;&#21040;&#36866;&#24403;&#30340;&#35777;&#25454;&#26469;&#24418;&#25104;&#21512;&#29702;&#12289;&#20840;&#38754;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#21442;&#19982;&#23545;&#19982;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#30340;&#28508;&#22312;&#22330;&#26223;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#12290;&#22312;&#26816;&#32034;&#27169;&#22359;&#30340;&#22686;&#24378;&#19979;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20294;&#22312;&#21487;&#38752;&#35777;&#25454;&#36873;&#25321;&#21644;&#28145;&#20837;&#38382;&#39064;&#20998;&#26512;&#26041;&#38754;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Chain-of-Discussion&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#24320;&#25918;&#24335;QA&#25552;&#20379;&#26356;&#27491;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20010;&#20307;&#19978;&#36824;&#19981;&#22815;&#24378;&#22823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#35752;&#35770;&#23545;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;\url{https://github.com/kobaya}&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
&lt;/p&gt;</description></item><item><title>Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13717</link><description>&lt;p&gt;
Neeko&#65306;&#21033;&#29992;&#21160;&#24577;LoRA&#23454;&#29616;&#39640;&#25928;&#22810;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13717
&lt;/p&gt;
&lt;p&gt;
Neeko&#21033;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#26377;&#25928;&#22788;&#29702;&#22810;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25552;&#21319;&#20102;&#23545;&#19981;&#21516;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#20013;&#36215;&#30528;&#38761;&#21629;&#24615;&#20316;&#29992;&#65292;&#20294;&#22312;&#22810;&#35282;&#33394;&#25198;&#28436;&#65288;MCRP&#65289;&#22330;&#26223;&#20013;&#36935;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Neeko&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;&#27169;&#20223;&#22810;&#20010;&#35282;&#33394;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;Neeko&#37319;&#29992;&#21160;&#24577;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#36866;&#24212;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35282;&#33394;&#25198;&#28436;&#36807;&#31243;&#20998;&#35299;&#20026;&#20195;&#29702;&#39044;&#35757;&#32451;&#12289;&#22810;&#20010;&#35282;&#33394;&#25198;&#28436;&#21644;&#35282;&#33394;&#22686;&#37327;&#23398;&#20064;&#65292;&#26377;&#25928;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#35282;&#33394;&#12290;&#36825;&#31181;&#21160;&#24577;&#26041;&#27861;&#65292;&#32467;&#21512;&#20026;&#27599;&#20010;&#35282;&#33394;&#35774;&#35745;&#30340;&#29420;&#29305;LoRA&#22359;&#65292;&#22686;&#24378;&#20102;Neeko&#23545;&#29420;&#29305;&#23646;&#24615;&#12289;&#20010;&#24615;&#21644;&#35828;&#35805;&#27169;&#24335;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;Neeko&#22312;MCRP&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#22810;&#26679;&#21270;&#30340;&#20114;&#21160;&#20307;&#39564;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#65288;&#38142;&#25509;&#20013;&#25552;&#20379;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13717v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12121</link><description>&lt;p&gt;
&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Image Review Ability of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26159;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LVLM&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;LVLM&#23545;&#22270;&#20687;&#30340;&#35780;&#20215;&#33021;&#21147;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#31361;&#26174;&#20102;&#23545;&#20854;&#35780;&#20215;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#19982;&#22270;&#20687;&#26631;&#39064;&#19981;&#21516;&#65292;&#35780;&#20215;&#25991;&#26412;&#21487;&#20197;&#20174;&#22270;&#20687;&#26500;&#22270;&#21644;&#26333;&#20809;&#31561;&#19981;&#21516;&#35270;&#35282;&#25776;&#20889;&#12290;&#36825;&#31181;&#35780;&#20215;&#35282;&#24230;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21807;&#19968;&#30830;&#23450;&#22270;&#20687;&#30340;&#27491;&#30830;&#35780;&#20215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;LVLM&#23545;&#35780;&#20215;&#25991;&#26412;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#27979;&#37327;&#36825;&#20123;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#26368;&#26032;LVLM&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05813</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#25512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#36951;&#24536;&#25216;&#26415;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#30340;&#36951;&#24536;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#24378;&#21270;&#36951;&#24536;&#26694;&#26550;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#35299;&#20915;&#31070;&#32463;&#27169;&#22411;&#24847;&#22806;&#20445;&#30041;&#20010;&#20154;&#25110;&#25935;&#24863;&#25968;&#25454;&#30340;&#38382;&#39064;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#31934;&#30830;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#12290;&#19982;&#20197;&#24448;&#23436;&#20840;&#30456;&#21453;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#36731;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#25935;&#24863;&#20449;&#24687;&#25552;&#21462;&#21487;&#33021;&#24615;&#65288;S-EL&#65289;&#21644;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20934;&#30830;&#24615;&#65288;S-MA&#65289;&#65292;&#26088;&#22312;&#34913;&#37327;&#25935;&#24863;&#20449;&#24687;&#28040;&#38500;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21152;&#24378;&#36951;&#24536;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26631;&#27880;&#25935;&#24863;&#33539;&#22260;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#12290;&#22312;&#32447;&#36873;&#25321;&#26426;&#21046;&#21033;&#29992;&#35821;&#35328;&#27010;&#29575;&#24471;&#20998;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#31163;&#32447;&#31574;&#30053;&#21017;&#21033;&#29992;&#22522;&#20110;&#36317;&#31163;&#30340;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.08883</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#35268;&#21017;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Learn from Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#30446;&#21069;LLMs&#30340;&#30693;&#35782;&#23398;&#20064;&#33539;&#24335;&#20027;&#35201;&#22522;&#20110;&#20174;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#20854;&#20013;LLMs&#20174;&#19968;&#23450;&#25968;&#37327;&#30340;&#30417;&#30563;&#31034;&#20363;&#20013;&#38544;&#24335;&#23398;&#20064;&#20869;&#37096;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#31034;&#20363;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#22797;&#26434;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#21463;&#21040;&#21551;&#21457;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#20174;&#35268;&#21017;&#20013;&#23398;&#20064;&#26469;&#21478;&#19968;&#31181;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#25110;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#36825;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30340;&#21487;&#34892;&#24615;&#65292;&#21363;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#65292;&#39318;&#20808;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#19978;&#19979;&#25991;&#33021;&#21147;&#26469;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08883v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, this learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can learn new tasks or grasps new knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which targets on encoding rule-based knowledge into LLMs. We further propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textu
&lt;/p&gt;</description></item><item><title>ChOiRe&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#29992;&#25143;&#26126;&#30830;&#21644;&#38544;&#24335;&#30340;&#20010;&#20154;&#35282;&#33394;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#31867;&#35266;&#28857;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2311.08385</link><description>&lt;p&gt;
ChOiRe&#65306;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08385
&lt;/p&gt;
&lt;p&gt;
ChOiRe&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#28857;&#38142;&#25512;&#29702;&#34920;&#24449;&#21644;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#29992;&#25143;&#26126;&#30830;&#21644;&#38544;&#24335;&#30340;&#20010;&#20154;&#35282;&#33394;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#31867;&#35266;&#28857;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#35266;&#28857;&#23545;&#40784;&#23545;&#20110;&#22686;&#24378;&#23427;&#20204;&#25226;&#25569;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#21916;&#22909;&#21644;&#20449;&#20208;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ChOiRe&#65292;&#19968;&#20010;&#22235;&#27493;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#35266;&#28857;&#65292;&#35813;&#26694;&#26550;&#19981;&#21516;&#22320;&#23545;&#24453;&#29992;&#25143;&#26126;&#30830;&#22768;&#26126;&#30340;&#20010;&#20154;&#35282;&#33394;&#65288;&#21363;&#20154;&#21475;&#32479;&#35745;&#25110;&#24847;&#35782;&#24418;&#24577;&#23646;&#24615;&#65289;&#21644;&#20174;&#29992;&#25143;&#21382;&#21490;&#35266;&#28857;&#25512;&#26029;&#20986;&#30340;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#12290;ChOiRe&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#29992;&#25143;&#26126;&#30830;&#30340;&#20010;&#20154;&#35282;&#33394;&#65292;&#20197;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#23646;&#24615;&#65307;&#65288;ii&#65289;&#35821;&#35328;&#27169;&#22411;&#23558;&#38544;&#24335;&#20154;&#29289;&#35266;&#28857;&#25490;&#21517;&#25104;&#20248;&#20808;&#21015;&#34920;&#65307;&#65288;iii&#65289;&#35266;&#28857;&#38142;&#25512;&#29702;&#65288;CoO&#65289;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#39034;&#24207;&#22320;&#20998;&#26512;&#26126;&#30830;&#30340;&#20010;&#20154;&#35282;&#33394;&#21644;&#26368;&#30456;&#20851;&#30340;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#20197;&#25191;&#34892;&#35266;&#28857;&#39044;&#27979;&#65307;&#65288;iv&#65289;&#20197;&#21450;ChOiRe&#25191;&#34892;&#31532;&#65288;iii&#65289;&#27493;CoO&#22810;&#27425;&#65292;&#38543;&#30528;&#38544;&#24335;&#20010;&#20154;&#35282;&#33394;&#21015;&#34920;&#19981;&#26029;&#22686;&#21152;&#26469;&#20811;&#26381;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#19981;&#36275;&#20197;&#25512;&#26029;&#26368;&#32456;&#32467;&#26524;&#12290;ChOiRe&#21462;&#24471;&#20102;&#26032;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08385v3 Announce Type: replace  Abstract: Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs. We present ChOiRe, a four-step framework to predict human opinion which differentially models the user explicit personae (i.e. demographic or ideological attributes) that are manually declared, and implicit personae inferred from user historical opinions. ChOiRe consists of (i) an LM analyzing the user explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii) CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result. ChOiRe achieves new
&lt;/p&gt;</description></item><item><title>XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2310.05502</link><description>&lt;p&gt;
XAL&#65306;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#23398;&#20064;&#32773;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05502
&lt;/p&gt;
&lt;p&gt;
XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#22320;&#31579;&#36873;&#26368;&#20855;&#24418;&#25104;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20998;&#31867;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#20998;&#27495;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20250;&#20986;&#29616;&#23545;&#34920;&#38754;&#27169;&#24335;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#20154;&#31867;&#26681;&#25454;&#22240;&#26524;&#20449;&#24687;&#25512;&#26029;&#21644;&#39044;&#27979;&#30340;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#29702;&#30001;&#34701;&#20837;AL&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65288;XAL&#65289;&#65292;&#26088;&#22312;&#40723;&#21169;&#20998;&#31867;&#22120;&#35777;&#26126;&#20854;&#25512;&#26029;&#24182;&#28145;&#20837;&#30740;&#31350;&#26080;&#27861;&#25552;&#20379;&#21512;&#29702;&#35299;&#37322;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#36827;&#34892;&#20998;&#31867;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05502v2 Announce Type: replace  Abstract: Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fast-DetectGPT&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#30456;&#23545;&#20110;DetectGPT&#22312;&#30333;&#30418;&#21644;&#20854;&#20182;&#27979;&#35797;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#36798;&#21040;&#32422;75%&#12290;</title><link>https://arxiv.org/abs/2310.05130</link><description>&lt;p&gt;
Fast-DetectGPT: &#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05130
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fast-DetectGPT&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#30456;&#23545;&#20110;DetectGPT&#22312;&#30333;&#30418;&#21644;&#20854;&#20182;&#27979;&#35797;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#36798;&#21040;&#32422;75%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#29616;&#20986;&#20135;&#29983;&#27969;&#30021;&#12289;&#36830;&#36143;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#29983;&#20135;&#21147;&#26426;&#20250;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#31038;&#20250;&#39118;&#38505;&#12290;&#20026;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#26377;&#24517;&#35201;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#21019;&#20316;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#24341;&#20837;&#26465;&#20214;&#27010;&#29575;&#26354;&#29575;&#27010;&#24565;&#65292;&#20197;&#38416;&#26126;LLMs&#21644;&#20154;&#31867;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;&#27719;&#36873;&#25321;&#24046;&#24322;&#12290;&#21033;&#29992;&#35813;&#26354;&#29575;&#20316;&#20026;&#22522;&#30784;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;**Fast-DetectGPT**&#65292;&#19968;&#20010;&#20248;&#21270;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#23558;DetectGPT&#30340;&#25200;&#21160;&#27493;&#39588;&#26367;&#25442;&#20026;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#28304;&#27169;&#22411;&#21644;&#27979;&#35797;&#26465;&#20214;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Fast-DetectGPT&#22312;&#30333;&#30418;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05130v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06561</link><description>&lt;p&gt;
Intention Analysis Prompting&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#33391;&#22909;&#30340;&#36234;&#29425;&#38450;&#24481;&#32773;
&lt;/p&gt;
&lt;p&gt;
Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#38544;&#34109;&#21644;&#22797;&#26434;&#30340;&#36234;&#29425;&#25915;&#20987;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21363;Intention Analysis Prompting&#65288;IAPrompt&#65289;&#12290;&#20854;&#21407;&#29702;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#35302;&#21457;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#65306;1&#65289;&#22522;&#26412;&#24847;&#22270;&#20998;&#26512;&#65292;2&#65289;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;IAPrompt&#26159;&#19968;&#31181;&#20165;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#26377;&#29992;&#24615;&#12290;&#22312;Vicuna&#12289;ChatGLM&#12289;MPT&#12289;DeepSeek&#21644;GPT-3.5&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;IAPrompt&#33021;&#22815;&#25345;&#32493;&#19988;&#26174;&#33879;&#22320;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65288;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#19979;&#38477;46.5%&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/alph&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2306.17184</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#65311;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26041;&#38754;&#35777;&#26126;&#20102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#25104;&#21151;&#30340;&#20005;&#26684;&#29702;&#35770;&#35299;&#37322;&#23578;&#26410;&#34987;&#25552;&#20986;&#65292;&#22240;&#20026;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#25511;&#21046;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#32452;&#21512;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21487;&#20197;&#29992;&#26469;&#27169;&#25311;&#33521;&#35821;&#21477;&#23376;&#30340;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#19988;&#38169;&#35823;&#29575;&#20026;&#38646;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#31361;&#20986;&#20102;&#23884;&#20837;&#23618;&#21644;&#20840;&#36830;&#25509;&#32452;&#20214;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
&lt;/p&gt;</description></item></channel></rss>