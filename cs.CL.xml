<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2402.19097</link><description>&lt;p&gt;
TEncDM: &#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#29702;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#20197;&#21450;&#33258;&#25105;&#35843;&#33410;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;TEncDM&#30340;&#25991;&#26412;&#32534;&#30721;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#35768;&#22810;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#23545;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Text Encoding Diffusion Model (TEncDM)&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#20013;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35299;&#30721;&#22120;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#25991;&#26412;&#37325;&#26500;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#33258;&#25105;&#35843;&#33410;&#65292;&#24182;&#21457;&#29616;&#36825;&#20250;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#38454;&#27573;&#30340;&#21435;&#22122;&#27493;&#39588;&#25968;&#37327;&#12290;&#22312;&#20004;&#20010;&#19979;&#28216;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;QQP&#21644;XSum&#19978;&#23545;TEncDM&#30340;&#35780;&#20272;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19097v1 Announce Type: new  Abstract: Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority ove
&lt;/p&gt;</description></item><item><title>&#26080;&#38656;&#25968;&#25454;&#21442;&#19982;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#24182;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16319</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25968;&#25454;&#26435;&#37325;&#21387;&#32553;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Data-freeWeight Compress and Denoise for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16319
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#25968;&#25454;&#21442;&#19982;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#24182;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#37325;&#22609;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#30340;&#26684;&#23616;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#30340;&#26174;&#33879;&#25193;&#22823;&#65292;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#38480;&#20110;GPU&#20869;&#23384;&#21644;&#35745;&#31639;&#36895;&#24230;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#22914;&#21098;&#26525;&#21644;&#37327;&#21270;&#12290;&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#29305;&#24615;&#65292;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#20943;&#23569;&#26435;&#37325;&#22312;&#21387;&#32553;&#21442;&#25968;&#26041;&#38754;&#26080;&#30097;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20511;&#37492;LLMs&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26080;&#25968;&#25454;&#32852;&#21512;&#31209;-k&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#28857;&#22312;&#20110;&#26080;&#38656;&#39069;&#22806;&#28041;&#21450;&#20219;&#20309;&#35821;&#26009;&#24211;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16319v1 Announce Type: new  Abstract: Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in con
&lt;/p&gt;</description></item><item><title>BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07536</link><description>&lt;p&gt;
BreakGPT: &#19968;&#31181;&#20855;&#26377;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07536
&lt;/p&gt;
&lt;p&gt;
BreakGPT&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#38454;&#27573;&#32467;&#26500;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#26131;&#21306;&#38388;&#31361;&#30772;&#65288;TRB&#65289;&#26159;&#37329;&#34701;&#20132;&#26131;&#25216;&#26415;&#20998;&#26512;&#20013;&#30340;&#19968;&#31181;&#20851;&#38190;&#26041;&#27861;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32929;&#31080;&#12289;&#26399;&#36135;&#21644;&#22806;&#27719;&#31561;&#37329;&#34701;&#24066;&#22330;&#30340;&#20132;&#26131;&#32773;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#30495;&#20551;&#31361;&#30772;&#24182;&#25552;&#20379;&#27491;&#30830;&#30340;&#29702;&#30001;&#23545;&#25237;&#36164;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#39046;&#22495;&#30340;&#25928;&#26524;&#20173;&#19981;&#29702;&#24819;&#12290;&#21407;&#22240;&#22312;&#20110;&#31361;&#30772;&#26816;&#27979;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#21644;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BreakGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#38454;&#27573;&#32467;&#26500;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;GPT-3.5&#30456;&#27604;&#65292;BreakGPT&#30340;&#31572;&#26696;&#21644;&#29702;&#30001;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;44%&#65292;&#26377;&#21161;&#20110;&#37329;&#34701;&#31361;&#30772;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the m
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2401.11323</link><description>&lt;p&gt;
&#36776;&#35782;&#24182;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Identifying and Analyzing Task-Encoding Tokens in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;ICL&#30340;&#24037;&#20316;&#26426;&#21046;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#22914;&#20309;&#20174;ICL&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#26495;&#26631;&#35760;&#21644;&#20572;&#29992;&#35789;&#26631;&#35760;&#26368;&#23481;&#26131;&#25104;&#20026;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#24847;&#24605;&#12289;&#37325;&#22797;&#21644;&#25991;&#26412;&#26684;&#24335;&#26159;&#36825;&#20123;&#26631;&#35760;&#30340;&#20027;&#35201;&#21306;&#21035;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11323v2 Announce Type: replace  Abstract: In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL's working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing task-encoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to per
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.02233</link><description>&lt;p&gt;
&#29992;&#21307;&#23398;&#25945;&#31185;&#20070;&#22686;&#24378;&#40657;&#30418;LLMs&#36827;&#34892;&#20020;&#24202;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.02233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#31034;&#20986;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#32570;&#20047;&#29305;&#23450;&#12289;&#28145;&#20837;&#30340;&#30693;&#35782;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#22686;&#24378;&#21307;&#23398;&#25945;&#31185;&#20070;&#65288;LLM-AMT&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;LLM-AMT&#36890;&#36807;&#25554;&#20837;&#24335;&#27169;&#22359;&#23558;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#38598;&#25104;&#21040;LLMs&#30340;&#26694;&#26550;&#20013;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;&#19968;&#20010;&#26597;&#35810;&#22686;&#24378;&#22120;&#12289;&#19968;&#20010;&#28151;&#21512;&#25945;&#31185;&#20070;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#30693;&#35782;&#33258;&#25105;&#23436;&#21892;&#12290;&#23427;&#20204;&#20849;&#21516;&#25972;&#21512;&#26435;&#23041;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;LLMs&#38405;&#35835;&#22120;&#26377;&#21161;&#20110;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMAMT&#26174;&#33879;&#25552;&#39640;&#20102;&#21709;&#24212;&#36136;&#37327;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.6%&#21040;16.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;GPT-4-Turbo&#20026;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.03249</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#22909;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#21527;&#65311;&#23545;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;PPNL&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#31354;&#38388;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#20173;&#28982;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#36335;&#24452;&#35268;&#21010;&#65288;PPNL&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#21046;&#23450;&#38656;&#35201;LLM&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#24182;&#36991;&#24320;&#38556;&#30861;&#29289;&#21644;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#30340;&#8220;&#36335;&#24452;&#35268;&#21010;&#8221;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#30340;&#31354;&#38388;-&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#25324;GPT-4&#22312;&#20869;&#30340;LLM&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#21644;&#21508;&#31181;&#35268;&#27169;&#30340;BART&#21644;T5&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;LLM&#36827;&#34892;&#25512;&#29702;&#21644;&#20132;&#20114;&#34892;&#21160;&#26102;&#65292;&#23569;&#26679;&#26412;&#30340;GPT-4&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26080;&#27861;&#36827;&#34892;&#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLM&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success across a wide spectrum of tasks; however, they still face limitations in scenarios that demand long-term planning and spatial reasoning. To facilitate this line of research, in this work, we propose a new benchmark, termed $\textbf{P}$ath $\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage ($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by formulating ''path planning'' tasks that require an LLM to navigate to target locations while avoiding obstacles and adhering to constraints. Leveraging this benchmark, we systematically investigate LLMs including GPT-4 via different few-shot prompting methodologies and BART and T5 of various sizes via fine-tuning. Our experimental results show the promise of few-shot GPT-4 in spatial reasoning, when it is prompted to reason and act interleavedly, although it still fails to make long-term temporal reasoning. In contrast, while fine-tuned LLMs achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13365</link><description>&lt;p&gt;
&#29992;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#23558;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#30021;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#37327;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#29616;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#20195;&#12290;&#26080;&#35770;&#36825;&#20123;&#27169;&#22411;&#33258;&#36523;&#30340;&#23481;&#37327;&#21644;&#32467;&#26500;&#22914;&#20309;&#65292;&#37117;&#23384;&#22312;&#23545;LLMs&#20855;&#26377;&#26356;&#38271;&#26356;&#22797;&#26434;&#19978;&#19979;&#25991;&#30340;&#22686;&#24378;&#29702;&#35299;&#30340;&#38656;&#27714;&#65292;&#32780;&#27169;&#22411;&#36890;&#24120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20854;&#29702;&#35299;&#33021;&#21147;&#33539;&#22260;&#30340;&#21477;&#23376;&#24207;&#21015;&#26102;&#20250;&#36935;&#21040;&#19978;&#38480;&#65292;&#23548;&#33268;&#20135;&#29983;&#31163;&#39064;&#25110;&#28151;&#20081;&#30340;&#22238;&#31572;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#20851;&#27880;&#8220;&#20026;&#20160;&#20040;&#27169;&#22411;&#26080;&#27861;&#33258;&#34892;&#24357;&#34917;&#25110;&#22686;&#24378;&#33258;&#24049;&#30340;&#33021;&#21147;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#20869;&#30340;&#20449;&#24687;&#20256;&#36882;&#24615;&#36136;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27880;&#24847;&#21147;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#20351;&#27169;&#22411;&#22312;&#26368;&#23567;&#21270;&#39069;&#22806;&#35757;&#32451;&#25110;&#23545;&#29983;&#25104;&#27969;&#21033;&#24615;&#30340;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#38271;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.14583</link><description>&lt;p&gt;
&#35753;&#38544;&#21547;&#30340;&#26174;&#24615;&#21270;&#65306;&#20197;NLP&#20013;&#30340;&#38544;&#24335;&#20869;&#23481;&#20026;&#31532;&#19968;&#20844;&#27665;
&lt;/p&gt;
&lt;p&gt;
Making the Implicit Explicit: Implicit Content as a First Class Citizen in NLP. (arXiv:2305.14583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#28857;&#25918;&#22312;&#38544;&#24335;&#20869;&#23481;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#21644;&#20998;&#35299;&#26041;&#27861;&#38477;&#20302;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22797;&#26434;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#23884;&#20837;&#65292;&#35745;&#31639;&#25919;&#27835;&#23398;&#21644;&#26500;&#24314;&#21457;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#22810;&#20803;&#21270;&#30340;&#65292;&#19968;&#20010;&#34920;&#36848;&#21487;&#20197;&#29992;&#31561;&#20215;&#30340;&#24418;&#24335;&#37325;&#30003;&#65292;&#32780;&#20854;&#20013;&#30340;&#38544;&#21547;&#21644;&#26174;&#24615;&#20869;&#23481;&#25903;&#25345;&#21508;&#31181;&#36923;&#36753;&#21644;&#35821;&#29992;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#34920;&#36848;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#38754;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#8220;&#36825;&#37324;&#24456;&#40657;&#8221;&#21487;&#33021;&#26159;&#19968;&#20010;&#26263;&#31034;&#38656;&#35201;&#25171;&#24320;&#28783;&#12290;&#28982;&#32780;&#65292;NLP&#26041;&#27861;&#36890;&#24120;&#20165;&#20165;&#22522;&#20110;&#34920;&#38754;&#24418;&#24335;&#25805;&#20316;&#65292;&#30465;&#30053;&#20102;&#36825;&#31181;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#35821;&#35328;&#26469;&#34920;&#31034;&#35821;&#35328;&#65292;&#24182;&#24341;&#23548;LLM&#23558;&#34920;&#36848;&#20998;&#35299;&#20026;&#36923;&#36753;&#21644;&#21487;&#20449;&#30340;&#25512;&#29702;&#12290;&#20998;&#35299;&#30340;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#20351;&#23427;&#20204;&#26356;&#23481;&#26131;&#23884;&#20837;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21464;&#21270;&#22312;&#21477;&#23376;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#65292;&#22312;&#35745;&#31639;&#25919;&#27835;&#23398;&#20013;&#26377;&#23454;&#36136;&#24615;&#24212;&#29992;&#65292;&#24182;&#24341;&#20986;&#19968;&#31181;&#26032;&#30340;&#26500;&#24314;&#21457;&#29616;&#36807;&#31243;&#65292;&#25105;&#20204;&#29992;&#20154;&#24037;&#27880;&#37322;&#39564;&#35777;&#20102;&#36825;&#31181;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is multifaceted. A given utterance can be re-expressed in equivalent forms, and its implicit and explicit content support various logical and pragmatic inferences. When processing an utterance, we consider these different aspects, as mediated by our interpretive goals -- understanding that "it's dark in here" may be a veiled direction to turn on a light. Nonetheless, NLP methods typically operate over the surface form alone, eliding this nuance.  In this work, we represent language with language, and direct an LLM to decompose utterances into logical and plausible inferences. The reduced complexity of the decompositions makes them easier to embed, opening up novel applications. Variations on our technique lead to state-of-the-art improvements on sentence embedding benchmarks, a substantive application in computational political science, and to a novel construct-discovery process, which we validate with human annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10615</link><description>&lt;p&gt;
ML-SUPERB: &#22810;&#35821;&#31181;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35206;&#30422;143&#31181;&#35821;&#35328;&#12289;&#29992;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22522;&#20934; ML-SUPERB&#65292;&#24182;&#21457;&#29616;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#19988;&#22810;&#35821;&#31181;&#27169;&#22411;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22788;&#29702;Universal PERformance Benchmark (SUPERB)&#26159;&#19968;&#20010;&#29992;&#20110;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#30340;&#25490;&#34892;&#27036;&#12290;&#28982;&#32780;&#65292;SUPERB&#22312;&#35780;&#20272;&#20013;&#20027;&#35201;&#32771;&#34385;&#33521;&#35821;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#35821;&#31181;SUPERB (ML-SUPERB)&#65292;&#35206;&#30422;&#20102;143&#31181;&#35821;&#35328;&#65288;&#20174;&#39640;&#36164;&#28304;&#21040;&#28626;&#21361;&#35821;&#35328;&#65289;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;&#19982;SUPERB&#27010;&#24565;&#31867;&#20284;&#65292;ML-SUPERB&#21033;&#29992;&#20923;&#32467;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#27973;&#23618;&#19979;&#28216;&#27169;&#22411;&#30340;&#31616;&#21333;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#35821;&#31181;&#20219;&#21153;&#12290;&#19982;SUPERB&#22522;&#20934;&#31867;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#38899;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#19982;FBANK&#29305;&#24449;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#31181;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;ML-SUPERB&#20316;&#20026;&#19968;&#20010;&#25361;&#25112;&#65292;&#25552;&#20379;&#32452;&#32455;&#22909;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#33050;&#26412;&#65292;&#29992;&#20110;&#26410;&#26469;&#30340;&#22810;&#35821;&#31181;&#34920;&#31034;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
&lt;/p&gt;</description></item></channel></rss>