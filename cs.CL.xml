<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.16527</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#65306;&#28789;&#27963;&#23450;&#20041;&#19982;&#29616;&#26377;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16527
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#21363;&#23558;&#26080;&#22788;&#19981;&#22312;&#65292;&#20174;&#21046;&#36896;&#19994;&#30340;&#33258;&#20027;&#24615;&#21040;&#20892;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#65292;&#20174;&#21307;&#30103;&#21161;&#29702;&#21040;&#23089;&#20048;&#20135;&#19994;&#12290;&#22823;&#22810;&#25968;&#31995;&#32479;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#23376;&#32452;&#20214;&#24320;&#21457;&#30340;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#24773;&#22659;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#32597;&#35265;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#29305;&#21035;&#24046;&#12290;&#22522;&#20110;&#22810;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20197;&#21450;&#20174;&#21508;&#20010;&#39046;&#22495;&#37319;&#38598;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#30456;&#20449;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25552;&#20379;&#29616;&#26377;&#35268;&#21010;&#32773;&#25152;&#32570;&#20047;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#36825;&#31181;&#24120;&#35782;&#25512;&#29702;&#23558;&#24357;&#21512;&#31639;&#27861;&#24320;&#21457;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16527v1 Announce Type: new  Abstract: Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.16751</link><description>&lt;p&gt;
&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#21644;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#29702;&#35299;&#20844;&#27665;&#30340;&#20215;&#20540;&#35266;&#23545;&#20110;&#20197;&#20844;&#27665;&#20026;&#20013;&#24515;&#30340;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#21442;&#19982;&#32773;&#20570;&#20986;&#36873;&#25321;&#24182;&#25552;&#20379;&#36873;&#25321;&#30340;&#21160;&#26426;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#19982;&#20182;&#20204;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#20215;&#20540;&#20559;&#22909;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#21644;&#21160;&#26426;&#20043;&#38388;&#26816;&#27979;&#21040;&#20914;&#31361;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20272;&#35745;&#20215;&#20540;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#26469;&#35299;&#20915;&#26816;&#27979;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#8220;&#29645;&#35270;&#26159;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26377;&#24847;&#20041;&#34892;&#20026;&#8221;&#36825;&#19968;&#21746;&#23398;&#31435;&#22330;&#25805;&#20316;&#21270;&#12290;&#20063;&#23601;&#26159;&#22914;&#26524;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#26159;&#22522;&#20110;&#23545;&#20215;&#20540;&#20559;&#22909;&#30340;&#28145;&#24605;&#29087;&#34385;&#65292;&#37027;&#20040;&#21487;&#20197;&#22312;&#21442;&#19982;&#32773;&#20026;&#36873;&#25321;&#25552;&#20379;&#30340;&#21160;&#26426;&#20013;&#35266;&#23519;&#21040;&#20215;&#20540;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20248;&#20808;&#32771;&#34385;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#32780;&#19981;&#26159;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#30340;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16751v1 Announce Type: cross  Abstract: Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that "valuing is deliberatively consequential." That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone.
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09401</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#26597;&#35810;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback with Active Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#22312;&#26500;&#24314;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#21463;&#21040;&#20027;&#21160;&#23398;&#20064;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#25552;&#20986;&#26597;&#35810;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#31454;&#20105;&#20108;&#33218;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;APPO&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;$\tilde{O}(d^2/\Delta)$&#30340;&#36951;&#25022;&#30028;&#21644;$\tilde{O}(d^2/\Delta^2)$&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;$\Delta$&#26159;&#25152;&#26377;&#19978;&#19979;&#25991;&#20013;&#30340;&#27425;&#20248;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADPO&#65292;&#36825;&#26159;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#38469;&#29256;&#26412;&#65292;&#22522;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09401v1 Announce Type: cross Abstract: Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item></channel></rss>