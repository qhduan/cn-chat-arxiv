<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01799</link><description>&lt;p&gt;
PATCH -- &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#65306;&#25968;&#23398;&#33021;&#21147;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A Case Study of Mathematics Proficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PATCH&#65292;&#29992;&#20110;&#23558;&#24515;&#29702;&#27979;&#37327;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#30340;&#27979;&#37327;&#36136;&#37327;&#12289;&#39033;&#30446;&#32423;&#21035;&#35780;&#20272;&#21644;&#21442;&#32771;&#20154;&#32676;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#22823;&#22411;&#65288;&#22810;&#27169;&#24577;&#65289;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22522;&#20934;&#27979;&#35797;&#30528;&#37325;&#20110;&#34913;&#37327;LLMs&#30340;&#23398;&#26415;&#33021;&#21147;&#65292;&#36890;&#24120;&#20063;&#23545;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#19982;&#20154;&#31867;&#32771;&#35797;&#32773;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#23545;LLMs&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#26377;&#38382;&#39064;&#30340;&#27979;&#37327;&#36136;&#37327;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#26159;&#21542;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#34913;&#37327;&#25152;&#38656;&#30340;&#20869;&#23481;&#65311;&#65289;&#12289;&#32570;&#20047;&#39033;&#30446;&#32423;&#21035;&#30340;&#36136;&#37327;&#35780;&#20272;&#65288;&#20363;&#22914;&#65292;&#26377;&#20123;&#39033;&#30446;&#26159;&#21542;&#27604;&#20854;&#20182;&#26356;&#37325;&#35201;&#25110;&#26356;&#22256;&#38590;&#65311;&#65289;&#20197;&#21450;&#20154;&#31867;&#20154;&#21475;&#21442;&#29031;&#27169;&#31946;&#65288;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#19982;&#35841;&#36827;&#34892;&#27604;&#36739;&#65311;&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#8212;&#8212;&#19968;&#38376;&#33268;&#21147;&#20110;&#27979;&#37327;&#28508;&#22312;&#21464;&#37327;&#22914;&#23398;&#26415;&#33021;&#21147;&#30340;&#39046;&#22495;&#8212;&#8212;&#26469;&#36827;&#34892;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PATCH&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#36741;&#21161;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01799v1 Announce Type: new  Abstract: Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers. While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics - a field dedicated to the measurement of latent variables like academic proficiency - into LLM benchmarking. We make three primary contributions. First, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.16075</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#23545;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65306;&#23545;&#33521;&#21360;&#35821;&#35328;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#26412;&#30740;&#31350;&#21017;&#32771;&#23519;&#20102;&#23558;&#22270;&#20687;&#29305;&#24449;&#28155;&#21152;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22270;&#20687;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#12290;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#12290;&#23454;&#39564;&#23558;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35270;&#35273;&#32972;&#26223;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#65306;&#23545;&#20110;&#38750;&#22122;&#22768;&#32763;&#35793;&#65292;&#19981;&#20351;&#29992;&#35270;&#35273;&#32972;&#26223;&#25928;&#26524;&#26368;&#22909;&#65307;&#23545;&#20110;&#20302;&#22122;&#22768;&#65292;&#35009;&#21098;&#30340;&#22270;&#20687;&#29305;&#24449;&#26368;&#20339;&#65307;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#22270;&#20687;&#29305;&#24449;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
&lt;/p&gt;</description></item></channel></rss>