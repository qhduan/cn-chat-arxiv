<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.15615</link><description>&lt;p&gt;
NaturalTurn&#65306;&#19968;&#31181;&#23558;&#36716;&#24405;&#20214;&#20998;&#21106;&#25104;&#33258;&#28982;&#23545;&#35805;&#36716;&#25240;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15615
&lt;/p&gt;
&lt;p&gt;
NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#35805;&#26159;&#31038;&#20250;&#12289;&#35748;&#30693;&#21644;&#35745;&#31639;&#31185;&#23398;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#36716;&#24405;&#36716;&#25442;&#20026;&#20250;&#35805;&#36718;&#27425;&#8212;&#8212;&#31038;&#20250;&#20114;&#21160;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;NaturalTurn&#8221;&#65292;&#19968;&#31181;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#12290;NaturalTurn&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#22914;&#32972;&#26223;&#22768;&#12289;&#31616;&#30701;&#25554;&#35805;&#21644;&#20854;&#20182;&#34920;&#29616;&#23545;&#35805;&#29305;&#24449;&#30340;&#24179;&#34892;&#35328;&#35821;&#24418;&#24335;&#65292;&#26469;&#36816;&#20316;&#12290;&#20351;&#29992;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#27966;&#29983;&#30340;&#36716;&#24405;&#30456;&#27604;&#65292;NaturalTurn&#27966;&#29983;&#30340;&#36716;&#24405;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32479;&#35745;&#21644;&#25512;&#26029;&#29305;&#24615;&#12290;NaturalTurn&#31639;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 Announce Type: new  Abstract: Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement i
&lt;/p&gt;</description></item><item><title>&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;</title><link>https://arxiv.org/abs/2207.01262</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21644;Leaderboarding&#29702;&#35299;&#38271;&#25991;&#26723;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01262
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;20&#22810;&#20010;&#29992;&#20110;&#38271;&#25991;&#26723;&#25490;&#21517;&#30340;Transformer&#27169;&#22411;&#65288;&#21253;&#25324;&#26368;&#36817;&#20351;&#29992;FlashAttention&#35757;&#32451;&#30340;LongP&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31616;&#21333;&#30340;FirstP&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&#23558;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#20110;&#36755;&#20837;&#25130;&#26029;&#20026;&#21069;512&#20010;&#26631;&#35760;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;MS MARCO&#25991;&#26723;v1&#20316;&#20026;&#20027;&#35201;&#35757;&#32451;&#38598;&#65292;&#24182;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#23545;&#20854;&#20182;&#25910;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item></channel></rss>