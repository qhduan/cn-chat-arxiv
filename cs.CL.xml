<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.04195</link><description>&lt;p&gt;
MildTriple Loss&#27169;&#22411;&#19979;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#38543;&#30528;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#21516;&#24314;&#27169;&#65292;&#35201;&#27714;&#20174;&#25991;&#26412;&#20013;&#29702;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#20013;&#23398;&#20064;&#34892;&#20026;&#29305;&#24449;&#12290;&#20197;&#24448;&#30340;&#36816;&#21160;&#25968;&#25454;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#21487;&#33021;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20174;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#23398;&#20064;&#34920;&#31034;&#24182;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
&lt;/p&gt;</description></item></channel></rss>