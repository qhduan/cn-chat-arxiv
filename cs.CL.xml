<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10424</link><description>&lt;p&gt;
&#20351;&#29992;&#40520;&#40533;&#27748;&#26694;&#26550;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning with a Pelican Soup Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#26159;&#22522;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#65292;&#23427;&#20204;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21363;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#27010;&#24565;&#65292;&#65288;2&#65289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33324;&#24418;&#24335;&#21270;&#65292;&#20197;&#21450;&#65288;3&#65289;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;$\mathcal{O}(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#36825;&#37324;$T$&#26159;&#28436;&#31034;&#20013;&#31034;&#20363;-&#26631;&#31614;&#23545;&#30340;&#25968;&#37327;&#12290;&#19982;&#20808;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21453;&#26144;&#20102;&#21160;&#35789;&#36873;&#25321;&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;"&#21407;&#23376;&#27010;&#24565;"&#27010;&#24565;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35299;&#37322;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29609;&#20855;&#35774;&#32622;&#65292;Calcutec&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10424v1 Announce Type: cross  Abstract: Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of \textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec,
&lt;/p&gt;</description></item></channel></rss>