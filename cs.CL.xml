<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;</title><link>https://arxiv.org/abs/2403.10258</link><description>&lt;p&gt;
&#32763;&#35793;&#21040;&#24213;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#21527;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#35821;&#26009;&#24211;&#19981;&#24179;&#34913;&#65292;&#23427;&#20204;&#22823;&#22810;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#36825;&#19968;&#29616;&#35937;&#26469;&#25552;&#39640;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25193;&#23637;&#21040;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23558;&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#35821;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25152;&#26377;&#22330;&#26223;&#12290;&#23545;&#20110;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#35821;&#35328;&#30340;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#65292;&#20197;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26356;&#20026;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#19982;&#25991;&#21270;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#30528;&#21147;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10258v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09107</link><description>&lt;p&gt;
GLoRE: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRE: Evaluating Logical Reasoning of Large Language Models. (arXiv:2310.09107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21253;&#25324;GPT-4&#21644;&#26032;&#20852;&#31038;&#21306;&#27169;&#22411;&#22312;&#20869;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#23581;&#35797;&#36824;&#24456;&#23569;&#65292;&#32780;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GLoRE&#65292;&#19968;&#20010;&#31934;&#24515;&#32452;&#32455;&#30340;&#36890;&#29992;&#36923;&#36753;&#25512;&#29702;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;12&#20010;&#35206;&#30422;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#31867;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65307;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#36739;&#24378;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;GPT-4&#22823;&#24133;&#36229;&#36807;&#20102;ChatGPT&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;ChatGPT&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We 
&lt;/p&gt;</description></item></channel></rss>