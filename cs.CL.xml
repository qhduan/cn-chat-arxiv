<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>LLM&#21463;&#21040;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#20854;&#24615;&#33021;&#19981;&#21487;&#38752;&#65292;&#25361;&#25112;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.00699</link><description>&lt;p&gt;
LLM&#21463;&#21040;&#22810;&#23569;&#27745;&#26579;&#65311;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;&#21644;LLMSanitize&#24211;
&lt;/p&gt;
&lt;p&gt;
How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00699
&lt;/p&gt;
&lt;p&gt;
LLM&#21463;&#21040;&#27745;&#26579;&#21487;&#33021;&#23548;&#33268;&#20854;&#24615;&#33021;&#19981;&#21487;&#38752;&#65292;&#25361;&#25112;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#65292;&#26032;&#30340;&#26426;&#20250;&#27491;&#22312;&#20986;&#29616;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#27745;&#26579;&#38382;&#39064;&#36805;&#36895;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20225;&#19994;&#24212;&#29992;&#21644;&#20154;&#24037;&#26234;&#33021;&#31609;&#27454;&#24050;&#32463;&#36798;&#21040;&#19968;&#23450;&#35268;&#27169;&#65292;&#27969;&#34892;&#30340;&#38382;&#31572;&#22522;&#20934;&#25552;&#39640;&#20960;&#20010;&#30334;&#20998;&#28857;&#21487;&#33021;&#24847;&#21619;&#30528;&#25968;&#30334;&#19975;&#32654;&#20803;&#65292;&#23545;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#26045;&#21152;&#20102;&#24040;&#22823;&#21387;&#21147;&#12290;&#21516;&#26102;&#65292;&#36861;&#36394;LLMs&#35265;&#36807;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65307;&#23545;&#20110;&#20687;GPT-4&#21644;Claude-3&#36825;&#26679;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#20182;&#20204;&#19981;&#36879;&#38706;&#20219;&#20309;&#26377;&#20851;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;LLMs&#30340;&#24615;&#33021;&#21487;&#33021;&#19981;&#20877;&#21487;&#38752;&#65292;&#22240;&#20026;&#20854;&#39640;&#24615;&#33021;&#33267;&#23569;&#37096;&#20998;&#24402;&#22240;&#20110;&#20854;&#20808;&#21069;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#23616;&#38480;&#24615;&#21361;&#21450;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25972;&#20307;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#32570;&#20047;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00699v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.03819</link><description>&lt;p&gt;
LEACE&#65306;&#38381;&#21512;&#24418;&#24335;&#20013;&#30340;&#23436;&#32654;&#32447;&#24615;&#27010;&#24565;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
LEACE: Perfect linear concept erasure in closed form. (arXiv:2306.03819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;LEACE&#65292;&#21487;&#22312;&#21024;&#38500;&#25351;&#23450;&#29305;&#24449;&#30340;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#24182;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#12290;&#20316;&#32773;&#29992;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#36825;&#19968;&#26032;&#26041;&#27861;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#30340;&#20381;&#36182;&#24615;&#21644;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#20219;&#21153;&#20013;&#24471;&#20986;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#20174;&#34920;&#24449;&#20013;&#21024;&#38500;&#25351;&#23450;&#30340;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#65292;&#38450;&#27490;&#20998;&#31867;&#22120;&#20351;&#29992;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#21644;&#21487;&#35299;&#37322;&#24615;&#65288;&#20363;&#22914;&#65292;&#21024;&#38500;&#27010;&#24565;&#20197;&#35266;&#23519;&#27169;&#22411;&#34892;&#20026;&#30340;&#21464;&#21270;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LEAst-squares&#27010;&#24565;&#25830;&#38500;&#65288;LEACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38381;&#21512;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#35777;&#26126;&#38450;&#27490;&#25152;&#26377;&#32447;&#24615;&#20998;&#31867;&#22120;&#26816;&#27979;&#21040;&#27010;&#24565;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#25913;&#21464;&#34920;&#31034;&#65292;&#22914;&#24191;&#27867;&#31867;&#21035;&#30340;&#33539;&#25968;&#25152;&#27979;&#37327;&#30340;&#37027;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21517;&#20026;&#8220;&#27010;&#24565;&#25830;&#38500;&#8221;&#30340;&#26032;&#26041;&#27861;&#23558;LEACE&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25830;&#38500;&#27599;&#20010;&#23618;&#20013;&#30340;&#30446;&#26631;&#27010;&#24565;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#24615;&#20449;&#24687;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#21450;&#20943;&#23569;BERT&#23884;&#20837;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/EleutherAI/concept-erasure&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.
&lt;/p&gt;</description></item></channel></rss>