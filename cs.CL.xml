<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10803</link><description>&lt;p&gt;
SD-HuBERT: &#33258;&#25105;&#33976;&#39311;&#35825;&#23548;HuBERT&#20013;&#30340;&#38899;&#33410;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT. (arXiv:2310.10803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SD-HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#33976;&#39311;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#21333;&#20803;&#21457;&#29616;&#24320;&#21551;&#20102;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#30340;&#21333;&#20803;&#24448;&#24448;&#20173;&#22788;&#20110;&#38899;&#32032;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;SSL&#34920;&#31034;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#26102;&#65292;&#38899;&#33410;&#32452;&#32455;&#30340;&#20986;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#8220;&#33258;&#25105;&#33976;&#39311;&#8221;&#30446;&#26631;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;HuBERT&#65292;&#24182;&#21152;&#20837;&#19968;&#20010;&#27719;&#32858;&#26631;&#35760;&#26469;&#24635;&#32467;&#25972;&#20010;&#21477;&#23376;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#20013;&#21010;&#23450;&#20102;&#26126;&#30830;&#30340;&#36793;&#30028;&#65292;&#24182;&#19988;&#24103;&#38388;&#30340;&#34920;&#31034;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#38899;&#33410;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#20986;&#29616;&#30340;&#32467;&#26500;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#30495;&#23454;&#38899;&#33410;&#23545;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;Spoken Speech ABX&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#30340;&#21477;&#23376;&#32423;&#34920;&#31034;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#38899;&#33410;&#21457;&#29616;&#21644;&#23398;&#20064;&#21477;&#23376;&#32423;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.15932</link><description>&lt;p&gt;
BUCA&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#19988;&#22312;&#33539;&#22260;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#38480;&#65292;&#26080;&#30417;&#30563;&#30340;&#24120;&#35782;&#25512;&#29702;(UCR)&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;UCR&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23558;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;(&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;)&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#25490;&#21517;&#26469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#21512;&#29702;&#21644;&#19981;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;KG&#30340;&#29616;&#26377;UCR&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#33410;&#30465;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/probe2/BUCA&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.17408</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#25552;&#31034;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#30340;&#21307;&#30103;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17408
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;(EHRs)&#20272;&#35745;&#21307;&#30103;&#24178;&#39044;&#30340;&#25345;&#32493;&#26102;&#38388;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer-based&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#30456;&#20851;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#65288;&#36830;&#32493;&#12289;&#20998;&#31867;&#12289;&#20108;&#36827;&#21046;&#21644;&#33258;&#30001;&#25991;&#26412;&#29305;&#24449;&#65289;&#25237;&#24433;&#21040;&#19968;&#20010;&#21327;&#35843;&#30340;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20511;&#21161;&#21307;&#23398;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#22312;&#21333;&#20803;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#65288;ICU&#20303;&#38498;&#26102;&#38388;&#20272;&#35745;&#65289;&#21644;&#20122;&#27954;&#65288;&#25163;&#26415;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, estimating the duration of medical intervention based on electronic health records (EHRs) has gained significant attention in the filed of clinical decision support. However, current models largely focus on structured data, leaving out information from the unstructured clinical free-text data. To address this, we present a novel language-enhanced transformer-based framework, which projects all relevant clinical data modalities (continuous, categorical, binary, and free-text features) into a harmonized language latent space using a pre-trained sentence encoder with the help of medical prompts. The proposed method enables the integration of information from different modalities within the cell transformer encoder and leads to more accurate duration estimation for medical intervention. Our experimental results on both US-based (length of stay in ICU estimation) and Asian (surgical duration prediction) medical datasets demonstrate the effectiveness of our proposed framewor
&lt;/p&gt;</description></item></channel></rss>