<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22810;&#35821;&#31181;&#35757;&#32451;&#30340;Poro 34B&#27169;&#22411;&#22312;&#33452;&#20848;&#35821;&#31561;&#23567;&#35821;&#31181;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01856</link><description>&lt;p&gt;
Poro 34B&#21644;&#22810;&#35821;&#31181;&#30340;&#31069;&#31119;
&lt;/p&gt;
&lt;p&gt;
Poro 34B and the Blessing of Multilinguality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01856
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#35757;&#32451;&#30340;Poro 34B&#27169;&#22411;&#22312;&#33452;&#20848;&#35821;&#31561;&#23567;&#35821;&#31181;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#20855;&#26377;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#29616;&#22312;&#38656;&#35201;&#25968;&#19975;&#20159;&#23383;&#30340;&#25991;&#26412;&#65292;&#36825;&#27604;&#32477;&#22823;&#22810;&#25968;&#35821;&#35328;&#21487;&#33719;&#24471;&#30340;&#25991;&#26412;&#25968;&#37327;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#23613;&#31649;&#21253;&#21547;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#26159;&#33719;&#21462;&#26356;&#22810;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#26126;&#26174;&#26041;&#27861;&#65292;&#20294;&#22810;&#35821;&#31181;&#24448;&#24448;&#34987;&#35270;&#20026;&#19968;&#31181;&#35781;&#21650;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#35757;&#32451;&#24037;&#20316;&#20173;&#28982;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#22823;&#35821;&#31181;&#19978;&#12290;&#25105;&#20204;&#30456;&#20449;&#22810;&#35821;&#31181;&#21487;&#20197;&#26159;&#19968;&#31181;&#31069;&#31119;&#65292;&#24182;&#19988;&#24212;&#35813;&#26377;&#21487;&#33021;&#36890;&#36807;&#22810;&#35821;&#31181;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#23567;&#35821;&#31181;&#30340;&#27169;&#22411;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Poro 34B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;1&#19975;&#20159;&#20010;&#33452;&#20848;&#35821;&#12289;&#33521;&#35821;&#21644;&#32534;&#31243;&#35821;&#35328;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#25317;&#26377;340&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22810;&#35821;&#31181;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#27169;&#22411;&#65292;&#19981;&#20165;&#22312;&#33452;&#20848;&#35821;&#30340;&#29616;&#26377;&#27169;&#22411;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#32780;&#19988;&#22312;&#34920;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01856v1 Announce Type: new  Abstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16937</link><description>&lt;p&gt;
&#36328;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#20064;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25552;&#39640;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#26041;&#38754;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65306;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#30456;&#23545;&#36739;&#23569;&#30340;&#24102;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32534;&#31243;&#35821;&#35328;&#26159;&#20302;&#36164;&#28304;&#30340;&#65292;&#32570;&#20047;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#24102;&#26631;&#31614;&#26679;&#26412;&#65292;&#29978;&#33267;&#32570;&#20047;&#26080;&#26631;&#31614;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;&#36951;&#30041;&#25110;&#26032;&#35821;&#35328;&#65289;&#30340;&#29992;&#25143;&#26080;&#27861;&#20139;&#21463;&#21040;LLM&#30340;&#22909;&#22788;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20351;&#29992;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;LLM&#21644;11&#21040;41&#31181;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer 
&lt;/p&gt;</description></item></channel></rss>