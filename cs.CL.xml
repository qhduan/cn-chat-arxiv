<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.</title><link>https://arxiv.org/abs/2403.01390</link><description>&lt;p&gt;
&#27491;&#24403;&#19988;&#20805;&#20998;&#65306;&#21487;&#39564;&#35777;&#30340;&#24120;&#35782;&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01390
&lt;/p&gt;
&lt;p&gt;
LLM-based KGQA methods struggle with hallucination on commonsense reasoning questions, hindering their applicability in real-world applications.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#38382;&#39064;&#22238;&#31572;&#65288;KGQA&#65289;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#30693;&#35782;&#22270;&#20013;&#23384;&#20648;&#30340;&#20851;&#31995;&#20449;&#24687;&#26469;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21450;&#20854;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;KGQA&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;Silvio Berlusconi&#30340;&#31532;&#19968;&#20219;&#22971;&#23376;&#20986;&#29983;&#22312;&#21738;&#24231;&#22478;&#24066;&#65311;&#8221;&#65292;&#32780;&#24573;&#30053;&#20102;&#28041;&#21450;&#24120;&#35782;&#25512;&#29702;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#29616;&#23454;&#19990;&#30028;&#29992;&#25143;&#21487;&#33021;&#26356;&#32463;&#24120;&#25552;&#20986;&#30340;&#65292;&#20363;&#22914;&#8220;&#25105;&#38656;&#35201;&#21333;&#29420;&#30340;&#31614;&#35777;&#25165;&#33021;&#30475;&#21040;&#23041;&#20262;&#22810;&#22827;&#30340;&#32500;&#32435;&#26031;&#24182;&#21442;&#21152;&#20170;&#24180;&#22799;&#22825;&#30340;&#22885;&#36816;&#20250;&#21527;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;KGQA&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#26102;&#38590;&#20197;&#20135;&#29983;&#30495;&#23454;&#30340;&#31572;&#26696;&#65292;&#23588;&#20854;&#26159;&#23545;&#38024;&#23545;&#38271;&#23614;&#23454;&#20307;&#30340;&#26597;&#35810;&#65288;&#20363;&#22914;&#38750;&#20027;&#27969;&#21644;&#26368;&#36817;&#30340;&#23454;&#20307;&#65289;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#21487;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01390v1 Announce Type: new  Abstract: Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., "In which city was Silvio Berlusconi's first wife born?", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., "Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?" unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especial
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#26469;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.08708</link><description>&lt;p&gt;
&#32463;&#30001;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#23454;&#29616;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#20869;&#23384;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#26469;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24191;&#27867;&#20869;&#23384;&#21344;&#29992;&#20250;&#38459;&#30861;&#20854;&#22312;&#20869;&#23384;&#21463;&#38480;&#29615;&#22659;&#65288;&#22914;&#20113;&#29615;&#22659;&#25110;&#35774;&#22791;&#19978;&#65289;&#30340;&#37096;&#32626;&#12290; PLMs&#20351;&#29992;&#23884;&#20837;&#30697;&#38453;&#26469;&#34920;&#31034;&#24191;&#27867;&#30340;&#35789;&#27719;&#65292;&#26500;&#25104;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#22823;&#37096;&#20998;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#32771;&#34385;&#20102;&#22312;Transformer&#23618;&#20869;&#21098;&#26525;&#21442;&#25968;&#20197;&#25552;&#39640;&#21442;&#25968;&#25928;&#29575;&#65292;&#20294;&#22312;&#24494;&#35843;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#21098;&#26525;&#23884;&#20837;&#30697;&#38453;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26377;&#19968;&#20010;&#26174;&#33879;&#27604;&#20363;&#30340;&#35789;&#27719;&#26410;&#34987;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#26469;&#26368;&#23567;&#21270;&#23884;&#20837;&#30697;&#38453;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20801;&#35768;&#26356;&#39640;&#25928;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive memory footprint of pre-trained language models (PLMs) can hinder deployment in memory-constrained settings, such as cloud environments or on-device. PLMs use embedding matrices to represent extensive vocabularies, forming a large proportion of the model parameters. While previous work towards parameter-efficient PLM development has considered pruning parameters within the transformer layers, pruning the embedding matrix as part of fine-tuning or inference has yet to be explored. We first demonstrate that a significant proportion of the vocabulary remains unused in these scenarios. We then propose a simple yet effective approach that leverages this finding to minimize the memory footprint of the embedding matrix. We show that this approach provides substantial reductions in memory usage across a wide range of models and tasks. Notably, our approach maintains equivalent downstream task performance while allowing a more efficient use of compute resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65288;ECT&#65289;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25552;&#39640;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04386</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#35780;&#20272;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Evaluation Models from Large Language Models for Sequence Generation. (arXiv:2308.04386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65288;ECT&#65289;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25552;&#39640;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#29983;&#25104;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#20855;&#26377;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#23427;&#20204;&#30340;&#35780;&#20272;&#33021;&#21147;&#26102;&#20250;&#24102;&#26469;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;ECT&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65292;&#23558;&#35780;&#20272;&#33021;&#21147;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;ECT&#65292;&#25105;&#20204;&#20174;ChatGPT&#20013;&#23398;&#20064;&#20102;&#21508;&#31181;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#25913;&#36827;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#21644;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;ECT&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#35780;&#20272;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#29983;&#25104;&#24207;&#21015;&#65292;&#36825;&#26159;&#36890;&#36807;&#24120;&#29992;&#30340;&#24230;&#37327;&#21644;ChatGPT&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models achieve state-of-the-art performance on sequence generation evaluation, but typically have a large number of parameters. This is a computational challenge as presented by applying their evaluation capability at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an \textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer the evaluation capability from LLMs to relatively lightweight language models. Based on the proposed ECT, we learn various evaluation models from ChatGPT, and employ them as reward models to improve sequence generation models via reinforcement learning and reranking approaches. Experimental results on machine translation, text style transfer, and summarization tasks demonstrate the effectiveness of our ECT. Notably, applying the learned evaluation models to sequence generation models results in better generated sequences as evaluated by commonly used metrics and ChatGPT.
&lt;/p&gt;</description></item></channel></rss>