<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.18668</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#24179;&#34913;&#20102;&#21484;&#22238;-&#21534;&#21520;&#37327;&#30340;&#25240;&#34935;
&lt;/p&gt;
&lt;p&gt;
Simple linear attention language models balance the recall-throughput tradeoff
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#21484;&#22238;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#24050;&#32463;&#30475;&#21040;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21463;&#21040;KV-cache&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#29942;&#39048;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65288;&#20363;&#22914;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65289;&#32780;&#19981;&#24433;&#21709;&#21484;&#22238;&#12290;&#36890;&#36807;&#23558;&#23454;&#39564;&#21644;&#29702;&#35770;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29366;&#24577;&#22823;&#23567;&#21644;&#21484;&#22238;&#33021;&#21147;&#20043;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65288;&#20363;&#22914;H3&#12289;Mamba&#12289;RWKV&#65289;&#20445;&#25345;&#22266;&#23450;&#22823;&#23567;&#30340;&#24490;&#29615;&#29366;&#24577;&#65292;&#20294;&#22312;&#21484;&#22238;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BASED&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32447;&#24615;&#21644;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#31616;&#21333;&#26550;&#26500;&#12290;&#36890;&#36807;&#25913;&#21464;BASED&#31383;&#21475;&#22823;&#23567;&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24449;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#29366;&#24577;&#22823;&#23567;&#65292;&#24182;&#36941;&#21382;&#21484;&#22238;-&#20869;&#23384;&#25240;&#34935;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18668v1 Announce Type: new  Abstract: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff cu
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.09433</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24322;&#26500;&#25968;&#25454;&#28246;&#32467;&#26500;&#21270;&#35270;&#22270;&#29983;&#25104;&#30340;&#31616;&#21333;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes. (arXiv:2304.09433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#21644;&#23450;&#21046;&#19979;&#23454;&#29616;&#29983;&#25104;&#21487;&#26597;&#35810;&#34920;&#26684;&#30340;&#31616;&#21333;&#31995;&#32479;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#31574;&#30053;&#65292;&#22312;&#19981;&#21516;&#36136;&#37327;&#21644;&#25104;&#26412;&#20013;&#24179;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#23545;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#29983;&#25104;&#30340;&#34920;&#26684;&#22343;&#39640;&#36136;&#37327;&#65292;&#19988;&#26080;&#38656;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#30028;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#20986;&#36890;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#21147;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#25668;&#21462;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#24182;&#36755;&#20986;&#21487;&#26597;&#35810;&#30340;&#34920;&#26684;&#12290;&#37492;&#20110;&#28508;&#22312;&#25991;&#26723;&#30340;&#22810;&#26679;&#24615;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#31616;&#21270;&#30340;&#20551;&#35774;&#24182;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20445;&#25345;&#24191;&#27867;&#24615;&#12290;&#22312;&#24191;&#27867;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#21487;&#20165;&#38480;&#20110;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#30001;LLMs&#39537;&#21160;&#30340;&#31616;&#21333;&#21407;&#22411;&#31995;&#32479;EVAPORATE&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#35813;&#31995;&#32479;&#30340;&#20004;&#31181;&#22522;&#26412;&#19981;&#21516;&#31574;&#30053;&#65306;&#25552;&#31034;LLM&#30452;&#25509;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20540;&#25110;&#25552;&#31034;LLM&#21512;&#25104;&#25191;&#34892;&#25552;&#21462;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#25104;&#26412;-&#36136;&#37327;&#26435;&#34913;&#12290;&#20195;&#30721;&#21512;&#25104;&#20415;&#23452;&#65292;&#20294;&#27604;&#30452;&#25509;&#25277;&#21462;&#36828;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EVAPORATE&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#25991;&#26723;&#29305;&#23450;&#30340;&#23450;&#21046;&#24773;&#20917;&#19979;&#20026;&#21508;&#31181;&#25991;&#26723;&#31867;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.  We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than dire
&lt;/p&gt;</description></item></channel></rss>