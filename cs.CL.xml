<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item></channel></rss>