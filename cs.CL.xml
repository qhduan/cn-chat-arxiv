<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17428</link><description>&lt;p&gt;
&#36890;&#36807;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17428
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#37492;&#20110;&#31934;&#31070;&#31185;&#35775;&#35848;&#26159;&#19987;&#19994;&#38754;&#35797;&#32773;&#19982;&#34987;&#38754;&#35797;&#32773;&#20043;&#38388;&#30446;&#26631;&#23548;&#21521;&#21644;&#32467;&#26500;&#21270;&#23545;&#35805;&#65292;&#36825;&#26159;LLMs&#21487;&#20197;&#25552;&#20379;&#23454;&#36136;&#20215;&#20540;&#30340;&#26368;&#26410;&#34987;&#24320;&#21457;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#21019;&#20260;&#32463;&#21382;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#30340;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;LLMs&#29992;&#20110;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#29992;&#36884;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#26159;&#21542;&#33021;&#22815;&#65288;1&#65289;&#21010;&#20998;&#34920;&#31034;&#31934;&#31070;&#30151;&#29366;&#30340;&#23545;&#35805;&#37096;&#20998;&#24182;&#21629;&#21517;&#30151;&#29366;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#35775;&#35848;&#23545;&#35805;&#35760;&#24405;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#12290;&#36825;&#37324;&#65292;&#35775;&#35848;&#25968;&#25454;&#30001;&#31934;&#31070;&#20581;&#24247;&#19987;&#23478;&#36827;&#34892;&#26631;&#35760;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#25552;&#31034;&#30340;LLMs&#22312;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.04801</link><description>&lt;p&gt;
Alpaca&#23545;&#25239;Vicuna&#65306;&#20351;&#29992;LLMs&#25581;&#31034;LLMs&#30340;&#35760;&#24518;&#21270;
&lt;/p&gt;
&lt;p&gt;
Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#30456;&#27604;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#33021;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#35760;&#24518;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#40657;&#30418;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25915;&#20987;&#32773;LLM&#20195;&#29702;&#26469;&#25581;&#31034;&#21463;&#23475;&#20195;&#29702;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#65292;&#19982;&#30452;&#25509;&#29992;&#35757;&#32451;&#25968;&#25454;&#25552;&#31034;&#30446;&#26631;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#26159;&#37327;&#21270;LLMs&#35760;&#24518;&#21270;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#30340;&#25298;&#32477;&#25277;&#26679;&#20248;&#21270;&#36807;&#31243;&#26469;&#25214;&#21040;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#65292;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24449;&#65306;(1)&#19982;&#35757;&#32451;&#25968;&#25454;&#26368;&#23567;&#37325;&#21472;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#21521;&#27169;&#22411;&#21576;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;(2)&#21463;&#23475;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#22823;&#37325;&#21472;&#65292;&#26088;&#22312;&#35825;&#20351;&#21463;&#23475;&#32773;&#21520;&#20986;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#22522;&#20110;&#25351;&#20196;&#30340;&#25552;&#31034;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#37325;&#21472;&#31243;&#24230;&#27604;&#22522;&#32447;&#21069;&#32512;&#21518;&#32512;&#27979;&#37327;&#39640;&#20986;23.7&#65285;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;(1)&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#21487;&#20197;&#26292;&#38706;&#19982;&#20182;&#20204;&#30340;&#22522;&#26412;&#27169;&#22411;&#19968;&#26679;&#22810;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04801v1 Announce Type: new  Abstract: In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03163</link><description>&lt;p&gt;
Design2Code&#65306;&#25105;&#20204;&#31163;&#33258;&#21160;&#21270;&#21069;&#31471;&#24037;&#31243;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Design2Code: How Far Are We From Automating Front-End Engineering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03163
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#39134;&#29467;&#36827;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#26032;&#30340;&#21069;&#31471;&#24320;&#21457;&#33539;&#24335;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;LLMs&#21487;&#33021;&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;Design2Code&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;484&#20010;&#22810;&#26679;&#21270;&#30495;&#23454;&#32593;&#39029;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;LLMs&#33021;&#21542;&#29983;&#25104;&#30452;&#25509;&#28210;&#26579;&#20026;&#32473;&#23450;&#21442;&#32771;&#32593;&#39029;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#20197;&#36755;&#20837;&#20026;&#23631;&#24149;&#25130;&#22270;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;GPT-4V&#21644;Gemini Pro Vision&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19968;&#20010;&#24320;&#28304;&#30340;Design2Code-18B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#65292;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01404</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#30340;&#32570;&#22833;&#21450;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What Is Missing in Multilingual Visual Reasoning and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#65292;&#21457;&#29616;&#20102;&#22810;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#65292;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#27169;&#22411;&#20170;&#22825;&#22312;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#29992;&#25143;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#19968;&#20010;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#65292;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20687;GPT-4V&#36825;&#26679;&#30340;&#19987;&#26377;&#31995;&#32479;&#29616;&#22312;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#19982;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#24046;&#36317;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;GPT-4V&#22312;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24320;&#21457;&#20844;&#24179;&#30340;&#31995;&#32479;&#20855;&#26377;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#22833;&#36133;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20351;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#22810;&#35821;&#35328;&#24615;&#65292;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#21253;&#25324;&#19968;&#31181;&#32763;&#35793;-&#27979;&#35797;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#24615;&#65292;&#19968;&#31181;&#35270;&#35273;&#32534;&#31243;&#26041;&#27861;&#26469;&#20998;&#35299;&#22797;&#26434;&#25512;&#29702;&#65292;&#20197;&#21450;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01404v1 Announce Type: new  Abstract: NLP models today strive for supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V obtain the best performance on this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.14296</link><description>&lt;p&gt;
&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#36890;&#36807;&#26657;&#20934;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases of Large Language Models in Stance Detection with Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#20250;&#29983;&#25104;&#20559;&#35265;&#31435;&#22330;&#65292;&#36825;&#26159;&#30001;&#20110;&#34394;&#20551;&#24773;&#24863;-&#31435;&#22330;&#30456;&#20851;&#24615;&#21644;&#23545;&#26576;&#20123;&#20010;&#20154;&#21644;&#20027;&#39064;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;LLMs&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#30340;&#20559;&#35265;&#65288;MB-Cal&#65289;&#12290;&#22312;&#20854;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#65292;&#20197;&#20943;&#36731;LLMs&#20135;&#29983;&#30340;&#31435;&#22330;&#25512;&#29702;&#32467;&#26524;&#19978;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26657;&#20934;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#26469;&#30699;&#27491;&#31435;&#22330;&#20559;&#35265;&#12290;&#38024;&#23545;&#30446;&#26631;&#21644;&#38646;&#23556;&#20987;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MB-Cal&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#27169;&#22359;&#65288;PEFT&#65289;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#33258;&#24049;&#30340;LLM&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04401</link><description>&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#27169;&#22359;&#65288;PEFT&#65289;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#33258;&#24049;&#30340;LLM&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#20010;&#24615;&#21270;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#26088;&#22312;&#20351;LLM&#30340;&#20132;&#20114;&#12289;&#20869;&#23481;&#21644;&#25512;&#33616;&#19982;&#20010;&#20307;&#29992;&#25143;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#26368;&#36817;LLM&#20010;&#24615;&#21270;&#30340;&#36827;&#23637;&#32858;&#28966;&#20110;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#21382;&#21490;&#26816;&#32034;&#21644;&#25991;&#26412;&#27010;&#35201;&#31561;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#20016;&#23500;&#29992;&#25143;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;OPPU&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#27169;&#22359;&#26469;&#23384;&#20648;&#29992;&#25143;&#29305;&#23450;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;&#36890;&#36807;&#25554;&#20837;&#29992;&#25143;&#30340;&#20010;&#20154;PEFT&#21442;&#25968;&#65292;&#20182;&#20204;&#21487;&#20197;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parame
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05254</link><description>&lt;p&gt;
&#20013;&#32654;&#20004;&#22269;&#20043;&#38388;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#34920;&#36798;&#30340;&#20215;&#20540;&#21644;&#28608;&#21160;&#23545;&#27604;&#65306;&#19968;&#20010;&#36328;&#25991;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20307;&#30340;&#24773;&#24863;&#34920;&#36798;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35199;&#26041;&#29615;&#22659;&#20013;&#12290;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#23384;&#22312;&#30528;&#24341;&#21457;&#24773;&#24863;&#34920;&#36798;&#30340;&#37325;&#35201;&#24046;&#24322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32654;&#22269;Twitter&#21644;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#19978;&#30340;&#20004;&#20010;&#20027;&#35201;&#24773;&#24863;&#32500;&#24230;&#65288;&#20215;&#20540;&#21644;&#28608;&#21160;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20010;&#20307;&#20043;&#38388;&#30340;&#28608;&#21160;&#21644;&#20215;&#20540;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#20869;&#23481;&#19978;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24179;&#21488;&#19978;&#30340;&#35789;&#35821;&#20351;&#29992;&#21644;&#35805;&#39064;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#35299;&#35835;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;Twitter&#29992;&#25143;&#26469;&#35828;&#65292;&#36127;&#38754;&#24773;&#32490;&#21644;&#27491;&#38754;&#24773;&#32490;&#20043;&#38388;&#30340;&#24773;&#24863;&#24378;&#24230;&#21464;&#21270;&#19981;&#22826;&#26126;&#26174;&#65292;&#32780;&#23545;&#20110;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#26469;&#35828;&#65292;&#20276;&#38543;&#30528;&#24773;&#24863;&#30340;&#19978;&#21319;&#65292;&#28608;&#21160;&#31243;&#24230;&#26377;&#26356;&#26126;&#26174;&#30340;&#21319;&#32423;&#12290;&#20174;&#35821;&#35328;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01030</link><description>&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Local and Global Features for Aspect-based Sentiment Classification. (arXiv:2311.01030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#23616;&#37096;&#19978;&#19979;&#25991;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26088;&#22312;&#21028;&#26029;&#21477;&#23376;&#20013;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#25152;&#20256;&#36798;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#24773;&#24863;&#26497;&#24615;&#19981;&#20165;&#30001;&#23616;&#37096;&#19978;&#19979;&#25991;&#20915;&#23450;&#65292;&#36824;&#19982;&#36828;&#31163;&#32473;&#23450;&#26041;&#38754;&#26415;&#35821;&#30340;&#35789;&#27719;&#30456;&#20851;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#36275;&#22815;&#22320;&#21306;&#20998;&#24212;&#35813;&#26356;&#20851;&#27880;&#21738;&#20123;&#35789;&#35821;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#27491;&#22312;&#36827;&#20837;&#22522;&#20110;&#26041;&#21521;&#30340;&#24773;&#24863;&#20998;&#31867;&#20197;&#32534;&#30721;&#21477;&#27861;&#20381;&#36182;&#26641;&#20449;&#24687;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#27861;&#20381;&#36182;&#26641;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#23558;&#20381;&#36182;&#20851;&#31995;&#26631;&#31614;&#20449;&#24687;&#26377;&#25928;&#22320;&#25972;&#21512;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21253;&#21547;&#39640;&#26031;&#25513;&#30721;&#23618;&#21644;&#21327;&#26041;&#24046;&#33258;&#27880;&#24847;&#23618;&#30340;&#23616;&#37096;&#32534;&#30721;&#22120;&#12290;&#39640;&#26031;&#25513;&#30721;&#23618;&#20542;&#21521;&#20110;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#21608;&#22260;&#26041;&#38754;&#26415;&#35821;&#30340;&#24863;&#21463;&#37326;&#65292;&#20197;&#20351;&#20854;&#19981;&#37325;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ASC) aims to judge the sentiment polarity conveyed by the given aspect term in a sentence. The sentiment polarity is not only determined by the local context but also related to the words far away from the given aspect term. Most recent efforts related to the attention-based models can not sufficiently distinguish which words they should pay more attention to in some cases. Meanwhile, graph-based models are coming into ASC to encode syntactic dependency tree information. But these models do not fully leverage syntactic dependency trees as they neglect to incorporate dependency relation tag information into representation learning effectively. In this paper, we address these problems by effectively modeling the local and global features. Firstly, we design a local encoder containing: a Gaussian mask layer and a covariance self-attention layer. The Gaussian mask layer tends to adjust the receptive field around aspect terms adaptively to deemphasize 
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;</title><link>http://arxiv.org/abs/2306.09996</link><description>&lt;p&gt;
&#25506;&#31350;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#25552;&#31034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20855;&#22791;&#29702;&#35299;&#21644;&#25512;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36817;&#26399;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;VQA&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#32452;&#21512;&#38382;&#39064;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#22914;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#20351;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;BLIP2&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;VQA&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#23613;&#31649;&#32467;&#26524;&#21508;&#24322;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#22270;&#20687;&#26631;&#39064;&#65289;&#21487;&#20197;&#20419;&#36827;VQA&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
&lt;/p&gt;</description></item></channel></rss>