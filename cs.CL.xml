<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.10634</link><description>&lt;p&gt;
&#20154;&#31867;&#22522;&#22240;&#26680;&#33527;&#37240;&#24207;&#21015;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DNA&#30456;&#20851;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#31867;&#20284;&#20110;GPT-3&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#32771;&#34385;&#21040;&#22788;&#29702;&#25972;&#20010;DNA&#24207;&#21015;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#20915;&#23450;&#22312;&#26356;&#23567;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;DNA&#12290;&#36825;&#20010;&#20915;&#31574;&#24182;&#19981;&#25913;&#21464;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;DNA&#21644;&#22522;&#22240;&#37117;&#21487;&#20197;&#30475;&#20316;&#30001;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#33527;&#37240;&#32452;&#25104;&#30340;&#19968;&#32500;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
&lt;/p&gt;</description></item></channel></rss>