<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18276</link><description>&lt;p&gt;
RankMamba&#65292;&#22312;Transformer&#26102;&#20195;&#23545;Mamba&#25991;&#26723;&#25490;&#21517;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18276
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;(IR)&#31561;&#22810;&#20010;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;--&#27880;&#24847;&#21147;&#65292;&#22312;&#35757;&#32451;&#20013;&#38656;&#35201;$O(n^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22312;&#25512;&#26029;&#20013;&#38656;&#35201;$O(n)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#27604;&#22914;Flash Attention&#21644;Multi-query Attention&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#26088;&#22312;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#26469;&#21462;&#20195;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#22411;&#32467;&#26500;--Mamba&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
&lt;/p&gt;</description></item><item><title>GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14934</link><description>&lt;p&gt;
GRACE: &#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GRACE: Discriminator-Guided Chain-of-Thought Reasoning. (arXiv:2305.14934v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14934
&lt;/p&gt;
&lt;p&gt;
GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27493;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#20363;&#22914;&#20351;&#29992;&#24605;&#32500;&#38142;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23545;&#38169;&#35823;&#30340;&#27493;&#39588;&#20998;&#37197;&#36739;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#24615;&#30340;&#35299;&#30721;&#31574;&#30053;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRACE&#30340;&#24341;&#23548;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#35757;&#32451;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#20135;&#29983;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;GRACE&#20351;&#29992;&#19968;&#20010;&#22312;&#27491;&#30830;&#21644;&#38169;&#35823;&#27493;&#39588;&#19978;&#36827;&#34892;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#30340;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22522;&#20110;&#27491;&#30830;&#24615;&#23545;&#19979;&#19968;&#27493;&#20505;&#36873;&#36827;&#34892;&#35780;&#20998;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GRACE&#21482;&#38656;&#35201;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;FLAN-T5&#21644;LLaMA&#31995;&#21015;&#30340;&#27169;&#22411;&#65292;&#23545;&#22235;&#20010;&#25968;&#23398;&#21644;&#20004;&#20010;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;GRACE&#30340;&#35780;&#20272;&#65292;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;&#19982;&#36138;&#23146;&#35299;&#30721;&#12289;&#39564;&#35777;&#22120;&#21644;&#33258;&#19968;&#33268;&#24615;&#30456;&#27604;&#65292;GRACE&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When 
&lt;/p&gt;</description></item></channel></rss>