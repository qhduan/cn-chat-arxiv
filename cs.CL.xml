<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#32418;&#38431;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00629</link><description>&lt;p&gt;
&#38024;&#23545;&#38463;&#21888;&#29705;&#26031;&#20043;&#36405;&#65306;&#29983;&#25104;&#27169;&#22411;&#32418;&#38431;&#27979;&#35797;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Against The Achilles' Heel: A Survey on Red Teaming for Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00629
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#32418;&#38431;&#27979;&#35797;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#27491;&#36805;&#36895;&#26222;&#21450;&#24182;&#34987;&#25972;&#21512;&#21040;&#26085;&#24120;&#24212;&#29992;&#20013;&#65292;&#20294;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#65292;&#22240;&#20026;&#21508;&#31181;&#28431;&#27934;&#19981;&#26029;&#26292;&#38706;&#12290;&#38754;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#32418;&#38431;&#27979;&#35797;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#23545;&#25972;&#20010;&#27969;&#31243;&#36827;&#34892;&#20840;&#38754;&#32452;&#32455;&#24182;&#35299;&#20915;&#31038;&#21306;&#26032;&#20852;&#20027;&#39064;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35843;&#26597;&#28085;&#30422;&#20102;120&#22810;&#31687;&#35770;&#25991;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25915;&#20987;&#31574;&#30053;&#20998;&#31867;&#20307;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#21508;&#31181;&#33258;&#21160;&#32418;&#38431;&#27979;&#35797;&#26041;&#27861;&#30340;&#25628;&#32034;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#26032;&#39046;&#22495;&#65292;&#21253;&#25324;&#22810;&#27169;&#24335;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#39118;&#38505;&#12289;&#26080;&#23475;&#26597;&#35810;&#30340;&#36807;&#24230;&#20351;&#29992;&#20197;&#21450;&#19979;&#28216;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00629v1 Announce Type: new  Abstract: Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32534;&#30721;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10779</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10779
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32534;&#30721;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#65288;ZSLP&#65289;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#26469;&#39044;&#27979;&#32473;&#23450;&#22836;&#23454;&#20307;&#21644;&#20854;&#20851;&#31995;&#26102;&#30340;&#23614;&#23454;&#20307;&#65292;&#28982;&#32780;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#26377;&#26102;&#32570;&#20047;&#36825;&#20123;&#35814;&#32454;&#20449;&#24687;&#65292;&#24182;&#19988;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#39044;&#27979;&#23614;&#23454;&#20307;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39044;&#27979;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#20043;&#38388;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#21463;&#38480;&#20110;&#26080;&#27861;&#21033;&#29992;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#25152;&#26377;&#65288;&#25351;&#25968;&#22810;&#65289;&#36335;&#24452;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#20849;&#21516;&#25351;&#31034;&#23427;&#20204;&#30340;&#20851;&#31995;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65288;CTLP&#65289;&#65292;&#23427;&#20197;&#32447;&#24615;&#26102;&#38388;&#32534;&#30721;&#20102;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10779v1 Announce Type: new  Abstract: Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically identifying relations between given entities. Existing methods primarily employ auxiliary information to predict tail entity given head entity and its relation, yet face challenges due to the occasional unavailability of such detailed information and the inherent simplicity of predicting tail entities based on semantic similarities. Even though Large Language Models (LLMs) offer a promising solution to predict unobserved relations between the head and tail entity in a zero-shot manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths' information between two entities, which are critical in collectively indicating their relation types. To address this, in this work, we introduce a Condensed Transition Graph Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths' information in linear time
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01710</link><description>&lt;p&gt;
&#26143;&#36784;&#21363;&#20320;&#25152;&#38656;&#65306;&#29992;&#36828;&#31243;&#30417;&#30563;&#37329;&#23383;&#22612;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#65292;&#23454;&#29616;&#26041;&#38754;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25991;&#26723;&#32423;&#31471;&#21040;&#31471;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#23545;&#22312;&#32447;&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#26041;&#38754;&#21644;&#35780;&#35770;&#24773;&#24863;&#36827;&#34892;&#26377;&#25928;&#30340;&#32479;&#19968;&#20998;&#26512;&#12290;&#25105;&#20204;&#20551;&#35774;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#26159;&#35780;&#35770;&#20013;&#21508;&#26041;&#38754;&#35780;&#20998;&#30340;&#8220;&#31895;&#31890;&#24230;&#32508;&#21512;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;DSPN&#65289;&#65292;&#21482;&#29992;&#25991;&#26723;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#21487;&#26377;&#25928;&#22320;&#25191;&#34892;&#26041;&#38754;-&#31867;&#21035;&#26816;&#27979;&#12289;&#26041;&#38754;-&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#21644;&#35780;&#20998;&#39044;&#27979;&#12290;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#25191;&#34892;&#36825;&#19977;&#20010;&#30456;&#20851;&#30340;&#24773;&#24863;&#23376;&#20219;&#21153;&#65292;DSPN&#21487;&#20197;&#25552;&#21462;&#35780;&#35770;&#20013;&#25552;&#21040;&#30340;&#26041;&#38754;&#65292;&#30830;&#23450;&#30456;&#24212;&#30340;&#24773;&#24863;&#65292;&#24182;&#39044;&#27979;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#33521;&#25991;&#21644;&#27721;&#35821;&#22810;&#26041;&#38754;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DSPN&#65292;&#21457;&#29616;&#20165;&#20351;&#29992;&#26143;&#32423;&#35780;&#20998;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#65292;DSPN&#30340;&#24615;&#33021;&#19982;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;DSPN&#22312;&#35780;&#35770;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#36755;&#20986;&#65292;&#20197;&#35828;&#26126;&#37329;&#23383;&#22612;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose document-level end-to-end sentiment analysis to efficiently understand aspect and review sentiment expressed in online reviews in a unified manner. In particular, we assume that star rating labels are a "coarse-grained synthesis" of aspect ratings across in the review. We propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating Prediction using only document star rating labels for training. By performing these three related sentiment subtasks in an end-to-end manner, DSPN can extract aspects mentioned in the review, identify the corresponding sentiments, and predict the star rating labels. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that with only star rating labels for supervision, DSPN can perform comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN's outputs on reviews to show the py
&lt;/p&gt;</description></item></channel></rss>