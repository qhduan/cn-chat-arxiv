<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10168</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20154;-&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#24037;&#20316;&#32773;&#65311;&#29992;LLM&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs. (arXiv:2307.10168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20247;&#21253;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#28508;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#20197;&#21069;&#34987;&#35748;&#20026;&#21482;&#26377;&#20154;&#31867;&#25165;&#33021;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#21407;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#25506;&#32034;LLM&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;LLM&#21487;&#20197;&#27169;&#25311;&#26576;&#20123;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;&#36825;&#20123;&#8220;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#8221;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#25104;&#21151;&#30340;&#31243;&#24230;&#26159;&#21487;&#21464;&#30340;&#65292;&#24182;&#21463;&#21040;&#35831;&#27714;&#32773;&#23545;LLM&#33021;&#21147;&#30340;&#29702;&#35299;&#12289;&#23376;&#20219;&#21153;&#25152;&#38656;&#30340;&#29305;&#23450;&#25216;&#33021;&#20197;&#21450;&#25191;&#34892;&#36825;&#20123;&#23376;&#20219;&#21153;&#30340;&#26368;&#20339;&#20132;&#20114;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#20154;&#31867;&#21644;LLM&#23545;&#25351;&#31034;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#65292;&#24378;&#35843;&#20026;LLM&#25552;&#20379;&#38754;&#21521;&#20154;&#31867;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20855;&#26377;&#20114;&#34917;&#25216;&#33021;&#30340;&#20154;&#31867;&#21644;LLM&#30340;&#28508;&#21147;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24179;&#21488;&#26469;&#30740;&#31350;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#65288;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross
&lt;/p&gt;</description></item><item><title>BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.00915</link><description>&lt;p&gt;
BiomedCLIP&#65306;&#19968;&#31181;&#20174;&#19968;&#21315;&#20116;&#30334;&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00915
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21253;&#25324;&#29289;&#29702;&#27979;&#37327;&#21644;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#12290;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#22788;&#29702;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20363;&#22914;&#24179;&#34892;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;PMC-15M&#65292;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;MIMIC-CXR&#65289;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31867;&#22411;&#12290;PMC-15M&#21253;&#21547;&#20102;&#26469;&#33258;440&#19975;&#31185;&#23398;&#35770;&#25991;&#30340;1500&#19975;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22522;&#20110;PMC-15M&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;BiomedCLIP&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#65292;&#20197;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#65292;&#20174;&#26816;&#32034;&#21040;&#20998;&#31867;&#21040;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedC
&lt;/p&gt;</description></item></channel></rss>