<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#23545;LLMs&#22312;Semantic Overlap Summarization&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;TELeR&#20998;&#31867;&#27861;&#35780;&#20272;&#20102;15&#20010;&#27969;&#34892;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#24635;&#32467;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17008</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#37325;&#21472;&#25688;&#35201;&#20219;&#21153;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLMs on the Semantic Overlap Summarization Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;LLMs&#22312;Semantic Overlap Summarization&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;TELeR&#20998;&#31867;&#27861;&#35780;&#20272;&#20102;15&#20010;&#27969;&#34892;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#24635;&#32467;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Semantic Overlap Summarization (SOS)&#26159;&#19968;&#39033;&#21463;&#38480;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#65292;&#20854;&#20013;&#32422;&#26463;&#26159;&#25429;&#33719;&#20004;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#30340;&#20849;&#21516;/&#37325;&#21472;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#25688;&#35201;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#36807;&#20351;&#29992;LLMs&#36827;&#34892;SOS&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#30001;&#20110;LLMs&#30340;&#21709;&#24212;&#23545;&#25552;&#31034;&#35774;&#35745;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#24456;&#25935;&#24863;&#65292;&#36827;&#34892;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#24471;&#20986;&#21487;&#38752;&#32467;&#35770;&#20043;&#21069;&#31995;&#32479;&#22320;&#25506;&#32034;&#21508;&#31181;&#25552;&#31034;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;TELeR&#20998;&#31867;&#27861;&#65292;&#21487;&#29992;&#20110;&#35774;&#35745;&#21644;&#25506;&#32034;LLMs&#30340;&#21508;&#31181;&#25552;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;TELeR&#20998;&#31867;&#27861;&#21644;15&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;SOS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#23427;&#20204;&#20174;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20013;&#24635;&#32467;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17008v1 Announce Type: new  Abstract: Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in Large Language Models (LLMs) have achieved superior performance in numerous summarization tasks, a benchmarking study of the SOS task using LLMs is yet to be performed. As LLMs' responses are sensitive to slight variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed which can be used to design and explore various prompts for LLMs. Using this TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs on the SOS Task, assessing their ability to summarize overlapping information from multiple alternative narratives. For 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.04941</link><description>&lt;p&gt;
&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#20197;&#29702;&#35299;&#38405;&#35835;&#21644;&#38405;&#35835;&#38556;&#30861;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#20027;&#21160;&#25512;&#29702;&#26469;&#27169;&#25311;&#38405;&#35835;&#21644;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#23558;&#35821;&#35328;&#22788;&#29702;&#25551;&#36848;&#20026;&#23545;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#20174;&#38899;&#33410;&#21040;&#21477;&#23376;&#30340;&#19981;&#21516;&#31890;&#24230;&#23454;&#29616;&#39044;&#27979;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23454;&#29616;&#36924;&#30495;&#30340;&#25991;&#26412;&#39044;&#27979;&#65292;&#20197;&#21450;&#20027;&#21160;&#25512;&#29702;&#29992;&#20110;&#24341;&#23548;&#30524;&#21160;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#39044;&#27979;&#36827;&#34892;&#27979;&#35797;&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29087;&#32451;&#38405;&#35835;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#65292;&#24182;&#36981;&#24490;&#38405;&#35835;&#21452;&#36335;&#29702;&#35770;&#20013;&#30340;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#36335;&#24452;&#30340;&#21306;&#20998;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#30524;&#21160;&#34892;&#20026;&#20135;&#29983;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#38405;&#35835;&#38556;&#30861;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#20943;&#24369;&#20102;&#20808;&#39564;&#30340;&#36129;&#29486;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#26356;&#21152;&#26029;&#29255;&#21270;&#30340;&#38405;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel computational model employing hierarchical active inference to simulate reading and eye movements. The model characterizes linguistic processing as inference over a hierarchical generative model, facilitating predictions and inferences at various levels of granularity, from syllables to sentences.  Our approach combines the strengths of large language models for realistic textual predictions and active inference for guiding eye movements to informative textual information, enabling the testing of predictions. The model exhibits proficiency in reading both known and unknown words and sentences, adhering to the distinction between lexical and nonlexical routes in dual-route theories of reading. Notably, our model permits the exploration of maladaptive inference effects on eye movements during reading, such as in dyslexia. To simulate this condition, we attenuate the contribution of priors during the reading process, leading to incorrect inferences and a more fragmented
&lt;/p&gt;</description></item></channel></rss>