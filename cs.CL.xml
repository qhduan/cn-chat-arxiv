<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.01225</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22914;BERT&#12289;Roberta&#12289;T5&#21644;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#24456;&#38590;&#29702;&#35299;&#23545;&#25239;&#24615;&#20998;&#31867;&#24182;&#35782;&#21035;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#12290;&#23427;&#19987;&#27880;&#20110;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26102;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;IT-DT&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#24615;&#26816;&#27979;&#12290;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#12290;&#22312;&#36716;&#25442;&#38454;&#27573;&#65292;IT-DT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#12290;&#36890;&#36807;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#25442;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
&lt;/p&gt;</description></item><item><title>EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.07805</link><description>&lt;p&gt;
EasyNER&#65306;&#19968;&#31181;&#21487;&#23450;&#21046;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#21307;&#23398;&#25991;&#26412;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text. (arXiv:2304.07805v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07805
&lt;/p&gt;
&lt;p&gt;
EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30740;&#31350;&#24050;&#32463;&#20135;&#29983;&#20102;&#22823;&#37327;&#20986;&#29256;&#29289;&#65292;PubMed&#25968;&#25454;&#24211;&#24050;&#32463;&#25910;&#24405;&#20102;&#36229;&#36807;3,500&#19975;&#31687;&#30740;&#31350;&#25991;&#31456;&#12290;&#25972;&#21512;&#36825;&#20123;&#20998;&#25955;&#22312;&#22823;&#37327;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#29983;&#29702;&#26426;&#21046;&#21644;&#23548;&#33268;&#26032;&#22411;&#21307;&#23398;&#24178;&#39044;&#30340;&#30142;&#30149;&#36807;&#31243;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25104;&#20026;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20154;&#31867;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#36825;&#23588;&#20854;&#25104;&#20026;&#38382;&#39064;&#12290;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#21487;&#20197;&#24110;&#21161;&#20174;&#22823;&#37327;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#25552;&#21462;&#21644;&#36830;&#25509;&#20449;&#24687;&#12290;&#25991;&#26412;&#25366;&#25496;&#30340;&#31532;&#19968;&#27493;&#36890;&#24120;&#26159;&#35782;&#21035;&#29305;&#23450;&#31867;&#21035;&#30340;&#20851;&#38190;&#23383;&#65288;&#20363;&#22914;&#25152;&#26377;&#34507;&#30333;&#36136;&#25110;&#30142;&#30149;&#21517;&#31216;&#65289;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;NER&#24037;&#20855;EasyNER&#65292;&#29992;&#20110;&#35782;&#21035;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#30340;&#20856;&#22411;&#23454;&#20307;&#65292;&#21253;&#25324;&#30142;&#30149;&#21517;&#31216;&#12289;&#33647;&#29289;&#21517;&#31216;&#21644;&#34507;&#30333;&#36136;&#21517;&#31216;&#12290;EasyNER&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#32463;&#39564;&#27700;&#24179;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#25105;&#20204;&#23558;EasyNER&#24212;&#29992;&#20110;COVID-19&#30456;&#20851;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#20013;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;&#65292;&#20026;&#19979;&#28216;&#20998;&#26512;&#25552;&#20379;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical research generates a large number of publications with the PubMed database already containing &gt;35 million research articles. Integration of the knowledge scattered across this large body of literature could provide key insights into physiological mechanisms and disease processes leading to novel medical interventions. However, it is a great challenge for researchers to utilize this information in full since the scale and complexity of the data greatly surpasses human processing abilities. This becomes especially problematic in cases of extreme urgency like the COVID-19 pandemic. Automated text mining can help extract and connect information from the large body of medical research articles. The first step in text mining is typically the identification of specific classes of keywords (e.g., all protein or disease names), so called Named Entity Recognition (NER). Here we present an end-to-end pipeline for NER of typical entities found in medical research articles, including diseas
&lt;/p&gt;</description></item></channel></rss>