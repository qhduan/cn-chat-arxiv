<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08728</link><description>&lt;p&gt;
LLM-FuncMapper:&#36890;&#36807;LLM&#35299;&#37322;&#24314;&#31569;&#27861;&#35268;&#20013;&#30340;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper: Function Identification for Interpreting Complex Clauses in Building Codes via LLM. (arXiv:2308.08728v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08728
&lt;/p&gt;
&lt;p&gt;
LLM-FuncMapper&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#23454;&#29616;&#23545;&#24314;&#31569;&#27861;&#35268;&#20013;&#22797;&#26434;&#26465;&#27454;&#30340;&#20989;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21407;&#23376;&#20989;&#25968;&#21644;&#24320;&#21457;&#25552;&#31034;&#27169;&#26495;&#26469;&#35299;&#20915;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#21160;&#21270;&#35268;&#21017;&#26816;&#26597;&#65288;ARC&#65289;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23545;&#30417;&#31649;&#24615;&#25991;&#26412;&#30340;&#35268;&#21017;&#35299;&#37322;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#21644;&#20256;&#32479;&#36923;&#36753;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35299;&#37322;&#20855;&#26377;&#38544;&#24335;&#23646;&#24615;&#25110;&#22797;&#26434;&#35745;&#31639;&#36923;&#36753;&#30340;&#30417;&#31649;&#26465;&#27454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35782;&#21035;&#21508;&#31181;&#30417;&#31649;&#26465;&#27454;&#25152;&#38656;&#30340;&#39044;&#23450;&#20041;&#20989;&#25968;&#30340;&#26041;&#27861;LLM-FuncMapper&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23545;&#24314;&#31569;&#27861;&#35268;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#21407;&#23376;&#20989;&#25968;&#65292;&#20197;&#25429;&#25417;&#38544;&#24335;&#23646;&#24615;&#21644;&#22797;&#26434;&#32422;&#26463;&#30340;&#20849;&#20139;&#35745;&#31639;&#36923;&#36753;&#65292;&#21019;&#24314;&#20102;&#24120;&#35265;&#22359;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#35299;&#37322;&#30417;&#31649;&#26465;&#27454;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20998;&#31867;&#30340;&#35843;&#20248;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20989;&#25968;&#35782;&#21035;&#21151;&#33021;&#12290;&#26368;&#21518;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a vital stage of automated rule checking (ARC), rule interpretation of regulatory texts requires considerable effort. However, interpreting regulatory clauses with implicit properties or complex computational logic is still challenging due to the lack of domain knowledge and limited expressibility of conventional logic representations. Thus, LLM-FuncMapper, an approach to identifying predefined functions needed to interpret various regulatory clauses based on the large language model (LLM), is proposed. First, by systematically analysis of building codes, a series of atomic functions are defined to capture shared computational logics of implicit properties and complex constraints, creating a database of common blocks for interpreting regulatory clauses. Then, a prompt template with the chain of thought is developed and further enhanced with a classification-based tuning strategy, to enable common LLMs for effective function identification. Finally, the proposed approach is validated
&lt;/p&gt;</description></item></channel></rss>