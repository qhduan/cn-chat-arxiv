<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.12170</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Unsupervised LLM Adaptation for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#26679;&#21270;&#30693;&#35782;&#12290;&#25509;&#30528;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#36820;&#22238;&#22810;&#26679;&#38382;&#39064;&#30340;&#27491;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;LLM&#35843;&#25972;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#22914;&#19981;&#21516;&#32452;&#32455;&#25110;&#26102;&#26399;&#65292;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#12289;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#28304;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65307;&#65288;i&#65289;&#24494;&#35843;&#27169;&#22411;&#23637;&#31034;&#20102;&#25552;&#20379;&#27491;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11005</link><description>&lt;p&gt;
&#25506;&#31350;&#20215;&#20540;&#20559;&#22909;&#65306;LLMs&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploring Value Biases: How LLMs Deviate Towards the Ideal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11005
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21709;&#24212;&#23545;&#31038;&#20250;&#20135;&#29983;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#29702;&#35299;LLMs&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#30340;&#38750;&#25925;&#24847;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#36776;&#21035;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#36825;&#31181;&#26080;&#24847;&#35782;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#25277;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#36825;&#31181;&#25277;&#26679;&#29616;&#35937;&#65292;&#21457;&#29616;LLMs&#30340;&#25277;&#26679;&#20542;&#21521;&#20110;&#20559;&#29233;&#39640;&#20215;&#20540;&#36873;&#39033;&#12290;&#20215;&#20540;&#20559;&#22909;&#23545;&#24212;&#20110;&#20174;&#26368;&#21487;&#33021;&#30340;&#21709;&#24212;&#21521;LLM&#20013;&#20195;&#34920;&#30340;&#29702;&#24819;&#20215;&#20540;&#30340;&#36716;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#21363;&#20415;&#26159;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#23398;&#20064;&#21040;&#30340;&#26032;&#23454;&#20307;&#65292;&#36825;&#31181;&#25928;&#26524;&#20063;&#33021;&#22815;&#20877;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20559;&#24046;&#34920;&#29616;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22320;&#26041;&#65292;&#24182;&#23545;&#36873;&#25321;&#20856;&#22411;&#23454;&#20363;&#31561;&#30456;&#20851;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20215;&#20540;&#20559;&#22909;&#22312;&#19981;&#21516;&#20998;&#31867;&#30340;LLMs&#20013;&#37117;&#24456;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
&lt;/p&gt;</description></item></channel></rss>