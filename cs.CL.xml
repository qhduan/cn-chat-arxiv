<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#32780;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#22312;&#21387;&#32553;&#25552;&#31034;&#21644;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.01077</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompting Methods for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#32780;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#22312;&#21387;&#32553;&#25552;&#31034;&#21644;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24050;&#25104;&#20026;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#20026;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21363;&#27169;&#22411;&#25512;&#29702;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#30340;&#20154;&#21147;&#21171;&#21160;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20887;&#38271;&#21644;&#22797;&#26434;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;&#21644;&#25511;&#21046;LLMs&#30340;&#34892;&#20026;&#26102;&#12290;&#22240;&#27492;&#65292;LLM&#39046;&#22495;&#35265;&#35777;&#20102;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#30340;&#26174;&#33879;&#28608;&#22686;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#22312;&#36739;&#39640;&#30340;&#23618;&#38754;&#19978;&#65292;&#39640;&#25928;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#20998;&#31867;&#20026;&#20004;&#31181;&#26041;&#24335;&#65306;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#30340;&#25552;&#31034;&#21644;&#20855;&#26377;&#39640;&#25928;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;&#21069;&#32773;&#28041;&#21450;&#21508;&#31181;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21518;&#32773;&#37319;&#29992;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#31034;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#22238;&#39038;&#20102;&#39640;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01077v1 Announce Type: new  Abstract: Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.06892</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#24320;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#21464;&#21387;&#22120;&#30340;&#26816;&#27979;&#22120;&#65288;DETRs&#65289;&#36890;&#36807;&#25972;&#21512;&#35821;&#35328;&#27169;&#24577;&#65292;&#22312;&#23553;&#38381;&#38598;&#21644;&#24320;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#65288;OVD&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#39640;&#35201;&#27714;&#30340;&#35745;&#31639;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;OVDEval&#22522;&#20934;&#27979;&#35797;&#20013;&#20004;&#20010;&#39046;&#20808;&#27169;&#22411;OmDet&#21644;Grounding-DINO&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;OmDet-Turbo&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23454;&#26102;OVD&#27169;&#22411;&#20855;&#26377;&#21019;&#26032;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#65288;EFH&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#32531;&#35299;OmDet&#21644;Grounding-DINO&#20013;&#35266;&#23519;&#21040;&#30340;&#29942;&#39048;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OmDet-Turbo-Base&#22312;&#24212;&#29992;TensorRT&#21644;&#35821;&#35328;&#32531;&#23384;&#25216;&#26415;&#21518;&#23454;&#29616;&#20102;100.2&#24103;&#27599;&#31186;&#65288;FPS&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;COCO&#21644;LVIS&#25968;&#25454;&#38598;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;OmDet-Turbo&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06892v1 Announce Type: cross  Abstract: End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01432</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29992;&#20110;&#19981;&#22826;&#27969;&#34892;&#30693;&#35782;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35760;&#24518;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#24403;&#22788;&#29702;&#19981;&#22826;&#27969;&#34892;&#25110;&#20302;&#39057;&#27010;&#24565;&#21644;&#23454;&#20307;&#26102;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#20363;&#22914;&#22312;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#23545;&#23450;&#21046;LLMs&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FT&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#21463;&#27426;&#36814;&#21644;&#26368;&#19981;&#21463;&#27426;&#36814;&#30340;&#32676;&#20307;&#20013;&#65292;&#32780;RAG&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#21478;&#22806;&#65292;&#26816;&#32034;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#36827;&#27493;&#21152;&#24378;&#20102;RAG&#21644;FT&#26041;&#27861;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11512</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#21040;&#24179;&#31561;&#65306;&#21435;&#20559;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#35789;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#23427;&#20204;&#26159;&#36825;&#20123;&#27169;&#22411;&#25226;&#25569;&#19978;&#19979;&#25991;&#20851;&#31995;&#12289;&#20419;&#36827;&#26356;&#32454;&#33268;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#22312;&#35768;&#22810;&#38656;&#35201;&#23545;&#20154;&#31867;&#35821;&#35328;&#26377;&#22522;&#26412;&#29702;&#35299;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#22522;&#30707;&#12290;&#37492;&#20110;&#36825;&#20123;&#23884;&#20837;&#24448;&#24448;&#33258;&#36523;&#21453;&#26144;&#25110;&#23637;&#31034;&#20559;&#35265;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20063;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#36825;&#31181;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24320;&#21019;&#24615;&#21069;&#20154;&#30740;&#31350;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;DeepSoftDebias&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#8220;&#36719;&#21435;&#20559;&#8221;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#20855;&#26377;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;DeepSoftDebias&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
&lt;/p&gt;</description></item><item><title>Reviewer2&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#65292;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.10886</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#20248;&#21270;&#35780;&#35770;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reviewer2: Optimizing Review Generation Through Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10886
&lt;/p&gt;
&lt;p&gt;
Reviewer2&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#65292;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLMs&#30340;&#21457;&#23637;&#20026;&#21327;&#21161;&#20316;&#32773;&#25913;&#36827;&#20854;&#20316;&#21697;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290; &#26412;&#25991;&#35774;&#24819;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#21363;&#20316;&#32773;&#21487;&#20197;&#25910;&#21040;LLM&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25581;&#31034;&#24403;&#21069;&#33609;&#31295;&#20013;&#30340;&#24369;&#28857;&#12290; &#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#35780;&#35770;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#29983;&#25104;&#32570;&#20047;&#32454;&#33410;&#30340;&#35780;&#35770;&#65292;&#24182;&#19988;&#19981;&#33021;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#20135;&#29983;&#30340;&#21508;&#31181;&#24847;&#35265;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reviewer2&#30340;&#39640;&#25928;&#20108;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#12290; &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#12290; &#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23558;&#23548;&#33268;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290; &#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;27,000&#31687;&#35770;&#25991;&#21644;99,000&#31687;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29992;&#26041;&#38754;&#25552;&#31034;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10886v1 Announce Type: new  Abstract: Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.02408</link><description>&lt;p&gt;
GLaPE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02408
&lt;/p&gt;
&lt;p&gt;
GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#33258;&#36523;&#20316;&#20026;&#20248;&#21270;&#22120;&#26469;&#35782;&#21035;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26368;&#20248;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25552;&#31034;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#26631;&#31614;&#65292;&#20197;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#25552;&#31034;&#30340;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#24191;&#27867;&#30340;&#23454;&#26045;&#21644;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65288;GLaPE&#65289;&#65292;&#20197;&#20943;&#23569;&#23545;&#37329;&#26631;&#31614;&#30340;&#20381;&#36182;&#12290;&#21463;&#21040;&#33258;&#19968;&#33268;&#24615;&#21644;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#36827;&#34892;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GLaPE&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20845;&#20010;&#20219;&#21153;&#65292;GLaPE&#22312;&#32477;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#35780;&#20272;&#32467;&#26524;&#19982;&#20351;&#29992;&#30495;&#23454;&#37329;&#26631;&#31614;&#35780;&#20272;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2310.18964</link><description>&lt;p&gt;
LLMs&#19982;Fine-tuning: &#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#36328;&#39046;&#22495;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;Large Language Models&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#22312;&#24615;&#33021;&#19978;&#20173;&#28982;&#20855;&#26377;&#26497;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20132;&#27969;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#25968;&#23383;&#24179;&#21488;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#35299;&#20915;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;1&#65289;&#27169;&#22411;&#24615;&#33021;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24494;&#35843;&#21644;&#35757;&#32451;&#21442;&#25968;&#65311;&#65288;2&#65289;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#31243;&#24230;&#22914;&#20309;&#65311;&#20197;&#21450;&#65288;3&#65289;&#24433;&#21709;&#27867;&#21270;&#28508;&#21147;&#30340;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#30340;&#20855;&#20307;&#29305;&#24449;&#26159;&#20160;&#20040;&#65311;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;LLMs&#20063;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#20026;&#20102;&#22238;&#31572;&#38382;&#39064;&#65288;1&#65289;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;36&#20010;&#39046;&#22495;&#20869;&#20998;&#31867;&#22120;&#65292;&#28085;&#30422;&#20102;LLaMA&#12289;Vicuna&#21450;&#20854;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#29366;&#24577;&#65292;&#36328;&#36234;&#20102;&#20061;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18964v2 Announce Type: replace  Abstract: In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. To answer (1) we analyze 36 in-domain classifiers comprising LLaMA, Vicuna, and their variations in pre-trained and fine-tuned states across nine publicly available datasets that span a wide range of platforms a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.14212</link><description>&lt;p&gt;
&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#25913;&#36827;&#20102;&#24847;&#22806;&#24773;&#20917;&#19979;&#30340;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations. (arXiv:2401.14212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#35782;&#21035;&#35270;&#35273;&#23454;&#20307;&#24182;&#23558;&#23427;&#20204;&#25490;&#21015;&#22312;&#20108;&#32500;&#31354;&#38388;&#24067;&#23616;&#20013;&#65292;&#38656;&#35201;&#23545;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#32452;&#21512;&#29702;&#35299;&#12290;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#23616;&#37096;&#21644;&#21463;&#25511;&#30340;&#20462;&#22797;&#12290;&#36890;&#36807;&#27604;&#36739;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#38544;&#24335;&#25110;&#26174;&#24335;&#32534;&#30721;&#21477;&#23376;&#35821;&#27861;&#30340;&#35821;&#35328;&#34920;&#31034;&#20013;&#39044;&#27979;&#24067;&#23616;&#65292;&#22914;&#26524;&#21477;&#23376;&#25552;&#21040;&#30340;&#23454;&#20307;&#20851;&#31995;&#19982;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#12290;&#20026;&#20102;&#27979;&#35797;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30001;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#24067;&#23616;&#32452;&#25104;&#30340;&#27979;&#35797;&#38598;&#65292;&#25551;&#36848;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#26410;&#26366;&#35265;&#36807;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#27169;&#22411;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#29702;&#35299;&#36755;&#20837;&#21477;&#23376;&#30340;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#25439;&#22833;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#24378;&#21270;&#20102;&#21477;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.00802</link><description>&lt;p&gt;
GRDD: &#24076;&#33098;&#26041;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20811;&#37324;&#29305;&#12289;&#24222;&#25552;&#12289;&#21271;&#24076;&#33098;&#21644;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#22235;&#31181;&#26041;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#24179;&#34913;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#24182;&#19988;&#26159;&#21019;&#24314;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#31867;&#20284;&#36164;&#28304;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25152;&#30740;&#31350;&#30340;&#26041;&#35328;&#20855;&#26377;&#36275;&#22815;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#38024;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#38169;&#35823;&#26159;&#30001;&#20110;&#25968;&#25454;&#38598;&#28165;&#29702;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a dataset for the computational study of a number of Modern Greek dialects. It consists of raw text data from four dialects of Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is of considerable size, albeit imbalanced, and presents the first attempt to create large scale dialectal resources of this type for Modern Greek dialects. We then use the dataset to perform dialect idefntification. We experiment with traditional ML algorithms, as well as simple DL architectures. The results show very good performance on the task, potentially revealing that the dialects in question have distinct enough characteristics allowing even simple ML models to perform well on the task. Error analysis is performed for the top performing algorithms showing that in a number of cases the errors are due to insufficient dataset cleaning.
&lt;/p&gt;</description></item></channel></rss>