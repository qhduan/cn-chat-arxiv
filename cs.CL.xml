<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.16442</link><description>&lt;p&gt;
&#22914;&#26524;CLIP&#33021;&#35828;&#35805;: &#36890;&#36807;&#23427;&#20204;&#30340;&#39318;&#36873;&#27010;&#24565;&#25551;&#36848;&#29702;&#35299;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#34920;&#31034;&#26159;&#22522;&#20110;&#24418;&#29366;&#31561;&#35270;&#35273;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;VLM&#22312;&#34920;&#31034;&#27010;&#24565;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Extract and Explore&#65288;EX2&#65289;&#65292;&#29992;&#20110;&#21051;&#30011;VLM&#30340;&#37325;&#35201;&#25991;&#26412;&#29305;&#24449;&#12290;EX2&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;VLM&#39318;&#36873;&#39033;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;VLM&#37325;&#35201;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#36825;&#20123;&#25551;&#36848;&#20197;&#30830;&#23450;&#23545;VLM&#34920;&#31034;&#26377;&#36129;&#29486;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;&#27809;&#26377;&#24110;&#21161;&#20449;&#24687;&#30340;&#34394;&#20551;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#21333;&#20987;&#25918;&#22823;&#27010;&#24565;&#30340;&#29031;&#29255;&#65289;&#65292;&#20294;&#22312;VLM&#34920;&#31034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#20013;&#65292;VLM&#22312;&#34920;&#31034;&#35270;&#35273;&#27010;&#24565;&#26102;&#26174;&#33879;&#20381;&#36182;&#38750;&#35270;&#35273;&#23646;&#24615;&#65288;&#22914;&#26646;&#24687;&#22320;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05152</link><description>&lt;p&gt;
&#26397;&#21521;&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Towards a Psychology of Machines: Large Language Models Predict Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#32570;&#20047;&#20154;&#31867;&#35748;&#30693;&#22522;&#30784;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#31616;&#21333;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#25552;&#20379;&#20851;&#20110;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#27934;&#35265;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25991;&#26412;&#29702;&#35299;&#29702;&#35770;&#65292;&#25105;&#20204;&#20551;&#35774;&#35782;&#21035;&#27169;&#26865;&#20004;&#21487;&#30340;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#22240;&#20026;&#27604;&#23572;&#21917;&#37202;&#65292;&#25152;&#20197;&#37202;&#20174;&#26410;&#30041;&#22312;&#25151;&#23376;&#37324;&#8221;&#65289;&#22312;&#21069;&#38754;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20250;&#24471;&#21040;&#20419;&#36827;&#12290;&#21442;&#19982;&#32773;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;ChatGPT&#65292;&#37117;&#34987;&#21576;&#29616;&#25104;&#23545;&#30340;&#21477;&#23376;&#12290;&#31532;&#20108;&#20010;&#21477;&#23376;&#24635;&#26159;&#19968;&#20010;&#26088;&#22312;&#22266;&#26377;&#22320;&#27169;&#26865;&#20004;&#21487;&#30340;&#33457;&#22253;&#36335;&#24452;&#21477;&#65292;&#32780;&#31532;&#19968;&#20010;&#21477;&#23376;&#21017;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#24739;&#26377;&#24930;&#24615;&#37202;&#31934;&#20013;&#27602;&#8221;&#65289;&#25110;&#19981;&#21512;&#36866;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#21916;&#27426;&#25171;&#39640;&#23572;&#22827;&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
&lt;/p&gt;</description></item><item><title>PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11534</link><description>&lt;p&gt;
PreAct: &#22312; ReAct &#20013;&#39044;&#27979;&#26410;&#26469;&#22686;&#24378;&#26234;&#33021;&#20307;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11534
&lt;/p&gt;
&lt;p&gt;
PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#39044;&#27979;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#24120;&#24120;&#26377;&#21161;&#20110;&#20010;&#20307;&#25299;&#23637;&#24605;&#32500;&#36807;&#31243;&#24182;&#36827;&#34892;&#21453;&#24605;&#65292;&#20174;&#32780;&#20419;&#36827;&#26397;&#27491;&#30830;&#26041;&#21521;&#25512;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; PreAct &#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#39044;&#27979;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#23548;&#21521;&#30340;&#25512;&#29702;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PreAct &#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#20248;&#20110; ReAct &#26041;&#27861;&#65292;&#24182;&#19988;&#24403;&#19982;&#21453;&#24605;&#26041;&#27861;&#32467;&#21512;&#26102;&#65292;PreAct &#30340;&#25928;&#26524;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#21382;&#21490;&#39044;&#27979;&#23545;LLM&#35268;&#21010;&#26377;&#25345;&#32493;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.00326</link><description>&lt;p&gt;
Agent-OM&#65306;&#21033;&#29992;LLM&#20195;&#29702;&#36827;&#34892;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Agent-OM: Leveraging LLM Agents for Ontology Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Agent-OM&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#20026;&#26412;&#20307;&#21305;&#37197;&#31995;&#32479;&#24341;&#20837;&#20102;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20114;&#25805;&#20316;&#24615;&#65292;&#36890;&#36807;&#23545;&#40784;&#30456;&#20851;&#23454;&#20307;&#26469;&#35299;&#20915;&#20854;&#27010;&#24565;&#24322;&#26500;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;LLM&#35774;&#35745;&#33539;&#24335;&#65292;&#21629;&#21517;&#20026;Agent-OM&#65292;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#26816;&#32034;&#21644;&#21305;&#37197;&#30340;&#21516;&#20307;&#20195;&#29702;&#20197;&#21450;&#19968;&#32452;&#22522;&#20110;&#25552;&#31034;&#30340;&#31616;&#21333;OM&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
&lt;/p&gt;</description></item><item><title>ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;</title><link>http://arxiv.org/abs/2401.00741</link><description>&lt;p&gt;
ToolEyes&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios. (arXiv:2401.00741v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00741
&lt;/p&gt;
&lt;p&gt;
ToolEyes&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#30340;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#30340;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25317;&#26377;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#20316;&#20026;&#20013;&#20171;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#20110;&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36873;&#25321;&#30340;&#24037;&#20855;&#19982;&#26399;&#26395;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#26223;&#65292;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#31572;&#26696;&#21487;&#20197;&#20107;&#20808;&#30830;&#23450;&#65292;&#19982;&#30495;&#23454;&#38656;&#27714;&#32972;&#36947;&#32780;&#39536;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#32467;&#26524;&#24573;&#35270;&#20102;LLMs&#26377;&#25928;&#21033;&#29992;&#24037;&#20855;&#25152;&#38656;&#30340;&#22797;&#26434;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolEyes&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;LLMs&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#35814;&#32454;&#20998;&#26512;&#20102;&#19971;&#20010;&#30495;&#23454;&#24773;&#26223;&#65292;&#20998;&#26512;&#20102;&#23545;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20116;&#20010;&#32500;&#24230;&#65306;&#26684;&#24335;&#23545;&#40784;&#65292;&#24847;&#22270;&#29702;&#35299;&#65292;&#34892;&#20026;&#35268;&#21010;&#65292;&#24037;&#20855;&#36873;&#25321;&#21644;&#31572;&#26696;&#32452;&#32455;&#12290;&#27492;&#22806;&#65292;ToolEyes&#36824;&#21253;&#21547;&#19968;&#20010;&#25317;&#26377;&#32422;600&#31181;&#24037;&#20855;&#30340;&#24037;&#20855;&#24211;&#65292;&#20316;&#20026;LLMs&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#22312;&#28041;&#21450;&#19977;&#20010;&#31867;&#21035;&#30340;&#21313;&#20010;LLMs&#30340;&#35780;&#20272;&#20013;&#65292;ToolEyes&#21462;&#24471;&#20102;&#22914;&#19979;&#30340;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined, diverging from genuine needs. Furthermore, a sole emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories revea
&lt;/p&gt;</description></item></channel></rss>