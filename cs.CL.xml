<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02882</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
Linear Attention Sequence Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24182;&#34892;&#65288;SP&#65289;&#20316;&#20026;&#19968;&#31181;&#22788;&#29702;&#36229;&#20986;&#21333;&#20010;GPU&#20869;&#23384;&#38480;&#21046;&#30340;&#38271;&#24207;&#21015;&#30340;&#27969;&#34892;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SP&#26041;&#27861;&#24182;&#26410;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24615;&#65292;&#23548;&#33268;&#22312;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#34892;&#25928;&#29575;&#21644;&#21487;&#29992;&#24615;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#39640;&#25928;SP&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21491;&#20056;&#20869;&#26680;&#25216;&#24039;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;SP&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#21644;&#20013;&#38388;&#29366;&#24577;&#32531;&#23384;&#26469;&#22686;&#24378;LASP&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#20351;LASP&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#30828;&#20214;&#21451;&#22909;&#24615;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31934;&#24515;&#30830;&#20445;&#24207;&#21015;&#32423;LASP&#19982;&#25152;&#26377;&#31867;&#22411;&#30340;&#25209;&#32423;&#25968;&#25454;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11083</link><description>&lt;p&gt;
&#20026;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24037;&#19994;&#22330;&#26223;&#20013;&#21313;&#20998;&#37325;&#35201;&#65292;&#21253;&#25324;&#29983;&#20135;&#32447;&#19978;&#24322;&#24120;&#27169;&#24335;&#30340;&#35782;&#21035;&#21644;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#30340;&#21046;&#36896;&#32570;&#38519;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25317;&#26377;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23450;&#21046;&#20026;&#24322;&#24120;&#26816;&#27979;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30340;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#26465;&#20214;&#24341;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#22810;&#27169;&#24577;&#25552;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#31867;&#21035;&#19978;&#19979;&#25991;&#12289;&#27491;&#24120;&#35268;&#21017;&#21644;&#21442;&#32771;&#22270;&#20687;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#34920;&#31034;&#32479;&#19968;&#20026;2D&#22270;&#20687;&#26684;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11083v1 Announce Type: cross  Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling m
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2311.07564</link><description>&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#28436;&#35762;&#25991;&#26412;&#20013;&#30340;&#21457;&#35328;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#39564;&#35777;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#20070;&#38754;&#26679;&#26412;&#26159;&#21542;&#21516;&#23646;&#19968;&#20316;&#32773;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#28041;&#21450;&#23545;&#20070;&#38754;&#25991;&#26412;&#30340;&#24402;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36716;&#24405;&#28436;&#35762;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#35768;&#22810;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#26631;&#28857;&#21644;&#22823;&#20889;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#24182;&#19981;&#20855;&#22791;&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36716;&#24405;&#30340;&#28436;&#35762;&#21576;&#29616;&#20854;&#20182;&#27169;&#24335;&#65292;&#22914;&#22635;&#20805;&#35789;&#21644;&#22238;&#24212;&#24615;&#22768;&#38899;&#65288;&#20363;&#22914;&#8220;&#21999;&#8221;&#65292;&#8220;&#21999;&#65292;&#21999;&#8221;&#65289;&#65292;&#36825;&#20123;&#21487;&#33021;&#26159;&#19981;&#21516;&#21457;&#35328;&#20154;&#30340;&#29305;&#24449;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;&#20026;&#20102;&#38480;&#21046;&#21457;&#35328;&#20154;&#19982;&#35805;&#39064;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#25105;&#20204;&#20351;&#29992;&#20250;&#35805;&#25552;&#31034;&#21644;&#21442;&#19982;&#21516;&#19968;&#23545;&#35805;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#19981;&#21516;&#38590;&#24230;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22312;&#36825;&#19968;&#26032;&#22522;&#20934;&#19978;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
&lt;/p&gt;</description></item></channel></rss>