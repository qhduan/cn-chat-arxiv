<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18540</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#21518;&#20445;&#25345;LLMs&#30340;&#23545;&#40784;&#24615;:&#25552;&#31034;&#27169;&#26495;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;LLMs&#65292;&#22914;Llama 2-Chat&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#30340;&#24040;&#22823;&#27963;&#21160;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#21382;&#20102;&#23545;&#40784;&#24615;&#35757;&#32451;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#26368;&#36817;&#65292;&#40784;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#25253;&#21578;&#31216;&#65292;&#21363;&#20351;&#26159;&#33391;&#24615;&#30340;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#65289;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20943;&#36731;&#36825;&#31181;&#23545;&#40784;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#32842;&#22825;&#27169;&#22411;&#65288;Meta&#30340;Llama 2-Chat&#65292;Mistral AI&#30340;Mistral 7B Instruct v0.2&#21644;OpenAI&#30340;GPT-3.5 Turbo&#65289;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#26412;&#25991;&#21457;&#29616;&#24494;&#35843;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#27169;&#26495;&#22312;&#20445;&#25345;&#23433;&#20840;&#23545;&#40784;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017; - &#22312;&#27979;&#35797;&#26102;&#19981;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21253;&#21547;&#23427;&#12290;&#23545;GSM8K&#65292;ChatDoctor&#21644;OpenOrca&#36827;&#34892;&#30340;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;PTST&#26174;&#30528;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#22686;&#21152;&#65292;&#29978;&#33267;&#20960;&#20046;&#28040;&#38500;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18540v1 Announce Type: cross  Abstract: Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost elimin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;</title><link>http://arxiv.org/abs/2304.11960</link><description>&lt;p&gt;
ThreatCrawl&#65306;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23433;&#20840;&#28966;&#28857;&#29228;&#34411;
&lt;/p&gt;
&lt;p&gt;
ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#20449;&#24687;&#23545;&#20110;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#26469;&#35828;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#38450;&#24050;&#32463;&#22312;&#20854;&#20182;&#31995;&#32479;&#19978;&#21457;&#29983;&#30340;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#34429;&#28982;&#26377;&#19981;&#21516;&#30340;&#26631;&#20934;&#26469;&#20132;&#27969;&#36825;&#20123;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#20449;&#24687;&#26159;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#22312;&#25991;&#31456;&#25110;&#21338;&#23458;&#24086;&#23376;&#20013;&#20849;&#20139;&#30340;&#12290;&#25163;&#21160;&#27983;&#35272;&#22810;&#20010;&#22312;&#32447;&#38376;&#25143;&#21644;&#26032;&#38395;&#39029;&#38754;&#20197;&#21457;&#29616;&#26032;&#23041;&#32961;&#24182;&#25552;&#21462;&#23427;&#20204;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#25195;&#25551;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#22810;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#23041;&#32961;&#25351;&#31034;&#22120;&#65288;IOCs&#65289;&#30340;&#25552;&#21462;&#22120;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#24050;&#32463;&#35299;&#20915;&#20102;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25628;&#32034;&#36825;&#20123;&#25991;&#26723;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#23427;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25628;&#32034;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#25991;&#26723;&#12290;ThreatCrawl&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#35782;&#21035;&#30456;&#20851;&#32593;&#31449;&#21644;&#32593;&#39029;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#26469;&#20248;&#20808;&#32771;&#34385;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item></channel></rss>