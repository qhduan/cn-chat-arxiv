<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.15751</link><description>&lt;p&gt;
&#31232;&#30095;MeZO&#65306;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#36890;&#24120;&#20250;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#32780;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#21033;&#29992;&#23384;&#20648;&#22120;&#30340;&#38646;&#38454;&#65288;MeZO&#65289;&#20248;&#21270;&#22120;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#20869;&#23384;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#24448;&#24448;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19982;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;MeZO&#20173;&#28982;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#21040;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;MeZO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#23558;ZO&#24212;&#29992;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#36873;&#25321;&#26041;&#26696;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
&lt;/p&gt;</description></item></channel></rss>