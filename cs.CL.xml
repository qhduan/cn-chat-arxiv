<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.04919</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#22359;&#21270;&#19982;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Chunking with Hierarchical RNN. (arXiv:2309.04919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#39044;&#27979;&#35821;&#35328;&#32467;&#26500;&#65292;&#22914;&#35299;&#26512;&#21644;&#21477;&#22359;&#21270;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#22359;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;&#38750;&#23618;&#27425;&#21270;&#26041;&#24335;&#23545;&#21333;&#35789;&#36827;&#34892;&#20998;&#32452;&#30340;&#21477;&#27861;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#20998;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;HRNN&#65289;&#26469;&#24314;&#27169;&#21333;&#35789;&#21040;&#21477;&#22359;&#21644;&#21477;&#22359;&#21040;&#21477;&#23376;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#65306;&#20351;&#29992;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;CoNLL-2000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23558;&#30701;&#35821;F1&#24471;&#20998;&#25552;&#39640;&#20102;6&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#36824;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21477;&#22359;&#32467;&#26500;&#22312;&#31070;&#32463;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#30701;&#26242;&#30340;&#12290;&#26412;&#30740;&#31350;&#23545;&#20110;&#25512;&#21160;&#26080;&#30417;&#30563;&#21477;&#22359;&#21270;&#30340;&#36827;&#23637;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This paper introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset reveal a notable improvement over existing unsupervised methods, enhancing phrase F1 score by up to 6 percentage points. Further, finetuning with downstream tasks results in an additional performance improvement. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model's downstream-task training. This study contributes to the advancement 
&lt;/p&gt;</description></item></channel></rss>