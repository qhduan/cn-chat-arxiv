<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16021</link><description>&lt;p&gt;
TMT: &#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#35270;&#20026;&#19981;&#21516;&#35821;&#35328;&#26469;&#23454;&#29616;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19977;&#27169;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16021
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20849;&#21516;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#27491;&#22312;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#37197;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27169;&#32763;&#35793;&#65288;TMT&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#24847;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#24182;&#23558;&#22810;&#27169;&#24577;&#32763;&#35793;&#35270;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#26631;&#35760;&#20026;&#31163;&#25955;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#36328;&#27169;&#24577;&#30340;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25552;&#20986;&#30340;TMT&#20013;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#36827;&#34892;&#26680;&#24515;&#32763;&#35793;&#65292;&#32780;&#27169;&#24577;&#29305;&#23450;&#22788;&#29702;&#20165;&#22312;&#26631;&#35760;&#21270;&#21644;&#21435;&#26631;&#35760;&#21270;&#38454;&#27573;&#20869;&#36827;&#34892;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#20845;&#31181;&#27169;&#24577;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;TMT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>CAT-LLM&#26159;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#25991;&#31456;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#39118;&#26684;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#24182;&#25903;&#25345;&#21160;&#24577;&#25193;&#23637;&#20869;&#37096;&#39118;&#26684;&#26641;&#65292;&#20351;&#24471;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.05707</link><description>&lt;p&gt;
CAT-LLM: &#20351;&#29992;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#20026;&#22522;&#30784;&#65292;&#20026;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#25552;&#20379;&#25351;&#23548;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer. (arXiv:2401.05707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05707
&lt;/p&gt;
&lt;p&gt;
CAT-LLM&#26159;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20013;&#25991;&#25991;&#31456;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#39118;&#26684;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#24182;&#25903;&#25345;&#21160;&#24577;&#25193;&#23637;&#20869;&#37096;&#39118;&#26684;&#26641;&#65292;&#20351;&#24471;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#22312;&#22312;&#32447;&#23089;&#20048;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#33521;&#25991;&#21477;&#23376;&#20869;&#30340;&#39118;&#26684;&#36716;&#25442;&#65292;&#32780;&#24573;&#30053;&#20102;&#38271;&#31687;&#20013;&#25991;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#38480;&#21046;&#20102;&#39118;&#26684;&#36716;&#25442;&#22312;&#25968;&#23383;&#23186;&#20307;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#36716;&#25442;&#26694;&#26550;&#65288;CAT-LLM&#65289;&#65292;&#21033;&#29992;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;CAT-LLM&#21253;&#25324;&#19968;&#20010;&#23450;&#21046;&#30340;&#12289;&#21487;&#26367;&#25442;&#30340;&#25991;&#26412;&#39118;&#26684;&#23450;&#20041;&#65288;TSD&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25991;&#31456;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#36716;&#25442;&#20013;&#25991;&#25991;&#31456;&#39118;&#26684;&#12290;TSD&#27169;&#22359;&#38598;&#25104;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#20998;&#26512;&#25991;&#31456;&#39118;&#26684;&#65292;&#20174;&#32780;&#24110;&#21161;LLM&#20840;&#38754;&#25226;&#25569;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#21407;&#22987;&#25991;&#26412;&#30340;&#23436;&#25972;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22359;&#25903;&#25345;&#20869;&#37096;&#39118;&#26684;&#26641;&#30340;&#21160;&#24577;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39118;&#26684;&#36716;&#25442;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text style transfer is increasingly prominent in online entertainment and social media. However, existing research mainly concentrates on style transfer within individual English sentences, while ignoring the complexity of long Chinese texts, which limits the wider applicability of style transfer in digital media realm. To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM), leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style. The TSD module integrates a series of machine learning algorithms to analyze article-style from both words and sentences levels, thereby aiding LLMs thoroughly grasp the target style without compromising the integrity of the original text. In addition, this module supports dynamic expansion of internal style trees, showcasing rob
&lt;/p&gt;</description></item></channel></rss>