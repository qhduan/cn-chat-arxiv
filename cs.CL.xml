<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.14773</link><description>&lt;p&gt;
StreamingT2V: &#19968;&#31181;&#19968;&#33268;&#12289;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#38271;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14773
&lt;/p&gt;
&lt;p&gt;
StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#20351;&#24471;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20010;&#24615;&#21270;&#20869;&#23481;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#35270;&#39057;&#65288;&#36890;&#24120;&#20026;16&#25110;24&#24103;&#65289;&#65292;&#24403;&#22825;&#30495;&#22320;&#25193;&#23637;&#21040;&#38271;&#35270;&#39057;&#21512;&#25104;&#30340;&#24773;&#20917;&#26102;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#30828;&#35009;&#21098;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StreamingT2V&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;80&#12289;240&#12289;600&#12289;1200&#25110;&#26356;&#22810;&#24103;&#30340;&#38271;&#35270;&#39057;&#65292;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CAM&#65289;&#30340;&#30701;&#26399;&#35760;&#24518;&#22359;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23558;&#24403;&#21069;&#29983;&#25104;&#26465;&#20214;&#35774;&#32622;&#20026;&#20808;&#21069;&#22359;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#22359;&#36807;&#28193;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21517;&#20026;&#22806;&#35266;&#20445;&#23384;&#27169;&#22359;&#30340;&#38271;&#26399;&#35760;&#24518;&#22359;&#65292;&#20174;&#31532;&#19968;&#20010;&#35270;&#39057;&#22359;&#20013;&#25552;&#21462;&#39640;&#32423;&#22330;&#26223;&#21644;&#23545;&#35937;&#29305;&#24449;&#65292;&#20197;&#38450;&#27490;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36941;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#34920;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;</title><link>http://arxiv.org/abs/2209.13636</link><description>&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Local Grammar-Based Coding Revisited. (arXiv:2209.13636v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36941;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#34920;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#22120;&#36880;&#20010;&#31526;&#21495;&#22320;&#23545;&#35821;&#27861;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#26368;&#23567;&#35821;&#27861;&#21464;&#25442;&#36890;&#36807;&#23616;&#37096;&#35821;&#27861;&#32534;&#30721;&#30340;&#38271;&#24230;&#22312;&#39044;&#35774;&#30340;&#35821;&#27861;&#31867;&#21035;&#20013;&#26368;&#23567;&#21270;&#35821;&#27861;&#38271;&#24230;&#12290;&#24050;&#30693;&#65292;&#36825;&#26679;&#30340;&#26368;&#23567;&#32534;&#30721;&#23545;&#20110;&#20005;&#26684;&#27491;&#29109;&#29575;&#30340;&#24773;&#20917;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#65292;&#32780;&#26368;&#23567;&#35821;&#27861;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#26500;&#25104;&#20102;&#28304;&#30340;&#20114;&#20449;&#24687;&#30340;&#19978;&#30028;&#12290;&#23613;&#31649;&#23436;&#20840;&#26368;&#23567;&#32534;&#30721;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#21463;&#38480;&#30340;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36866;&#30340;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#24378;&#22823;&#26222;&#36941;&#24615;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#19981;&#21463;&#29109;&#29575;&#30340;&#38480;&#21046;&#12290;&#35813;&#35777;&#26126;&#22522;&#20110;&#23545;&#25490;&#21517;&#27010;&#29575;&#30340;&#31616;&#21333;&#30340;Zipfian&#30028;&#38480;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of minimal local grammar-based coding. In this setting, the local grammar encoder encodes grammars symbol by symbol, whereas the minimal grammar transform minimizes the grammar length in a preset class of grammars as given by the length of local grammar encoding. It has been known that such minimal codes are strongly universal for a strictly positive entropy rate, whereas the number of rules in the minimal grammar constitutes an upper bound for the mutual information of the source. Whereas the fully minimal code is likely intractable, the constrained minimal block code can be efficiently computed. In this article, we present a new, simpler, and more general proof of strong universality of the minimal block code, regardless of the entropy rate. The proof is based on a simple Zipfian bound for ranked probabilities. By the way, we also show empirically that the number of rules in the minimal block code cannot clearly discriminate between long-memory and memoryless s
&lt;/p&gt;</description></item></channel></rss>