<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08382</link><description>&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Punctuation Restoration Improves Structure Understanding without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08382
&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#21435;&#22122;&#31561;&#65292;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#20174;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21040;&#20250;&#35805;&#20219;&#21153;&#30340;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#25991;&#26412;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#35821;&#35328;&#24615;&#33021;&#21644;&#26426;&#22120;&#33021;&#21147;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#24402;&#22240;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26410;&#33021;&#20805;&#20998;&#20256;&#36882;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;&#32473;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#23545;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#29616;&#30340;&#25913;&#21892;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#12289;&#20998;&#22359;&#21644;&#35789;&#24615;&#26631;&#27880;&#12290;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#20135;&#29983;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more rob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.02868</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#29609;&#25991;&#23383;&#28216;&#25103;&#65311;&#29616;&#29366;&#21644;&#26410;&#26469;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. (arXiv:2304.02868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#26234;&#33021;&#65292;&#26377;&#24453;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#29992;&#25143;&#36890;&#20449;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#26088;&#22312;&#35843;&#26597;&#23427;&#20204;&#22312;&#29609;&#25991;&#23383;&#28216;&#25103;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#29609;&#23478;&#36890;&#36807;&#19982;&#28216;&#25103;&#19990;&#30028;&#30340;&#23545;&#35805;&#26469;&#29702;&#35299;&#29615;&#22659;&#24182;&#23545;&#24773;&#20917;&#20570;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;ChatGPT&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;&#20173;&#28982;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;ChatGPT&#26080;&#27861;&#36890;&#36807;&#29609;&#28216;&#25103;&#25110;&#38405;&#35835;&#28216;&#25103;&#25163;&#20876;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#65307;&#23427;&#21487;&#33021;&#26080;&#27861;&#21033;&#29992;&#23427;&#24050;&#32463;&#25317;&#26377;&#30340;&#19990;&#30028;&#30693;&#35782;&#65307;&#23427;&#26080;&#27861;&#25512;&#26029;&#20986;&#38543;&#30528;&#28216;&#25103;&#36827;&#23637;&#30340;&#27599;&#19968;&#27493;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20132;&#21449;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.
&lt;/p&gt;</description></item></channel></rss>