<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08454</link><description>&lt;p&gt;
&#28151;&#21512;&#32534;&#30721;&#22120;&#25903;&#25345;&#36830;&#32493;&#35821;&#38899;&#20998;&#31163;&#29992;&#20110;&#20250;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23558;&#35821;&#38899;&#20998;&#31163;&#25104;&#26080;&#37325;&#21472;&#30340;&#27969;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;ASR&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22312;ASR&#27169;&#22411;&#20013;&#21253;&#21547;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#28151;&#21512;&#32534;&#30721;&#22120;&#21033;&#29992;&#21407;&#22987;&#37325;&#21472;&#30340;&#35821;&#38899;&#26469;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#24341;&#20837;&#30340;&#20266;&#24433;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31163;&#22120;&#65288;&#21253;&#25324;&#24378;&#22823;&#30340;TF-GridNet&#27169;&#22411;&#65289;&#35780;&#20272;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36824;&#23637;&#31034;&#20102;TF-GridNet&#30340;&#24378;&#22823;&#20998;&#31163;&#33021;&#21147;&#65292;&#22823;&#22823;&#32553;&#23567;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A commonmethod involves first separating the speech into overlap-free streams and then performing ASR on the resulting signals. Recently, the inclusion of a mixture encoder in the ASR model has been proposed. This mixture encoder leverages the original overlapped speech to mitigate the effect of artifacts introduced by the speech separation. Previously, however, the method only addressed two-speaker scenarios. In this work, we extend this approach to more natural meeting contexts featuring an arbitrary number of speakers and dynamic overlaps. We evaluate the performance using different speech separators, including the powerful TF-GridNet model. Our experiments show state-of-the-art performance on the LibriCSS dataset and highlight the advantages of the mixture encoder. Furthermore, they demonstrate the strong separation of TF-GridNet which largely closes the gap between previous m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#65292;&#21487;&#20197;&#23558;&#19990;&#30028;&#27169;&#22411;&#20998;&#37197;&#32473;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;NLP&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.04347</link><description>&lt;p&gt;
&#25968;&#23398;&#35299;&#39064;&#30340;&#19990;&#30028;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
World Models for Math Story Problems. (arXiv:2306.04347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#65292;&#21487;&#20197;&#23558;&#19990;&#30028;&#27169;&#22411;&#20998;&#37197;&#32473;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;NLP&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23398;&#29983;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32780;&#35328;&#65292;&#35299;&#20915;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20182;&#20204;&#29702;&#35299;&#25925;&#20107;&#20013;&#25152;&#25551;&#36848;&#30340;&#19990;&#30028;&#24182;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#35745;&#31639;&#20986;&#31572;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21019;&#26032;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#33258;&#21160;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25968;&#23398;&#27010;&#24565;&#30340;&#20934;&#30830;&#34920;&#31034;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#23548;&#33268;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25972;&#21512;&#21040;&#20998;&#31867;&#21644;&#34920;&#36798;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#19978;&#65292;&#24182;&#24320;&#21457;&#20986;&#38024;&#23545;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#39046;&#22495;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#12290;&#21033;&#29992;MathWorld&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#20998;&#37197;&#19990;&#30028;&#27169;&#22411;&#65292;&#23427;&#20204;&#34920;&#31034;&#22312;&#25991;&#26412;&#20013;&#20171;&#32461;&#30340;&#24773;&#20917;&#21644;&#34892;&#21160;&#20197;&#21450;&#23427;&#20204;&#30340;&#25968;&#23398;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#20960;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;&#26032;&#25968;&#25454;&#38598;Story-Gen Math&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MathWorld&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several exis
&lt;/p&gt;</description></item></channel></rss>