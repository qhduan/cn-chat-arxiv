<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.05518</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#22686;&#24378;&#19968;&#33268;&#24615;&#35757;&#32451;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05518
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CoT&#65289;&#26377;&#28508;&#21147;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#31995;&#32479;&#24615;&#22320;&#27498;&#26354;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#22240;&#32032;--&#27604;&#22914;&#65292;&#21512;&#29702;&#21270;&#31572;&#26696;&#20197;&#31526;&#21512;&#29992;&#25143;&#24847;&#35265;&#32780;&#19981;&#25552;&#21450;&#27492;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26041;&#26696;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#21333;&#20803;&#65292;&#38024;&#23545;&#19971;&#20010;&#38382;&#31572;&#20219;&#21153;&#27979;&#35797;&#20102;&#20061;&#31181;&#24418;&#24335;&#30340;&#26377;&#20559;&#25512;&#29702;&#65292;&#21457;&#29616;&#23558;BCT&#24212;&#29992;&#20110;&#24102;&#26377;&#19968;&#31181;&#20559;&#35265;&#30340;GPT-3.5-Turbo&#21487;&#20197;&#23558;&#26377;&#20559;&#25512;&#29702;&#30340;&#27604;&#20363;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#38477;&#20302;86%&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#20559;&#35265;&#65292;&#24179;&#22343;&#23558;&#26410;&#30693;&#20559;&#35265;&#19978;&#30340;&#26377;&#20559;&#25512;&#29702;&#20943;&#23569;&#20102;37%&#12290;&#30001;&#20110;BCT&#23558;&#26410;&#30693;&#20559;&#35265;&#27867;&#21270;&#24182;&#19988;&#19981;&#38656;&#35201;&#37329;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05518v1 Announce Type: cross  Abstract: While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may h
&lt;/p&gt;</description></item></channel></rss>