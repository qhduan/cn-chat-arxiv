<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.01528</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01528
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;BL&#65289;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#28304;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20016;&#23500;&#22810;&#38754;&#25551;&#36848;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#35745;&#31639;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#36798;&#30340;&#24494;&#22937;&#21465;&#36848;&#19982;&#36890;&#36807;&#21508;&#31181;&#20998;&#23376;&#24314;&#27169;&#25216;&#26415;&#25551;&#36848;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#32454;&#33410;&#34701;&#21512;&#65292;&#25171;&#24320;&#20102;&#20840;&#38754;&#34920;&#24449;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#30340;&#26032;&#36884;&#24452;&#12290;&#36890;&#36807;&#23558;&#22260;&#32469;&#29983;&#29289;&#20998;&#23376;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#25968;&#25454;&#32435;&#20837;&#24314;&#27169;&#20013;&#65292;BL&#26088;&#22312;&#25429;&#25417;&#21253;&#21547;&#35821;&#35328;&#20256;&#36798;&#30340;&#31526;&#21495;&#29305;&#24615;&#20197;&#21450;&#25968;&#37327;&#21270;&#32467;&#26500;&#29305;&#24449;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11414</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#32454;&#31890;&#24230;&#21487;&#35299;&#37322;&#20107;&#23454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11414
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25688;&#35201;&#26088;&#22312;&#29983;&#25104;&#22522;&#20110;&#36755;&#20837;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20107;&#23454;&#24615;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65288;FALLACIOUS&#65289;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21363;&#22522;&#20110;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#21644;&#26080;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#19981;&#38656;&#35201;&#22522;&#20934;&#30495;&#20540;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#36890;&#36807;GitHub&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11414v1 Announce Type: new  Abstract: Multimodal summarization aims to generate a concise summary based on the input text and image. However, the existing methods potentially suffer from unfactual output. To evaluate the factuality of multimodal summarization models, we propose two fine-grained and explainable evaluation frameworks (FALLACIOUS) for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn't need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method. We will release our code and dataset via github.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07787</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#35780;&#20272;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#20197;&#29702;&#35299;&#24773;&#24863;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#21152;&#24378;ABSA&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20381;&#36182;&#21644;&#32452;&#25104;&#26641;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#12290;&#38543;&#30528;ABSA&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21019;&#26032;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#34987;&#34701;&#20837;&#20854;&#20013;&#65288;&#20363;&#22914;&#28508;&#22312;&#22270;&#65289;&#65292;&#20294;&#36825;&#20063;&#24341;&#20837;&#20102;&#22797;&#26434;&#24615;&#21644;&#28151;&#28102;&#12290;&#30446;&#21069;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#38598;&#25104;&#21040;ABSA&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#65288;EMGF&#65289;&#32593;&#32476;&#65292;&#23427;&#25972;&#21512;&#20102;&#26469;&#33258;&#21477;&#27861;&#20381;&#36182;&#21644;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;EMGF&#37197;&#22791;&#20102;&#22810;&#38170;&#28857;&#19977;&#20803;&#23398;&#20064;&#21644;&#27491;&#20132;&#25237;&#24433;&#65292;&#39640;&#25928;&#22320;&#21033;&#29992;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#32508;&#21512;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2205.12186</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;: &#23545;&#25239;&#36127;&#34920;&#31034;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#25968;&#25454;&#26102;&#65292;&#26087;&#20219;&#21153;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#33021;&#20250;&#28418;&#31227;&#12290;&#19968;&#20123;&#36127;&#38754;&#30340;&#34920;&#31034;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#20026;&#20250;&#23548;&#33268;&#20174;&#26412;&#22320;&#23398;&#20064;&#30340;&#31867;&#21035;&#21407;&#22411;&#21644;&#25968;&#25454;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21407;&#22411;&#25351;&#23548;&#23398;&#20064;&#65292;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;NLP&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20197;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#37051;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36739;&#23569;&#34920;&#31034;&#28418;&#31227;&#30340;&#30456;&#24403;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#37325;&#26032;&#37319;&#26679;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item></channel></rss>