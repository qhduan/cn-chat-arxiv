<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.00932</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Knowledge Distillation via Synthetic Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#24046;&#20998;&#31169;&#23494;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38544;&#31169;&#30340;&#22686;&#21152;&#32039;&#36843;&#24615;&#35201;&#27714;LLMs&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;(DP)&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#38656;&#35201;&#21387;&#32553;LLMs&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#25110;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#30495;&#23454;&#37096;&#32626;&#12290;&#24046;&#20998;&#38544;&#31169;&#21644;&#27169;&#22411;&#21387;&#32553;&#36890;&#24120;&#24517;&#39035;&#22312;&#23454;&#29616;&#20854;&#30446;&#26631;&#30340;&#36807;&#31243;&#20013;&#26435;&#34913;&#25928;&#29992;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#32773;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#30340;&#25928;&#29992;&#25439;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#31169;&#23494;&#30693;&#35782;&#33976;&#39311;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#30001;&#24046;&#20998;&#31169;&#23494;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20197;&#20004;&#31181;&#26041;&#24335;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#19978;&#65306;&#19968;&#31181;&#26159;&#26469;&#33258;&#21512;&#25104;&#25968;&#25454;&#26412;&#36523;&#30340;&#30828;&#26631;&#31614;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Question Answering Over Spatio-Temporal Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11542
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#30693;&#35782;&#22270;&#65288;STKG&#65289;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#21644;&#20301;&#32622;&#20449;&#24687;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#30740;&#31350;&#30028;&#20851;&#27880;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA&#65289;&#65292;&#20294;&#22522;&#20110;STKG&#30340;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#20173;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20063;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STQAD&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;10,000&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#38754;&#21521;&#26102;&#31354;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;STKGQA&#65289;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;KGQA&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36828;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STCQA&#65292;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;KGQA&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;STComplEx&#30340;&#26032;&#22411;STKG&#23884;&#20837;&#26041;&#27861;&#12290;&#36890;&#36807;&#20174;&#38382;&#39064;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#38382;&#31572;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
&lt;/p&gt;</description></item></channel></rss>