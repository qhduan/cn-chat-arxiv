<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12766</link><description>&lt;p&gt;
NovelQA&#65306;&#29992;&#20110;&#38271;&#36317;&#31163;&#23567;&#35828;&#38382;&#31572;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NovelQA: A Benchmark for Long-Range Novel Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12766
&lt;/p&gt;
&lt;p&gt;
NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#20449;&#24687;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NovelQA&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#20855;&#26377;&#25193;&#23637;&#25991;&#26412;&#30340;LLM&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;NovelQA&#30001;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;LLM&#20013;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#29702;&#24819;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NovelQA&#30340;&#35774;&#35745;&#19982;&#26500;&#24314;&#65292;&#31361;&#20986;&#20102;&#20854;&#25163;&#21160;&#27880;&#37322;&#21644;&#22810;&#26679;&#30340;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#22312;NovelQA&#19978;&#23545;&#38271;&#25991;&#26412;LLM&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#22810;&#36339;&#25512;&#29702;&#12289;&#32454;&#33410;&#23548;&#21521;&#31561;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item></channel></rss>