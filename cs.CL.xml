<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13343</link><description>&lt;p&gt;
&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System. (arXiv:2304.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13343
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25511;&#20869;&#23384;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21046;&#20110;&#26080;&#27861;&#22788;&#29702;&#36807;&#38271;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25511;&#20869;&#23384;&#65288;SCM&#65289;&#31995;&#32479;&#65292;&#20197;&#37322;&#25918;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#38480;&#36755;&#20837;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65306;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12289;&#20869;&#23384;&#27969;&#21644;&#20869;&#23384;&#25511;&#21046;&#22120;&#12290;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36845;&#20195;&#22320;&#22788;&#29702;&#36229;&#38271;&#36755;&#20837;&#65292;&#24182;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#23384;&#20648;&#22312;&#20869;&#23384;&#27969;&#20013;&#12290;&#20869;&#23384;&#25511;&#21046;&#22120;&#20026;&#20195;&#29702;&#25552;&#20379;&#38271;&#26399;&#23384;&#20648;&#22120;&#65288;&#24402;&#26723;&#23384;&#20648;&#22120;&#65289;&#21644;&#30701;&#26399;&#23384;&#20648;&#22120;&#65288;&#38378;&#23384;&#65289;&#65292;&#20197;&#29983;&#25104;&#31934;&#30830;&#36830;&#36143;&#30340;&#21709;&#24212;&#12290;&#25511;&#21046;&#22120;&#30830;&#23450;&#24212;&#28608;&#27963;&#21738;&#20123;&#26469;&#33258;&#24402;&#26723;&#23384;&#20648;&#22120;&#30340;&#35760;&#24518;&#65292;&#24182;&#22914;&#20309;&#23558;&#23427;&#20204;&#21512;&#24182;&#21040;&#27169;&#22411;&#36755;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#21487;&#20197;&#19982;&#20219;&#20309;LLMs&#38598;&#25104;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#36229;&#38271;&#25991;&#26412;&#32780;&#26080;&#38656;&#20462;&#25913;&#25110;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SCM&#31995;&#32479;&#20351;&#24471;LLMs&#33021;&#22815;&#22788;&#29702;&#38271;&#24230;&#39640;&#36798;8192&#20010;&#20196;&#29260;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Language Models (LLMs) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory (SCM) system to unleash infinite-length input capacity for large-scale language models. Our SCM system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our SCM system can be integrated with any LLMs to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our SCM system enables LLMs, which are not
&lt;/p&gt;</description></item></channel></rss>