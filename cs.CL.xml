<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2401.17256</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Weak-to-Strong Jailbreaking on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17256
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#36827;&#34892;&#30772;&#35299;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20294;&#32418;&#38431;&#27979;&#35797;&#25253;&#21578;&#34920;&#26126;&#65292;&#36825;&#20123;&#32463;&#36807;&#31934;&#24515;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25552;&#31034;&#12289;&#35843;&#20248;&#25110;&#35299;&#30721;&#36827;&#34892;&#30772;&#35299;&#12290;&#22312;&#35843;&#26597;&#23545;&#40784;LLMs&#30340;&#30772;&#35299;&#28431;&#27934;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30772;&#35299;&#21644;&#23545;&#40784;&#27169;&#22411;&#30340;&#35299;&#30721;&#20998;&#24067;&#20165;&#22312;&#21021;&#22987;&#29983;&#25104;&#20013;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24369;&#21040;&#24378;&#30772;&#35299;&#25915;&#20987;&#65292;&#25932;&#23545;&#26041;&#21487;&#20197;&#21033;&#29992;&#36739;&#23567;&#30340;&#19981;&#23433;&#20840;/&#23545;&#40784;LLMs&#65288;&#20363;&#22914;7B&#65289;&#25351;&#23548;&#23545;&#26174;&#33879;&#36739;&#22823;&#30340;&#23545;&#40784;LLMs&#65288;&#20363;&#22914;70B&#65289;&#36827;&#34892;&#30772;&#35299;&#12290;&#35201;&#36827;&#34892;&#30772;&#35299;&#65292;&#21482;&#38656;&#39069;&#22806;&#35299;&#30721;&#20004;&#20010;&#36739;&#23567;&#30340;LLMs&#19968;&#27425;&#65292;&#19982;&#35299;&#30721;&#36739;&#22823;&#30340;LLMs&#30456;&#27604;&#65292;&#20854;&#35745;&#31639;&#21644;&#24310;&#36831;&#25104;&#26412;&#36739;&#23567;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#32452;&#32455;&#30340;&#20116;&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#20197;&#21069;&#26410;&#27880;&#24847;&#21040;&#20294;&#39640;&#25928;&#30340;&#30772;&#35299;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, expo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01405</link><description>&lt;p&gt;
&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#19982;&#20854;&#20182;&#24635;&#32479;&#22312;&#28436;&#35762;&#20013;&#26159;&#21542;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#39118;&#26684;&#65311;&#22914;&#26524;&#26159;&#65292;&#26377;&#21738;&#20123;&#26041;&#38754;&#30340;&#19981;&#21516;&#65311;&#36825;&#20123;&#24046;&#24322;&#26159;&#21542;&#23616;&#38480;&#20110;&#20219;&#20309;&#21333;&#19968;&#30340;&#27807;&#36890;&#23186;&#20171;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#35010;&#24615;&#28436;&#35762;&#35789;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#27604;&#36739;&#25919;&#27835;&#23545;&#25163;&#35789;&#27719;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#22810;&#31181;&#24635;&#32479;&#28436;&#35762;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#30456;&#24403;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#29305;&#26391;&#26222;&#30340;&#35762;&#35805;&#27169;&#24335;&#19982;&#36817;&#20195;&#21382;&#20219;&#20027;&#35201;&#24635;&#32479;&#20505;&#36873;&#20154;&#19981;&#21516;&#12290;&#19968;&#20123;&#26174;&#33879;&#30340;&#21457;&#29616;&#21253;&#25324;&#29305;&#26391;&#26222;&#20351;&#29992;&#29305;&#21035;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#38024;&#23545;&#20182;&#30340;&#25919;&#27835;&#23545;&#25163;&#65292;&#24182;&#19988;&#20182;&#37325;&#22797;&#24378;&#35843;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20182;&#30340;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#21152;&#29420;&#29305;&#65292;&#32780;&#20182;&#20204;&#30340;&#29420;&#29305;&#24615;&#20540;&#19982;&#27665;&#20027;&#20826;&#30456;&#23545;&#36739;&#25509;&#36817;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Donald Trump speak differently from other presidents? If so, in what ways? Are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for comparing the lexical features of political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Some notable findings include Trump's employment of particularly divisive and antagonistic language targeting of his political opponents and his patterns of repetition for emphasis. Furthermore, Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values are comparably closer to those of the Democrats. These differences hold across a variety of measurement 
&lt;/p&gt;</description></item><item><title>Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09954</link><description>&lt;p&gt;
Eva-KELLM&#65306;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09954
&lt;/p&gt;
&lt;p&gt;
Eva-KELLM&#26159;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#21644;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#26469;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#25910;&#38598;&#25104;&#26412;&#39640;&#12289;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#22256;&#38590;&#12289;&#35780;&#20272;&#35270;&#35282;&#21463;&#38480;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30528;&#20016;&#23500;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#25110;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#24182;&#35780;&#20272;&#20854;&#25928;&#26524;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#36825;&#19981;&#20165;&#22312;&#25910;&#38598;&#19978;&#20135;&#29983;&#39640;&#25104;&#26412;&#65292;&#32780;&#19988;&#22312;&#34920;&#36798;&#22797;&#26434;&#20107;&#23454;&#26102;&#20063;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#35780;&#20272;&#35270;&#35282;&#19978;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Eva-KELLM&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;LLM&#20351;&#29992;&#21407;&#22987;&#25991;&#26723;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#65292;&#19982;&#20351;&#29992;&#20107;&#23454;&#19977;&#20803;&#32452;&#30456;&#27604;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#12289;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#26356;&#26032;&#21518;&#30340;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) possess a wealth of knowledge encoded in their parameters. However, this knowledge may become outdated or unsuitable over time. As a result, there has been a growing interest in knowledge editing for LLMs and evaluating its effectiveness. Existing studies primarily focus on knowledge editing using factual triplets, which not only incur high costs for collection but also struggle to express complex facts. Furthermore, these studies are often limited in their evaluation perspectives. In this paper, we propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs. This benchmark includes an evaluation framework and a corresponding dataset. Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets. We then evaluate the updated LLM from multiple perspectives. In addition to assessing the effectiveness of knowledge editing and
&lt;/p&gt;</description></item></channel></rss>