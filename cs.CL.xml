<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01172</link><description>&lt;p&gt;
&#27969;&#24335;&#24207;&#21015;&#36716;&#23548;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Sequence Transduction through Dynamic Compression
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01172
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#21387;&#32553;&#21644;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#65292;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#65292;&#24182;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;STAR&#65288;&#24102;&#26377;&#38170;&#23450;&#34920;&#31034;&#30340;&#27969;&#24335;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#27969;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;&#12290;STAR&#21160;&#24577;&#22320;&#23545;&#36755;&#20837;&#27969;&#36827;&#34892;&#20998;&#27573;&#65292;&#21019;&#24314;&#21387;&#32553;&#30340;&#38170;&#23450;&#34920;&#31034;&#65292;&#23454;&#29616;&#36817;&#20046;&#26080;&#25439;&#30340;&#21387;&#32553;&#65288;12&#20493;&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;STAR&#22312;&#21516;&#26102;&#36827;&#34892;&#35821;&#38899;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#21644;&#24310;&#36831;-&#36136;&#37327;&#25240;&#34935;&#65292;&#20248;&#21270;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.15309</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlled Training Data Generation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#19987;&#38376;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#37027;&#20123;&#37319;&#29992;&#24320;&#29615;&#26041;&#27861;&#24182;&#39044;&#20808;&#23450;&#20041;&#25552;&#31034;&#35789;&#26469;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#38381;&#29615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#31532;&#19968;&#20010;&#26426;&#21046;&#20351;&#29992;&#26469;&#33258;&#32473;&#23450;&#30417;&#30563;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#30340;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#25552;&#31034;&#35789;&#23548;&#33268;&#20102;&#32463;&#36807;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30693;&#36947;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31532;&#20108;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#24341;&#23548;&#21040;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#31216;&#23558;&#36825;&#20004;&#20010;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#20026;&#24341;&#23548;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08946</link><description>&lt;p&gt;
&#21487;&#29992;&#30340;XAI&#65306;&#22312;LLM&#26102;&#20195;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;10&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08946
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25351;&#30340;&#26159;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27934;&#35265;&#65292;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36816;&#20316;&#26041;&#24335;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;XAI&#30340;&#37325;&#28857;&#27491;&#34987;&#25193;&#23637;&#21040;&#24120;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#32780;&#22791;&#21463;&#25209;&#35780;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#19968;&#25299;&#23637;&#38656;&#35201;&#23545;XAI&#26041;&#27861;&#35770;&#36827;&#34892;&#26174;&#33879;&#36716;&#21464;&#65292;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;XAI&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;LLMs&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#34892;&#19994;&#24212;&#29992;&#20013;&#65292;XAI&#30340;&#35282;&#33394;&#20174;&#20165;&#20165;&#25171;&#24320;&#8220;&#40657;&#21283;&#23376;&#8221;&#36716;&#21464;&#20026;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#20316;&#20026;XAI&#27934;&#35265;&#30340;&#34987;&#21160;&#25509;&#21463;&#32773;&#65292;LLMs&#30340;&#29420;&#29305;&#33021;&#21147;&#33021;&#22815;&#30456;&#20114;&#22686;&#24378;XAI&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;1&#65289;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.12692</link><description>&lt;p&gt;
FormulaQA&#65306;&#19968;&#20010;&#22522;&#20110;&#20844;&#24335;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12692
&lt;/p&gt;
&lt;p&gt;
FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20844;&#24335;&#26159;&#20154;&#31867;&#22312;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#24456;&#23569;&#26126;&#30830;&#25351;&#20986;&#25512;&#29702;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#20844;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;FormulaQA&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#23567;&#20174;7B&#21040;&#36229;&#36807;100B&#21442;&#25968;&#30340;LLMs&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#22806;&#37096;&#20844;&#24335;&#25968;&#25454;&#24211;&#26102;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#22823;&#23567;&#19981;&#36229;&#36807;2B&#30340;&#36739;&#23567;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#24378;&#35843;&#20102;&#24403;&#24212;&#29992;&#20110;&#25105;&#20204;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25913;&#36827;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27010;&#24565;&#35868;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#12290;&#36890;&#36807;&#24212;&#29992;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#35868;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2310.18290</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35868;&#39064;&#20197;&#36741;&#21161;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach to Automatically generating Riddles aiding Concept Attainment. (arXiv:2310.18290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27010;&#24565;&#35868;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#12290;&#36890;&#36807;&#24212;&#29992;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#35868;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#20445;&#25345;&#23398;&#20064;&#32773;&#30340;&#31215;&#26497;&#21442;&#19982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#20026;&#22686;&#24378;&#23398;&#20064;&#32773;&#30340;&#21442;&#19982;&#24230;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#31163;&#32447;&#29615;&#22659;&#20013;&#12290;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#23601;&#26159;&#19968;&#31181;&#25945;&#23398;&#31574;&#30053;&#65292;&#23427;&#30528;&#37325;&#20110;&#23398;&#20064;&#32773;&#23545;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#27010;&#24565;&#30340;&#23383;&#20856;&#23450;&#20041;&#12290;&#36890;&#36807;&#25628;&#32034;&#21644;&#21015;&#20030;&#29992;&#20110;&#21306;&#20998;&#21508;&#31181;&#27010;&#24565;&#30340;&#23454;&#20363;&#21644;&#38750;&#23454;&#20363;&#20043;&#38388;&#30340;&#23646;&#24615;&#65292;&#26469;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#23558;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#26500;&#24314;&#27010;&#24565;&#35868;&#39064;&#65292;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#23398;&#20064;&#36164;&#28304;&#20013;&#21019;&#24314;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#26681;&#25454;&#20854;&#23545;&#27010;&#24565;&#30340;&#21807;&#19968;&#24615;&#36827;&#34892;&#20998;&#31867;&#20026;&#8220;&#20027;&#39064;&#26631;&#35760;&#8221;&#21644;&#8220;&#20849;&#21516;&#8221;&#65292;&#28982;&#21518;&#26681;&#25454;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#30340;&#26684;&#24335;&#29983;&#25104;&#35868;&#39064;&#65292;&#24182;&#25429;&#33719;&#36825;&#20123;&#35868;&#39064;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#12290;&#20174;&#20154;&#31867;&#35780;&#20272;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12555</link><description>&lt;p&gt;
PlanFitting&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models. (arXiv:2309.12555v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12555
&lt;/p&gt;
&lt;p&gt;
PlanFitting&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#24110;&#21161;&#29992;&#25143;&#23450;&#21046;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#23427;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#23545;&#20110;&#30830;&#20445;&#36275;&#22815;&#30340;&#20307;&#32946;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20154;&#20204;&#30340;&#22797;&#26434;&#26085;&#31243;&#21644;&#32771;&#34385;&#22240;&#32032;&#20197;&#21450;&#35745;&#21010;&#30340;&#21019;&#24314;&#36890;&#24120;&#38656;&#35201;&#19982;&#19987;&#23478;&#30340;&#21453;&#22797;&#27807;&#36890;&#65292;&#36825;&#19968;&#36807;&#31243;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PlanFitting&#65292;&#23427;&#26159;&#19968;&#20010;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#36741;&#21161;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;PlanFitting&#20351;&#29992;&#25143;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21508;&#31181;&#32422;&#26463;&#21644;&#26597;&#35810;&#65292;&#20174;&#32780;&#20415;&#20110;&#21019;&#24314;&#21644;&#20248;&#21270;&#36866;&#21512;&#20854;&#29305;&#23450;&#24773;&#20917;&#30340;&#27599;&#21608;&#36816;&#21160;&#35745;&#21010;&#65292;&#24182;&#20445;&#25345;&#22522;&#26412;&#21407;&#21017;&#30340;&#25166;&#26681;&#12290;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#65288;N=18&#65289;&#20351;&#29992;PlanFitting&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19987;&#23478;&#35268;&#21010;&#32773;&#65288;N=3&#65289;&#23545;&#36825;&#20123;&#35745;&#21010;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;PlanFitting&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#12289;&#21487;&#25805;&#20316;&#21644;&#26377;&#25454;&#21487;&#20381;&#30340;&#36816;&#21160;&#35745;&#21010;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;AI&#21161;&#25163;&#22312;&#21019;&#24314;&#35745;&#21010;&#26041;&#38754;&#30340;&#26410;&#26469;&#35774;&#35745;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.00948</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20998;&#26512;LLM&#30340;&#29702;&#35770;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#24615;&#33021;&#19981;&#26029;&#25552;&#39640;&#65292;&#19988;&#39318;&#27425;&#23637;&#31034;&#20102;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#20998;&#26512;&#12290;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#24182;&#23545;&#35821;&#35328;&#23398;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#26368;&#36817;&#24050;&#32463;&#25552;&#39640;&#21040;&#20102;&#33021;&#22815;&#22312;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#26377;&#25928;&#30340;&#35821;&#35328;&#25968;&#25454;&#30340;&#24418;&#24335;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#20803;&#35821;&#35328;&#33021;&#21147;&#20998;&#26512;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;LLMs&#20027;&#35201;&#26159;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#30340;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;&#20998;&#26512;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#25913;&#36827;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#24182;&#23545;&#35821;&#35328;&#23398;&#20013;&#30340;&#29702;&#35770;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#24418;&#24335;&#35821;&#35328;&#23398;&#30340;&#19977;&#20010;&#23376;&#39046;&#22495;&#65306;&#21477;&#27861;&#12289;&#38899;&#38901;&#23398;&#21644;&#35821;&#20041;&#23398;&#65292;&#25506;&#31350;&#20102;GPT-4&#30340;&#20803;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20803;&#35821;&#35328;&#20998;&#26512;&#30340;&#30740;&#31350;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#23454;&#39564;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#33324;&#25351;&#23548;&#26041;&#38024;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#65292;&#24182;&#20026;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#36825;&#20010;&#30740;&#31350;&#36824;&#26377;&#21161;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#29702;&#35770;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
&lt;/p&gt;</description></item></channel></rss>