<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CherryQ&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20851;&#38190;&#21442;&#25968;&#30340;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#39640;&#25928;&#37327;&#21270;&#33267;&#20302;&#31934;&#24230;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02837</link><description>&lt;p&gt;
&#26368;&#21518;&#25910;&#23448;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#24322;&#36136;&#24615;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02837
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CherryQ&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#30041;&#20851;&#38190;&#21442;&#25968;&#30340;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#39640;&#25928;&#37327;&#21270;&#33267;&#20302;&#31934;&#24230;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21442;&#25968;&#24322;&#36136;&#24615;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23569;&#37327;&#8220;&#27185;&#26691;&#8221;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#32780;&#32477;&#22823;&#22810;&#25968;&#21442;&#25968;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#22312;&#19981;&#21516;&#27169;&#22411;&#31995;&#21015;&#12289;&#35268;&#27169;&#21644;&#31867;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CherryQ&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#32479;&#19968;&#20102;&#28151;&#21512;&#31934;&#24230;&#21442;&#25968;&#30340;&#20248;&#21270;&#12290;CherryQ&#33021;&#22815;&#35782;&#21035;&#24182;&#20445;&#30041;&#39640;&#31934;&#24230;&#19979;&#20851;&#38190;&#30340;&#27185;&#26691;&#21442;&#25968;&#65292;&#21516;&#26102;&#23558;&#20854;&#20313;&#21442;&#25968;&#31215;&#26497;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;CherryQ&#30340;&#26377;&#25928;&#24615;&#12290;CherryQ&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;3&#20301;&#37327;&#21270;Vicuna-1.5&#19982;&#23427;&#20204;&#30340;16&#20301;&#23545;&#24212;&#29289;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02837v1 Announce Type: new  Abstract: This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. Th
&lt;/p&gt;</description></item><item><title>&#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#20102;&#19968;&#20010;&#35206;&#30422;22&#31181;&#35821;&#35328;&#12289;&#21253;&#21547;251B&#26631;&#35760;&#21644;74.8M&#25351;&#23548;-&#21709;&#24212;&#23545;&#30340;&#36164;&#28304;&#22871;&#20214;&#65292;&#32467;&#21512;&#39640;&#24230;&#31579;&#36873;&#30340;&#25968;&#25454;&#12289;&#26377;&#20215;&#20540;&#30340;&#26410;&#39564;&#35777;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24178;&#20928;&#24320;&#28304;&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06350</link><description>&lt;p&gt;
IndicLLMSuite: &#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#34013;&#22270;
&lt;/p&gt;
&lt;p&gt;
IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06350
&lt;/p&gt;
&lt;p&gt;
&#20026;&#21360;&#24230;&#35821;&#35328;&#21019;&#24314;&#20102;&#19968;&#20010;&#35206;&#30422;22&#31181;&#35821;&#35328;&#12289;&#21253;&#21547;251B&#26631;&#35760;&#21644;74.8M&#25351;&#23548;-&#21709;&#24212;&#23545;&#30340;&#36164;&#28304;&#22871;&#20214;&#65292;&#32467;&#21512;&#39640;&#24230;&#31579;&#36873;&#30340;&#25968;&#25454;&#12289;&#26377;&#20215;&#20540;&#30340;&#26410;&#39564;&#35777;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#29992;&#20110;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24178;&#20928;&#24320;&#28304;&#27969;&#27700;&#32447;&#65292;&#20197;&#21450;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33521;&#25991;LLM&#65288;Large Language Models&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23450;&#21046;&#36164;&#28304;&#65292;&#26500;&#24314;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#27169;&#22411;&#30340;&#36827;&#23637;&#21463;&#38459;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19987;&#38376;&#20026;&#21457;&#23637;&#21360;&#24230;&#35821;&#35328;LLM&#32780;&#35774;&#35745;&#30340;&#22823;&#37327;&#36164;&#28304;&#22871;&#20214;&#26469;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#28085;&#30422;&#20102;22&#31181;&#35821;&#35328;&#65292;&#21253;&#21547;&#24635;&#20849;251B&#26631;&#35760;&#21644;7480&#19975;&#20010;&#25351;&#23548;-&#21709;&#24212;&#23545;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25163;&#21160;&#39564;&#35777;&#25968;&#25454;&#12289;&#23578;&#26410;&#39564;&#35777;&#20294;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24178;&#20928;&#30340;&#12289;&#24320;&#28304;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#26469;&#28304;&#31579;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#21253;&#25324;&#32593;&#31449;&#12289;PDF&#21644;&#35270;&#39057;&#65292;&#34701;&#20837;&#20102;&#29228;&#21462;&#12289;&#28165;&#29702;&#12289;&#26631;&#35760;&#21644;&#21435;&#37325;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#23545;&#20110;&#25351;&#23548;&#24494;&#35843;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#29616;&#26377;&#30340;&#21360;&#24230;&#25968;&#25454;&#38598;&#65292;&#23558;&#33521;&#25991;&#25968;&#25454;&#38598;&#32763;&#35793;/&#36716;&#20889;&#25104;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#21033;&#29992;&#20102;LLaMa2&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06350v1 Announce Type: new  Abstract: Despite the considerable advancements in English LLMs, the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12025</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#32763;&#35793;&#65306;&#23384;&#22312;&#21644;&#32570;&#22833;&#30340;&#20869;&#23481;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#26368;&#36817;&#21457;&#29983;&#20102;&#19968;&#22330;&#21464;&#38761;&#24615;&#30340;&#36716;&#21464;&#65292;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#24443;&#24213;&#25913;&#21464;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;NLP&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#31181;&#33539;&#24335;&#24050;&#32463;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#65292;&#21253;&#25324;&#35821;&#38899;&#65292;&#22312;&#37027;&#37324;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFMs&#65289;&#21644;LLMs&#32467;&#21512;&#25104;&#21333;&#19968;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#26412;&#25991;&#30528;&#37325;&#20110;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#12290;&#36890;&#36807;&#23457;&#26597;&#35813;&#20027;&#39064;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#20026;&#27490;&#25552;&#20986;&#30340;&#26550;&#26500;&#35299;&#20915;&#26041;&#26696;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;&#22522;&#20110;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#19981;&#20165;&#25972;&#29702;&#20102;&#25152;&#23398;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#36824;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#35774;&#32622;&#21644;&#35780;&#20272;&#26041;&#27861;&#22914;&#20309;&#38459;&#30861;&#23545;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12025v1 Announce Type: new  Abstract: The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block 
&lt;/p&gt;</description></item><item><title>Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11217</link><description>&lt;p&gt;
Asclepius&#65306;&#29992;&#20110;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39057;&#35889;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11217
&lt;/p&gt;
&lt;p&gt;
Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Med-MLLMs&#65289;&#30340;&#37325;&#22823;&#31361;&#30772;&#36890;&#36807;&#24378;&#22823;&#30340;&#20449;&#24687;&#32508;&#21512;&#21644;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#25913;&#36896;&#20102;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#35786;&#26029;&#26694;&#26550;&#30340;&#22797;&#26434;&#24615;&#28085;&#30422;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#65292;&#24182;&#28041;&#21450;&#22797;&#26434;&#30340;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19981;&#36866;&#21512;Med-MLLMs&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Med-MLLMs&#26159;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#27844;&#38706;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#29420;&#31435;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#29992;&#20110;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Asclepius&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;Med-MLLM&#22522;&#20934;&#65292;&#20005;&#26684;&#21644;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#23398;&#19987;&#19994;&#65288;&#24515;&#34880;&#31649;&#12289;&#32963;&#32928;&#31561;&#65289;&#21644;&#19981;&#21516;&#35786;&#26029;&#33021;&#21147;&#65288;&#30693;&#35273;&#12289;&#30142;&#30149;&#20998;&#26512;&#31561;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 Announce Type: new  Abstract: The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, e
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10527</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#37319;&#26679;&#23545;&#25239;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sampling of adversarial entities in biomedical question answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10527
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21442;&#25968;&#22495;&#30693;&#35782;&#30340;&#22686;&#21152;&#28145;&#24230;&#25512;&#21160;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37096;&#32626;&#12290;&#22312;&#39640;&#39118;&#38505;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#29702;&#35299;&#27169;&#22411;&#30340;&#28431;&#27934;&#23545;&#20110;&#37327;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#21644;&#35268;&#33539;&#20854;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21457;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20316;&#20026;&#23545;&#25239;&#31034;&#20363;&#30340;&#21629;&#21517;&#23454;&#20307;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21487;&#33021;&#30340;&#20266;&#35013;&#30340;&#30097;&#38382;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#24130;&#32553;&#25918;&#36317;&#31163;&#21152;&#26435;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#22238;&#31572;&#20013;&#20248;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#25506;&#32034;&#25915;&#20987;&#34920;&#38754;&#19978;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#36825;&#25581;&#31034;&#20102;&#20004;&#31181;&#22312;&#29305;&#24449;&#19978;&#26126;&#26174;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#23454;&#20307;&#30340;&#21046;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#26041;&#24335;&#22914;&#20309;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10527v1 Announce Type: new  Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08349</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65288;&#20063;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#65289;&#24050;&#25104;&#20026;&#24357;&#21512;&#29992;&#25143;&#33021;&#21147;&#19982;&#22522;&#20110;SQL&#30340;&#25968;&#25454;&#35775;&#38382;&#20043;&#38388;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35831;&#27714;&#36716;&#21270;&#20026;&#29305;&#23450;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;SQL&#35821;&#21477;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24120;&#24120;&#26159;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#39640;&#20998;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#26126;&#26174;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#22269;&#38469;&#39033;&#30446;&#20851;&#20110;&#25991;&#26412;&#21040;SQL&#30028;&#38754;&#30340;&#38598;&#20013;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#39318;&#27425;&#28145;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;FootballDB&#30340;&#30495;&#23454;&#37096;&#32626;&#65292;&#35813;&#31995;&#32479;&#22312;FIFA World Cup&#30340;&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#32479;&#19968;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Speech-Text Pretraining for Spoken Dialog Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#30340;&#24191;&#27867;&#35821;&#38899;&#25991;&#26412;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#21644;&#24191;&#20041;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#24182;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#30452;&#25509;&#29702;&#35299;&#21644;&#21512;&#25104;&#35821;&#38899;&#20855;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#24314;&#27169;&#30340;&#22522;&#20110;LLM&#30340;&#31574;&#30053;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35821;&#38899;&#25991;&#26412;LLM&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#27169;&#22411;&#65288;USDM&#65289;&#65292;&#20197;&#22312;&#19981;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25110;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19982;&#32473;&#23450;&#36755;&#20837;&#35821;&#38899;&#30456;&#20851;&#30340;&#36830;&#36143;&#21475;&#35821;&#22238;&#22797;&#21644;&#26377;&#26426;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#30340;&#35821;&#38899;&#25991;&#26412;&#25512;&#29702;&#26041;&#24335;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;LLM&#25152;&#23637;&#31034;&#30340;&#25512;&#29702;&#38142;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#29983;&#25104;&#33258;&#28982;&#27969;&#30021;&#30340;&#21475;&#35821;&#22238;&#22797;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#21644;&#32423;&#32852;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#35814;&#32454;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#35760;&#24518;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.03749</link><description>&lt;p&gt;
&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Memorization of Named Entities in Fine-tuned BERT Models. (arXiv:2212.03749v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#35760;&#24518;&#31243;&#24230;&#65292;&#24182;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#20854;&#20013;&#19968;&#20010;&#39118;&#38505;&#26159;&#20174;&#35757;&#32451;&#22312;&#20010;&#20154;&#21644;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21629;&#21517;&#23454;&#20307;&#35760;&#24518;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20316;&#20026;&#20195;&#34920;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#23454;&#39564;&#20013;&#37319;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#32454;&#35843;&#35774;&#32622;&#65292;&#21253;&#25324;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35774;&#32622;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#23450;&#20041;&#30340;&#39034;&#24207;&#25277;&#26679;&#31574;&#30053;&#21644;&#20004;&#31181;&#25552;&#31034;&#31574;&#30053;&#20174;&#32454;&#35843;BERT&#27169;&#22411;&#20013;&#21019;&#24314;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#26679;&#26412;&#20013;&#25628;&#32034;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#26597;&#30475;&#23427;&#20204;&#26159;&#21542;&#20063;&#23384;&#22312;&#20110;&#32454;&#35843;&#25968;&#25454;&#38598;&#20013;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#37038;&#20214;&#21644;&#21338;&#23458;&#39046;&#22495;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;DP&#30340;&#24212;&#29992;&#23545;&#27979;&#35797;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preserving deep learning is an emerging field in machine learning that aims to mitigate the privacy risks in the use of deep neural networks. One such risk is training data extraction from language models that have been trained on datasets, which contain personal and privacy sensitive information. In our study, we investigate the extent of named entity memorization in fine-tuned BERT models. We use single-label text classification as representative downstream task and employ three different fine-tuning setups in our experiments, including one with Differentially Privacy (DP). We create a large number of text samples from the fine-tuned BERT models utilizing a custom sequential sampling strategy with two prompting strategies. We search in these samples for named entities and check if they are also present in the fine-tuning datasets. We experiment with two benchmark datasets in the domains of emails and blogs. We show that the application of DP has a detrimental effect on the te
&lt;/p&gt;</description></item></channel></rss>