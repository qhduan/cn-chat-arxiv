<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.17887</link><description>&lt;p&gt;
&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23618;&#21098;&#26525;&#30340;&#19981;&#21512;&#29702;&#26080;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Ineffectiveness of the Deeper Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17887
&lt;/p&gt;
&lt;p&gt;
&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#23618;&#21098;&#26525;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31227;&#38500;&#22823;&#37096;&#20998;&#23618;&#65288;&#26368;&#39640;&#36798;&#19968;&#21322;&#65289;&#20043;&#21069;&#65292;&#19981;&#21516;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#21098;&#26525;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#23618;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#21098;&#26525;&#23618;&#22359;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#8220;&#20462;&#22797;&#8221;&#25439;&#23475;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23569;&#37327;&#24494;&#35843;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#37327;&#21270;&#21644;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;QLoRA&#65289;&#65292;&#36825;&#26679;&#25105;&#20204;&#30340;&#27599;&#20010;&#23454;&#39564;&#37117;&#21487;&#20197;&#22312;&#21333;&#20010;A100 GPU&#19978;&#25191;&#34892;&#12290;&#20174;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#34917;&#20805;&#20854;&#20182;PEFT&#31574;&#30053;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#24494;&#35843;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21478;&#19968;&#26041;&#38754;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;&#20174;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#30340;&#21098;&#26525;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.08743</link><description>&lt;p&gt;
&#23558;LLMs&#24341;&#23548;&#21040;&#26080;&#20559;&#21709;&#24212;&#65306;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#30340;&#21709;&#24212;&#12290;&#30001;&#20110;LLMs&#28041;&#21450;&#21040;&#37325;&#35201;&#30340;&#20915;&#31574;&#21046;&#23450;&#65288;&#20363;&#22914;&#25307;&#32856;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#65292;&#24320;&#21457;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#31038;&#20250;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19982;LLM&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;LLMs&#36755;&#20837;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;LLM&#25512;&#29702;&#30340;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#29702;&#35299;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#21435;&#20559;&#20542;LLM&#36755;&#20986;&#30340;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21435;&#20559;&#25351;&#31034;&#26041;&#27861;&#65292;&#22914;&#25233;&#21046;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#20363;&#23376;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#26080;&#20559;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21551;&#31034;&#20102;&#26032;&#30340;&#21435;&#20559;&#20542;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
&lt;/p&gt;</description></item><item><title>TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08694</link><description>&lt;p&gt;
TeaMs-RL&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25945;&#25480;LLMs&#26356;&#22909;&#22320;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08694
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#36890;&#24120;&#38754;&#20020;&#30528;&#22312;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#20013;&#23545;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#20005;&#37325;&#20381;&#36182;&#25110;&#19982;&#33258;&#25105;&#25351;&#23548;&#33539;&#24335;&#30456;&#20851;&#30340;&#39057;&#32321;&#19988;&#26114;&#36149;&#30340;&#22806;&#37096;&#26597;&#35810;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36716;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-- &#20294;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20559;&#31163;&#20102;&#20856;&#22411;&#30340;RLHF&#65292;&#21518;&#32773;&#22312;&#25351;&#23548;&#25968;&#25454;&#35757;&#32451;&#21518;&#20248;&#21270;LLMs&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;RL&#30452;&#25509;&#29983;&#25104;&#21333;&#29420;&#36275;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TeaMs-RL&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25805;&#20316;&#21644;&#35268;&#21017;&#65292;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#12290;&#23427;&#20419;&#36827;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#32780;&#19981;&#36807;&#20110;&#20381;&#36182;&#22806;&#37096;&#20808;&#36827;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28040;&#38500;&#20102;&#38543;&#21518;&#30340;RLHF&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65306;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08694v1 Announce Type: new  Abstract: The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human inv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#29983;&#25104;&#20114;&#21160;&#32773;&#30340;&#24120;&#35782;&#30693;&#35782;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07260</link><description>&lt;p&gt;
CKERC&#65306;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#29983;&#25104;&#20114;&#21160;&#32773;&#30340;&#24120;&#35782;&#30693;&#35782;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;(ERC)&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#23427;&#22312;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#35805;&#35821;&#30340;&#24773;&#24863;&#12290;&#23427;&#39640;&#24230;&#20381;&#36182;&#20110;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#32773;&#36523;&#20221;&#20449;&#24687;&#12289;&#22810;&#26041;&#23545;&#35805;&#22330;&#26223;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;instructERC&#65289;&#20165;&#20165;&#35782;&#21035;&#35828;&#35805;&#32773;&#65292;&#24573;&#30053;&#20102;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#35828;&#35805;&#32773;&#32972;&#21518;&#30340;&#24120;&#35782;&#30693;&#35782;(&#21363;&#65292;&#21548;&#20247;&#30340;&#21453;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#31561;)&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#28145;&#20837;&#25366;&#25496;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32852;&#21512;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#26694;&#26550;&#65292;&#21363;CKERC&#12290;&#25105;&#20204;&#35774;&#35745;&#25552;&#31034;&#26469;&#29983;&#25104;&#22522;&#20110;&#21382;&#21490;&#35805;&#35821;&#30340;&#23545;&#35805;&#32773;&#24120;&#35782;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#21160;&#32773;&#24120;&#35782;&#35782;&#21035;&#20219;&#21153;&#36827;&#34892;LLM&#39044;&#35757;&#32451;&#65292;&#20197;&#24494;&#35843;&#35828;&#35805;&#32773;&#38544;&#21547;&#32447;&#32034;&#20449;&#24687;&#12290;&#36890;&#36807;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07260v1 Announce Type: new  Abstract: Emotion recognition in conversation (ERC) is a task which predicts the emotion of an utterance in the context of a conversation. It tightly depends on dialogue context, speaker identity information, multiparty dialogue scenario and so on. However, the state-of-the-art method (instructERC) solely identifying speaker, and ignores commonsense knowledge(i.e., reaction of the listeners and intention of the speaker, etc.) behind speakers during a conversation, which can deeply mine speaker information. To this end, we propose a novel joint large language models with commonsense knowledge framework for emotion recognition in conversation, namely CKERC.We design prompts to generate interlocutors' commonsense based on historical utterances with large language model. And we use the interlocutor commonsense identification task for LLM pre-training to fine-tune speaker implicit clues information.By solving above challenge, our method achieve state-o
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.18945</link><description>&lt;p&gt;
Syntactic Ghost&#65306;&#19968;&#31181;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#26080;&#24863;&#30693;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18945
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Syntactic Ghost&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#24863;&#30693;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#28431;&#27934;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PLM&#21518;&#38376;&#25915;&#20987;&#37319;&#29992;&#26126;&#26174;&#30340;&#35302;&#21457;&#22120;&#65292;&#22312;&#25163;&#21160;&#23545;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#22240;&#27492;&#22312;&#25928;&#26524;&#12289;&#38544;&#21311;&#24615;&#21644;&#36890;&#29992;&#24615;&#26041;&#38754;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#26399;&#26395;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19981;&#21487;&#35265;&#21644;&#36890;&#29992;&#30340;&#21518;&#38376;&#26893;&#20837;&#65292;&#31216;&#20026;Syntactic Ghost&#65288;&#31616;&#31216;&#20026;synGhost&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#25932;&#24847;&#22320;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#39044;&#23450;&#20041;&#21477;&#27861;&#32467;&#26500;&#30340;&#27602;&#23475;&#26679;&#26412;&#20316;&#20026;&#38544;&#34109;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#23558;&#21518;&#38376;&#26893;&#20837;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#30693;&#35782;&#12290;&#27602;&#23475;&#26679;&#26412;&#30340;&#36755;&#20986;&#34920;&#31034;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23613;&#21487;&#33021;&#22343;&#21248;&#22320;&#20998;&#24067;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#24418;&#25104;&#24191;&#27867;&#30340;&#21518;&#38376;&#12290;&#27492;&#22806;&#65292;&#22312;&#20142;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18945v1 Announce Type: cross  Abstract: Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#25991;&#20214;&#37096;&#20998;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21487;&#26174;&#30528;&#25552;&#39640;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#65292;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.16406</link><description>&lt;p&gt;
&#20174; RAGs &#21040;&#36130;&#23500;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20020;&#24202;&#35797;&#39564;&#25776;&#20889;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
From RAGs to riches: Using large language models to write documents for clinical trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#25991;&#20214;&#37096;&#20998;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21487;&#26174;&#30528;&#25552;&#39640;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#65292;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#25776;&#20889;&#22823;&#37327;&#25991;&#20214;&#65292;&#21253;&#25324;&#21327;&#35758;&#12289;&#21516;&#24847;&#20070;&#12289;&#20020;&#24202;&#30740;&#31350;&#25253;&#21578;&#31561;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#24555;&#36895;&#29983;&#25104;&#36825;&#20123;&#25991;&#20214;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#36755;&#20986;&#36136;&#37327;&#23384;&#22312;&#25285;&#24551;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#29983;&#25104;&#20854;&#20013;&#19968;&#20010;&#25991;&#20214;&#65288;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#65289;&#30340;&#37096;&#20998;&#20869;&#23481;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;LLM&#22312;&#20869;&#23481;&#30456;&#20851;&#24615;&#21644;&#26415;&#35821;&#20351;&#29992;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#21512;&#29702;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65306;&#29305;&#21035;&#26159;&#20020;&#24202;&#24605;&#32500;&#21644;&#36923;&#36753;&#65292;&#20197;&#21450;&#21442;&#32771;&#25991;&#29486;&#30340;&#36866;&#24403;&#20351;&#29992;&#12290;&#20026;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25552;&#31034;LLM&#20351;&#29992;&#20934;&#30830;&#30340;&#26368;&#26032;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;RAG&#65292;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16406v1 Announce Type: new  Abstract: Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11094</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35789;&#23884;&#20837;&#65306;LLMs&#26159;&#21542;&#25552;&#20379;&#26032;&#30340;&#19996;&#35199;&#65311;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings Revisited: Do LLMs Offer Something New?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35789;&#23884;&#20837;&#23545;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#26032;&#30340;&#21333;&#35789;/&#21477;&#23376;/&#25991;&#26723;&#23884;&#20837;&#27169;&#22411;&#12290;&#23613;&#31649;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#24615;&#33021;&#30340;&#25552;&#21319;&#20165;&#20165;&#26159;&#22240;&#20026;&#35268;&#27169;&#36824;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#24213;&#23618;&#23884;&#20837;&#19982;&#21477;&#23376;-BERT&#65288;SBERT&#65289;&#25110;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;USE&#65289;&#20043;&#31867;&#30340;&#20256;&#32479;&#32534;&#30721;&#27169;&#22411;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#19982;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#23427;&#20204;&#30340;&#28508;&#22312;&#21521;&#37327;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;LLMs&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20063;&#39640;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#19968;&#20123;LLMs&#20542;&#21521;&#20110;&#20135;&#29983;&#35789;&#23884;&#20837;si&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10767</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference to the Best Explanation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#22522;&#26412;&#35299;&#37322;&#36807;&#31243;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IBE-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21746;&#23398;&#20851;&#20110;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;&#65288;IBE&#65289;&#30340;&#21551;&#21457;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#36827;&#23545;LLMs&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#12290;IBE-Eval&#36890;&#36807;&#32467;&#21512;&#21253;&#25324;&#19968;&#33268;&#24615;&#12289;&#31616;&#27905;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;&#20869;&#30340;&#26174;&#24335;&#36923;&#36753;&#21644;&#35821;&#35328;&#29305;&#24449;&#26469;&#20272;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#22240;&#26524;&#38382;&#31572;&#65288;CQA&#65289;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20854;&#20013;IBE-Eval&#34987;&#35201;&#27714;&#22312;&#22810;&#20010;&#30001;LLMs&#65288;&#21363;GPT 3.5&#21644;Llama 2&#65289;&#29983;&#25104;&#30340;&#31454;&#20105;&#24615;&#22240;&#26524;&#35299;&#37322;&#20013;&#36873;&#25321;&#26368;&#21512;&#29702;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;IBE-Eval&#21487;&#20197;&#25104;&#21151;&#22320;&#20197;&#39640;&#36798;77\%&#30340;&#20934;&#30830;&#29575;&#65288;&#27604;&#38543;&#26426;&#39640;&#32422;27%&#65289;&#35782;&#21035;&#26368;&#20339;&#35299;&#37322;&#65292;&#20248;&#20110;GPT 3.5&#20316;&#20026;&#21028;&#23450;&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\appr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;</title><link>https://arxiv.org/abs/2402.09911</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09911
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20266;&#21644;&#22810;&#28304;&#30693;&#35782;&#22270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#20854;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#21487;&#20197;&#25552;&#39640;ROUGE-L&#20998;&#25968;&#33267;&#23569;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#24182;&#22686;&#24378;&#23427;&#20204;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#25105;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#26377;&#25928;&#35299;&#20915;&#26410;&#30693;&#20107;&#23454;&#24187;&#35273;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20351;&#29992;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;&#19981;&#21516;KG&#26469;&#28304;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#24320;&#25918;&#24335;&#31572;&#26696;&#38382;&#39064;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20266;&#22270;&#29983;&#25104;&#21644;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#20266;&#22270;&#29983;&#25104;&#26469;&#23454;&#29616;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#29615;&#22659;&#20013;&#20351;&#29992;KG&#22686;&#24378;LLM&#12290;&#21407;&#23376;&#32423;&#30693;&#35782;&#39564;&#35777;&#21033;&#29992;&#21407;&#23376;&#32423;&#30693;&#35782;&#26597;&#35810;&#21644;&#39564;&#35777;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;KG&#26469;&#28304;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;ROUGE-L&#20998;&#25968;&#19978;&#33267;&#23569;&#25552;&#21319;&#20102;11.5&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09911v1 Announce Type: cross  Abstract: Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ende
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.04236</link><description>&lt;p&gt;
CogCoM: &#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#22312;&#23558;&#35270;&#35273;&#25351;&#20196;&#19982;&#31572;&#26696;&#23545;&#40784;&#26041;&#38754;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30830;&#23450;&#24615;&#30340;&#23545;&#40784;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#35270;&#35273;&#25512;&#29702;&#65292;&#24182;&#23548;&#33268;&#22312;&#32454;&#33268;&#30340;&#35270;&#35273;&#38382;&#39064;&#21644;&#19981;&#24544;&#23454;&#30340;&#21709;&#24212;&#26041;&#38754;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25805;&#20316;&#38142;&#8221;&#30340;&#26426;&#21046;&#65292;&#20351;VLM&#33021;&#22815;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#25805;&#20316;&#37117;&#25351;&#30340;&#26159;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#25805;&#20316;&#65292;&#21487;&#20197;&#26159;&#36890;&#36807;&#20808;&#21069;&#35757;&#32451;&#33719;&#24471;&#30340;&#20869;&#22312;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#22522;&#30784;&#65289;&#25110;&#32773;&#26159;&#27169;&#20223;&#31867;&#20154;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#12290;&#36825;&#20010;&#26426;&#21046;&#40723;&#21169;VLM&#29983;&#25104;&#24102;&#26377;&#35777;&#25454;&#30340;&#35270;&#35273;&#25512;&#29702;&#30340;&#24544;&#23454;&#30340;&#21709;&#24212;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#19978;&#36861;&#36394;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#26377;&#20869;&#32622;&#25512;&#29702;&#26426;&#21046;&#30340;17B&#36890;&#29992;VLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.17514</link><description>&lt;p&gt;
FEUDA&#65306;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17514
&lt;/p&gt;
&lt;p&gt;
FEUDA&#26159;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#22522;&#30784;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#25506;&#32034;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#25903;&#21033;&#29992;&#26469;&#33258;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#40723;&#21169;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#20998;&#31867;&#26694;&#26550;&#20013;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25110;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#24517;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#20854;&#20013;&#19968;&#20010;&#36755;&#20837;&#31034;&#20363;&#30001;&#27169;&#26495;&#20462;&#25913;&#21518;&#65292;&#20877;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20013;&#29983;&#25104;&#19968;&#20010;&#26631;&#31614;&#23383;&#31526;&#20018;&#12290;&#20026;&#20102;&#30740;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#36825;&#31181;&#26032;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;FEUDA&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#65292;&#22312;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#30340;&#31034;&#20363;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;LM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#36890;&#36807;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26412;&#19978;&#35757;&#32451;LM&#65292;&#31532;&#20108;&#20010;&#20219;&#21153;&#20351;&#29992;&#28304;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#25351;&#20196;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25351;&#20196;&#35843;&#25972;&#23545;&#27599;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#39033;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2310.19651</link><description>&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#21160;&#24577;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27599;&#20010;&#33021;&#21147;&#37117;&#26377;&#20854;&#33258;&#24049;&#30340;&#22686;&#38271;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25351;&#20196;&#35843;&#25972;&#23545;&#27599;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#39033;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#29992;&#20110;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#36941;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#21019;&#24314;&#20173;&#28982;&#20027;&#35201;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#23548;&#33268;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#19978;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#25454;&#37327;&#12289;&#21442;&#25968;&#22823;&#23567;&#21644;&#25968;&#25454;&#26500;&#24314;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;LLM&#30340;&#27599;&#20010;&#22522;&#26412;&#33021;&#21147;&#65288;&#22914;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#30340;&#21457;&#23637;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#26500;&#24314;&#20934;&#21017;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#33021;&#21147;&#30340;&#36229;&#36807;40k&#20010;&#23454;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#20855;&#26377;70&#20159;&#33267;330&#20159;&#21442;&#25968;&#30340;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19651v2 Announce Type: replace  Abstract: Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quantity and quality across existing datasets. While some research advocates for expanding the number of instructions, others suggest that a small set of well-chosen examples is adequate. To better understand data construction guidelines, our research provides a granular analysis of how data volume, parameter size, and data construction methods influence the development of each underlying ability of LLM, such as creative writing, code generation, and logical reasoning. We present a meticulously curated dataset with over 40k instances across ten abilities and examine instruction-tuned models with 7b to 33b parameters. Our study reveals three primary findings: (i) Despite the models' overall performance being tied to data a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#65292;&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#20316;&#12290;LLM&#20805;&#24403;&#25945;&#24072;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20805;&#24403;&#23398;&#29983;&#65292;&#23427;&#20204;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#23454;&#29616;&#20102;&#30456;&#20114;&#21327;&#21161;&#12290;&#36825;&#31181;&#21512;&#20316;&#25552;&#20379;&#20102;&#39640;&#32423;&#20449;&#24687;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.06603</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#30456;&#20114;&#21512;&#20316;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study. (arXiv:2401.06603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06603
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#65292;&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#20316;&#12290;LLM&#20805;&#24403;&#25945;&#24072;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20805;&#24403;&#23398;&#29983;&#65292;&#23427;&#20204;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#23454;&#29616;&#20102;&#30456;&#20114;&#21327;&#21161;&#12290;&#36825;&#31181;&#21512;&#20316;&#25552;&#20379;&#20102;&#39640;&#32423;&#20449;&#24687;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;(&#22914;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;)&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#28982;&#32780;LLMs&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24072;&#29983;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20855;&#20307;&#26159;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;LLMs&#21453;&#39304;&#65292;&#24182;&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#39640;&#32423;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;LLM&#25198;&#28436;&#25945;&#24072;&#35282;&#33394;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21017;&#25198;&#28436;&#23398;&#29983;&#35282;&#33394;&#12290;&#36825;&#20004;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#30340;&#26041;&#24335;&#30456;&#20114;&#21327;&#21161;&#65292;&#22914;&#8220;&#25105;&#24110;&#20320;&#24110;&#25105;&#24110;&#8221;&#31561;&#12290;LLM&#26234;&#33021;&#20307;&#21521;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#25277;&#35937;&#20449;&#24687;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#31574;&#30053;&#25913;&#36827;&#12290;&#21453;&#36807;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21521;LLM&#26234;&#33021;&#20307;&#25552;&#20379;&#21453;&#39304;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#23454;&#26102;&#20449;&#24687;&#65292;&#24110;&#21161;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#26631;&#35760;&#12290;&#36825;&#31181;&#21452;&#21521;&#21453;&#39304;&#24490;&#29615;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as "I help you help I help." The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization,
&lt;/p&gt;</description></item><item><title>Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17953</link><description>&lt;p&gt;
Whisper-MCE: &#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#24615;&#33021;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17953
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Whisper&#22312;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#24050;&#32463;&#25509;&#36817;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#32454;&#35843;&#30340;Whisper&#27169;&#22411;Whisper-MCE&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#30340;whisper-large-v2&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#30340;&#33021;&#21147;&#26356;&#24378;&#12289;&#35782;&#21035;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#35782;&#21035;&#36895;&#24230;&#26356;&#24555;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2205.12554</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#20854;&#24110;&#21161;&#24615;&#30340;&#24863;&#30693;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20154;&#31867;&#24863;&#30693;&#24110;&#21161;&#24615;&#20197;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24110;&#21161;&#24615;&#30340;&#35745;&#31639;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#26500;&#24314;&#20998;&#31867;&#22120;&#65292;&#33258;&#21160;&#30830;&#23450;&#21709;&#24212;&#30340;&#24110;&#21161;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20351;&#29992;&#23545;&#19981;&#21516;&#29992;&#25143;&#26597;&#35810;&#30340;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19977;&#31181;&#20449;&#24687;&#26597;&#35810;&#22330;&#26223;&#19979;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant &amp; coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries th
&lt;/p&gt;</description></item></channel></rss>