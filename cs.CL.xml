<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65288;MPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Direct Preference Optimization&#65288;DPO&#65289;&#65292;&#28982;&#21518;&#22312;&#22256;&#38590;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;Reinforcement Learning with Human Feedback&#65288;RLHF&#65289;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.19443</link><description>&lt;p&gt;
&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#36873;&#25321;&#19982;&#26356;&#22909;&#30340;&#21442;&#32771;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19443
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65288;MPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Direct Preference Optimization&#65288;DPO&#65289;&#65292;&#28982;&#21518;&#22312;&#22256;&#38590;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;Reinforcement Learning with Human Feedback&#65288;RLHF&#65289;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22788;&#29702;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#26159;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#26377;&#23475;&#20559;&#35265;&#65292;&#24182;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#23545;&#40784;&#30340;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#24102;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#12290;&#36890;&#36807;&#20998;&#26512;RLHF&#21644;DPO&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPO&#65288;&#28151;&#21512;&#20559;&#22909;&#20248;&#21270;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32531;&#35299;&#20004;&#31181;&#26041;&#27861;&#24369;&#28857;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65306;&#39318;&#20808;&#22312;&#19968;&#20010;&#31616;&#21333;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DPO&#65292;&#28982;&#21518;&#20877;&#22312;&#24102;&#26377;DPO&#27169;&#22411;&#20316;&#20026;&#21442;&#32771;&#27169;&#22411;&#30340;&#22256;&#38590;&#38598;&#19978;&#25191;&#34892;RLHF&#12290;&#22312;&#36825;&#37324;&#65292;&#31616;&#21333;&#21644;&#22256;&#38590;&#38598;&#26159;&#30001;&#35757;&#32451;&#33391;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#23558;&#21709;&#24212;&#23545;&#20998;&#25104;&#20855;&#26377;&#36739;&#22823;&#24046;&#36317;&#30340;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19443v1 Announce Type: new  Abstract: Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item></channel></rss>