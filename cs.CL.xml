<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.15426</link><description>&lt;p&gt;
&#25945;&#32946;&#29615;&#22659;&#19979;&#38598;&#25104;&#24378;&#20808;&#39564;&#27169;&#22359;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#30340;&#19977;&#38454;&#27573;SFT&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20808;&#39564;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#35777;&#26126;&#27604;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#26356;&#26377;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22120;&#21644;&#37325;&#21472;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20581;&#22766;&#30340;&#20998;&#31867;&#65292;&#23558;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#20998;&#19977;&#25209;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;LORA&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#27169;&#22359;&#65292;&#23558;&#31995;&#32479;&#25552;&#31034;&#12289;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#20219;&#21153;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;&#20808;&#39564;&#30340;&#24494;&#35843;&#27169;&#22411;&#24212;&#29992;&#20102;&#21387;&#32553;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#38543;&#21518;&#22312;&#36755;&#20986;&#31471;&#36827;&#34892;&#25991;&#26412;&#36807;&#28388;&#20197;&#33719;&#24471;&#22686;&#37327;&#24341;&#23548;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#30495;&#27491;&#20197;&#20016;&#23500;&#30340;&#25945;&#32946;&#30693;&#35782;&#12289;&#20998;&#27493;&#25351;&#23548;&#30340;&#29305;&#28857;&#20307;&#29616;&#23548;&#24072;&#35282;&#33394;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item></channel></rss>