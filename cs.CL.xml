<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.02317</link><description>&lt;p&gt;
GNN2R: &#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#29702;&#30001;&#30340;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs. (arXiv:2312.02317v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02317
&lt;/p&gt;
&lt;p&gt;
GNN2R&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#20197;&#21450;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#26041;&#27861;&#21482;&#25552;&#20379;&#26368;&#32456;&#30340;&#30830;&#23450;&#31572;&#26696;&#65292;&#32780;&#27809;&#26377;&#35299;&#37322;&#65292;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#21644;&#26597;&#30475;&#30340;KG&#23454;&#20307;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#27493;&#25512;&#29702;&#27169;&#22411;&#65288;GNN2R&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNN2R&#33021;&#22815;&#36890;&#36807;&#20165;&#26377;&#30340;&#38382;&#39064;-&#26368;&#32456;&#31572;&#26696;&#23545;&#25552;&#20379;&#26368;&#32456;&#31572;&#26696;&#20197;&#21450;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#32972;&#21518;&#30340;&#25512;&#29702;&#23376;&#22270;&#30340;&#29702;&#30001;&#65292;&#19988;&#20165;&#38656;&#35201;&#36890;&#36807;&#24369;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;GNN2R&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#23383;&#22788;&#29702;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#23545;&#20854;&#24433;&#21709;&#30340;&#19978;&#19979;&#25991;&#21270;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#20116;&#20010;&#31561;&#32423;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#21482;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17035</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#24433;&#21709;&#25991;&#26412;&#25968;&#23383;&#22788;&#29702;&#30340;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contextualising Levels of Language Resourcedness affecting Digital Processing of Text. (arXiv:2309.17035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#23383;&#22788;&#29702;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#36164;&#28304;&#27700;&#24179;&#23545;&#20854;&#24433;&#21709;&#30340;&#19978;&#19979;&#25991;&#21270;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#20116;&#20010;&#31561;&#32423;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#21482;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#39046;&#22495;&#22914;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#24037;&#20855;&#37117;&#28041;&#21450;&#21040;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#22788;&#29702;&#65292;&#20174;&#25968;&#23383;&#21270;&#32440;&#36136;&#25991;&#20214;&#21040;&#35821;&#38899;&#29983;&#25104;&#12290;&#20869;&#23481;&#30340;&#35821;&#35328;&#36890;&#24120;&#34987;&#21010;&#20998;&#20026;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#65288;LRL&#65289;&#25110;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#65288;HRL&#65289;&#12290;&#38750;&#27954;&#35821;&#35328;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#65292;&#32780;&#33521;&#35821;&#21017;&#26159;&#36164;&#28304;&#26368;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#20026;&#20102;&#20026;&#36825;&#20123;&#35821;&#35328;&#24320;&#21457;&#36719;&#20214;&#31995;&#32479;&#20197;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35821;&#35328;&#36164;&#28304;&#12290;&#26412;&#25991;&#35748;&#20026;&#23545;&#20110;&#25152;&#26377;&#35821;&#35328;&#26469;&#35828;&#65292;&#23558;&#20854;&#21010;&#20998;&#20026;LRL&#21644;HRL&#20004;&#31181;&#23545;&#31435;&#30340;&#31867;&#22411;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#36890;&#36807;&#23545;&#31038;&#20250;&#20013;&#35821;&#35328;&#36164;&#28304;&#30340;&#28165;&#26224;&#29702;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30697;&#38453;&#26469;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;"&#38750;&#24120;LRL"&#12289;"LRL"&#12289;"RL"&#12289;"HRL"&#21644;"&#38750;&#24120;HRL"&#12290;&#36825;&#31181;&#21010;&#20998;&#22522;&#20110;&#36830;&#25509;&#24773;&#26223;&#22522;&#30784;&#35774;&#26045;&#12289;&#24773;&#26223;&#23545;&#35805;&#27969;&#12289;&#24773;&#26223;&#30693;&#35782;&#31561;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application domains such as digital humanities and tool like chatbots involve some form of processing natural language, from digitising hardcopies to speech generation. The language of the content is typically characterised as either a low resource language (LRL) or high resource language (HRL), also known as resource-scarce and well-resourced languages, respectively. African languages have been characterized as resource-scarce languages (Bosch et al. 2007; Pretorius &amp; Bosch 2003; Keet &amp; Khumalo 2014) and English is by far the most well-resourced language. Varied language resources are used to develop software systems for these languages to accomplish a wide range of tasks. In this paper we argue that the dichotomous typology LRL and HRL for all languages is problematic. Through a clear understanding of language resources situated in a society, a matrix is developed that characterizes languages as Very LRL, LRL, RL, HRL and Very HRL. The characterization is based on the typology of con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.10634</link><description>&lt;p&gt;
&#20154;&#31867;&#22522;&#22240;&#26680;&#33527;&#37240;&#24207;&#21015;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DNA&#30456;&#20851;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#31867;&#20284;&#20110;GPT-3&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#32771;&#34385;&#21040;&#22788;&#29702;&#25972;&#20010;DNA&#24207;&#21015;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#20915;&#23450;&#22312;&#26356;&#23567;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;DNA&#12290;&#36825;&#20010;&#20915;&#31574;&#24182;&#19981;&#25913;&#21464;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;DNA&#21644;&#22522;&#22240;&#37117;&#21487;&#20197;&#30475;&#20316;&#30001;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#33527;&#37240;&#32452;&#25104;&#30340;&#19968;&#32500;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
&lt;/p&gt;</description></item></channel></rss>