<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02690</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#39640;&#26031;&#20998;&#24067;&#36755;&#20837;&#19979;&#33258;&#28982;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
Attention is Naturally Sparse with Gaussian Distributed Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#24378;&#24230;&#26159;&#20851;&#38190;&#29942;&#39048;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;$O(n^2)$&#22797;&#26434;&#24230;&#12290;&#31232;&#30095;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#33655;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;LLMs&#20869;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#26031;&#36755;&#20837;&#26694;&#26550;&#19979;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#32452;&#22522;&#30784;&#20551;&#35774;&#24182;&#37319;&#29992;&#19968;&#31181;&#31995;&#32479;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#30340;&#20869;&#22312;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#34920;&#29616;&#24418;&#24335;&#30340;&#35814;&#32454;&#29702;&#35770;&#26816;&#26597;&#65292;&#25581;&#31034;&#20102;&#22312;&#35745;&#31639;&#33410;&#32422;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#20043;&#38388;&#28508;&#22312;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25972;&#21512;&#26032;&#20449;&#24687;&#12289;&#20445;&#30041;&#24050;&#23398;&#30693;&#35782;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17400</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#35265;&#35299;&#19982;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating Continual Pretraining in Large Language Models: Insights and Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25972;&#21512;&#26032;&#20449;&#24687;&#12289;&#20445;&#30041;&#24050;&#23398;&#30693;&#35782;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#19981;&#26029;&#23398;&#20064;&#65288;CL&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#26159;&#21046;&#23450;&#39640;&#25928;&#21487;&#25345;&#32493;&#22521;&#35757;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#32780;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#35782;&#21035;&#30340;&#36807;&#31243;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#38598;&#20013;&#20110;&#26377;&#38480;&#20219;&#21153;&#25110;&#39046;&#22495;&#24182;&#20027;&#35201;&#26088;&#22312;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;LLMs&#23545;&#23454;&#38469;&#24773;&#26223;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#26223;&#35266;&#30340;&#36866;&#24212;&#24615;&#21644;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#34913;&#37327;LLMs&#23545;&#36825;&#20123;&#19981;&#26029;&#28436;&#21464;&#30340;&#25968;&#25454;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17400v1 Announce Type: new  Abstract: This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item></channel></rss>