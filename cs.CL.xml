<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.10552</link><description>&lt;p&gt;
Conversational SimulMT: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21516;&#26102;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#25361;&#25112;&#24615;&#30340;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;SimulMT&#20219;&#21153;&#20013;&#21487;&#20197;&#21462;&#24471;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#24448;&#24448;&#26159;&#20197;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#30340;&#22686;&#21152;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#35299;&#30721;&#26469;&#25552;&#39640;&#22522;&#20110;LLM&#30340;SimulMT&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;SimulMT&#22522;&#20934;&#19978;&#20351;&#29992;Llama2-7b-chat&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#24403;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10552v1 Announce Type: new  Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.06706</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Machine Translation with Large Language Models. (arXiv:2309.06706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;&#20132;&#20114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#21487;&#20197;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31163;&#32447;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;LLM&#24212;&#29992;&#20110;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793; (SimulMT) &#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#19982;&#19981;&#21516;&#35299;&#30721;&#27169;&#24335;&#20135;&#29983;&#30340;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#36827;&#34892;SimulMT&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;SimulMT&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20840;&#21477;&#21644;&#21069;&#32512;&#21477;&#23376;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#21518;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;MUST-C&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#21487;&#20197;&#23454;&#29616;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have demonstrated their abilities to solve various natural language processing tasks through dialogue-based interactions. For instance, research indicates that LLMs can achieve competitive performance in offline machine translation tasks for high-resource languages. However, applying LLMs to simultaneous machine translation (SimulMT) poses many challenges, including issues related to the training-inference mismatch arising from different decoding patterns. In this paper, we explore the feasibility of utilizing LLMs for SimulMT. Building upon conventional approaches, we introduce a simple yet effective mixture policy that enables LLMs to engage in SimulMT without requiring additional training. Furthermore, after Supervised Fine-Tuning (SFT) on a mixture of full and prefix sentences, the model exhibits significant performance improvements. Our experiments, conducted with Llama2-7B-chat on nine language pairs from the MUST-C dataset, demonstrate that LLM can ac
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.06031</link><description>&lt;p&gt;
FinGPT&#65306;&#24320;&#28304;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06031
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#26159;&#37329;&#34701;LLMs&#65288;FinLLMs&#65289;&#30340;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;FinGPT&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FinGPT&#37319;&#29992;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#20182;&#20204;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#24378;&#35843;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#24314;&#31435;FinGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#20316;&#20026;&#29992;&#25143;&#30340;&#22522;&#30784;&#65292;&#22914;&#26426;&#22120;&#39038;&#38382;&#12289;&#31639;&#27861;&#20132;&#26131;&#21644;&#35770; &#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and l
&lt;/p&gt;</description></item></channel></rss>