<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08462</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#27169;&#22411;&#20284;&#28982;&#27604;&#30340;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Authorship Verification based on the Likelihood Ratio of Grammar Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#20998;&#26512;&#19968;&#32452;&#25991;&#20214;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#30001;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;AV&#26041;&#27861;&#20351;&#29992;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#20110;&#20854;&#21151;&#33021;&#27809;&#26377;&#21512;&#29702;&#30340;&#31185;&#23398;&#35299;&#37322;&#65292;&#24182;&#19988;&#24120;&#24120;&#38590;&#20197;&#35299;&#37322;&#32473;&#20998;&#26512;&#20154;&#21592;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#35745;&#31639;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026; $\lambda_G$&#65288;LambdaG&#65289;&#30340;&#37327;&#65306;&#20505;&#36873;&#20316;&#32773;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#19982;&#21442;&#32771;&#32676;&#20307;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#30456;&#21516;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#36825;&#20123;&#35821;&#27861;&#27169;&#22411;&#26159;&#20351;&#29992;&#20165;&#38024;&#23545;&#35821;&#27861;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#30340; $n$-gram&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#30340;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;LambdaG...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08462v1 Announce Type: new  Abstract: Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.14770</link><description>&lt;p&gt;
KERMIT: &#24102;&#26377;&#21453;&#36716;&#21464;&#25442;&#30340;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation. (arXiv:2309.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20851;&#31995;&#24314;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#24211;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#26469;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26159;&#19968;&#39033;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#29992;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#21487;&#33021;&#36935;&#21040;&#19968;&#20123;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#24403;&#25551;&#36848;&#19981;&#33021;&#20934;&#30830;&#20805;&#20998;&#22320;&#34920;&#36798;&#39044;&#26399;&#21547;&#20041;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#20010;&#39069;&#22806;&#26426;&#21046;&#26469;&#22686;&#21152;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#24211;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#25551;&#36848;&#20197;&#24357;&#34917;&#26597;&#35810;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#21521;&#20851;&#31995;&#21019;&#24314;&#23545;&#31216;&#22270;&#65292;&#20174;&#32780;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#39069;&#22806;&#30340;&#26631;&#31614;&#21644;&#34917;&#20805;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#31995;&#23454;&#20307;&#20043;&#38388;&#39069;&#22806;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#36825;&#20123;&#21162;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion is a task that revolves around filling in missing triples based on the information available in a knowledge graph. Among the current studies, text-based methods complete the task by utilizing textual descriptions of triples. However, this modeling approach may encounter limitations, particularly when the description fails to accurately and adequately express the intended meaning. To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers. Secondly, we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction. This approach offers additional insights into the relationships between entities. Through these efforts, we have observed significant improvements in knowledge graph completion
&lt;/p&gt;</description></item><item><title>Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10305</link><description>&lt;p&gt;
Baichuan 2: &#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10305
&lt;/p&gt;
&lt;p&gt;
Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#24191;&#27867;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;LLMs&#26159;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#65292;&#25110;&#32773;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Baichuan 2&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#29992;26&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#12290;Baichuan 2&#22312;MMLU&#12289;CMMLU&#12289;GSM8K&#21644;HumanEval&#31561;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#30456;&#21516;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#12290;&#27492;&#22806;&#65292;Baichuan 2&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#22402;&#30452;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#26816;&#26597;&#28857;&#65292;&#20197;&#20351;&#30740;&#31350;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;Baichuan 2&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
&lt;/p&gt;</description></item></channel></rss>