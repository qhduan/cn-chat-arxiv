<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.15615</link><description>&lt;p&gt;
NaturalTurn&#65306;&#19968;&#31181;&#23558;&#36716;&#24405;&#20214;&#20998;&#21106;&#25104;&#33258;&#28982;&#23545;&#35805;&#36716;&#25240;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NaturalTurn: A Method to Segment Transcripts into Naturalistic Conversational Turns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15615
&lt;/p&gt;
&lt;p&gt;
NaturalTurn&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#23545;&#35805;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#21462;&#36716;&#24405;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#23545;&#35805;&#26159;&#31038;&#20250;&#12289;&#35748;&#30693;&#21644;&#35745;&#31639;&#31185;&#23398;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#21487;&#20280;&#32553;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#36716;&#24405;&#36716;&#25442;&#20026;&#20250;&#35805;&#36718;&#27425;&#8212;&#8212;&#31038;&#20250;&#20114;&#21160;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;NaturalTurn&#8221;&#65292;&#19968;&#31181;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#20132;&#27969;&#21160;&#24577;&#30340;&#36718;&#27425;&#20998;&#21106;&#31639;&#27861;&#12290;NaturalTurn&#36890;&#36807;&#21306;&#20998;&#35828;&#35805;&#32773;&#30340;&#20027;&#35201;&#23545;&#35805;&#36718;&#27425;&#21644;&#21548;&#20247;&#30340;&#27425;&#35201;&#35805;&#35821;&#65292;&#22914;&#32972;&#26223;&#22768;&#12289;&#31616;&#30701;&#25554;&#35805;&#21644;&#20854;&#20182;&#34920;&#29616;&#23545;&#35805;&#29305;&#24449;&#30340;&#24179;&#34892;&#35328;&#35821;&#24418;&#24335;&#65292;&#26469;&#36816;&#20316;&#12290;&#20351;&#29992;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#27966;&#29983;&#30340;&#36716;&#24405;&#30456;&#27604;&#65292;NaturalTurn&#27966;&#29983;&#30340;&#36716;&#24405;&#34920;&#29616;&#20986;&#26377;&#21033;&#30340;&#32479;&#35745;&#21644;&#25512;&#26029;&#29305;&#24615;&#12290;NaturalTurn&#31639;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15615v1 Announce Type: new  Abstract: Conversation is the subject of increasing interest in the social, cognitive, and computational sciences. And yet, as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational turns--the basic building blocks of social interaction. We introduce "NaturalTurn," a turn segmentation algorithm designed to accurately capture the dynamics of naturalistic exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize conversation. Using data from a large conversation corpus, we show how NaturalTurn-derived transcripts demonstrate favorable statistical and inferential characteristics compared to transcripts derived from existing methods. The NaturalTurn algorithm represents an improvement i
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17237</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching with Multi-View Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#21452;&#27969;&#27169;&#22411;&#22312;&#30830;&#20445;&#26816;&#32034;&#36895;&#24230;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21463;&#21040;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#21333;&#19968;&#34920;&#31034;&#26469;&#20998;&#21035;&#32534;&#30721;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#25110;&#21521;&#37327;&#20869;&#31215;&#24471;&#21040;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;&#19968;&#26041;&#38754;&#65292;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#36825;&#31181;&#32570;&#20047;&#20132;&#20114;&#30340;&#26694;&#26550;&#20013;&#65292;&#21305;&#37197;&#22810;&#37325;&#21547;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20449;&#24687;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#20419;&#36827;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;MVAM&#65288;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17237v1 Announce Type: cross  Abstract: Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by
&lt;/p&gt;</description></item></channel></rss>