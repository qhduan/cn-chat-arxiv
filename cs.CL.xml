<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14327</link><description>&lt;p&gt;
&#23376;&#23545;&#35937;&#32423;&#22270;&#20687;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subobject-level Image Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#23558;&#22270;&#20687;&#26631;&#35760;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#26041;&#24418;&#34917;&#19969;&#20316;&#20026;&#36755;&#20837;&#21333;&#20803;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#20687;&#32032;&#20998;&#32452;&#32467;&#26500;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#37319;&#29992;&#30340;&#23376;&#35789;&#26631;&#35760;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23376;&#23545;&#35937;&#30001;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65289;&#33719;&#24471;&#30340;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#22270;&#20687;&#27573;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#22522;&#20110;&#23376;&#23545;&#35937;&#26631;&#35760;&#21270;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65288;SeqAE&#65289;&#65292;&#23558;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#23376;&#23545;&#35937;&#23884;&#20837;&#39304;&#36865;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23376;&#23545;&#35937;&#32423;&#21035;&#26631;&#35760;&#21270;&#26174;&#33879;&#20419;&#36827;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14327v1 Announce Type: cross  Abstract: Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descr
&lt;/p&gt;</description></item><item><title>&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08382</link><description>&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Punctuation Restoration Improves Structure Understanding without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08382
&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#21435;&#22122;&#31561;&#65292;&#22312;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#20174;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21040;&#20250;&#35805;&#20219;&#21153;&#30340;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#25991;&#26412;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#35821;&#35328;&#24615;&#33021;&#21644;&#26426;&#22120;&#33021;&#21147;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#24402;&#22240;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26410;&#33021;&#20805;&#20998;&#20256;&#36882;&#35821;&#35328;&#32467;&#26500;&#30693;&#35782;&#32473;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#23545;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#29616;&#30340;&#25913;&#21892;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#12289;&#20998;&#22359;&#21644;&#35789;&#24615;&#26631;&#27880;&#12290;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21487;&#20197;&#25913;&#21892;&#32467;&#26500;&#29702;&#35299;&#24182;&#20135;&#29983;&#26356;&#21152;&#40065;&#26834;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. However, despite impressive conversational capabilities of recent large language model, their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more rob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;CARE&#65292;&#29992;&#20110;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992; Motivational Interviewing &#26694;&#26550;&#65292;CARE &#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#24110;&#21161;&#36741;&#23548;&#21592;&#35786;&#26029;&#21738;&#31181;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21709;&#24212;&#31034;&#20363;&#20316;&#20026;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.08982</link><description>&lt;p&gt;
&#24110;&#21161;&#24110;&#21161;&#32773;&#65306;&#36890;&#36807; AI &#24378;&#21270;&#23454;&#36341;&#21644;&#21453;&#39304;&#26469;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback. (arXiv:2305.08982v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;CARE&#65292;&#29992;&#20110;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992; Motivational Interviewing &#26694;&#26550;&#65292;CARE &#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#24110;&#21161;&#36741;&#23548;&#21592;&#35786;&#26029;&#21738;&#31181;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21709;&#24212;&#31034;&#20363;&#20316;&#20026;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#29992;&#25143;&#26469;&#21040;&#22312;&#32447;&#21516;&#20394;&#36741;&#23548;&#24179;&#21488;&#23547;&#27714;&#20851;&#20110;&#20174;&#20851;&#31995;&#21387;&#21147;&#21040;&#28966;&#34385;&#31561;&#22810;&#31181;&#20027;&#39064;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32447;&#21516;&#20394;&#25903;&#25345;&#32676;&#20307;&#24182;&#19981;&#24635;&#26159;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#26377;&#25928;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#29992;&#25143;&#19982;&#26080;&#29992;&#30340;&#36741;&#23548;&#21592;&#20135;&#29983;&#20102;&#36127;&#38754;&#20307;&#39564;&#12290;&#21516;&#20394;&#36741;&#23548;&#21592;&#26159;&#22312;&#32447;&#21516;&#20394;&#36741;&#23548;&#24179;&#21488;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#20294;&#20182;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#36890;&#24120;&#27809;&#26377;&#31995;&#32479;&#22320;&#25509;&#25910;&#25351;&#23548;&#25110;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461; CARE&#65306;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110; AI &#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#22686;&#24378;&#21516;&#20394;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#12290;&#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#65292;CARE &#24110;&#21161;&#35786;&#26029;&#22312;&#32473;&#23450;&#24773;&#22659;&#19979;&#21738;&#20123;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#37327;&#36523;&#23450;&#21046;&#30340;&#31034;&#20363;&#21709;&#24212;&#20316;&#20026;&#24314;&#35758;&#12290;&#36741;&#23548;&#21592;&#21487;&#20197;&#36873;&#25321;&#22312;&#22238;&#22797;&#27714;&#21161;&#32773;&#20043;&#21069;&#36873;&#25321;&#12289;&#20462;&#25913;&#25110;&#24573;&#30053;&#20219;&#20309;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of users come to online peer counseling platforms to seek support on diverse topics ranging from relationship stress to anxiety. However, studies show that online peer support groups are not always as effective as expected largely due to users' negative experiences with unhelpful counselors. Peer counselors are key to the success of online peer counseling platforms, but most of them often do not have systematic ways to receive guidelines or supervision. In this work, we introduce CARE: an interactive AI-based tool to empower peer counselors through automatic suggestion generation. During the practical training stage, CARE helps diagnose which specific counseling strategies are most suitable in the given context and provides tailored example responses as suggestions. Counselors can choose to select, modify, or ignore any suggestion before replying to the support seeker. Building upon the Motivational Interviewing framework, CARE utilizes large-scale counseling conversation data
&lt;/p&gt;</description></item></channel></rss>